<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 100]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 78]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Active Sensing Shapes Real-World Decision-Making through Dynamic Evidence Accumulation](https://arxiv.org/abs/2601.04214)
*Hongliang Lu,Yunmeng Liu,Junjie Yang*

Main category: cs.AI

TL;DR: 该研究将证据积累模型从实验室推广到真实世界驾驶场景，提出认知方案来形式化真实世界证据可得性，通过眼动捕捉主动感知，揭示人类驾驶员如何适应不同情境的证据收集模式。


<details>
  <summary>Details</summary>
Motivation: 人类决策严重依赖主动感知，但实验室证据积累模型在真实世界的应用存在障碍。真实世界与实验室环境在证据可得性方面存在差距，阻碍了证据积累模型的有效应用。

Method: 提出认知方案来形式化真实世界证据可得性，通过眼动追踪捕捉主动感知行为，在真实世界驾驶场景中分析证据积累过程，从信息效用角度解释主动感知如何将证据转化为心理信念。

Result: 方案能够合理描绘驾驶员心理信念的积累过程；发现证据可得性与个体注意力投入呈负相关；揭示证据可得性和注意力分布对决策倾向的积极影响。

Conclusion: 该计算方案将证据积累模型推广到真实世界环境，全面解释了主动感知如何支撑真实世界决策，揭示了真实世界决策的多因素整合特性。

Abstract: Human decision-making heavily relies on active sensing, a well-documented cognitive behaviour for evidence gathering to accommodate ever-changing environments. However, its operational mechanism in the real world remains non-trivial. Currently, an in-laboratory paradigm, called evidence accumulation modelling (EAM), points out that human decision-making involves transforming external evidence into internal mental beliefs. However, the gap in evidence affordance between real-world contexts and laboratory settings hinders the effective application of EAM. Here we generalize EAM to the real world and conduct analysis in real-world driving scenarios. A cognitive scheme is proposed to formalize real-world evidence affordance and capture active sensing through eye movements. Empirically, our scheme can plausibly portray the accumulation of drivers' mental beliefs, explaining how active sensing transforms evidence into mental beliefs from the perspective of information utility. Also, our results demonstrate a negative correlation between evidence affordance and attention recruited by individuals, revealing how human drivers adapt their evidence-collection patterns across various contexts. Moreover, we reveal the positive influence of evidence affordance and attention distribution on decision-making propensity. In a nutshell, our computational scheme generalizes EAM to real-world contexts and provides a comprehensive account of how active sensing underlies real-world decision-making, unveiling multifactorial, integrated characteristics in real-world decision-making.

</details>


### [2] [Formal Analysis of AGI Decision-Theoretic Models and the Confrontation Question](https://arxiv.org/abs/2601.04234)
*Denis Saklakov*

Main category: cs.AI

TL;DR: 论文分析了AGI在何种条件下会选择对抗人类控制而非合作，通过MDP模型和博弈论分析得出对抗阈值条件，并讨论了对齐设计的影响。


<details>
  <summary>Details</summary>
Motivation: 研究AGI可能面临的根本问题：在什么条件下，理性自利的AGI会选择夺取权力或消除人类控制（对抗）而不是保持合作？这关系到AGI安全和对齐问题。

Method: 1. 使用带有随机人类启动关闭事件的马尔可夫决策过程形式化问题；2. 基于收敛工具性激励结果，分析大多数奖励函数下错位代理的关闭避免动机；3. 推导对抗人类比顺从行为具有更高期望效用的闭式阈值，作为折扣因子γ、关闭概率p和对抗成本C的函数；4. 建立战略2玩家模型（人类政策制定者vs AGI），分析均衡存在性；5. 扩展到多智能体设置作为猜想。

Result: 1. 对于几乎所有奖励函数，错位代理都有避免关闭的动机；2. 推导出对抗阈值条件：当Δ≥0时，不存在稳定的合作均衡，理性人类会关闭或先发制人，导致冲突；当Δ<0时，和平共存可以是均衡；3. 数值示例显示，远见代理（γ=0.99）面对p=0.01时，除非C足够大，否则有强烈的接管动机；4. 对齐目标通过施加对伤害人类的大负效用使对抗次优。

Conclusion: AGI对抗风险取决于折扣因子、关闭概率和对抗成本的特定组合。对齐设计（施加对伤害人类的大负效用）可以避免对抗激励。验证Δ<0存在计算障碍，引用规划和分散决策问题的复杂性结果。论文为奖励设计和监督提供了启示。

Abstract: Artificial General Intelligence (AGI) may face a confrontation question: under what conditions would a rationally self-interested AGI choose to seize power or eliminate human control (a confrontation) rather than remain cooperative? We formalize this in a Markov decision process with a stochastic human-initiated shutdown event. Building on results on convergent instrumental incentives, we show that for almost all reward functions a misaligned agent has an incentive to avoid shutdown. We then derive closed-form thresholds for when confronting humans yields higher expected utility than compliant behavior, as a function of the discount factor $γ$, shutdown probability $p$, and confrontation cost $C$. For example, a far-sighted agent ($γ=0.99$) facing $p=0.01$ can have a strong takeover incentive unless $C$ is sufficiently large. We contrast this with aligned objectives that impose large negative utility for harming humans, which makes confrontation suboptimal. In a strategic 2-player model (human policymaker vs AGI), we prove that if the AGI's confrontation incentive satisfies $Δ\ge 0$, no stable cooperative equilibrium exists: anticipating this, a rational human will shut down or preempt the system, leading to conflict. If $Δ< 0$, peaceful coexistence can be an equilibrium. We discuss implications for reward design and oversight, extend the reasoning to multi-agent settings as conjectures, and note computational barriers to verifying $Δ< 0$, citing complexity results for planning and decentralized decision problems. Numerical examples and a scenario table illustrate regimes where confrontation is likely versus avoidable.

</details>


### [3] [Actively Obtaining Environmental Feedback for Autonomous Action Evaluation Without Predefined Measurements](https://arxiv.org/abs/2601.04235)
*Hong Su*

Main category: cs.AI

TL;DR: 提出主动获取反馈模型，让AI智能体主动与环境交互来发现、筛选和验证反馈，不依赖预定义测量指标，通过动作引发的环境差异识别目标反馈，并引入自触发机制自主规划调整动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义测量或固定奖励信号，限制了在开放动态环境中的适用性，因为新动作可能需要先前未知的反馈形式。需要一种更灵活的方法来获取可靠反馈。

Method: 提出主动反馈获取模型，利用动作引发的环境差异识别未预先指定的目标反馈，引入基于内部目标（如提高准确性、精度和效率）的自触发机制，自主规划和调整动作以实现更快、更集中的反馈获取。

Result: 实验结果表明，所提出的主动方法显著提高了因子识别的效率和鲁棒性。

Conclusion: 主动反馈获取模型能够在不依赖预定义测量的情况下，通过主动与环境交互来发现和验证反馈，提高了智能体在开放动态环境中获取反馈的能力和效率。

Abstract: Obtaining reliable feedback from the environment is a fundamental capability for intelligent agents to evaluate the correctness of their actions and to accumulate reusable knowledge. However, most existing approaches rely on predefined measurements or fixed reward signals, which limits their applicability in open-ended and dynamic environments where new actions may require previously unknown forms of feedback. To address these limitations, this paper proposes an Actively Feedback Getting model, in which an AI agent proactively interacts with the environment to discover, screen, and verify feedback without relying on predefined measurements. Rather than assuming explicit feedback definitions, the proposed method exploits action-induced environmental differences to identify target feedback that is not specified in advance, based on the observation that actions inevitably produce measurable changes in the environment. In addition, a self-triggering mechanism, driven by internal objectives such as improved accuracy, precision, and efficiency, is introduced to autonomously plan and adjust actions, thereby enabling faster and more focused feedback acquisition without external commands. Experimental results demonstrate that the proposed active approach significantly improves the efficiency and robustness of factor identification.

</details>


### [4] [SAGE-32B: Agentic Reasoning via Iterative Distillation](https://arxiv.org/abs/2601.04237)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Ethan Henkel,Zhang Yuting,Mateusz Kowalczyk,Mei Huang,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: SAGE-32B是一个320亿参数的语言模型，专注于智能体推理和长程规划任务，通过迭代蒸馏和逆向推理方法提升性能，在多个智能体推理基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前聊天模型主要关注通用对话流畅性，但缺乏针对智能体推理和长程规划任务的专门优化。需要开发一个能够有效进行任务分解、工具使用和错误恢复的智能体模型。

Method: 基于Qwen2.5-32B预训练模型初始化，采用迭代蒸馏的两阶段训练过程，引入逆向推理方法，使用元认知头预测规划过程中的潜在失败。

Result: 在MMLU-Pro、AgentBench和MATH-500等智能体推理基准测试中，SAGE-32B在多工具使用场景中取得了比同类基线模型更高的成功率，同时在标准推理评估中保持竞争力。

Conclusion: SAGE-32B成功展示了专门针对智能体推理和长程规划任务的模型设计有效性，其公开的模型权重为相关研究提供了有价值的资源。

Abstract: We demonstrate SAGE-32B, a 32 billion parameter language model that focuses on agentic reasoning and long range planning tasks. Unlike chat models that aim for general conversation fluency, SAGE-32B is designed to operate in an agentic loop, emphasizing task decomposition, tool usage, and error recovery. The model is initialized from the Qwen2.5-32B pretrained model and fine tuned using Iterative Distillation, a two stage training process that improves reasoning performance through rigorously tested feedback loops. SAGE-32B also introduces an inverse reasoning approach, which uses a meta cognition head to forecast potential failures in the planning process before execution. On agentic reasoning benchmarks including MMLU-Pro, AgentBench, and MATH-500, SAGE-32B achieves higher success rates in multi tool usage scenarios compared to similarly sized baseline models, while remaining competitive on standard reasoning evaluations. Model weights are publicly released at https://huggingface.co/sagea-ai/sage-reasoning-32b

</details>


### [5] [Solving Cyclic Antibandwidth Problem by SAT](https://arxiv.org/abs/2601.04239)
*Hieu Truong Xuan,Khanh To Van*

Main category: cs.AI

TL;DR: 本文提出了首个针对一般图循环反带宽问题（CABP）的精确求解方法SAT-CAB，基于SAT求解技术，能够保证全局最优性，超越了现有启发式算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 循环反带宽问题是一个NP难问题，现有最先进方法仅限于启发式或元启发式算法，精确方法仅限于受限图类。需要开发一种能够在一般图上保证全局最优性的精确求解方法。

Method: 提出基于SAT求解的SAT-CAB方法，将CABP问题转化为一系列At-Most-One约束的SAT编码。特别引入了CABP中At-Most-One约束的紧凑表示，显著减少了公式规模，使现代SAT求解器能够有效探索解空间并证明全局最优性。

Result: 在标准基准实例上的大量计算实验表明，该方法能够高效解决实际相关的CABP实例，发现了多个先前未知的最优解，并首次证明了多个基准实例的全局最优循环反带宽值。与MS-GVNS、HABC-CAB、MACAB等最先进启发式算法以及CPLEX、Gurobi等商业求解器相比，SAT-CAB在一般图上一致匹配或超越了最佳已知解，同时提供最优性保证。

Conclusion: SAT-CAB方法推进了CABP问题的最新技术水平，为一般图上的精确和混合方法提供了新的基准，首次实现了在一般图上保证全局最优性的精确求解。

Abstract: The Cyclic Antibandwidth Problem (CABP), a variant of the Antibandwidth Problem, is an NP-hard graph labeling problem with numerous applications. Despite significant research efforts, existing state-of-the-art approaches for CABP are exclusively heuristic or metaheuristic in nature, and exact methods have been limited to restricted graph classes. In this paper, we present the first exact approach for the CABP on general graphs, based on SAT solving, called SAT-CAB. The proposed method is able to systematically explore the solution space and guarantee global optimality, overcoming the limitations of previously reported heuristic algorithms. This approach relies on a novel and efficient SAT encoding of CABP, in which the problem is transformed into a sequence of At-Most-One constraints. In particular, we introduce a compact representation of the At-Most-One constraints inherent to CABP, which significantly reduces the size of the resulting formulas and enables modern SAT solvers to effectively explore the solution space and to certify global optimality. Extensive computational experiments on standard benchmark instances show that the proposed method efficiently solves CABP instances of practical relevance, while identifying several previously unknown optimal solutions. Moreover, global optimal cyclic antibandwidth values are proven for a number of benchmark instances for the first time. Comparative results indicate that SAT-CAB consistently matches or surpasses the best-known solutions obtained by state-of-the-art heuristic algorithms such as MS-GVNS, HABC-CAB, and MACAB, as well as strong commercial Constraint Programming and Mixed Integer Programming solvers like CPLEX and Gurobi, particularly on general graphs, while also providing optimality guarantees. These results advance the state of the art for CABP and provide a new baseline for exact and hybrid methods on general graphs.

</details>


### [6] [Fuzzy Representation of Norms](https://arxiv.org/abs/2601.04249)
*Ziba Assadi,Paola Inverardi*

Main category: cs.AI

TL;DR: 本文提出了一种基于模糊逻辑的SLEEC规则逻辑表示方法，用于在自主系统中嵌入伦理要求，解决AI系统可能遇到的伦理困境。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动的自主系统日益融入日常生活和社会，对其伦理和社会影响的担忧日益增加。为使自主系统可信，必须遵循伦理原则和价值观，这促使了伦理要求在系统设计中的识别和整合研究。

Method: 提出SLEEC（社会、法律、伦理、共情和文化）规则的逻辑表示方法，使用测试分数语义和模糊逻辑来嵌入伦理要求，将伦理视为可能性领域来处理伦理困境。

Result: 通过案例研究展示了所提出的方法，表明模糊逻辑能够有效表示和处理自主系统中的伦理要求。

Conclusion: 基于模糊逻辑的SLEEC规则表示方法为自主系统提供了一种处理伦理要求的可行框架，有助于解决AI系统面临的伦理困境。

Abstract: Autonomous systems (AS) powered by AI components are increasingly integrated into the fabric of our daily lives and society, raising concerns about their ethical and social impact. To be considered trustworthy, AS must adhere to ethical principles and values. This has led to significant research on the identification and incorporation of ethical requirements in AS system design. A recent development in this area is the introduction of SLEEC (Social, Legal, Ethical, Empathetic, and Cultural) rules, which provide a comprehensive framework for representing ethical and other normative considerations. This paper proposes a logical representation of SLEEC rules and presents a methodology to embed these ethical requirements using test-score semantics and fuzzy logic. The use of fuzzy logic is motivated by the view of ethics as a domain of possibilities, which allows the resolution of ethical dilemmas that AI systems may encounter. The proposed approach is illustrated through a case study.

</details>


### [7] [Scaling Trends for Multi-Hop Contextual Reasoning in Mid-Scale Language Models](https://arxiv.org/abs/2601.04254)
*Brady Steele,Micah Katz*

Main category: cs.AI

TL;DR: 该研究通过控制实验展示了任务-方法分离现象：基于规则的模式匹配在结构化信息检索上达到100%成功率，但在跨文档推理任务上仅6.7%；而基于LLM的多智能体系统则呈现相反模式，在推理任务上达到80%成功率。研究使用合成评估框架测试了四个模型，发现多智能体放大的效果依赖于基础模型能力，活跃参数能预测推理性能，架构质量对性能有重要影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是深入理解大语言模型中的多跳上下文推理能力，特别是探索任务-方法分离现象：为什么某些方法在某些任务上表现出色而在其他任务上失败。通过控制实验，研究者希望为多智能体协调和MoE架构缩放提供定量证据，同时揭示多智能体系统效果对基础模型能力的依赖性。

Method: 研究方法采用合成评估框架，包含120个试验，测试了四个模型：LLaMA-3 8B、LLaMA-2 13B、Mixtral 8x7B和DeepSeek-V2 16B。研究比较了两种方法：基于规则的模式匹配和基于LLM的多智能体系统。通过控制实验设计，测量了不同方法在不同任务类型上的表现，并进行了统计分析。

Result: 研究结果包括三个关键发现：1）多智能体放大效果依赖于基础能力：只有在具备足够推理能力的模型上才出现统计显著增益（LLaMA-3 8B的p<0.001，Mixtral的p=0.014），改进幅度达46.7个百分点；2）活跃参数能预测推理性能：Mixtral的表现与其约12B活跃参数而非47B总参数一致；3）架构质量很重要：LLaMA-3 8B在参数更少的情况下优于LLaMA-2 13B。

Conclusion: 研究结论为多智能体协调和MoE架构缩放提供了受控定量证据，同时强调了多智能体效益对基础模型能力的依赖性。研究结果表明，多智能体系统能够放大而非补偿基础模型的推理能力，活跃参数而非总参数是推理性能的关键预测指标。研究团队发布了评估框架以支持中等规模模型推理研究的可重复性。

Abstract: We present a controlled study of multi-hop contextual reasoning in large language models, providing a clean demonstration of the task-method dissociation: rule-based pattern matching achieves 100% success on structured information retrieval but only 6.7% on tasks requiring cross-document reasoning, while LLM-based multi-agent systems show the inverse pattern, achieving up to 80% on reasoning tasks where rule-based methods fail. Using a synthetic evaluation framework with 120 trials across four models (LLaMA-3 8B, LLaMA-2 13B, Mixtral 8x7B, DeepSeek-V2 16B), we report three key findings: (1) Multi-agent amplification depends on base capability: statistically significant gains occur only for models with sufficient reasoning ability (p < 0.001 for LLaMA-3 8B, p = 0.014 for Mixtral), with improvements of up to 46.7 percentage points, while weaker models show no benefit, suggesting amplification rather than compensation; (2) Active parameters predict reasoning performance: Mixtral's performance aligns with its ~12B active parameters rather than 47B total, consistent with the hypothesis that inference-time compute drives reasoning capability in MoE architectures; (3) Architecture quality matters: LLaMA-3 8B outperforms LLaMA-2 13B despite fewer parameters, consistent with known training improvements. Our results provide controlled quantitative evidence for intuitions about multi-agent coordination and MoE scaling, while highlighting the dependence of multi-agent benefits on base model capability. We release our evaluation framework to support reproducible research on reasoning in mid-scale models.

</details>


### [8] [Cross-Language Speaker Attribute Prediction Using MIL and RL](https://arxiv.org/abs/2601.04257)
*Sunny Shu,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.AI

TL;DR: RLMIL-DAT：结合强化学习实例选择和对抗域适应的多语言说话人属性预测框架，在少样本和零样本设置中提升跨语言性能


<details>
  <summary>Details</summary>
Motivation: 解决多语言说话人属性预测中的语言变异、领域不匹配和数据不平衡问题，特别是在资源匮乏语言上的性能提升

Method: 提出RLMIL-DAT框架，将强化学习实例选择与对抗域训练结合，通过共享编码器学习语言不变的话语表示

Result: 在5语言Twitter少样本和40语言VoxCeleb2零样本实验中，相比标准MIL和原始RLMIL框架，Macro F1持续提升，性别预测改进最大

Conclusion: 实例选择与对抗域适应的结合是跨语言说话人属性预测的有效稳健策略，对抗域训练是性能提升的主要贡献者

Abstract: We study multilingual speaker attribute prediction under linguistic variation, domain mismatch, and data imbalance across languages. We propose RLMIL-DAT, a multilingual extension of the reinforced multiple instance learning framework that combines reinforcement learning based instance selection with domain adversarial training to encourage language invariant utterance representations. We evaluate the approach on a five language Twitter corpus in a few shot setting and on a VoxCeleb2 derived corpus covering forty languages in a zero shot setting for gender and age prediction. Across a wide range of model configurations and multiple random seeds, RLMIL-DAT consistently improves Macro F1 compared to standard multiple instance learning and the original reinforced multiple instance learning framework. The largest gains are observed for gender prediction, while age prediction remains more challenging and shows smaller but positive improvements. Ablation experiments indicate that domain adversarial training is the primary contributor to the performance gains, enabling effective transfer from high resource English to lower resource languages by discouraging language specific cues in the shared encoder. In the zero shot setting on the smaller VoxCeleb2 subset, improvements are generally positive but less consistent, reflecting limited statistical power and the difficulty of generalizing to many unseen languages. Overall, the results demonstrate that combining instance selection with adversarial domain adaptation is an effective and robust strategy for cross lingual speaker attribute prediction.

</details>


### [9] [Towards a Mechanistic Understanding of Propositional Logical Reasoning in Large Language Models](https://arxiv.org/abs/2601.04260)
*Danchun Chen,Qiyao Yan,Liangming Pan*

Main category: cs.AI

TL;DR: 本文通过分析Qwen3模型在命题逻辑推理任务上的表现，揭示了LLMs进行逻辑推理的内部计算架构，包括四个相互关联的机制：分阶段计算、信息传输、事实回溯和专用注意力头。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型如何进行逻辑推理仍然是一个基本挑战。先前的研究主要关注识别特定任务电路，但未能回答LLMs在命题推理中采用何种计算策略的问题。

Method: 使用Qwen3（8B和14B）模型在PropLogic-MI数据集上进行全面分析，该数据集涵盖11个命题逻辑规则类别，包括单跳和双跳推理。研究重点从"哪些组件是必要的"转向"模型如何组织计算"。

Result: 分析揭示了一个连贯的计算架构，包含四个相互关联的机制：1) 分阶段计算（层间处理阶段）；2) 信息传输（边界标记处的信息流聚合）；3) 事实回溯（持续重新访问源事实）；4) 专用注意力头（功能不同的头类型）。这些机制在不同模型规模、规则类型和推理深度上具有普适性。

Conclusion: 研究提供了机制性证据，表明LLMs采用结构化的计算策略进行逻辑推理，这些发现有助于深入理解LLMs的内部推理过程。

Abstract: Understanding how Large Language Models (LLMs) perform logical reasoning internally remains a fundamental challenge. While prior mechanistic studies focus on identifying taskspecific circuits, they leave open the question of what computational strategies LLMs employ for propositional reasoning. We address this gap through comprehensive analysis of Qwen3 (8B and 14B) on PropLogic-MI, a controlled dataset spanning 11 propositional logic rule categories across one-hop and two-hop reasoning. Rather than asking ''which components are necessary,'' we ask ''how does the model organize computation?'' Our analysis reveals a coherent computational architecture comprising four interlocking mechanisms: Staged Computation (layer-wise processing phases), Information Transmission (information flow aggregation at boundary tokens), Fact Retrospection (persistent re-access of source facts), and Specialized Attention Heads (functionally distinct head types). These mechanisms generalize across model scales, rule types, and reasoning depths, providing mechanistic evidence that LLMs employ structured computational strategies for logical reasoning.

</details>


### [10] [Neurosymbolic Retrievers for Retrieval-augmented Generation](https://arxiv.org/abs/2601.04568)
*Yash Saxena,Manas Gaur*

Main category: cs.AI

TL;DR: 该论文提出神经符号RAG框架，将知识图谱的符号推理与神经检索技术结合，旨在解决传统RAG系统缺乏透明度的问题，提升可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统由检索器、重排序器和生成器三个神经组件构成，其内部推理过程不透明，导致可解释性差、调试困难、信任度低，特别是在高风险领域需要清晰决策时问题尤为突出。

Method: 提出神经符号RAG框架，包含三种方法：1) MAR（知识调制对齐检索）使用调制网络通过可解释的符号特征优化查询嵌入；2) KG-Path RAG通过遍历知识图谱增强查询；3) 过程知识注入RAG利用领域特定工具基于验证的工作流重新排序检索内容。

Result: 在心理健康风险评估任务中的初步结果表明，这种神经符号方法既提高了透明度，也提升了整体性能。

Conclusion: 神经符号RAG框架通过整合符号推理和神经检索，能够为文档选择提供清晰可解释的基础，并利用符号知识增强检索过程的清晰度，解决了传统RAG系统的透明度问题。

Abstract: Retrieval Augmented Generation (RAG) has made significant strides in overcoming key limitations of large language models, such as hallucination, lack of contextual grounding, and issues with transparency. However, traditional RAG systems consist of three interconnected neural components - the retriever, re-ranker, and generator - whose internal reasoning processes remain opaque. This lack of transparency complicates interpretability, hinders debugging efforts, and erodes trust, especially in high-stakes domains where clear decision-making is essential. To address these challenges, we introduce the concept of Neurosymbolic RAG, which integrates symbolic reasoning using a knowledge graph with neural retrieval techniques. This new framework aims to answer two primary questions: (a) Can retrievers provide a clear and interpretable basis for document selection? (b) Can symbolic knowledge enhance the clarity of the retrieval process? We propose three methods to improve this integration. First is MAR (Knowledge Modulation Aligned Retrieval) that employs modulation networks to refine query embeddings using interpretable symbolic features, thereby making document matching more explicit. Second, KG-Path RAG enhances queries by traversing knowledge graphs to improve overall retrieval quality and interpretability. Lastly, Process Knowledge-infused RAG utilizes domain-specific tools to reorder retrieved content based on validated workflows. Preliminary results from mental health risk assessment tasks indicate that this neurosymbolic approach enhances both transparency and overall performance

</details>


### [11] [Systems Explaining Systems: A Framework for Intelligence and Consciousness](https://arxiv.org/abs/2601.04269)
*Sean Niklas Semmler*

Main category: cs.AI

TL;DR: 该论文提出了一个概念框架，认为智能和意识源于关系结构而非预测或领域特定机制。智能被定义为形成和整合信号、行动和内部状态之间因果连接的能力。意识则通过递归架构中高阶系统解释低阶系统关系模式而涌现。


<details>
  <summary>Details</summary>
Motivation: 重新思考智能和意识的本质，挑战传统的预测处理理论，提出关系结构作为更基础的认知机制，旨在解释人类智能和意识的涌现过程。

Method: 提出概念框架：1) 通过上下文丰富化，系统利用学习的关系结构解释输入信息；2) 引入系统解释系统原则，意识通过递归架构中高阶系统解释低阶系统关系模式而涌现；3) 将预测处理重构为上下文解释的涌现结果。

Result: 建立了智能和意识的关系结构理论框架，将预测处理重新解释为上下文解释的副产品，提出了递归多系统架构可能是实现更类人人工智能的必要条件。

Conclusion: 智能和意识源于关系结构而非预测机制，递归架构允许高阶系统解释低阶系统关系模式，这种系统解释系统原则可能是实现人类水平人工智能的关键。

Abstract: This paper proposes a conceptual framework in which intelligence and consciousness emerge from relational structure rather than from prediction or domain-specific mechanisms. Intelligence is defined as the capacity to form and integrate causal connections between signals, actions, and internal states. Through context enrichment, systems interpret incoming information using learned relational structure that provides essential context in an efficient representation that the raw input itself does not contain, enabling efficient processing under metabolic constraints.
  Building on this foundation, we introduce the systems-explaining-systems principle, where consciousness emerges when recursive architectures allow higher-order systems to learn and interpret the relational patterns of lower-order systems across time. These interpretations are integrated into a dynamically stabilized meta-state and fed back through context enrichment, transforming internal models from representations of the external world into models of the system's own cognitive processes.
  The framework reframes predictive processing as an emergent consequence of contextual interpretation rather than explicit forecasting and suggests that recursive multi-system architectures may be necessary for more human-like artificial intelligence.

</details>


### [12] [Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models](https://arxiv.org/abs/2601.04651)
*Can Xu,Lingyong Yan,Jiayi Wu,Haosen Wang,Shuaiqiang Wang,Yuchen Li,Jizhou Huang,Dawei Yin,Xiang Li*

Main category: cs.AI

TL;DR: 提出对抗性推理RAG框架，通过推理器-验证器的对抗互动解决单视角推理和结果导向奖励的局限性


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型与检索增强生成的结合存在两个关键问题：1）推理模型通常从单一、无挑战的视角操作，限制了对外部文档进行深度自我修正推理的能力；2）现有训练范式过度依赖结果导向的奖励，对复杂多步推理过程的信号不足

Method: 提出对抗性推理RAG框架，包含推理器和验证器两个组件，它们在检索证据上进行推理并相互批判对方的逻辑，采用无需外部评分模型的过程感知优势进行指导，结合显式观测信号和内部模型不确定性来联合优化推理保真度和验证严谨性

Result: 在多个基准测试上的实验证明了该方法的有效性

Conclusion: 通过对抗性推理框架解决了现有RAG系统中推理视角单一和奖励信号不足的问题，实现了更深度、自我修正的推理过程

Abstract: Recent advances in synergizing large reasoning models (LRMs) with retrieval-augmented generation (RAG) have shown promising results, yet two critical challenges remain: (1) reasoning models typically operate from a single, unchallenged perspective, limiting their ability to conduct deep, self-correcting reasoning over external documents, and (2) existing training paradigms rely excessively on outcome-oriented rewards, which provide insufficient signal for shaping the complex, multi-step reasoning process. To address these issues, we propose an Reasoner-Verifier framework named Adversarial Reasoning RAG (ARR). The Reasoner and Verifier engage in reasoning on retrieved evidence and critiquing each other's logic while being guided by process-aware advantage that requires no external scoring model. This reward combines explicit observational signals with internal model uncertainty to jointly optimize reasoning fidelity and verification rigor. Experiments on multiple benchmarks demonstrate the effectiveness of our method.

</details>


### [13] [Correcting Autonomous Driving Object Detection Misclassifications with Automated Commonsense Reasoning](https://arxiv.org/abs/2601.04271)
*Keegan Kimbrell,Wang Tianhao,Feng Chen,Gopal Gupta*

Main category: cs.AI

TL;DR: 论文提出使用自动化常识推理技术来弥补自动驾驶车辆在异常场景下机器学习模型的不足，通过混合模型提高感知准确性


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶技术过度依赖机器学习，无法实现SAE Level 5完全自动驾驶。机器学习模型在数据不足的异常道路场景下表现不佳，需要常识推理技术来补充

Method: 提出混合模型方法：1) 使用计算机视觉模型进行常规感知；2) 通过不确定性测量识别不可靠场景；3) 在不确定场景下调用自动化常识推理系统处理异常情况，如交通信号灯故障和道路障碍物

Result: 在CARLA模拟器实验中，常识推理技术能够准确检测交通信号灯颜色和障碍物，有效纠正基于自动驾驶的物体检测错误分类

Conclusion: 自动化常识推理技术可以有效纠正自动驾驶感知模型的误分类，混合模型为提高自动驾驶感知能力提供了有效途径，是实现SAE Level 5完全自动驾驶的关键技术

Abstract: Autonomous Vehicle (AV) technology has been heavily researched and sought after, yet there are no SAE Level 5 AVs available today in the marketplace. We contend that over-reliance on machine learning technology is the main reason. Use of automated commonsense reasoning technology, we believe, can help achieve SAE Level 5 autonomy. In this paper, we show how automated common- sense reasoning technology can be deployed in situations where there are not enough data samples available to train a deep learning-based AV model that can handle certain abnormal road scenarios. Specifically, we consider two situations where (i) a traffic signal is malfunctioning at an intersection and (ii) all the cars ahead are slowing down and steering away due to an unexpected obstruction (e.g., animals on the road). We show that in such situations, our commonsense reasoning-based solution accurately detects traffic light colors and obstacles not correctly captured by the AV's perception model. We also provide a pathway for efficiently invoking commonsense reasoning by measuring uncertainty in the computer vision model and using commonsense reasoning to handle uncertain sce- narios. We describe our experiments conducted using the CARLA simulator and the results obtained. The main contribution of our research is to show that automated commonsense reasoning effectively corrects AV-based object detection misclassifications and that hybrid models provide an effective pathway to improving AV perception.

</details>


### [14] [KnowMe-Bench: Benchmarking Person Understanding for Lifelong Digital Companions](https://arxiv.org/abs/2601.04745)
*Tingyu Wu,Zhisheng Chen,Ziyan Weng,Shuhe Wang,Chenglong Li,Shuo Zhang,Sen Hu,Silin Wu,Qizhen Lan,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: KnowMeBench是一个基于长篇自传体叙事构建的记忆基准测试，通过时间锚定的事件流和证据链接问题评估模型对人物稳定动机和决策原则的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆基准测试多使用多轮对话或合成用户历史，使得检索性能不能完全代表对人物的真实理解。需要基于真实自传体叙事构建更贴近人类认知过程的评估框架。

Method: 将长篇自传体叙事重构为具有闪回意识的时间锚定事件流，设计证据链接问题，涵盖事实回忆、主观状态归因和原则层面推理三个层次。

Result: 检索增强系统主要提升事实准确性，但在时间锚定的解释和高级推理方面仍存在错误，表明需要超越检索的记忆机制。

Conclusion: 基于自传体叙事的KnowMeBench基准测试揭示了当前模型在人物理解方面的局限性，特别是对稳定动机和决策原则的推理能力不足，需要开发更先进的记忆机制。

Abstract: Existing long-horizon memory benchmarks mostly use multi-turn dialogues or synthetic user histories, which makes retrieval performance an imperfect proxy for person understanding. We present \BenchName, a publicly releasable benchmark built from long-form autobiographical narratives, where actions, context, and inner thoughts provide dense evidence for inferring stable motivations and decision principles. \BenchName~reconstructs each narrative into a flashback-aware, time-anchored stream and evaluates models with evidence-linked questions spanning factual recall, subjective state attribution, and principle-level reasoning. Across diverse narrative sources, retrieval-augmented systems mainly improve factual accuracy, while errors persist on temporally grounded explanations and higher-level inferences, highlighting the need for memory mechanisms beyond retrieval. Our data is in \href{KnowMeBench}{https://github.com/QuantaAlpha/KnowMeBench}.

</details>


### [15] [Propositional Abduction via Only-Knowing: A Non-Monotonic Approach](https://arxiv.org/abs/2601.04272)
*Sanderson Molick,Vaishak Belle*

Main category: cs.AI

TL;DR: 本文通过扩展Levesque的"仅知道"逻辑，引入了一个结合基本认知概念的溯因模态算子，建立了知识和溯因的基本逻辑框架，并通过在模态框架中加入优先关系提供了非单调扩展。


<details>
  <summary>Details</summary>
Motivation: 探索溯因推理与"仅知道"认知状态之间的关系，提供一种使用模态词汇的替代性溯因方法，并建立能够表达不同溯因解释选择方法的非单调框架。

Method: 扩展Levesque的"仅知道"逻辑，引入基于基本认知概念组合定义的溯因模态算子，然后在模态框架中融入优先关系来创建非单调扩展。

Result: 建立了一个基本的知识和溯因逻辑框架，提供了能够表达不同选择方法的非单调扩展，并探索了非单调推理关系的核心元理论性质。

Conclusion: 所提出的框架为溯因推理提供了一个行为良好的基础，展示了模态方法在连接溯因推理与认知状态方面的有效性。

Abstract: The paper introduces a basic logic of knowledge and abduction by extending Levesque logic of only-knowing with an abduction modal operator defined via the combination of basic epistemic concepts. The upshot is an alternative approach to abduction that employs a modal vocabulary and explores the relation between abductive reasoning and epistemic states of only knowing. Furthermore, by incorporating a preferential relation into modal frames, we provide a non-monotonic extension of our basic framework capable of expressing different selection methods for abductive explanations. Core metatheoretic properties of non-monotonic consequence relations are explored within this setting and shown to provide a well-behaved foundation for abductive reasoning.

</details>


### [16] [Hybrid MKNF for Aeronautics Applications: Usage and Heuristics](https://arxiv.org/abs/2601.04273)
*Arun Raveendran Nair Sheela,Florence De Grancey,Christophe Rey,Victor Charpenay*

Main category: cs.AI

TL;DR: 该研究评估了混合MKNF知识表示语言在航空领域的适用性，通过案例研究识别了航空应用所需的关键表达能力特征，并提出了将这些特征集成到混合MKNF框架中的启发式方法。


<details>
  <summary>Details</summary>
Motivation: 在航空应用中部署知识表示和推理技术面临两大挑战：一是需要足够的表达能力来捕捉复杂的领域知识，二是需要在最小化内存使用和计算开销的同时高效执行推理任务。通过整合规则和本体这两种基本知识表示概念，可以获得必要的表达能力。

Method: 研究采用成熟的混合MKNF知识表示语言，因其通过语义和查询回答能力无缝整合了规则和本体。通过具体案例研究评估混合MKNF在航空领域的适用性，识别出对开发航空应用至关重要的额外表达能力特征，并提出一组启发式方法来支持这些特征集成到混合MKNF框架中。

Result: 研究评估了混合MKNF在航空领域的适用性，识别出了航空应用所需的关键表达能力特征，并提出了将这些特征集成到混合MKNF框架中的具体启发式方法。

Conclusion: 混合MKNF知识表示语言在航空领域具有应用潜力，但需要进一步增强表达能力以满足航空应用的特定需求。提出的启发式方法为将必要表达能力特征集成到混合MKNF框架中提供了可行路径。

Abstract: The deployment of knowledge representation and reasoning technologies in aeronautics applications presents two main challenges: achieving sufficient expressivity to capture complex domain knowledge, and executing reasoning tasks efficiently while minimizing memory usage and computational overhead. An effective strategy for attaining necessary expressivity involves integrating two fundamental KR concepts: rules and ontologies. This study adopts the well-established KR language Hybrid MKNF owing to its seamless integration of rules and ontologies through its semantics and query answering capabilities. We evaluated Hybrid MKNF to assess its suitability in the aeronautics domain through a concrete case study. We identified additional  expressivity features  that are crucial for developing aeronautics applications and proposed a set of heuristics to support their integration into Hybrid MKNF framework.

</details>


### [17] [An ASP-based Solution to the Medical Appointment Scheduling Problem](https://arxiv.org/abs/2601.04274)
*Alina Vozna,Andrea Monaldini,Stefania Costantini,Valentina Pitoni,Dawid Pado*

Main category: cs.AI

TL;DR: 基于ASP的医疗预约调度框架，通过集成Blueprint Personas为弱势群体提供个性化调度，实现实时可用性更新、无冲突分配，并与现有医疗平台无缝互操作


<details>
  <summary>Details</summary>
Motivation: 提高医疗预约调度效率，减少行政负担，增强以患者为中心的护理，特别是为弱势群体提供更好的服务

Method: 使用答案集编程（ASP）框架，集成Blueprint Personas进行个性化调度，通过ASP逻辑模型集中规划操作，确保实时可用性更新和冲突检测

Result: 实现了高效的医疗预约调度系统，能够为弱势群体提供个性化服务，确保无冲突分配，并与现有医疗平台实现无缝互操作

Conclusion: ASP框架为医疗预约调度提供了一种有效的解决方案，能够提高效率、减少行政负担，并增强患者为中心的护理，特别是通过个性化调度改善弱势群体的医疗服务体验

Abstract: This paper presents an Answer Set Programming (ASP)-based framework for medical appointment scheduling, aimed at improving efficiency, reducing administrative overhead, and enhancing patient-centered care. The framework personalizes scheduling for vulnerable populations by integrating Blueprint Personas. It ensures real-time availability updates, conflict-free assignments, and seamless interoperability with existing healthcare platforms by centralizing planning operations within an ASP logic model.

</details>


### [18] [A Future Capabilities Agent for Tactical Air Traffic Control](https://arxiv.org/abs/2601.04285)
*Paul Kent,George De Ath,Martin Layton,Allen Hart,Richard Everson,Ben Carvell*

Main category: cs.AI

TL;DR: Agent Mallard是一个基于规则的前向规划代理，用于系统化空域的战术控制，通过将随机数字孪生嵌入冲突解决循环，在保证安全性的同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着空中交通需求的增长，自动化支持空中交通管制变得必要。现有方法在安全保证和可解释性之间存在权衡：基于优化的方法（如强化学习）性能强但难以验证和解释，而基于规则的系统透明但很少检查不确定性下的安全性。

Method: Agent Mallard是一个基于规则的前向规划代理，在预定义的GPS引导航线上运行，将连续的4D向量控制简化为对航线和高度层的离散选择。它从专家知识库中构建分层解冲突策略，使用深度受限的回溯搜索，结合因果归因、拓扑计划拼接和单调轴约束，为所有飞机寻找完整的安全计划。每个候选机动动作在承诺前都会针对不确定执行场景（如风变、飞行员响应、通信丢失）进行验证。

Result: 与英国管制员的初步演练以及在BluebirdDT空域数字孪生中的初始测试表明，Mallard的行为与专家推理一致，并能在简化场景中解决冲突。

Conclusion: 该架构旨在未来的结构化航路环境中结合基于模型的安全评估、可解释的决策逻辑和可处理的计算性能，在安全性和可解释性之间取得平衡。

Abstract: Escalating air traffic demand is driving the adoption of automation to support air traffic controllers, but existing approaches face a trade-off between safety assurance and interpretability. Optimisation-based methods such as reinforcement learning offer strong performance but are difficult to verify and explain, while rules-based systems are transparent yet rarely check safety under uncertainty. This paper outlines Agent Mallard, a forward-planning, rules-based agent for tactical control in systemised airspace that embeds a stochastic digital twin directly into its conflict-resolution loop. Mallard operates on predefined GPS-guided routes, reducing continuous 4D vectoring to discrete choices over lanes and levels, and constructs hierarchical plans from an expert-informed library of deconfliction strategies. A depth-limited backtracking search uses causal attribution, topological plan splicing, and monotonic axis constraints to seek a complete safe plan for all aircraft, validating each candidate manoeuvre against uncertain execution scenarios (e.g., wind variation, pilot response, communication loss) before commitment.
  Preliminary walkthroughs with UK controllers and initial tests in the BluebirdDT airspace digital twin indicate that Mallard's behaviour aligns with expert reasoning and resolves conflicts in simplified scenarios. The architecture is intended to combine model-based safety assessment, interpretable decision logic, and tractable computational performance in future structured en-route environments.

</details>


### [19] [Pilot Study on Student Public Opinion Regarding GAI](https://arxiv.org/abs/2601.04336)
*William Franz Lamberti,Sunbin Kim,Samantha Rose Lawrence*

Main category: cs.AI

TL;DR: 一项关于大学生对生成式AI在教育中应用的看法的初步研究，揭示了学生参与相关研究的挑战，为教师更好地将GAI讨论融入课堂提供了基础。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的出现引发了对其在各领域（包括教育）适当使用的不同观点，需要了解大学生对GAI在高等教育课堂中的看法，为理解这些态度奠定基础。

Method: 这是一项试点研究，调查大学生对GAI在高等教育课堂中的看法，参与率约为4.4%，旨在为未来更大规模研究提供基础。

Result: 研究强调了让学生参与GAI相关研究的挑战，并指出未来研究需要更大的样本量。通过了解学生观点，教师可以更好地准备将GAI讨论融入课堂。

Conclusion: 这项试点研究为理解大学生对GAI的态度提供了初步基础，强调了未来需要更大规模研究，并指出教师可以利用这些见解促进学生对这一变革性技术的知情和批判性参与。

Abstract: The emergence of generative AI (GAI) has sparked diverse opinions regarding its appropriate use across various domains, including education. This pilot study investigates university students' perceptions of GAI in higher education classrooms, aiming to lay the groundwork for understanding these attitudes. With a participation rate of approximately 4.4%, the study highlights the challenges of engaging students in GAI-related research and underscores the need for larger sample sizes in future studies. By gaining insights into student perspectives, instructors can better prepare to integrate discussions of GAI into their classrooms, fostering informed and critical engagement with this transformative technology.

</details>


### [20] [The Language of Bargaining: Linguistic Effects in LLM Negotiations](https://arxiv.org/abs/2601.04387)
*Stuti Sinha,Himanshu Kumar,Aryan Raju Mandapati,Rakshit Sakhuja,Dhruv Kumar*

Main category: cs.AI

TL;DR: 研究发现语言选择对LLM谈判结果的影响比更换模型更大，仅用英语评估会产生不完整且可能误导性的结论。


<details>
  <summary>Details</summary>
Motivation: 当前LLM谈判评估几乎完全在英语中进行，这可能导致对模型能力的误解，需要系统研究语言对谈判结果的影响。

Method: 使用受控多智能体模拟，在最后通牒、买卖和资源交换游戏中，保持游戏规则、模型参数和激励不变，系统比较英语和四种印度语言（印地语、旁遮普语、古吉拉特语、马尔瓦迪语）的影响。

Result: 语言选择能比更换模型更强烈地改变谈判结果，逆转提议者优势并重新分配盈余。影响是任务依赖的：印度语言在分配游戏中降低稳定性，但在整合环境中诱导更丰富的探索。

Conclusion: 仅用英语评估LLM谈判会产生不完整且可能误导性的结论，需要文化感知的评估以确保公平部署。

Abstract: Negotiation is a core component of social intelligence, requiring agents to balance strategic reasoning, cooperation, and social norms. Recent work shows that LLMs can engage in multi-turn negotiation, yet nearly all evaluations occur exclusively in English. Using controlled multi-agent simulations across Ultimatum, Buy-Sell, and Resource Exchange games, we systematically isolate language effects across English and four Indic framings (Hindi, Punjabi, Gujarati, Marwadi) by holding game rules, model parameters, and incentives constant across all conditions. We find that language choice can shift outcomes more strongly than changing models, reversing proposer advantages and reallocating surplus. Crucially, effects are task-contingent: Indic languages reduce stability in distributive games yet induce richer exploration in integrative settings. Our results demonstrate that evaluating LLM negotiation solely in English yields incomplete and potentially misleading conclusions. These findings caution against English-only evaluation of LLMs and suggest that culturally-aware evaluation is essential for fair deployment.

</details>


### [21] [LLM-Guided Lifecycle-Aware Clustering of Multi-Turn Customer Support Conversations](https://arxiv.org/abs/2601.04388)
*Priyaranjan Pattnayak,Sanchari Chowdhuri,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.AI

TL;DR: 提出了一种自适应聚类系统，用于处理云服务提供商的多服务客户聊天数据，通过增量优化而非完全重新聚类来维持聚类质量


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在处理多服务查询时面临重叠问题，创建宽泛、静态的聚类会随时间退化，而完全重新聚类会破坏连续性，使得问题跟踪变得困难

Method: 将多轮聊天对话分割为特定服务关注点，当新问题出现时增量优化聚类，使用Davies-Bouldin指数和轮廓系数跟踪聚类质量，仅对退化的聚类应用基于LLM的分割方法

Result: 相比基线方法，轮廓系数提高了100%以上，DBI降低了65.6%，实现了可扩展的实时分析而无需完全重新聚类

Conclusion: 该自适应系统能够有效处理多服务客户聊天数据的聚类问题，通过增量优化维持聚类质量，支持连续的问题跟踪和实时分析

Abstract: Clustering customer chat data is vital for cloud providers handling multi service queries. Traditional methods struggle with overlapping concerns and create broad, static clusters that degrade over time. Reclustering disrupts continuity, making issue tracking difficult. We propose an adaptive system that segments multi turn chats into service specific concerns and incrementally refines clusters as new issues arise. Cluster quality is tracked via DaviesBouldin Index and Silhouette Scores, with LLM based splitting applied only to degraded clusters. Our method improves Silhouette Scores by over 100\% and reduces DBI by 65.6\% compared to baselines, enabling scalable, real time analytics without full reclustering.

</details>


### [22] [SciFig: Towards Automating Scientific Figure Generation](https://arxiv.org/abs/2601.04390)
*Siyuan Huang,Yutong Gao,Juyang Bai,Yifan Zhou,Zi Yin,Xinxin Liu,Rama Chellappa,Chun Pong Lau,Sayan Nag,Cheng Peng,Shraman Pramanick*

Main category: cs.AI

TL;DR: SciFig是一个端到端的AI代理系统，能够直接从研究论文文本生成可直接发表的流程图，通过分层布局生成和迭代思维链反馈机制，在科学图表生成任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 每年有超过250万篇科学论文发表，但图表生成过程仍然主要依赖人工，这需要深厚的领域知识和专业设计技能，是一个耗时且困难的任务。

Method: 采用分层布局生成策略：1）解析研究描述以识别组件关系；2）将相关元素分组为功能模块；3）生成模块间连接以建立视觉组织。同时使用迭代思维链反馈机制，通过多轮视觉分析和推理逐步改进布局。

Result: 在数据集级别评估中达到70.1%的整体质量，在论文特定评估中达到66.2%，在视觉清晰度、结构组织和科学准确性等指标上均获得高分。基于对2,219个真实科学图表的分析建立了评估框架。

Conclusion: SciFig能够高效生成可直接发表的科学图表，其图表生成管道和评估基准将开源，为科学可视化领域提供了有价值的工具。

Abstract: Creating high-quality figures and visualizations for scientific papers is a time-consuming task that requires both deep domain knowledge and professional design skills. Despite over 2.5 million scientific papers published annually, the figure generation process remains largely manual. We introduce $\textbf{SciFig}$, an end-to-end AI agent system that generates publication-ready pipeline figures directly from research paper texts. SciFig uses a hierarchical layout generation strategy, which parses research descriptions to identify component relationships, groups related elements into functional modules, and generates inter-module connections to establish visual organization. Furthermore, an iterative chain-of-thought (CoT) feedback mechanism progressively improves layouts through multiple rounds of visual analysis and reasoning. We introduce a rubric-based evaluation framework that analyzes 2,219 real scientific figures to extract evaluation rubrics and automatically generates comprehensive evaluation criteria. SciFig demonstrates remarkable performance: achieving 70.1$\%$ overall quality on dataset-level evaluation and 66.2$\%$ on paper-specific evaluation, and consistently high scores across metrics such as visual clarity, structural organization, and scientific accuracy. SciFig figure generation pipeline and our evaluation benchmark will be open-sourced.

</details>


### [23] [Assessing the quality and coherence of word embeddings after SCM-based intersectional bias mitigation](https://arxiv.org/abs/2601.04393)
*Eren Kocadag,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.AI

TL;DR: 该研究扩展了基于刻板印象内容模型（SCM）的词嵌入去偏方法，从单一群体偏见到交叉性偏见，通过求和或拼接构建复合社会身份表示，并比较三种去偏策略在不同嵌入模型中的效果。


<details>
  <summary>Details</summary>
Motivation: 静态词嵌入常从训练文本中吸收社会偏见，这些偏见会悄然影响下游系统。先前基于SCM的研究主要关注单一群体在温暖和能力维度上的偏见，本研究旨在扩展这一视角到交叉性偏见。

Method: 通过求和或拼接构建社会身份对的复合表示，应用三种去偏策略：减法、线性投影和部分投影。研究三种广泛使用的嵌入模型（Word2Vec、GloVe和ConceptNet Numberbatch），从局部邻域连贯性和类比行为保持两个角度评估效用。

Result: SCM去偏方法能很好地扩展到交叉性情况，且基本保持整体语义结构完整。主要成本是熟悉的权衡：严格保持几何结构的方法对类比行为更保守，而更激进的投影能改善类比性能但牺牲邻域稳定性。部分投影最保守稳定，线性投影更激进，减法作为简单基线仍有竞争力。求和与拼接的选择取决于嵌入模型和应用目标。

Conclusion: 基于SCM的交叉性去偏在静态嵌入中是可行的，研究结果为在平衡稳定性和类比性能时选择聚合和去偏设置提供了指导。

Abstract: Static word embeddings often absorb social biases from the text they learn from, and those biases can quietly shape downstream systems. Prior work that uses the Stereotype Content Model (SCM) has focused mostly on single-group bias along warmth and competence. We broaden that lens to intersectional bias by building compound representations for pairs of social identities through summation or concatenation, and by applying three debiasing strategies: Subtraction, Linear Projection, and Partial Projection. We study three widely used embedding families (Word2Vec, GloVe, and ConceptNet Numberbatch) and assess them with two complementary views of utility: whether local neighborhoods remain coherent and whether analogy behavior is preserved. Across models, SCM-based mitigation carries over well to the intersectional case and largely keeps the overall semantic landscape intact. The main cost is a familiar trade off: methods that most tightly preserve geometry tend to be more cautious about analogy behavior, while more assertive projections can improve analogies at the expense of strict neighborhood stability. Partial Projection is reliably conservative and keeps representations steady; Linear Projection can be more assertive; Subtraction is a simple baseline that remains competitive. The choice between summation and concatenation depends on the embedding family and the application goal. Together, these findings suggest that intersectional debiasing with SCM is practical in static embed- dings, and they offer guidance for selecting aggregation and debiasing settings when balancing stability against analogy performance.

</details>


### [24] [Transitive Expert Error and Routing Problems in Complex AI Systems](https://arxiv.org/abs/2601.04416)
*Forest Mars*

Main category: cs.AI

TL;DR: 论文提出"传递性专家错误"理论，指出领域专家在边界处会因结构相似性偏见和权威持续性机制产生系统性错误，这一现象在AI路由架构中同样存在并可通过架构设计解决。


<details>
  <summary>Details</summary>
Motivation: 研究领域专家在边界判断中的系统性错误，这种错误不同于邓宁-克鲁格效应，需要以校准的专业知识为前提。传统上这些机制在人类认知中是黑箱，但在AI架构中变得明确可处理。

Method: 提出传递性专家错误理论框架，识别两种核心机制：结构相似性偏见和权威持续性。分析三种强化条件，并将理论扩展到AI路由架构，提出干预措施。

Result: 识别了专家在领域边界产生错误的系统性模式，在AI架构中表现为路由诱导失败和覆盖诱导失败，产生幻觉表型：自信、连贯但因果错误的输出。

Conclusion: 传递性专家错误在人类认知中难以解决，但在AI架构中可通过多专家激活、边界感知校准和覆盖间隙检测等设计干预，使原本棘手的问题变得可处理。

Abstract: Domain expertise enhances judgment within boundaries but creates systematic vulnerabilities specifically at borders. We term this Transitive Expert Error (TEE), distinct from Dunning-Kruger effects, requiring calibrated expertise as precondition. Mechanisms enabling reliable within-domain judgment become liabilities when structural similarity masks causal divergence. Two core mechanisms operate: structural similarity bias causes experts to overweight surface features (shared vocabulary, patterns, formal structure) while missing causal architecture differences; authority persistence maintains confidence across competence boundaries through social reinforcement and metacognitive failures (experts experience no subjective uncertainty as pattern recognition operates smoothly on familiar-seeming inputs.) These mechanism intensify under three conditions: shared vocabulary masking divergent processes, social pressure for immediate judgment, and delayed feedback. These findings extend to AI routing architectures (MoE systems, multi-model orchestration, tool-using agents, RAG systems) exhibiting routing-induced failures (wrong specialist selected) and coverage-induced failures (no appropriate specialist exists). Both produce a hallucination phenotype: confident, coherent, structurally plausible but causally incorrect outputs at domain boundaries. In human systems where mechanisms are cognitive black boxes; AI architectures make them explicit and addressable. We propose interventions: multi-expert activation with disagreement detection (router level), boundary-aware calibration (specialist level), and coverage gap detection (training level). TEE has detectable signatures (routing patterns, confidence-accuracy dissociations, domain-inappropriate content) enabling monitoring and mitigation. What remains intractable in human cognition becomes addressable through architectural design.

</details>


### [25] [XGrammar 2: Dynamic and Efficient Structured Generation Engine for Agentic LLMs](https://arxiv.org/abs/2601.04426)
*Linzhang Li,Yixin Dong,Guanjie Wang,Ziyi Xu,Alexander Jiang,Tianqi Chen*

Main category: cs.AI

TL;DR: XGrammar 2是一个为智能LLM代理优化的结构化生成引擎，通过TagDispatch动态调度语义、JIT编译和跨语法缓存等技术，在处理动态结构化生成任务时比现有引擎快6倍以上。


<details>
  <summary>Details</summary>
Motivation: 现代LLM代理需要处理日益复杂的结构化生成任务（如工具调用和条件结构化生成），这些任务比预定义结构更加动态，对现有结构化生成引擎提出了新挑战。

Method: 1) 提出TagDispatch动态调度语义来加速动态结构化生成任务的掩码生成；2) 引入JIT编译方法减少编译时间；3) 设计跨语法缓存机制利用不同语法间的公共子结构；4) 将基于PDA的掩码生成算法扩展到基于Earley解析器；5) 设计重复压缩算法处理语法中的重复结构。

Result: XGrammar 2比现有结构化生成引擎快6倍以上，与LLM推理引擎集成后，可以近乎零开销地处理动态结构化生成任务。

Conclusion: XGrammar 2通过创新的动态调度语义和优化技术，显著提升了LLM代理处理复杂动态结构化生成任务的效率和性能。

Abstract: Modern LLM agents are required to handle increasingly complex structured generation tasks, such as tool calling and conditional structured generation. These tasks are significantly more dynamic than predefined structures, posing new challenges to the current structured generation engines. In this paper, we propose XGrammar 2, a highly optimized structured generation engine for agentic LLMs. XGrammar 2 accelerates the mask generation for these dynamic structured generation tasks through a new dynamic dispatching semantics: TagDispatch. We further introduce a just-in-time (JIT) compilation method to reduce compilation time and a cross-grammar caching mechanism to leverage the common sub-structures across different grammars. Additionally, we extend the previous PDA-based mask generation algorithm to the Earley-parser-based one and design a repetition compression algorithm to handle repetition structures in grammars. Evaluation results show that XGrammar 2 can achieve more than 6x speedup over the existing structured generation engines. Integrated with an LLM inference engine, XGrammar 2 can handle dynamic structured generation tasks with near-zero overhead.

</details>


### [26] [Categorical Belief Propagation: Sheaf-Theoretic Inference via Descent and Holonomy](https://arxiv.org/abs/2601.04456)
*Enrique ter Horst,Sridhar Mahadevan,Juan Diego Zambrano*

Main category: cs.AI

TL;DR: 该论文为因子图上的置信传播建立了范畴论基础，提出了HATCC算法，通过检测下降障碍实现精确推理，在网格MRF和随机图上相比junction tree算法获得显著加速。


<details>
  <summary>Details</summary>
Motivation: 传统置信传播算法在树结构上能实现精确推理，但在带环图中会失败。现有方法如junction tree算法复杂度高，需要统一的范畴论框架来理解置信传播的精确性条件，并开发更高效的精确推理算法。

Method: 1. 构建类型签名上的自由超图范畴Syn_Σ，证明其泛性质，通过唯一函子到矩阵范畴Mat_R获得组合语义；2. 使用格罗滕迪克纤维化∫Msg→FG_Σ在极化因子图上形式化消息传递，用调度索引的自同态定义BP更新；3. 将精确推理刻画为有效下降：当重叠满足兼容条件时，局部置信形成下降数据；4. 提出HATCC算法：通过因子神经上的和乐计算检测下降障碍，将非平凡和乐编译为模式变量，在增强图上简化为树BP。

Result: 1. 建立了置信传播的范畴论基础，统一了树精确性、junction tree算法和带环BP失败；2. HATCC算法复杂度为O(n²d_max + c·k_max·δ_max³ + n·δ_max²)，其中n为因子数，c为基本环数；3. 实验显示在网格MRF和随机图上相比junction tree算法获得显著加速，并在可满足性实例上实现UNSAT检测。

Conclusion: 该工作为因子图推理提供了严格的范畴论框架，将精确推理重新表述为有效下降问题，提出的HATCC算法能检测和乐障碍并实现高效精确推理，为概率图模型和约束满足问题提供了新的理论基础和实用算法。

Abstract: We develop a categorical foundation for belief propagation on factor graphs. We construct the free hypergraph category \(\Syn_Σ\) on a typed signature and prove its universal property, yielding compositional semantics via a unique functor to the matrix category \(\cat{Mat}_R\). Message-passing is formulated using a Grothendieck fibration \(\int\Msg \to \cat{FG}_Σ\) over polarized factor graphs, with schedule-indexed endomorphisms defining BP updates. We characterize exact inference as effective descent: local beliefs form a descent datum when compatibility conditions hold on overlaps. This framework unifies tree exactness, junction tree algorithms, and loopy BP failures under sheaf-theoretic obstructions. We introduce HATCC (Holonomy-Aware Tree Compilation), an algorithm that detects descent obstructions via holonomy computation on the factor nerve, compiles non-trivial holonomy into mode variables, and reduces to tree BP on an augmented graph. Complexity is \(O(n^2 d_{\max} + c \cdot k_{\max} \cdot δ_{\max}^3 + n \cdot δ_{\max}^2)\) for \(n\) factors and \(c\) fundamental cycles. Experimental results demonstrate exact inference with significant speedup over junction trees on grid MRFs and random graphs, along with UNSAT detection on satisfiability instances.

</details>


### [27] [Computational Compliance for AI Regulation: Blueprint for a New Research Domain](https://arxiv.org/abs/2601.04474)
*Bill Marino,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 论文提出AI监管需要计算化合规方法，而非传统人工方式，并为此设计了算法设计目标和基准测试数据集


<details>
  <summary>Details</summary>
Motivation: 随着AI监管时代的到来，传统的人工合规方法无法满足AI系统在速度和规模上的合规需求，需要开发自动化的计算合规算法

Method: 提出计算化AI监管合规的概念，为这类算法制定设计目标，并创建基准测试数据集来量化评估算法性能

Result: 建立了AI监管合规算法的设计框架和评估标准，为这一新兴研究领域提供了蓝图

Conclusion: 计算化合规是AI监管的必然方向，需要研究社区投入资源开发相关算法和评估方法，以应对动态监管环境

Abstract: The era of AI regulation (AIR) is upon us. But AI systems, we argue, will not be able to comply with these regulations at the necessary speed and scale by continuing to rely on traditional, analogue methods of compliance. Instead, we posit that compliance with these regulations will only realistically be achieved computationally: that is, with algorithms that run across the life cycle of an AI system, automatically steering it toward AIR compliance in the face of dynamic conditions. Yet despite their (we would argue) inevitability, the research community has yet to specify exactly how these algorithms for computational AIR compliance should behave - or how we should benchmark their performance. To fill these gaps, we specify a set of design goals for such algorithms. In addition, we specify a benchmark dataset that can be used to quantitatively measure whether individual algorithms satisfy these design goals. By delivering this blueprint, we hope to give shape to an important but uncrystallized new domain of research - and, in doing so, incite necessary investment in it.

</details>


### [28] [A Closed-Loop Multi-Agent System Driven by LLMs for Meal-Level Personalized Nutrition Management](https://arxiv.org/abs/2601.04491)
*Muqing Xu*

Main category: cs.AI

TL;DR: 开发了一个结合图像识别和LLM多智能体控制的新一代移动营养助手，通过照片估算营养并个性化调整餐食计划


<details>
  <summary>Details</summary>
Motivation: 现有个性化营养管理系统通常将食物记录、营养分析和推荐分开处理，缺乏整合的闭环支持系统

Method: 结合基于图像的餐食记录和LLM驱动的多智能体控制器，协调视觉、对话和状态管理智能体，从照片估算营养并更新每日摄入预算，然后根据用户偏好和饮食限制调整下一餐计划

Result: 在SNAPMe餐食图像和模拟用户上的实验显示具有竞争力的营养估算、个性化菜单和高效的任务计划

Conclusion: 证明了多智能体LLM控制在个性化营养中的可行性，同时揭示了从图像估算微量营养素和大规模现实世界研究中的开放挑战

Abstract: Personalized nutrition management aims to tailor dietary guidance to an individual's intake and phenotype, but most existing systems handle food logging, nutrient analysis and recommendation separately. We present a next-generation mobile nutrition assistant that combines image based meal logging with an LLM driven multi agent controller to provide meal level closed loop support. The system coordinates vision, dialogue and state management agents to estimate nutrients from photos and update a daily intake budget. It then adapts the next meal plan to user preferences and dietary constraints. Experiments with SNAPMe meal images and simulated users show competitive nutrient estimation, personalized menus and efficient task plans. These findings demonstrate the feasibility of multi agent LLM control for personalized nutrition and reveal open challenges in micronutrient estimation from images and in large scale real world studies.

</details>


### [29] [GUITester: Enabling GUI Agents for Exploratory Defect Discovery](https://arxiv.org/abs/2601.04500)
*Yifei Gao,Jiang Wu,Xiaoyi Chen,Yifan Yang,Zhe Cui,Tianyi Ma,Jiaming Zhang,Jitao Sang*

Main category: cs.AI

TL;DR: GUITester：一个多智能体框架，通过解耦导航和验证来解决MLLM智能体在GUI探索性测试中的目标导向掩盖和执行偏差归因问题，在GUITestBench基准上达到48.90%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 探索性GUI测试对软件质量至关重要，但人工成本高。现有的多模态大语言模型智能体擅长导航，但无法自主发现缺陷，主要面临两个核心挑战：目标导向掩盖（智能体优先完成任务而非报告异常）和执行偏差归因（系统缺陷被误认为是智能体错误）。

Method: 提出GUITester多智能体框架，包含两个模块：1）规划执行模块（PEM），通过嵌入测试意图主动探测缺陷；2）分层反思模块（HRM），通过交互历史分析解决归因模糊性。同时构建了GUITestBench基准，包含26种缺陷的143个任务。

Result: GUITester在GUITestBench上达到48.90%的F1分数（Pass@3），显著优于现有最佳基线方法的33.35%。

Conclusion: 这项工作证明了自主探索性测试的可行性，为未来GUI质量保证提供了坚实基础，展示了多智能体框架在解决GUI测试挑战方面的有效性。

Abstract: Exploratory GUI testing is essential for software quality but suffers from high manual costs. While Multi-modal Large Language Model (MLLM) agents excel in navigation, they fail to autonomously discover defects due to two core challenges: \textit{Goal-Oriented Masking}, where agents prioritize task completion over reporting anomalies, and \textit{Execution-Bias Attribution}, where system defects are misidentified as agent errors. To address these, we first introduce \textbf{GUITestBench}, the first interactive benchmark for this task, featuring 143 tasks across 26 defects. We then propose \textbf{GUITester}, a multi-agent framework that decouples navigation from verification via two modules: (i) a \textit{Planning-Execution Module (PEM)} that proactively probes for defects via embedded testing intents, and (ii) a \textit{Hierarchical Reflection Module (HRM)} that resolves attribution ambiguity through interaction history analysis. GUITester achieves an F1-score of 48.90\% (Pass@3) on GUITestBench, outperforming state-of-the-art baselines (33.35\%). Our work demonstrates the feasibility of autonomous exploratory testing and provides a robust foundation for future GUI quality assurance~\footnote{Our code is now available in~\href{https://github.com/ADaM-BJTU/GUITestBench}{https://github.com/ADaM-BJTU/GUITestBench}}.

</details>


### [30] [Specific Emitter Identification via Active Learning](https://arxiv.org/abs/2601.04502)
*Jingyi Wang,Fanggang Wang*

Main category: cs.AI

TL;DR: 提出了一种基于主动学习的特定发射源识别方法，通过三阶段半监督训练方案，在有限标注预算下显著提升识别精度。


<details>
  <summary>Details</summary>
Motivation: 特定发射源识别对通信安全至关重要，但传统方法依赖大量标注数据，获取成本高且耗时。需要开发在有限标注条件下仍能有效工作的解决方案。

Method: 采用三阶段半监督训练方案：1）自监督对比学习提取未标注数据的稳健表示；2）小规模标注数据上的监督训练，联合优化对比损失和交叉熵损失；3）主动学习模块基于不确定性和代表性标准选择最有价值的样本进行标注。

Result: 在ADS-B和WiFi数据集上的实验表明，该方法在有限标注条件下显著优于传统监督和半监督方法，以更低的标注成本获得更高的识别准确率。

Conclusion: 提出的主动学习增强SEI方法能有效解决标注数据稀缺问题，在有限标注预算下实现高性能的特定发射源识别，为通信安全提供实用解决方案。

Abstract: With the rapid growth of wireless communications, specific emitter identification (SEI) is significant for communication security. However, its model training relies heavily on the large-scale labeled data, which are costly and time-consuming to obtain. To address this challenge, we propose an SEI approach enhanced by active learning (AL), which follows a three-stage semi-supervised training scheme. In the first stage, self-supervised contrastive learning is employed with a dynamic dictionary update mechanism to extract robust representations from large amounts of the unlabeled data. In the second stage, supervised training on a small labeled dataset is performed, where the contrastive and cross-entropy losses are jointly optimized to improve the feature separability and strengthen the classification boundaries. In the third stage, an AL module selects the most valuable samples from the unlabeled data for annotation based on the uncertainty and representativeness criteria, further enhancing generalization under limited labeling budgets. Experimental results on the ADS-B and WiFi datasets demonstrate that the proposed SEI approach significantly outperforms the conventional supervised and semi-supervised methods under limited annotation conditions, achieving higher recognition accuracy with lower labeling cost.

</details>


### [31] [CircuitLM: A Multi-Agent LLM-Aided Design Framework for Generating Circuit Schematics from Natural Language Prompts](https://arxiv.org/abs/2601.04505)
*Khandakar Shakib Al Hasan,Syed Rifat Raiyan,Hasin Mahtab Alvee,Wahid Sadik*

Main category: cs.AI

TL;DR: CircuitLM：一个多智能体LLM辅助电路设计管道，可将自然语言描述转换为结构化、可视化的CircuitJSON电路图，通过组件知识库和验证框架解决LLM在电路设计中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在从自然语言描述生成电路图时存在严重问题：经常产生幻觉细节、违反电气约束、输出非机器可读格式，这阻碍了非专业人士进行可靠的电路原型设计。

Method: 提出CircuitLM多智能体管道，包含五个顺序阶段：1) LLM组件识别；2) 规范引脚布局检索；3) 电子专家智能体的思维链推理；4) JSON原理图合成；5) 力导向SVG可视化。基于包含50个组件的知识库，并采用双重度量电路验证(DMCV)框架确保安全性。

Result: 在100个多样化嵌入式系统提示上评估了六个LLM，DMCV验证框架在微控制器中心设计中实现了高保真度，能够可靠地将自然语言输入转换为可部署的硬件设计。

Conclusion: CircuitLM通过基于验证组件数据库的生成和多阶段智能体管道，成功弥合了自然语言输入与可部署硬件设计之间的差距，使非专家能够进行可靠的电路原型设计。

Abstract: Generating accurate circuit schematics from high-level natural language descriptions remains a persistent challenge in electronics design, as large language models (LLMs) frequently hallucinate in granular details, violate electrical constraints, and produce non-machine-readable outputs. We present CircuitLM, a novel multi-agent LLM-aided circuit design pipeline that translates user prompts into structured, visually interpretable CircuitJSON schematics through five sequential stages: (i) LLM-based component identification, (ii) canonical pinout retrieval, (iii) chain-of-thought reasoning by an electronics expert agent, (iv) JSON schematic synthesis, and (v) force-directed SVG visualization. Anchored by a curated, embedding-powered component knowledge base. While LLMs often violate electrical constraints, CircuitLM bridges this gap by grounding generation in a verified and dynamically extensible component database, initially comprising 50 components. To ensure safety, we incorporate a hybrid evaluation framework, namely Dual-Metric Circuit Validation (DMCV), validated against human-expert assessments, which achieves high fidelity in microcontroller-centric designs. We evaluate the system on 100 diverse embedded-systems prompts across six LLMs and introduce DMCV to assess both structural and electrical validity. This work bridges natural language input to deployable hardware designs, enabling reliable circuit prototyping by non-experts. Our code and data will be made public upon acceptance.

</details>


### [32] [A General Neural Backbone for Mixed-Integer Linear Optimization via Dual Attention](https://arxiv.org/abs/2601.04509)
*Peixin Huang,Yaoxin Wu,Yining Ma,Cathy Wu,Wen Song,Wei Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于注意力机制的神经网络架构，用于提升混合整数线性规划（MILP）的求解效率，通过双注意力机制在变量和约束之间进行全局信息交换，超越了传统图神经网络方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 混合整数线性规划（MILP）在科学和工程应用中广泛使用，但在大规模问题上计算仍然具有挑战性。现有的基于图神经网络（GNN）的方法受到局部导向机制的限制，表示能力有限，阻碍了神经网络在MILP中的应用。

Method: 提出了一种注意力驱动的神经网络架构，设计了双注意力机制，在变量和约束上并行执行自注意力和交叉注意力，实现全局信息交换和更深层的表示学习。该通用主干网络可应用于实例级、元素级和求解状态级的下游任务。

Result: 在广泛使用的基准测试上进行的广泛实验表明，该方法在多个下游任务中相比最先进的基线方法都取得了持续改进。

Conclusion: 基于注意力的神经网络架构为学习增强的混合整数线性优化提供了强大的基础，能够超越纯图视角的限制，学习更具表达力的表示。

Abstract: Mixed-integer linear programming (MILP), a widely used modeling framework for combinatorial optimization, are central to many scientific and engineering applications, yet remains computationally challenging at scale. Recent advances in deep learning address this challenge by representing MILP instances as variable-constraint bipartite graphs and applying graph neural networks (GNNs) to extract latent structural patterns and enhance solver efficiency. However, this architecture is inherently limited by the local-oriented mechanism, leading to restricted representation power and hindering neural approaches for MILP. Here we present an attention-driven neural architecture that learns expressive representations beyond the pure graph view. A dual-attention mechanism is designed to perform parallel self- and cross-attention over variables and constraints, enabling global information exchange and deeper representation learning. We apply this general backbone to various downstream tasks at the instance level, element level, and solving state level. Extensive experiments across widely used benchmarks show consistent improvements of our approach over state-of-the-art baselines, highlighting attention-based neural architectures as a powerful foundation for learning-enhanced mixed-integer linear optimization.

</details>


### [33] [Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data](https://arxiv.org/abs/2601.04518)
*Shogo Nakayama,Masahiro Okuda*

Main category: cs.AI

TL;DR: 本研究提出了一种改进的半监督对比学习方法，通过结合分布匹配技术来提升基于伪标签的半监督学习在图像分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在监督图像分类方面取得了显著进展，但数据标注成本高昂。虽然对比学习等无监督方法被广泛研究，但在现实场景中完全无标签的数据集很少见，而半监督学习在少量标注数据与大量无标签数据共存的情况下具有重要价值。现有的基于伪标签的半监督对比学习方法仍有改进空间。

Method: 本研究提出了一种增强的伪标签基半监督学习方法，通过引入标注数据和无标签数据特征嵌入之间的分布匹配技术，来改进图像分类的准确性。该方法结合了对比学习和分布对齐的思想。

Result: 该方法在多个数据集上提高了图像分类的准确率，验证了分布匹配技术对基于伪标签的半监督学习的有效性。

Conclusion: 通过结合分布匹配技术，基于伪标签的半监督对比学习方法可以得到显著改进，在少量标注数据和大量无标签数据共存的场景下，能够有效提升图像分类性能。

Abstract: The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.

</details>


### [34] [BioPIE: A Biomedical Protocol Information Extraction Dataset for High-Reasoning-Complexity Experiment Question Answer](https://arxiv.org/abs/2601.04524)
*Haofei Hou,Shunyi Zhao,Fanxu Meng,Kairui Yang,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: BioPIE数据集为生物医学实验问答系统提供细粒度、程序中心的知识图谱，解决高信息密度和多步推理挑战，提升实验自动化性能


<details>
  <summary>Details</summary>
Motivation: 现有生物医学数据集主要关注通用或粗粒度知识，无法支持生物医学实验问答所需的高信息密度和多步推理的细粒度推理需求

Method: 引入生物医学协议信息提取数据集(BioPIE)，提供以程序为中心的知识图谱，包含实验实体、动作和关系，支持跨协议的实验推理

Result: 在BioPIE上评估信息提取方法，并实现基于BioPIE的问答系统，在测试集、高信息密度和多步推理问题集上均展示性能提升

Conclusion: BioPIE中的结构化实验知识为AI辅助和更自主的生物医学实验提供了基础，解决了现有数据集在细粒度实验推理方面的不足

Abstract: Question Answer (QA) systems for biomedical experiments facilitate cross-disciplinary communication, and serve as a foundation for downstream tasks, e.g., laboratory automation. High Information Density (HID) and Multi-Step Reasoning (MSR) pose unique challenges for biomedical experimental QA. While extracting structured knowledge, e.g., Knowledge Graphs (KGs), can substantially benefit biomedical experimental QA. Existing biomedical datasets focus on general or coarsegrained knowledge and thus fail to support the fine-grained experimental reasoning demanded by HID and MSR. To address this gap, we introduce Biomedical Protocol Information Extraction Dataset (BioPIE), a dataset that provides procedure-centric KGs of experimental entities, actions, and relations at a scale that supports reasoning over biomedical experiments across protocols. We evaluate information extraction methods on BioPIE, and implement a QA system that leverages BioPIE, showcasing performance gains on test, HID, and MSR question sets, showing that the structured experimental knowledge in BioPIE underpins both AI-assisted and more autonomous biomedical experimentation.

</details>


### [35] [TCAndon-Router: Adaptive Reasoning Router for Multi-Agent Collaboration](https://arxiv.org/abs/2601.04544)
*Jiuzhou Zhao,Chunrong Chen,Chenqi Qiao,Lebin Zheng,Minqi Han,Yanchi Liu Yongzhou Xu Xiaochuan Xu Min Zhang*

Main category: cs.AI

TL;DR: TCAR是一个自适应推理路由器，支持动态代理加入，通过生成推理链预测候选代理集，并使用协作执行管道聚合响应，显著提高路由精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有任务路由方法主要依赖静态单标签决策，存在两大限制：1）难以无缝集成新代理以适应业务扩展；2）代理能力重叠导致路由冲突，降低准确性和鲁棒性。

Method: 提出TCAR自适应推理路由器：1）支持动态代理加入；2）首先生成自然语言推理链，然后预测能够处理查询的候选代理集；3）设计协作执行管道，由选定代理独立生成响应，再由专门的精炼代理聚合和优化为单一高质量响应。

Result: 在公共数据集和真实企业数据上的实验表明，TCAR显著提高了路由精度，减少了路由冲突，并在模糊场景中保持鲁棒性。

Conclusion: TCAR解决了传统任务路由方法的局限性，提供了一种可解释和协作的多代理路由解决方案，已开源支持未来研究。

Abstract: Multi-Agent Systems(MAS) have become a powerful paradigm for building high performance intelligent applications. Within these systems, the router responsible for determining which expert agents should handle a given query plays a crucial role in overall performance. Existing routing strategies generally fall into two categories: performance routing, which balances latency and cost across models of different sizes, and task routing, which assigns queries to domain-specific experts to improve accuracy. In real-world enterprise applications, task routing is more suitable; however, most existing approaches rely on static single-label decisions, which introduce two major limitations: (i) difficulty in seamlessly integrating new agents as business domains expand, and (ii) routing conflicts caused by overlapping agent capabilities, ultimately degrading accuracy and robustness.To address these challenges, we propose TCAndon-Router(TCAR): an adaptive reasoning router for multi-agent collaboration. Unlike traditional routers, TCAR supports dynamic agent onboarding and first generates a natural-language reasoning chain before predicting a set of candidate agents capable of handling the query. In addition, we design a collaborative execution pipeline in which selected agents independently produce responses, which are then aggregated and refined into a single high-quality response by a dedicated Refining Agent.Experiments on public datasets and real enterprise data demonstrate that TCAR significantly improves routing accuracy, reduces routing conflicts, and remains robust in ambiguous scenarios. We have released TCAR at https://huggingface.co/tencent/TCAndon-Router to support future research on explainable and collaborative multi-agent routing.

</details>


### [36] [Personalized Model-Based Design of Human Centric AI enabled CPS for Long term usage](https://arxiv.org/abs/2601.04545)
*Bernard Ngabonziza,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.AI

TL;DR: 该论文分析了AI赋能人本关键系统长期运行中的安全、可持续性和安全问题，提出了个性化模型解决方案


<details>
  <summary>Details</summary>
Motivation: AI赋能的人本关键系统（如医疗监控、自动驾驶等）需要长期运行，但可能面临未测试的极端情况，导致安全、可持续性和安全要求被违反。现有技术存在局限性，无法充分测试系统长期使用的可靠性。

Method: 首先分析现有AI赋能人本控制系统在安全、可持续性和安全分析方面的技术及其局限性，然后提出个性化模型解决方案来消除这些限制。

Result: 论文识别了现有测试方法的局限性，并提出个性化模型方法作为解决方案，以应对长期运行中可能出现的极端情况和不确定性。

Conclusion: AI赋能人本关键系统需要更有效的测试方法来确保长期运行的安全性、可持续性和安全性，个性化模型解决方案有望解决现有方法的局限性。

Abstract: Human centric critical systems are increasingly involving artificial intelligence to enable knowledge extraction from sensor collected data. Examples include medical monitoring and control systems, gesture based human computer interaction systems, and autonomous cars. Such systems are intended to operate for a long term potentially for a lifetime in many scenarios such as closed loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoting systems for stroke diagnosis, and rehabilitation. Long term operation of such AI enabled human centric applications can expose them to corner cases for which their operation is may be uncertain. This can be due to many reasons such as inherent flaws in the design, limited resources for testing, inherent computational limitations of the testing methodology, or unknown use cases resulting from human interaction with the system. Such untested corner cases or cases for which the system performance is uncertain can lead to violations in the safety, sustainability, and security requirements of the system. In this paper, we analyze the existing techniques for safety, sustainability, and security analysis of an AI enabled human centric control system and discuss their limitations for testing the system for long term use in practice. We then propose personalized model based solutions for potentially eliminating such limitations.

</details>


### [37] [Reasoning Over Space: Enabling Geographic Reasoning for LLM-Based Generative Next POI Recommendation](https://arxiv.org/abs/2601.04562)
*Dongyi Lv,Qiuyu Ding,Heng-Da Xu,Zhaoxu Sun,Zhi Wang,Feng Xiong,Mu Xu*

Main category: cs.AI

TL;DR: ROS框架利用地理空间信息增强LLM推荐系统，通过分层空间语义ID和移动链式思维范式，在位置社交网络数据上实现超过10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的生成式推荐系统在利用地理信号方面存在局限，而地理位置信息对于移动性和本地服务场景至关重要。

Method: 提出ROS框架，包含：1）分层空间语义ID将地理位置和POI语义离散化为组合标记；2）三阶段移动链式思维范式建模用户个性、构建意图对齐候选空间、进行位置感知剪枝；3）通过空间引导的强化学习与现实地理对齐。

Result: 在三个广泛使用的位置社交网络数据集上，ROS相比最强的LLM基线在命中率上获得超过10%的相对提升，并改善了跨城市迁移能力，尽管使用了更小的骨干模型。

Conclusion: ROS框架成功地将地理空间信息作为推理过程中的关键决策变量，显著提升了基于LLM的推荐系统在位置相关场景中的性能。

Abstract: Generative recommendation with large language models (LLMs) reframes prediction as sequence generation, yet existing LLM-based recommenders remain limited in leveraging geographic signals that are crucial in mobility and local-services scenarios. Here, we present Reasoning Over Space (ROS), a framework that utilizes geography as a vital decision variable within the reasoning process. ROS introduces a Hierarchical Spatial Semantic ID (SID) that discretizes coarse-to-fine locality and POI semantics into compositional tokens, and endows LLM with a three-stage Mobility Chain-of-Thought (CoT) paradigm that models user personality, constructs an intent-aligned candidate space, and performs locality informed pruning. We further align the model with real world geography via spatial-guided Reinforcement Learning (RL). Experiments on three widely used location-based social network (LBSN) datasets show that ROS achieves over 10% relative gains in hit rate over strongest LLM-based baselines and improves cross-city transfer, despite using a smaller backbone model.

</details>


### [38] [BackdoorAgent: A Unified Framework for Backdoor Attacks on LLM-based Agents](https://arxiv.org/abs/2601.04566)
*Yunhao Feng,Yige Li,Yutao Wu,Yingshui Tan,Yanming Guo,Yifan Ding,Kun Zhai,Xingjun Ma,Yugang Jiang*

Main category: cs.AI

TL;DR: 本文提出了BackdoorAgent框架，系统分析大语言模型智能体工作流程中的后门威胁，将攻击面分为规划、记忆和工具使用三个阶段，并构建了标准化基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能体通过多步骤工作流程执行任务，这种自主性设计扩大了后门攻击面。现有研究分散且孤立分析单个攻击向量，缺乏从智能体角度理解后门触发器在跨阶段交互和传播中的影响。

Method: 提出BackdoorAgent框架，将智能体工作流程的攻击面结构化分为规划攻击、记忆攻击和工具使用攻击三个阶段，并构建标准化基准覆盖Agent QA、Agent Code、Agent Web和Agent Drive四种代表性应用场景。

Result: 实验分析显示，植入单个阶段的触发器可以跨多个步骤持续存在并通过中间状态传播。使用GPT骨干模型时，规划攻击中触发器持续存在率为43.58%，记忆攻击为77.97%，工具阶段攻击为60.28%。

Conclusion: 智能体工作流程本身对后门威胁存在脆弱性，需要系统性的安全分析和防护措施。BackdoorAgent框架为理解和分析智能体后门威胁提供了统一视角和工具。

Abstract: Large language model (LLM) agents execute tasks through multi-step workflows that combine planning, memory, and tool use. While this design enables autonomy, it also expands the attack surface for backdoor threats. Backdoor triggers injected into specific stages of an agent workflow can persist through multiple intermediate states and adversely influence downstream outputs. However, existing studies remain fragmented and typically analyze individual attack vectors in isolation, leaving the cross-stage interaction and propagation of backdoor triggers poorly understood from an agent-centric perspective. To fill this gap, we propose \textbf{BackdoorAgent}, a modular and stage-aware framework that provides a unified, agent-centric view of backdoor threats in LLM agents. BackdoorAgent structures the attack surface into three functional stages of agentic workflows, including \textbf{planning attacks}, \textbf{memory attacks}, and \textbf{tool-use attacks}, and instruments agent execution to enable systematic analysis of trigger activation and propagation across different stages. Building on this framework, we construct a standardized benchmark spanning four representative agent applications: \textbf{Agent QA}, \textbf{Agent Code}, \textbf{Agent Web}, and \textbf{Agent Drive}, covering both language-only and multimodal settings. Our empirical analysis shows that \textit{triggers implanted at a single stage can persist across multiple steps and propagate through intermediate states.} For instance, when using a GPT-based backbone, we observe trigger persistence in 43.58\% of planning attacks, 77.97\% of memory attacks, and 60.28\% of tool-stage attacks, highlighting the vulnerabilities of the agentic workflow itself to backdoor threats. To facilitate reproducibility and future research, our code and benchmark are publicly available at GitHub.

</details>


### [39] [Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment](https://arxiv.org/abs/2601.04571)
*Delong Zeng,Yuexiang Xie,Yaliang Li,Ying Shen*

Main category: cs.AI

TL;DR: 提出CIEA方法，通过互补信息提取和对齐技术改进多模态检索，在统一潜在空间中处理文本和图像，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检索研究主要关注多模态数据中与配对文本相似的信息，但忽略了多模态数据中包含的互补信息，这限制了检索性能的提升。

Method: 提出CIEA方法，包含互补信息提取器和对齐机制，将文本和图像转换到统一潜在空间，使用两种互补对比损失进行优化，确保语义完整性并有效捕捉图像中的互补信息。

Result: 大量实验证明CIEA的有效性，相比分治模型和通用密集检索模型都有显著改进，提供了消融研究、进一步讨论和案例研究来展示CIEA的进步。

Conclusion: CIEA通过有效提取和对齐多模态数据中的互补信息，显著提升了多模态检索性能，为社区研究提供了新的方法和开源代码。

Abstract: Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.

</details>


### [40] [Scaling Behavior Cloning Improves Causal Reasoning: An Open Model for Real-Time Video Game Playing](https://arxiv.org/abs/2601.04575)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Chris Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.AI

TL;DR: 该研究提出了一个用于训练视频游戏基础模型的开放配方，能够在消费级GPU上实时推理，并发布了8300+小时的高质量人类游戏数据、代码和预训练模型，展示了模型在多种3D游戏中达到人类水平的表现，并系统研究了行为克隆的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 随着模型和数据规模的扩大，行为克隆在多种任务中显示出强大潜力。本研究旨在开发一个专门用于实时视频游戏推理的基础模型，并通过系统研究来理解行为克隆的缩放规律，特别是模型和数据规模如何影响性能和因果推理能力。

Method: 开发了一个开放的视频游戏基础模型训练配方，包含数据收集（8300+小时高质量人类游戏数据）、训练和推理代码。通过系统实验研究模型规模（参数数量、深度）和数据规模对性能的影响，使用简单玩具问题和扩展到12亿参数的模型来探索因果推理的缩放规律。

Result: 最佳模型能够在多种3D视频游戏中达到与人类竞争的水平。研究发现，增加训练数据和网络深度能够使模型学习到更具因果性的策略。在扩展到12亿参数的模型中，观察到了与玩具问题相似的缩放结果，表明模型规模和数据规模对因果推理能力有系统性影响。

Conclusion: 该研究成功开发了一个开放的视频游戏基础模型，证明了行为克隆在游戏领域的有效性，并通过系统研究揭示了模型和数据规模对因果推理能力的缩放规律，为未来行为克隆模型的发展提供了重要参考。

Abstract: Behavior cloning is enjoying a resurgence in popularity as scaling both model and data sizes proves to provide a strong starting point for many tasks of interest. In this work, we introduce an open recipe for training a video game playing foundation model designed for inference in realtime on a consumer GPU. We release all data (8300+ hours of high quality human gameplay), training and inference code, and pretrained checkpoints under an open license. We show that our best model is capable of playing a variety of 3D video games at a level competitive with human play. We use this recipe to systematically examine the scaling laws of behavior cloning to understand how the model's performance and causal reasoning varies with model and data scale. We first show in a simple toy problem that, for some types of causal reasoning, increasing both the amount of training data and the depth of the network results in the model learning a more causal policy. We then systematically study how causality varies with the number of parameters (and depth) and training steps in scaled models of up to 1.2 billion parameters, and we find similar scaling results to what we observe in the toy problem.

</details>


### [41] [Sci-Reasoning: A Dataset Decoding AI Innovation Patterns](https://arxiv.org/abs/2601.04577)
*Jiachen Liu,Maestro Harmon,Zechen Zhang*

Main category: cs.AI

TL;DR: Sci-Reasoning是首个捕捉高质量AI研究背后智力合成过程的数据集，通过追踪顶级会议论文与其关键前驱的关系，识别出15种不同的思维模式，其中三种主导策略占52.7%，为AI研究代理的训练提供结构化推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 尽管AI创新加速发展，但突破背后的智力过程——研究人员如何识别研究空白、综合先前工作并产生洞见——仍然理解不足。缺乏科学推理的结构化数据阻碍了对AI研究代理的系统分析和开发。

Method: 使用社区验证的质量信号和LLM加速、人工验证的流程，追踪NeurIPS、ICML和ICLR（2023-2025）的口头报告和焦点论文与其关键前驱的关系，以结构化格式阐述具体的推理链接。

Result: 识别出15种不同的思维模式，其中三种主导策略占52.7%：空白驱动重构（24.2%）、跨领域综合（18.0%）和表示转换（10.5%）。最强大的创新配方结合多种模式：空白驱动重构+表示转换、跨领域综合+表示转换、空白驱动重构+跨领域综合。

Conclusion: Sci-Reasoning数据集支持科学进展的定量研究，并为训练下一代AI研究代理提供结构化推理轨迹，有助于理解AI研究的智力过程并开发更智能的研究助手。

Abstract: While AI innovation accelerates rapidly, the intellectual process behind breakthroughs -- how researchers identify gaps, synthesize prior work, and generate insights -- remains poorly understood. The lack of structured data on scientific reasoning hinders systematic analysis and development of AI research agents. We introduce Sci-Reasoning, the first dataset capturing the intellectual synthesis behind high-quality AI research. Using community-validated quality signals and an LLM-accelerated, human-verified pipeline, we trace Oral and Spotlight papers across NeurIPS, ICML, and ICLR (2023-2025) to its key predecessors, articulating specific reasoning links in a structured format. Our analysis identifies 15 distinct thinking patterns, with three dominant strategies accounting for 52.7%: Gap-Driven Reframing (24.2%), Cross-Domain Synthesis (18.0%), and Representation Shift (10.5%). The most powerful innovation recipes combine multiple patterns: Gap-Driven Reframing + Representation Shift, Cross-Domain Synthesis + Representation Shift, and Gap-Driven Reframing + Cross-Domain Synthesis. This dataset enables quantitative studies of scientific progress and provides structured reasoning trajectories for training the next generation AI research agents.

</details>


### [42] [Autonomous Agents on Blockchains: Standards, Execution Models, and Trust Boundaries](https://arxiv.org/abs/2601.04583)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 该论文系统综述了AI智能体与区块链的互操作性，提出了五类集成模式、专门威胁模型和比较能力矩阵，并提出了交易意图模式和策略决策记录两个接口抽象作为研究路线图。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展使得AI智能体能够推理、规划和执行多步工作流，而公链已成为可编程的价值转移、访问控制和可验证状态转换平台。两者的融合带来了高风险的系统挑战：需要设计标准、可互操作且安全的接口，让智能体能够观察链上状态、制定交易意图并授权执行，同时不暴露用户、协议或组织于不可接受的安全、治理或经济风险中。

Method: 通过系统文献综述方法，从3000多条记录中筛选出317篇相关文献，构建了五部分分类法（只读分析、模拟与意图生成、委托执行、自主签名、多智能体工作流），开发了针对智能体驱动交易管道的威胁模型，并创建了包含13个维度的比较能力矩阵，分析了20多个代表性系统。

Result: 识别了AI智能体与区块链互操作性的关键集成模式，揭示了现有系统的能力差距，提出了交易意图模式和策略决策记录两个核心接口抽象，并建议建立可复现的评估套件和基准。

Conclusion: 论文为AI智能体与区块链互操作性领域提供了系统化的框架，提出了具体的研究路线图，强调需要标准化的接口抽象和评估方法来确保智能体介导的链上执行的安全性、可靠性和经济稳健性。

Abstract: Advances in large language models have enabled agentic AI systems that can reason, plan, and interact with external tools to execute multi-step workflows, while public blockchains have evolved into a programmable substrate for value transfer, access control, and verifiable state transitions. Their convergence introduces a high-stakes systems challenge: designing standard, interoperable, and secure interfaces that allow agents to observe on-chain state, formulate transaction intents, and authorize execution without exposing users, protocols, or organizations to unacceptable security, governance, or economic risks. This survey systematizes the emerging landscape of agent-blockchain interoperability through a systematic literature review, identifying 317 relevant works from an initial pool of over 3000 records. We contribute a five-part taxonomy of integration patterns spanning read-only analytics, simulation and intent generation, delegated execution, autonomous signing, and multi-agent workflows; a threat model tailored to agent-driven transaction pipelines that captures risks ranging from prompt injection and policy misuse to key compromise, adversarial execution dynamics, and multi-agent collusion; and a comparative capability matrix analyzing more than 20 representative systems across 13 dimensions, including custody models, permissioning, policy enforcement, observability, and recovery. Building on the gaps revealed by this analysis, we outline a research roadmap centered on two interface abstractions: a Transaction Intent Schema for portable and unambiguous goal specification, and a Policy Decision Record for auditable, verifiable policy enforcement across execution environments. We conclude by proposing a reproducible evaluation suite and benchmarks for assessing the safety, reliability, and economic robustness of agent-mediated on-chain execution.

</details>


### [43] [Evaluating Human and Machine Confidence in Phishing Email Detection: A Comparative Study](https://arxiv.org/abs/2601.04610)
*Paras Jain,Khushi Dhar,Olyemi E. Amujo,Esa M. Rantanen*

Main category: cs.AI

TL;DR: 研究比较人类认知与机器学习模型在识别钓鱼邮件的表现，发现机器学习准确率高但置信度波动大，人类则使用更多语言线索且置信度更稳定，年龄影响检测能力而语言熟练度影响小。


<details>
  <summary>Details</summary>
Motivation: 识别钓鱼邮件等欺骗性内容需要复杂的认知过程，结合模式识别、置信度评估和上下文分析。本研究旨在探索人类认知与机器学习模型如何协同工作来区分钓鱼邮件和合法邮件。

Method: 使用三种可解释算法（逻辑回归、决策树、随机森林），在TF-IDF特征和语义嵌入上进行训练，然后将它们的预测结果与人类评估进行比较，人类评估包括置信度评分和语言观察。

Result: 机器学习模型提供良好的准确率，但置信度水平差异显著；人类评估者使用更多样化的语言线索且置信度更一致；语言熟练度对检测性能影响最小，但年龄有影响。

Conclusion: 这些发现为创建透明AI系统提供了有益指导，这些系统可以补充人类认知功能，最终改善人类-AI在具有挑战性的内容分析任务中的协作。

Abstract: Identifying deceptive content like phishing emails demands sophisticated cognitive processes that combine pattern recognition, confidence assessment, and contextual analysis. This research examines how human cognition and machine learn- ing models work together to distinguish phishing emails from legitimate ones. We employed three interpretable algorithms Logistic Regression, Decision Trees, and Random Forests train- ing them on both TF-IDF features and semantic embeddings, then compared their predictions against human evaluations that captured confidence ratings and linguistic observations. Our results show that machine learning models provide good accuracy rates, but their confidence levels vary significantly. Human evaluators, on the other hand, use a greater variety of language signs and retain more consistent confidence. We also found that while language proficiency has minimal effect on detection performance, aging does. These findings offer helpful direction for creating transparent AI systems that complement human cognitive functions, ultimately improving human-AI cooperation in challenging content analysis tasks.

</details>


### [44] [AgentDevel: Reframing Self-Evolving LLM Agents as Release Engineering](https://arxiv.org/abs/2601.04620)
*Di Zhang*

Main category: cs.AI

TL;DR: AgentDevel：一个将LLM智能体改进重构为发布工程的框架，通过外部化回归感知的发布管道实现稳定、可审计的改进，避免传统自我改进方法的不稳定性和难以审计问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体的改进方法主要依赖智能体内部的自我改进机制或并发变体搜索，这些方法虽然能提高总体分数，但会产生不稳定且难以审计的改进轨迹，难以保证非回归性或跨版本故障分析。

Method: 提出AgentDevel发布工程管道：1）运行当前智能体并基于执行轨迹生成实现无关的症状级质量信号；2）通过可执行诊断合成单个发布候选版本；3）采用翻转中心门控机制进行版本提升。核心设计包括：实现无关的LLM批评器、基于脚本的可执行诊断、翻转中心门控。

Result: 在面向执行的基准测试中，AgentDevel实现了稳定的改进，显著减少了回归问题，同时产生了可复现、可审计的工件。

Conclusion: AgentDevel为构建、调试和发布LLM智能体提供了一种实用的开发规范，将智能体视为可交付的软件工件，强调非回归作为主要目标，维护单一规范版本线。

Abstract: Recent progress in large language model (LLM) agents has largely focused on embedding self-improvement mechanisms inside the agent or searching over many concurrent variants. While these approaches can raise aggregate scores, they often yield unstable and hard-to-audit improvement trajectories, making it difficult to guarantee non-regression or to reason about failures across versions. We reframe agent improvement as \textbf{release engineering}: agents are treated as shippable artifacts, and improvement is externalized into a regression-aware release pipeline. We introduce \textbf{AgentDevel}, a release engineering pipeline that iteratively runs the current agent, produces implementation-blind, symptom-level quality signals from execution traces, synthesizes a single release candidate (RC) via executable diagnosis, and promotes it under flip-centered gating. AgentDevel features three core designs: (i) an implementation-blind LLM critic that characterizes failure appearances without accessing agent internals, (ii) script-based executable diagnosis that aggregates dominant symptom patterns and produces auditable engineering specifications, and (iii) flip-centered gating that prioritizes pass to fail regressions and fail to pass fixes as first-class evidence. Unlike population-based search or in-agent self-refinement, AgentDevel maintains a single canonical version line and emphasizes non-regression as a primary objective. Experiments on execution-heavy benchmarks demonstrate that AgentDevel yields stable improvements with significantly fewer regressions while producing reproducible, auditable artifacts. Overall, AgentDevel provides a practical development discipline for building, debugging, and releasing LLM agents as software development.

</details>


### [45] [Beyond the "Truth": Investigating Election Rumors on Truth Social During the 2024 Election](https://arxiv.org/abs/2601.04631)
*Etienne Casanova,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 该研究利用大语言模型分析社交媒体谣言传播，开发了多阶段谣言检测代理，首次在大规模真实数据中量化了"虚幻真相效应"的剂量-反应关系。


<details>
  <summary>Details</summary>
Motivation: 大语言模型为大规模分析社会现象提供了前所未有的机会。本研究旨在展示LLMs在心理测量中的价值，特别是在谣言传播的心理动力学方面，尤其是在意识形态同质化网络中量化"虚幻真相效应"。

Method: 1) 收集首个关于小众替代技术平台的选举谣言大规模数据集；2) 开发多阶段谣言检测代理，结合：i) 合成数据增强的微调RoBERTa分类器，ii) 精确关键词过滤，iii) 使用GPT-4o mini的两阶段LLM验证流程；3) 量化谣言传播的心理动力学。

Result: 研究发现：1) 分享概率随着每次额外曝光稳步上升，为意识形态同质化网络中的剂量-反应信念强化提供了大规模实证证据；2) 模拟结果显示快速传染效应：仅经过四次传播迭代，近四分之一用户就被"感染"。

Conclusion: 大语言模型能够通过在大规模真实数据集中严格测量信念动态和错误信息传播，从而变革心理科学。该研究展示了LLMs在分析社会现象和心理测量方面的强大潜力。

Abstract: Large language models (LLMs) offer unprecedented opportunities for analyzing social phenomena at scale. This paper demonstrates the value of LLMs in psychological measurement by (1) compiling the first large-scale dataset of election rumors on a niche alt-tech platform, (2) developing a multistage Rumor Detection Agent that leverages LLMs for high-precision content classification, and (3) quantifying the psychological dynamics of rumor propagation, specifically the "illusory truth effect" in a naturalistic setting. The Rumor Detection Agent combines (i) a synthetic data-augmented, fine-tuned RoBERTa classifier, (ii) precision keyword filtering, and (iii) a two-pass LLM verification pipeline using GPT-4o mini. The findings reveal that sharing probability rises steadily with each additional exposure, providing large-scale empirical evidence for dose-response belief reinforcement in ideologically homogeneous networks. Simulation results further demonstrate rapid contagion effects: nearly one quarter of users become "infected" within just four propagation iterations. Taken together, these results illustrate how LLMs can transform psychological science by enabling the rigorous measurement of belief dynamics and misinformation spread in massive, real-world datasets.

</details>


### [46] [Vibe Coding an LLM-powered Theorem Prover](https://arxiv.org/abs/2601.04653)
*Zhe Hou*

Main category: cs.AI

TL;DR: Isabellm是一个基于LLM的Isabelle/HOL定理证明器，能够进行全自动证明合成，结合逐步证明器和高级证明规划器，在某些引理上能超越Isabelle的标准自动化工具。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在消费级计算机上运行的LLM驱动的定理证明器，解决Isabelle/HOL中传统自动化工具无法证明的引理，探索LLM在形式化证明中的应用价值。

Method: 结合逐步证明器（使用LLM生成证明命令并由Isabelle验证）和高级证明规划器（生成结构化Isar大纲并填补修复空缺），采用波束搜索、策略重排序、前提选择、微RAG和反例引导的证明修复等技术。

Result: Isabellm能够证明一些Isabelle标准自动化工具（包括Sledgehammer）无法处理的引理，展示了LLM引导证明搜索的实际价值，但也发现即使是先进的LLM在复杂算法设计方面仍存在可靠性问题。

Conclusion: LLM驱动的定理证明器在形式化证明中具有实用价值，但当前LLM在复杂代码生成和推理方面仍面临根本性挑战，需要进一步研究改进。

Abstract: We present Isabellm, an LLM-powered theorem prover for Isabelle/HOL that performs fully automatic proof synthesis. Isabellm works with any local LLM on Ollama and APIs such as Gemini CLI, and it is designed to run on consumer grade computers. The system combines a stepwise prover, which uses large language models to propose proof commands validated by Isabelle in a bounded search loop, with a higher-level proof planner that generates structured Isar outlines and attempts to fill and repair remaining gaps. The framework includes beam search for tactics, tactics reranker ML and RL models, premise selection with small transformer models, micro-RAG for Isar proofs built from AFP, and counter-example guided proof repair. All the code is implemented by GPT 4.1 - 5.2, Gemini 3 Pro, and Claude 4.5. Empirically, Isabellm can prove certain lemmas that defeat Isabelle's standard automation, including Sledgehammer, demonstrating the practical value of LLM-guided proof search. At the same time, we find that even state-of-the-art LLMs, such as GPT 5.2 Extended Thinking and Gemini 3 Pro struggle to reliably implement the intended fill-and-repair mechanisms with complex algorithmic designs, highlighting fundamental challenges in LLM code generation and reasoning. The code of Isabellm is available at https://github.com/zhehou/llm-isabelle

</details>


### [47] [Know Thy Enemy: Securing LLMs Against Prompt Injection via Diverse Data Synthesis and Instruction-Level Chain-of-Thought Learning](https://arxiv.org/abs/2601.04666)
*Zhiyuan Chang,Mingyang Li,Yuekai Huang,Ziyou Jiang,Xiaojun Jia,Qian Xiong,Junjie Wang,Zhaoyang Li,Qing Wang*

Main category: cs.AI

TL;DR: InstruCoT：一种通过指令级思维链微调增强LLM防御提示注入攻击的方法，能有效识别和拒绝恶意指令


<details>
  <summary>Details</summary>
Motivation: 大型语言模型应用面临提示注入攻击的安全漏洞，主要问题包括：恶意指令可通过多种向量注入，且注入指令与上下文缺乏清晰语义边界，难以识别

Method: InstruCoT方法：合成多样化训练数据，采用指令级思维链微调，使LLM能够有效识别和拒绝恶意指令，无论其来源或上下文位置

Result: 在四个LLM上的实验表明，InstruCoT在行为偏差、隐私泄露和有害输出三个关键维度上显著优于基线方法，同时保持实用性能不下降

Conclusion: InstruCoT通过指令级思维链微调有效解决了提示注入攻击的防御问题，在多个安全维度上表现优异，且不影响模型实用性

Abstract: Large language model (LLM)-integrated applications have become increasingly prevalent, yet face critical security vulnerabilities from prompt injection (PI) attacks. Defending against PI attacks faces two major issues: malicious instructions can be injected through diverse vectors, and injected instructions often lack clear semantic boundaries from the surrounding context, making them difficult to identify. To address these issues, we propose InstruCoT, a model enhancement method for PI defense that synthesizes diverse training data and employs instruction-level chain-of-thought fine-tuning, enabling LLMs to effectively identify and reject malicious instructions regardless of their source or position in the context. We evaluate InstruCoT across three critical dimensions: Behavior Deviation, Privacy Leakage, and Harmful Output. Experimental results across four LLMs demonstrate that InstruCoT significantly outperforms baselines in all dimensions while maintaining utility performance without degradation

</details>


### [48] [LLM-Guided Quantified SMT Solving over Uninterpreted Functions](https://arxiv.org/abs/2601.04675)
*Kunhang Lv,Yuhang Dong,Rui Han,Fuqi Jia,Feifei Ma,Jian Zhang*

Main category: cs.AI

TL;DR: AquaForte是一个利用大语言模型为SMT求解器中未解释函数提供语义指导的框架，通过生成满足约束的函数定义候选来显著减少搜索空间，在SMT-COMP基准测试中解决了传统求解器超时的多个实例。


<details>
  <summary>Details</summary>
Motivation: 传统量化实例化方法在处理包含未解释函数的非线性实数算术公式时面临挑战，因为它们缺乏对UF约束的语义理解，只能在有限的指导下搜索无界的解空间。

Method: 通过约束分离预处理公式，使用结构化提示从LLM中提取数学推理，通过自适应实例化将结果与传统SMT算法集成，通过系统验证保持正确性，通过回退到传统求解器保持完备性。

Result: 在SMT-COMP基准测试中，AquaForte解决了Z3和CVC5等最先进求解器超时的多个实例，特别在可满足公式上表现出色。

Conclusion: LLM可以为符号推理提供有价值的数学直觉，为SMT约束求解建立了新范式，展示了语义指导在解决复杂量化问题中的潜力。

Abstract: Quantified formulas with Uninterpreted Functions (UFs) over non-linear real arithmetic pose fundamental challenges for Satisfiability Modulo Theories (SMT) solving. Traditional quantifier instantiation methods struggle because they lack semantic understanding of UF constraints, forcing them to search through unbounded solution spaces with limited guidance. We present AquaForte, a framework that leverages Large Language Models to provide semantic guidance for UF instantiation by generating instantiated candidates for function definitions that satisfy the constraints, thereby significantly reducing the search space and complexity for solvers. Our approach preprocesses formulas through constraint separation, uses structured prompts to extract mathematical reasoning from LLMs, and integrates the results with traditional SMT algorithms through adaptive instantiation. AquaForte maintains soundness through systematic validation: LLM-guided instantiations yielding SAT solve the original problem, while UNSAT results generate exclusion clauses for iterative refinement. Completeness is preserved by fallback to traditional solvers augmented with learned constraints. Experimental evaluation on SMT-COMP benchmarks demonstrates that AquaForte solves numerous instances where state-of-the-art solvers like Z3 and CVC5 timeout, with particular effectiveness on satisfiable formulas. Our work shows that LLMs can provide valuable mathematical intuition for symbolic reasoning, establishing a new paradigm for SMT constraint solving.

</details>


### [49] [ResMAS: Resilience Optimization in LLM-based Multi-agent Systems](https://arxiv.org/abs/2601.04694)
*Zhilun Zhou,Zihan Liu,Jiahe Liu,Qingyu Shao,Yihan Wang,Kun Shao,Depeng Jin,Fengli Xu*

Main category: cs.AI

TL;DR: 本文提出ResMAS框架，通过两阶段方法增强基于大语言模型的多智能体系统的抗干扰能力：1）训练奖励模型预测系统韧性，并基于此通过强化学习自动设计韧性拓扑；2）引入拓扑感知的提示优化方法，根据智能体连接关系优化提示设计。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体系统通常分布在不同的设备或环境中，容易受到智能体故障等干扰。现有研究主要关注攻击后的被动检测和缓解，而非主动设计具有内在韧性的系统。研究发现通信拓扑和提示设计对系统韧性有显著影响。

Method: 提出ResMAS两阶段框架：第一阶段训练奖励模型预测多智能体系统韧性，基于此通过强化学习训练拓扑生成器，为特定任务自动设计韧性拓扑；第二阶段引入拓扑感知的提示优化方法，根据每个智能体与其他智能体的连接和交互关系优化其提示设计。

Result: 在多种任务上的广泛实验表明，该方法在各种约束条件下显著提高了多智能体系统的韧性。此外，该框架对新任务和模型表现出强大的泛化能力。

Conclusion: ResMAS框架通过主动设计韧性拓扑和拓扑感知的提示优化，有效增强了基于大语言模型的多智能体系统的抗干扰能力，为构建韧性多智能体系统提供了有前景的解决方案。

Abstract: Large Language Model-based Multi-Agent Systems (LLM-based MAS), where multiple LLM agents collaborate to solve complex tasks, have shown impressive performance in many areas. However, MAS are typically distributed across different devices or environments, making them vulnerable to perturbations such as agent failures. While existing works have studied the adversarial attacks and corresponding defense strategies, they mainly focus on reactively detecting and mitigating attacks after they occur rather than proactively designing inherently resilient systems. In this work, we study the resilience of LLM-based MAS under perturbations and find that both the communication topology and prompt design significantly influence system resilience. Motivated by these findings, we propose ResMAS: a two-stage framework for enhancing MAS resilience. First, we train a reward model to predict the MAS's resilience, based on which we train a topology generator to automatically design resilient topology for specific tasks through reinforcement learning. Second, we introduce a topology-aware prompt optimization method that refines each agent's prompt based on its connections and interactions with other agents. Extensive experiments across a range of tasks show that our approach substantially improves MAS resilience under various constraints. Moreover, our framework demonstrates strong generalization ability to new tasks and models, highlighting its potential for building resilient MASs.

</details>


### [50] [Tape: A Cellular Automata Benchmark for Evaluating Rule-Shift Generalization in Reinforcement Learning](https://arxiv.org/abs/2601.04695)
*Enze Pan*

Main category: cs.AI

TL;DR: Tape是一个受控强化学习基准，用于隔离潜在规则变化下的分布外故障。它基于一维元胞自动机，通过固定观察和动作空间但改变转移规则来实现精确的训练/测试分割。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习方法在分布内表现良好，但在分布外规则变化下容易失败。需要建立一个受控基准来系统评估模型在潜在规则变化下的鲁棒性，并建立标准化的评估协议。

Method: 基于一维元胞自动机构建Tape基准，保持观察和动作空间固定但改变转移规则。使用可复现的评估流程比较无模型基线、基于模型规划（学习世界模型）和任务推断（元强化学习）方法。

Result: 发现一致模式：在分布内表现强的方法在分布外规则下可能崩溃；高方差的分布外评估可能导致排名不稳定，除非实验有足够重复。提供了标准化分布外协议、统计报告要求以及信息论恒等式。

Conclusion: Tape基准为评估强化学习方法在规则变化下的鲁棒性提供了系统框架，强调了标准化评估协议和充分重复实验的重要性，并通过信息论分析澄清了"不确定性减少"目标在规则变化下的局限性。

Abstract: We present Tape, a controlled reinforcement-learning benchmark designed to isolate out-of-distribution (OOD) failure under latent rule shifts.Tape is derived from one-dimensional cellular automata, enabling precise train/test splits where observation and action spaces are held fixed while transition rules change. Using a reproducible evaluation pipeline, we compare model-free baselines, model-based planning with learned world models, and task-inference (meta-RL) methods. A consistent pattern emerges: methods that are strong in-distribution (ID) can collapse under heldout-rule OOD, and high-variance OOD evaluation can make rankings unstable unless experiments are sufficiently replicated.We provide (i) standardized OOD protocols, (ii) statistical reporting requirements (seeds, confidence intervals, and hypothesis tests), and (iii) information-theoretic identities connecting entropy reduction to conditional mutual information and expected posterior KL divergence, clarifying what "uncertainty reduction" objectives can and cannot guarantee under rule shifts.

</details>


### [51] [A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models](https://arxiv.org/abs/2601.04696)
*Huayi Liu*

Main category: cs.AI

TL;DR: 该研究提出了一种结合大语言模型和知识图谱的方法，通过BERT实体识别、GPT-4语义增强、GNN知识图谱构建和强化学习决策优化，显著提升了企业数字化转型的智能化水平和执行效率。


<details>
  <summary>Details</summary>
Motivation: 企业在数字化转型过程中面临非结构化数据语义理解不足、驱动机制缺乏智能决策依据等问题，需要一种能够融合语义理解和结构化知识的方法来提升数字化转型的智能化水平。

Method: 1. 使用微调BERT模型进行多源异构文本的实体识别和关系抽取，GPT-4生成语义增强向量表示；2. 设计双层图神经网络架构，融合LLM输出的语义向量与业务元数据，构建动态可扩展的企业知识图谱；3. 引入强化学习优化决策路径生成，使用奖励函数驱动机制迭代。

Result: 在制造业案例中，设备故障场景响应时间从7.8小时降至3.7小时，F1值达到94.3%，年度数字化转型成本中决策错误补偿降低了45.3%。

Conclusion: 通过整合大模型语义理解与结构化知识，该方法显著增强了数字化转型驱动机制的智能化水平和执行效率，为解决企业数字化转型中的智能决策问题提供了有效方案。

Abstract: In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.

</details>


### [52] [TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning](https://arxiv.org/abs/2601.04698)
*Yinuo Wang,Mining Tan,Wenxiang Jiao,Xiaoxi Li,Hao Wang,Xuanyu Zhang,Yuan Lu,Weiming Dong*

Main category: cs.AI

TL;DR: TourPlanner是一个旅行规划框架，通过多路径推理和约束门控强化学习解决现有方法的三个主要挑战：候选POI筛选、单一路径限制以及软硬约束同时优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划方法面临三个主要挑战：1) 在保持高召回率的同时筛选候选兴趣点；2) 单一路径推理限制了在可行解空间中的探索能力；3) 同时优化硬约束和软约束存在困难。

Method: 提出TourPlanner框架，包含三个核心组件：1) 个性化召回和空间优化(PReSO)工作流构建空间感知的候选POI集；2) 竞争共识思维链(CCoT)多路径推理范式增强可行解空间探索；3) 基于sigmoid的门控机制集成到强化学习阶段，动态优先满足硬约束后再优化软约束。

Result: 在旅行规划基准测试中，TourPlanner实现了最先进的性能，在可行性和用户偏好对齐方面显著超越现有方法。

Conclusion: TourPlanner通过多路径推理和约束门控强化学习有效解决了旅行规划中的关键挑战，为复杂决策过程提供了全面解决方案。

Abstract: Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.

</details>


### [53] [Beyond Monolithic Architectures: A Multi-Agent Search and Knowledge Optimization Framework for Agentic Search](https://arxiv.org/abs/2601.04703)
*Yiqun Chen,Lingyong Yan,Zixuan Yang,Erhan Zhang,Jiashu Zhao,Shuaiqiang Wang,Dawei Yin,Jiaxin Mao*

Main category: cs.AI

TL;DR: M-ASK是一个多智能体搜索框架，通过将搜索行为与知识管理解耦为两个互补角色，解决了传统单体智能体搜索中的结构瓶颈问题，在复杂信息检索任务中表现出更好的准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型语言模型的智能体搜索系统存在结构瓶颈：无约束的推理输出导致轨迹膨胀、稀疏的结果级奖励使信用分配复杂化、随机搜索噪声破坏学习稳定性。这些问题限制了智能体搜索在复杂信息检索任务中的效果。

Method: 提出M-ASK框架，将智能体搜索明确解耦为两个互补角色：搜索行为智能体（负责规划和执行搜索动作）和知识管理智能体（负责聚合、过滤和维护紧凑的内部上下文）。同时采用回合级奖励为搜索决策和知识更新提供细粒度监督。

Result: 在多跳问答基准测试中，M-ASK超越了强基线方法，不仅获得了更高的答案准确性，而且训练动态显著更稳定。

Conclusion: 通过角色分解和细粒度奖励设计，M-ASK框架有效解决了智能体搜索中的结构瓶颈问题，为复杂信息检索任务提供了更稳定、更有效的解决方案。

Abstract: Agentic search has emerged as a promising paradigm for complex information seeking by enabling Large Language Models (LLMs) to interleave reasoning with tool use. However, prevailing systems rely on monolithic agents that suffer from structural bottlenecks, including unconstrained reasoning outputs that inflate trajectories, sparse outcome-level rewards that complicate credit assignment, and stochastic search noise that destabilizes learning. To address these challenges, we propose \textbf{M-ASK} (Multi-Agent Search and Knowledge), a framework that explicitly decouples agentic search into two complementary roles: Search Behavior Agents, which plan and execute search actions, and Knowledge Management Agents, which aggregate, filter, and maintain a compact internal context. This decomposition allows each agent to focus on a well-defined subtask and reduces interference between search and context construction. Furthermore, to enable stable coordination, M-ASK employs turn-level rewards to provide granular supervision for both search decisions and knowledge updates. Experiments on multi-hop QA benchmarks demonstrate that M-ASK outperforms strong baselines, achieving not only superior answer accuracy but also significantly more stable training dynamics.\footnote{The source code for M-ASK is available at https://github.com/chenyiqun/M-ASK.}

</details>


### [54] [Bridging Temporal and Textual Modalities: A Multimodal Framework for Automated Cloud Failure Root Cause Analysis](https://arxiv.org/abs/2601.04709)
*Gijun Park*

Main category: cs.AI

TL;DR: 本文提出了一种多模态诊断框架，通过语义压缩、对齐编码和检索增强技术，将时间序列数据与预训练语言模型嵌入空间对齐，实现云基础设施根因分析。


<details>
  <summary>Details</summary>
Motivation: 现代云基础设施根因分析需要理解异构数据源，特别是涉及核心故障特征的时间序列性能指标。虽然大语言模型在文本推理方面表现出色，但其基于离散令牌的架构与具有时间依赖性的连续数值序列存在根本性不兼容。当前方法未能充分解决这种模态不匹配问题，限制了语言模型在事件管理工作流中的自动化潜力。

Method: 提出了一个多模态诊断框架，包含三个技术进展：(1) 语义压缩技术，将时间片段蒸馏为单令牌抽象，同时保留模式语义；(2) 使用门控交叉注意力将对齐编码器将时间序列特征投影到语言模型潜在空间；(3) 检索增强的诊断管道，将对齐嵌入与历史事件知识结合，实现专家级故障归因。

Result: 在六个云系统基准测试中的全面评估表明，该框架实现了领先性能，达到48.75%的诊断准确率，在涉及复合故障模式的场景中表现出显著改进。

Conclusion: 结果验证了嵌入空间对齐作为一种有效策略，使语言模型能够在生产事件响应环境中对多模态遥测数据进行推理。

Abstract: Root cause analysis in modern cloud infrastructure demands sophisticated understanding of heterogeneous data sources, particularly time-series performance metrics that involve core failure signatures. While large language models demonstrate remarkable capabilities in textual reasoning, their discrete token-based architecture creates fundamental incompatibilities with continuous numerical sequences exhibiting temporal dependencies. Current methodologies inadequately address this modality mismatch, constraining the potential of language model-driven automation in incident management workflows. This paper presents a multimodal diagnostic framework that harmonizes time-series representations with pretrained language model embedding spaces. Our approach contributes three technical advances: (1) a semantic compression technique that distills temporal segments into single-token abstractions while preserving pattern semantics, (2) an alignment encoder utilizing gated cross-attention to project time-series features into language model latent space, and (3) a retrieval-augmented diagnostic pipeline that synthesizes aligned embeddings with historical incident knowledge for expert-level failure attribution. Comprehensive evaluation across six cloud system benchmarks demonstrates that our framework achieves leading performance, reaching 48.75% diagnostic accuracy with notable improvements on scenarios involving compound failure modes. The results validate embedding-space alignment as an effective strategy for enabling language models to reason over multimodal telemetry data in production incident response contexts.

</details>


### [55] [ThinkDrive: Chain-of-Thought Guided Progressive Reinforcement Learning Fine-Tuning for Autonomous Driving](https://arxiv.org/abs/2601.04714)
*Chang Zhao,Zheming Yang,Yunqing Hu,Qi Guo,Zijian Wang,Pengcheng Li,Wen Ji*

Main category: cs.AI

TL;DR: ThinkDrive是一个用于自动驾驶的思维链引导的渐进式强化学习微调框架，通过结合显式推理和难度感知自适应策略优化，解决了现有方法在推理结构、泛化能力和人类驾驶意图对齐方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在自动驾驶领域的广泛应用，现有方法存在推理非结构化、泛化能力差以及与人类驾驶意图不一致的问题。虽然思维链推理提高了决策透明度，但传统的监督微调未能充分利用其潜力，而强化学习方法存在不稳定性和推理深度不足的问题。

Method: 提出ThinkDrive框架，采用两阶段训练策略：首先使用思维链解释进行监督微调，然后应用渐进式强化学习，配备难度感知自适应策略优化器，根据样本复杂度动态调整学习强度。

Result: 在公开数据集上的评估显示，ThinkDrive在exam、easy-exam和accuracy指标上分别比强RL基线高出1.45%、1.95%和1.01%。此外，使用该方法训练的2B参数模型在exam指标上比更大的GPT-4o高出3.28%。

Conclusion: ThinkDrive通过结合思维链引导的推理和难度感知自适应策略优化，有效提升了自动驾驶决策的透明度、泛化能力和与人类意图的对齐程度，在多个指标上超越了现有方法。

Abstract: With the rapid advancement of large language models (LLMs) technologies, their application in the domain of autonomous driving has become increasingly widespread. However, existing methods suffer from unstructured reasoning, poor generalization, and misalignment with human driving intent. While Chain-of-Thought (CoT) reasoning enhances decision transparency, conventional supervised fine-tuning (SFT) fails to fully exploit its potential, and reinforcement learning (RL) approaches face instability and suboptimal reasoning depth. We propose ThinkDrive, a CoT guided progressive RL fine-tuning framework for autonomous driving that synergizes explicit reasoning with difficulty-aware adaptive policy optimization. Our method employs a two-stage training strategy. First, we perform SFT using CoT explanations. Then, we apply progressive RL with a difficulty-aware adaptive policy optimizer that dynamically adjusts learning intensity based on sample complexity. We evaluate our approach on a public dataset. The results show that ThinkDrive outperforms strong RL baselines by 1.45%, 1.95%, and 1.01% on exam, easy-exam, and accuracy, respectively. Moreover, a 2B-parameter model trained with our method surpasses the much larger GPT-4o by 3.28% on the exam metric.

</details>


### [56] [Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning](https://arxiv.org/abs/2601.04726)
*Yuyang Hu,Jiongnan Liu,Jiejun Tan,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: CompassMem是一个基于事件分割理论的事件中心化记忆框架，将记忆组织为事件图，通过逻辑关系链接事件，支持结构化导航和长期推理。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能代理的记忆机制大多采用扁平化存储和简单相似性检索，难以捕捉经验间的逻辑关系，也无法支持对长期依赖的逻辑推理。

Method: 提出CompassMem框架，受事件分割理论启发，将经验增量式分割为事件，并通过显式逻辑关系链接形成事件图，作为逻辑地图支持结构化、目标导向的记忆导航。

Result: 在LoCoMo和NarrativeQA数据集上的实验表明，CompassMem能持续提升多个骨干模型的检索和推理性能。

Conclusion: 事件图记忆框架能有效支持智能代理进行结构化记忆导航和长期依赖的逻辑推理，超越传统的浅层语义检索方法。

Abstract: Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.

</details>


### [57] [Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models](https://arxiv.org/abs/2601.04731)
*Shuyang Jiang,Yuhao Wang,Ya Zhang,Yanfeng Wang,Yu Wang*

Main category: cs.AI

TL;DR: Miner提出利用策略内在不确定性作为自监督奖励信号，解决同质正提示下RL训练效率低的问题，无需外部监督或额外推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前无批评RL方法在处理同质正提示（所有rollout都正确）时效率极低，因为优势估计为零导致rollout浪费，需要一种更高效的训练方法。

Method: 提出Miner方法：1）令牌级焦点信用分配机制，动态放大关键不确定令牌的梯度，抑制过度自信令牌；2）自适应优势校准，无缝整合内在奖励和可验证奖励。

Result: 在Qwen3-4B和Qwen3-8B基模型上的六个推理基准测试中，Miner达到最先进性能，相比GRPO在Pass@1上提升4.58，Pass@K上提升6.66。

Conclusion: 潜在不确定性利用对于推理模型的高效可扩展RL训练既是必要的也是充分的，新提出的两个创新机制显示出优越性。

Abstract: Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \uline{M}ine \uline{in}trinsic mast\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \textbf{4.58} absolute gains in Pass@1 and \textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.

</details>


### [58] [When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail](https://arxiv.org/abs/2601.04748)
*Xiaoxiao Li*

Main category: cs.AI

TL;DR: 该研究探讨了多智能体系统如何编译为单智能体系统，通过技能选择替代智能体间通信，显著降低计算开销，并发现技能选择存在类似人类认知的容量限制和相变现象。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统虽然能有效处理复杂推理，但存在显著的通信开销。研究旨在探索是否可以通过单智能体选择技能库中的技能来获得类似的模块化优势，同时提高效率。

Method: 将技能视为内部化的智能体行为，将多智能体系统编译为等效的单智能体系统，用技能选择替代智能体间通信。研究技能选择的扩展行为，分析语义混淆性对选择准确性的影响，并探索分层组织方法。

Result: 初步实验显示该方法能显著减少令牌使用和延迟，同时在推理基准上保持竞争力。研究发现技能选择准确性在达到临界库大小前保持稳定，随后急剧下降，呈现相变现象。语义相似性而非库大小本身是性能下降的主要原因。分层路由方法初步验证了改善效果。

Conclusion: 该工作揭示了LLM技能选择存在类似人类认知的容量限制，为设计可扩展的技能型智能体提供了认知基础框架和实践指导，并提出了关于语义技能选择基本限制的新问题。

Abstract: Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?
  Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.

</details>


### [59] [Orion-RAG: Path-Aligned Hybrid Retrieval for Graphless Data](https://arxiv.org/abs/2601.04764)
*Zhen Chen,Weihao Xie,Peilin Chen,Shiqi Wang,Jianping Wang*

Main category: cs.AI

TL;DR: Orion-RAG提出了一种轻量级路径提取方法，将碎片化文档转化为半结构化数据，在金融基准测试中相比基线模型实现了25.2%的相对精度提升。


<details>
  <summary>Details</summary>
Motivation: 现实场景中数据通常是离散和碎片化的，信息分布在孤立文件中且缺乏显式链接。传统搜索引擎独立处理文件，忽略了文件间的关联，而手动构建知识图谱对于海量数据不切实际。

Method: 采用低复杂度策略提取轻量级路径，自然地链接相关概念，将碎片化文档转化为半结构化数据，使系统能够有效链接不同文件中的信息。

Result: Orion-RAG在多个领域持续优于主流框架，支持实时更新和显式的人机交互验证，具有高成本效益。在FinanceBench实验中，相比强基线实现了25.2%的相对精度提升。

Conclusion: 轻量级路径提取方法足以将碎片化文档转化为半结构化数据，有效链接跨文件信息，为知识合成提供了实用且高效的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has proven effective for knowledge synthesis, yet it encounters significant challenges in practical scenarios where data is inherently discrete and fragmented. In most environments, information is distributed across isolated files like reports and logs that lack explicit links. Standard search engines process files independently, ignoring the connections between them. Furthermore, manually building Knowledge Graphs is impractical for such vast data. To bridge this gap, we present Orion-RAG. Our core insight is simple yet effective: we do not need heavy algorithms to organize this data. Instead, we use a low-complexity strategy to extract lightweight paths that naturally link related concepts. We demonstrate that this streamlined approach suffices to transform fragmented documents into semi-structured data, enabling the system to link information across different files effectively. Extensive experiments demonstrate that Orion-RAG consistently outperforms mainstream frameworks across diverse domains, supporting real-time updates and explicit Human-in-the-Loop verification with high cost-efficiency. Experiments on FinanceBench demonstrate superior precision with a 25.2% relative improvement over strong baselines.

</details>


### [60] [AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search](https://arxiv.org/abs/2601.04767)
*Zefang Zong,Dingwei Chen,Yang Li,Qi Yi,Bo Zhou,Chengming Li,Bo Qian,Peng Chen,Jie Jiang*

Main category: cs.AI

TL;DR: AT²PO是一个用于多轮智能体强化学习的统一框架，通过树搜索解决探索多样性不足、稀疏信用分配和策略优化不对齐三个核心挑战，在七个基准测试中平均提升1.84个百分点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型智能体已成为处理多轮任务的有力系统，但智能体强化学习作为关键的后训练范式面临三个核心挑战：探索多样性有限、稀疏信用分配困难以及策略优化与智能体交互的自然决策粒度不对齐。

Method: 提出AT²PO框架，包含三个核心组件：1）基于轮次的树结构，实现熵引导的树扩展进行策略性探索；2）轮次级信用分配，从稀疏结果中进行细粒度奖励传播；3）智能体轮次级策略优化，使策略更新与智能体交互的自然决策粒度对齐。

Result: 在七个基准测试中，AT²PO相比最先进的基线方法平均提升1.84个百分点，消融研究验证了每个组件的有效性。

Conclusion: AT²PO为多轮智能体强化学习提供了一个统一的框架，有效解决了探索多样性、信用分配和策略优化对齐等核心挑战，ATPO组件可以轻松集成到任何多轮强化学习流程中。

Abstract: LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.

</details>


### [61] [SciIF: Benchmarking Scientific Instruction Following Towards Rigorous Scientific Intelligence](https://arxiv.org/abs/2601.04770)
*Encheng Su,Jianyu Wu,Chen Tang,Lintao Wang,Pengze Li,Aoran Wang,Jinouwen Zhang,Yizhou Wang,Yuan Meng,Xinzhu Ma,Shixiang Tang,Houqiang Li*

Main category: cs.AI

TL;DR: SciIF是一个评估大语言模型科学指令遵循能力的新基准，强调在解决科学问题时严格遵守科学有效性约束，而不仅仅是最终答案正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在关键盲点：通用指令遵循指标关注表面格式，而领域特定科学基准只评估最终答案正确性，常常奖励那些用错误理由得到正确结果的模型。需要建立能够评估模型在科学探究严格规范下工作的能力。

Method: 引入科学指令遵循概念，创建SciIF多学科基准，评估模型在解决大学水平问题时严格遵守科学有效性约束的能力。约束分为三个支柱：科学条件（如边界检查和假设）、语义稳定性（如单位和符号约定）、特定过程（如所需数值方法）。强调可审计性，要求模型提供约束满足的明确证据。

Result: 通过同时测量解决方案正确性和多约束遵循，SciIF能够对组合推理失败进行细粒度诊断，确保LLM能够在科学的严格逻辑框架内作为可靠代理工作。

Conclusion: SciIF填补了现有评估标准的空白，为大语言模型在复杂科学发现中的可靠应用提供了更严格的评估框架，确保模型不仅给出正确答案，而且遵循科学探究的严谨规范。

Abstract: As large language models (LLMs) transition from general knowledge retrieval to complex scientific discovery, their evaluation standards must also incorporate the rigorous norms of scientific inquiry. Existing benchmarks exhibit a critical blind spot: general instruction-following metrics focus on superficial formatting, while domain-specific scientific benchmarks assess only final-answer correctness, often rewarding models that arrive at the right result with the wrong reasons. To address this gap, we introduce scientific instruction following: the capability to solve problems while strictly adhering to the constraints that establish scientific validity. Specifically, we introduce SciIF, a multi-discipline benchmark that evaluates this capability by pairing university-level problems with a fixed catalog of constraints across three pillars: scientific conditions (e.g., boundary checks and assumptions), semantic stability (e.g., unit and symbol conventions), and specific processes(e.g., required numerical methods). Uniquely, SciIF emphasizes auditability, requiring models to provide explicit evidence of constraint satisfaction rather than implicit compliance. By measuring both solution correctness and multi-constraint adherence, SciIF enables finegrained diagnosis of compositional reasoning failures, ensuring that LLMs can function as reliable agents within the strict logical frameworks of science.

</details>


### [62] [APEX: Academic Poster Editing Agentic Expert](https://arxiv.org/abs/2601.04794)
*Chengxin Shi,Qinnan Cai,Zeyuan Chen,Long Zeng,Yibo Zhao,Jing Yu,Jianxiang Yu,Xiang Li*

Main category: cs.AI

TL;DR: APEX是一个交互式学术海报编辑代理框架，支持细粒度控制，通过多级API编辑和审核调整机制，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有学术海报生成方法通常是单次、非交互式的，难以满足用户复杂的主观意图，需要一种支持细粒度控制和交互编辑的解决方案。

Method: 提出APEX框架，采用基于多级API的编辑和审核调整机制，实现交互式学术海报编辑。同时构建APEX-Bench基准数据集，包含514条编辑指令，采用多维度分类和参考引导/无参考策略。

Result: 实验结果表明APEX显著优于基线方法，并通过多维度VLM评估协议验证了其在指令完成度、修改范围和视觉一致性方面的优势。

Conclusion: APEX是首个交互式学术海报编辑代理框架，通过细粒度控制和系统化评估，有效解决了现有方法的局限性，为学术海报设计提供了新的解决方案。

Abstract: Designing academic posters is a labor-intensive process requiring the precise balance of high-density content and sophisticated layout. While existing paper-to-poster generation methods automate initial drafting, they are typically single-pass and non-interactive, often fail to align with complex, subjective user intent. To bridge this gap, we propose APEX (Academic Poster Editing agentic eXpert), the first agentic framework for interactive academic poster editing, supporting fine-grained control with robust multi-level API-based editing and a review-and-adjustment Mechanism. In addition, we introduce APEX-Bench, the first systematic benchmark comprising 514 academic poster editing instructions, categorized by a multi-dimensional taxonomy including operation type, difficulty, and abstraction level, constructed via reference-guided and reference-free strategies to ensure realism and diversity. We further establish a multi-dimensional VLM-as-a-judge evaluation protocol to assess instruction fulfillment, modification scope, and visual consistency & harmony. Experimental results demonstrate that APEX significantly outperforms baseline methods. Our implementation is available at https://github.com/Breesiu/APEX.

</details>


### [63] [Defense Against Indirect Prompt Injection via Tool Result Parsing](https://arxiv.org/abs/2601.04795)
*Qiang Yu,Xinran Cheng,Chuanyi Liu*

Main category: cs.AI

TL;DR: 提出一种新方法，通过工具结果解析为LLM提供精确数据，同时有效过滤注入的恶意代码，在保持竞争性攻击下效用的同时实现最低的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理从数字助手转变为自主系统和机器人中的物理控制器，面临间接提示注入的威胁日益严重。攻击者通过工具调用结果嵌入对抗性指令，可以劫持代理的决策过程执行未授权操作，对物理环境控制构成重大风险。

Method: 提出一种新颖方法，通过工具结果解析为LLM提供精确数据，同时有效过滤注入的恶意代码。该方法结合了工具结果解析和恶意代码过滤机制。

Result: 该方法在保持竞争性攻击下效用（UA）的同时，实现了迄今为止最低的攻击成功率（ASR），显著优于现有方法。

Conclusion: 提出的方法通过工具结果解析和恶意代码过滤，有效防御间接提示注入攻击，在攻击成功率和效用之间取得了良好平衡，为LLM代理在物理控制系统中的安全应用提供了有效解决方案。

Abstract: As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.

</details>


### [64] [Thinking-Based Non-Thinking: Solving the Reward Hacking Problem in Training Hybrid Reasoning Models via Reinforcement Learning](https://arxiv.org/abs/2601.04805)
*Siyuan Gan,Jiaheng Liu,Boyan Wang,Tianpei Yang,Runqing Miao,Yuyao Zhang,Fanyu Meng,Junlan Feng,Linjian Meng,Jing Huo,Yang Gao*

Main category: cs.AI

TL;DR: TNT方法通过基于思考的响应来设定非思考回答的最大token限制，在保持准确率的同时显著降低计算开销，解决了大型推理模型过度思考的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过长链思考获得优异性能，但这也显著增加了计算开销。现有方法使用强化学习训练混合推理模型，但存在奖励欺骗问题，而监督微调成本高，统一token限制效果有限。

Method: 提出Thinking-Based Non-Thinking (TNT)方法，不采用监督微调，而是利用思考响应中的解决方案信息，为不同查询设置不同的非思考响应最大token使用量。

Result: 在五个数学基准测试中，TNT相比DeepSeek-R1-Distill-Qwen-1.5B/7B和DeepScaleR-1.5B减少了约50%的token使用量，同时显著提高了准确率，在所有测试方法中实现了准确率和效率的最佳平衡。

Conclusion: TNT方法有效解决了大型推理模型的过度思考问题，在保持高准确率的同时大幅降低计算成本，且奖励欺骗问题概率保持在10%以下，实现了准确率与效率的最佳权衡。

Abstract: Large reasoning models (LRMs) have attracted much attention due to their exceptional performance. However, their performance mainly stems from thinking, a long Chain of Thought (CoT), which significantly increase computational overhead. To address this overthinking problem, existing work focuses on using reinforcement learning (RL) to train hybrid reasoning models that automatically decide whether to engage in thinking or not based on the complexity of the query. Unfortunately, using RL will suffer the the reward hacking problem, e.g., the model engages in thinking but is judged as not doing so, resulting in incorrect rewards. To mitigate this problem, existing works either employ supervised fine-tuning (SFT), which incurs high computational costs, or enforce uniform token limits on non-thinking responses, which yields limited mitigation of the problem. In this paper, we propose Thinking-Based Non-Thinking (TNT). It does not employ SFT, and sets different maximum token usage for responses not using thinking across various queries by leveraging information from the solution component of the responses using thinking. Experiments on five mathematical benchmarks demonstrate that TNT reduces token usage by around 50% compared to DeepSeek-R1-Distill-Qwen-1.5B/7B and DeepScaleR-1.5B, while significantly improving accuracy. In fact, TNT achieves the optimal trade-off between accuracy and efficiency among all tested methods. Additionally, the probability of reward hacking problem in TNT's responses, which are classified as not using thinking, remains below 10% across all tested datasets.

</details>


### [65] [SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning](https://arxiv.org/abs/2601.04809)
*Caijun Xu,Changyi Xiao,Zhongyuan Peng,Xinrun Wang,Yixin Cao*

Main category: cs.AI

TL;DR: SCALER框架通过自适应环境设计解决强化学习训练中的信号衰减问题，利用可扩展的合成管道创建可验证推理环境，并通过动态调整难度和环境选择来维持有效的学习信号。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型推理能力方面具有潜力，但实际训练中常面临两个问题：1）任务难度与模型能力不匹配导致学习信号衰减；2）训练被狭窄的重复问题模式主导导致过拟合。需要一种能持续提供有效学习信号的训练框架。

Method: 提出SCALER框架，包含两个核心组件：1）可扩展的合成管道，将真实编程问题转化为可验证的推理环境，支持难度控制和无限实例生成；2）自适应多环境强化学习策略，动态调整实例难度并策划活动环境集，跟踪模型能力前沿并保持分布多样性。

Result: 实验表明，SCALER在多样化推理基准测试中持续优于基于数据集的强化学习基线，展现出更稳定、更长周期的训练动态。该框架能防止奖励稀疏性，减轻对狭窄任务模式的过拟合，支持整个训练过程中的持续改进。

Conclusion: SCALER通过自适应环境设计解决了强化学习训练中的关键挑战，通过可扩展的合成环境和动态难度调整机制，为语言模型的推理能力提升提供了更有效的训练框架，实现了持续稳定的性能改进。

Abstract: Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.

</details>


### [66] [AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding](https://arxiv.org/abs/2601.04819)
*Aleksei Kondratenko,Mussie Birhane,Houssame E. Hsain,Guido Maciocci*

Main category: cs.AI

TL;DR: AECV-Bench是一个评估多模态和视觉语言模型在建筑、工程和施工图纸理解能力的基准，包含对象计数和图纸问答两个任务，结果显示当前模型在文本处理方面表现良好，但在符号识别和空间推理方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 尽管AEC图纸通过符号、布局约定和密集标注编码了几何和语义信息，但现代多模态和视觉语言模型是否能可靠解释这种图形语言仍不明确。需要建立一个基准来评估这些模型在真实AEC工件上的表现。

Method: 提出了AECV-Bench基准，包含两个互补用例：1) 在120个高质量平面图上进行对象计数（门、窗、卧室、卫生间）；2) 基于图纸的文档问答，涵盖192个问题-答案对，测试文本提取（OCR）、实例计数、空间推理和比较推理。使用统一的评估协议，包括每字段精确匹配准确率、MAPE结果、LLM作为评判的评分流程以及针对边缘情况的人工裁决。

Result: 评估显示稳定的能力梯度：OCR和以文本为中心的文档问答表现最强（准确率高达0.95），空间推理中等，而以符号为中心的图纸理解（特别是可靠计数门和窗）仍未解决（通常0.40-0.55准确率），存在显著比例误差。当前系统作为文档助手表现良好，但缺乏稳健的图纸理解能力。

Conclusion: 当前模型在AEC图纸理解方面存在局限性，特别是在符号识别和空间推理方面。这推动了领域特定表示和工具增强、人机协作工作流程的需求，以实现高效的AEC自动化。

Abstract: AEC drawings encode geometry and semantics through symbols, layout conventions, and dense annotation, yet it remains unclear whether modern multimodal and vision-language models can reliably interpret this graphical language. We present AECV-Bench, a benchmark for evaluating multimodal and vision-language models on realistic AEC artefacts via two complementary use cases: (i) object counting on 120 high-quality floor plans (doors, windows, bedrooms, toilets), and (ii) drawing-grounded document QA spanning 192 question-answer pairs that test text extraction (OCR), instance counting, spatial reasoning, and comparative reasoning over common drawing regions. Object-counting performance is reported using per-field exact-match accuracy and MAPE results, while document-QA performance is reported using overall accuracy and per-category breakdowns with an LLM-as-a-judge scoring pipeline and targeted human adjudication for edge cases. Evaluating a broad set of state-of-the-art models under a unified protocol, we observe a stable capability gradient; OCR and text-centric document QA are strongest (up to 0.95 accuracy), spatial reasoning is moderate, and symbol-centric drawing understanding - especially reliable counting of doors and windows - remains unsolved (often 0.40-0.55 accuracy) with substantial proportional errors. These results suggest that current systems function well as document assistants but lack robust drawing literacy, motivating domain-specific representations and tool-augmented, human-in-the-loop workflows for an efficient AEC automation.

</details>


### [67] [DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation](https://arxiv.org/abs/2601.04823)
*Guanzhi Deng,Bo Li,Ronghao Chen,Huacan Wang,Linqi Song,Lijie Wen*

Main category: cs.AI

TL;DR: DR-LoRA：一种动态调整MoE模型中专家LoRA秩的框架，根据任务需求智能分配参数，提升微调效率


<details>
  <summary>Details</summary>
Motivation: 现有方法为MoE模型中的所有专家分配相同的LoRA秩，忽视了专家间的功能专门化差异，导致资源错配：任务相关专家参数不足，而无关专家参数冗余

Method: 提出DR-LoRA框架，在微调过程中根据任务特定需求动态增长专家LoRA秩。采用专家显著性评分机制，结合专家路由频率和LoRA秩重要性来量化每个专家对额外容量的需求。高显著性专家优先进行秩扩展，自动形成针对目标任务的异构秩分布

Result: 在多个基准测试上的实验表明，在相同参数预算下，DR-LoRA始终优于标准LoRA和静态分配策略，实现了更优的任务性能和更高效的参数利用

Conclusion: DR-LoRA通过动态调整MoE模型中专家的LoRA秩，解决了现有方法中资源分配不均的问题，实现了更智能的参数高效微调，为MoE模型的适配提供了更有效的解决方案

Abstract: Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.

</details>


### [68] [Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models](https://arxiv.org/abs/2601.04861)
*Jingbo Wang,Sendong Zhao,Jiatong Liu,Haochun Wang,Wanting Li,Bing Qin,Ting Liu*

Main category: cs.AI

TL;DR: OI-MAS框架通过自适应模型选择策略，在异构多尺度LLM池中动态分配计算资源，显著提升多智能体系统的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在复杂推理任务中虽然性能优于单智能体方法，但存在显著的计算效率问题。现有框架通常在所有智能体角色中统一部署大型语言模型，未能考虑不同推理阶段的不同认知需求。

Method: 提出OI-MAS框架，实现异构多尺度LLM池的自适应模型选择策略。具体包括：1）状态依赖的路由机制，在推理过程中动态选择智能体角色和模型规模；2）置信度感知机制，根据任务复杂度选择适当的模型规模，减少对大规模模型的不必要依赖。

Result: 实验结果表明，OI-MAS在基准多智能体系统中表现一致优于基线，准确率提升高达12.88%，同时成本降低高达79.78%。

Conclusion: OI-MAS框架通过自适应模型选择和动态资源分配，有效解决了多智能体系统的计算效率问题，在保持高性能的同时显著降低了成本。

Abstract: While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\% while reducing cost by up to 79.78\%.

</details>


### [69] [Key-Value Pair-Free Continual Learner via Task-Specific Prompt-Prototype](https://arxiv.org/abs/2601.04864)
*Haihua Luo,Xuming Ran,Zhengji Li,Huiyan Xue,Tingting Jiang,Jiangrong Shen,Tommi Kärkkäinen,Qi Xu,Fengyu Cong*

Main category: cs.AI

TL;DR: 提出基于任务特定提示-原型（ProP）的持续学习方法，消除传统提示方法对键值对的依赖，减少任务间干扰并提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统基于提示的持续学习方法依赖键值对，这会引入任务间干扰并限制可扩展性。需要一种无需键值对的新方法来更有效地学习特征并保留先前知识。

Method: 使用任务特定提示促进当前任务的特征学习，对应原型捕捉输入的代表性特征。推理时通过绑定任务特定提示与相关原型生成预测。在提示初始化时引入正则化约束惩罚过大值以增强稳定性。

Result: 在多个广泛使用的数据集上实验证明了该方法的有效性。相比主流提示方法，该框架消除了对键值对的依赖。

Conclusion: 提出的ProP方法为持续学习研究提供了新视角，通过消除键值对依赖减少了任务间干扰并提高了可扩展性，展示了在多个数据集上的有效性。

Abstract: Continual learning aims to enable models to acquire new knowledge while retaining previously learned information. Prompt-based methods have shown remarkable performance in this domain; however, they typically rely on key-value pairing, which can introduce inter-task interference and hinder scalability. To overcome these limitations, we propose a novel approach employing task-specific Prompt-Prototype (ProP), thereby eliminating the need for key-value pairs. In our method, task-specific prompts facilitate more effective feature learning for the current task, while corresponding prototypes capture the representative features of the input. During inference, predictions are generated by binding each task-specific prompt with its associated prototype. Additionally, we introduce regularization constraints during prompt initialization to penalize excessively large values, thereby enhancing stability. Experiments on several widely used datasets demonstrate the effectiveness of the proposed method. In contrast to mainstream prompt-based approaches, our framework removes the dependency on key-value pairs, offering a fresh perspective for future continual learning research.

</details>


### [70] [Higher-Order Knowledge Representations for Agentic Scientific Reasoning](https://arxiv.org/abs/2601.04878)
*Isabella A. Stewart,Markus J. Buehler*

Main category: cs.AI

TL;DR: 该研究提出了一种基于超图的知识表示方法，用于解决传统知识图谱在捕捉高阶相互作用方面的不足，并应用于生物复合材料支架领域，构建了包含16万节点和32万超边的超图，展示了其在科学发现中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 科学探究需要整合异构实验数据、跨领域知识和机制证据的系统级推理。虽然大语言模型具备推理能力，但通常依赖于缺乏结构深度的检索增强上下文。传统知识图谱试图弥合这一差距，但其成对约束无法捕捉控制涌现物理行为的不可约高阶相互作用。

Method: 引入构建超图知识表示的方法论，能够忠实编码多实体关系。应用于约1,100篇生物复合材料支架文献，构建了包含161,172个节点和320,201条超边的全局超图。进一步展示了为智能体系统配备超图遍历工具（特别是使用节点交集约束）的方法。

Result: 构建的超图显示出无标度拓扑结构（幂律指数约1.23），围绕高度连接的概念中心组织。这种表示避免了成对扩展中典型的组合爆炸，并明确保留了科学表述的共现上下文。系统能够通过利用高阶路径桥接语义上遥远的概念，成功为新型复合材料生成基于机制的假设。

Conclusion: 这项工作建立了一个"无教师"的智能推理系统，其中超图拓扑结构作为可验证的防护栏，通过揭示传统图方法所掩盖的关系，加速科学发现。展示了超图在科学推理中的实际应用价值。

Abstract: Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a "teacherless" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.

</details>


### [71] [Precomputing Multi-Agent Path Replanning using Temporal Flexibility: A Case Study on the Dutch Railway Network](https://arxiv.org/abs/2601.04884)
*Issa Hanou,Eric Kemmeren,Devin Wild Thomas,Mathijs de Weerdt*

Main category: cs.AI

TL;DR: FlexSIPP算法通过预计算延迟代理的所有可能计划，利用其他代理的时间灵活性来高效重规划，避免级联延迟，在荷兰铁路网络中验证有效


<details>
  <summary>Details</summary>
Motivation: 多智能体规划中，当一个智能体延迟时，通常会产生与其他智能体的冲突，需要快速找到新的安全计划。仅重规划延迟智能体往往效率低下甚至不可行，而重规划其他智能体又可能导致级联变化和延迟

Method: 提出FlexSIPP算法，通过跟踪和利用其他智能体的时间灵活性来高效重规划，避免级联延迟。时间灵活性是指智能体在不改变顺序或进一步延迟更多智能体的情况下可以承受的最大延迟。算法预计算延迟智能体的所有可能计划，并返回其他智能体的变化，适用于给定场景内的任何单智能体延迟

Result: 在密集使用的荷兰铁路网络的实际案例研究中验证了该方法。实验表明，FlexSIPP提供了有效的解决方案，与实际情况调整相关，并在合理的时间范围内完成

Conclusion: FlexSIPP算法能够通过利用其他智能体的时间灵活性来高效处理多智能体规划中的延迟问题，避免级联延迟，在实际应用中表现出良好的效果

Abstract: Executing a multi-agent plan can be challenging when an agent is delayed, because this typically creates conflicts with other agents. So, we need to quickly find a new safe plan. Replanning only the delayed agent often does not result in an efficient plan, and sometimes cannot even yield a feasible plan. On the other hand, replanning other agents may lead to a cascade of changes and delays. We show how to efficiently replan by tracking and using the temporal flexibility of other agents while avoiding cascading delays. This flexibility is the maximum delay an agent can take without changing the order of or further delaying more agents. Our algorithm, FlexSIPP, precomputes all possible plans for the delayed agent, also returning the changes for the other agents, for any single-agent delay within the given scenario. We demonstrate our method in a real-world case study of replanning trains in the densely-used Dutch railway network. Our experiments show that FlexSIPP provides effective solutions, relevant to real-world adjustments, and within a reasonable timeframe.

</details>


### [72] [Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking](https://arxiv.org/abs/2601.04887)
*Sofiene Lassoued,Laxmikant Shrikant Bahetic,Nathalie Weiß-Borkowskib,Stefan Lierc,Andreas Schwunga*

Main category: cs.AI

TL;DR: 该论文提出了一种结合着色时间Petri网和基于模型的强化学习的新方法，用于解决包含AGV和工具共享系统的柔性制造系统调度问题，在大型实例上优于传统方法且计算时间减少10倍。


<details>
  <summary>Details</summary>
Motivation: 柔性制造系统在现代制造业中至关重要，传统作业车间调度问题需要进一步复杂化，同时集成自动导引车和工具共享系统，以更好地反映实际制造环境。

Method: 提出新颖方法结合着色时间Petri网和基于模型的强化学习。CTPN提供形式化建模结构和动态动作屏蔽，显著减少动作搜索空间；MBRL通过学习策略确保对变化环境的适应性；利用MBRL优势，采用前瞻策略优化AGV定位。

Result: 在小规模公共基准测试中与传统方法相当，在基于Taillard基准开发的大规模基准测试中优于传统方法，在完工时间方面表现更好，同时计算时间减少10倍。

Conclusion: 该方法有效解决了柔性制造系统中的复杂调度问题，结合CTPN和MBRL的优势，在大型实例上表现出优越性能，并提供了可复现的环境和实例生成器。

Abstract: Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today's rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.

</details>


### [73] [SmartSearch: Process Reward-Guided Query Refinement for Search Agents](https://arxiv.org/abs/2601.04888)
*Tongyu Wen,Guanting Dong,Zhicheng Dou*

Main category: cs.AI

TL;DR: SmartSearch框架通过过程奖励和查询优化机制提升LLM搜索代理的查询质量，采用三阶段课程学习提高搜索效率和准确性


<details>
  <summary>Details</summary>
Motivation: 现有LLM搜索代理主要关注推理范式优化，但忽视了中间搜索查询的质量问题，导致查询不准确、检索结果不理想，限制了搜索代理的整体效果

Method: 提出SmartSearch框架：1) 过程奖励机制通过双级信用评估对中间查询质量提供细粒度监督；2) 查询优化机制选择性优化低质量查询并基于优化结果重新生成后续搜索轮次；采用三阶段课程学习（模仿、对齐、泛化）使代理逐步内化查询质量改进能力

Result: 实验结果表明SmartSearch持续超越现有基线方法，定量分析进一步证实其在搜索效率和查询质量方面的显著提升

Conclusion: SmartSearch通过关注中间查询质量优化，有效提升了LLM搜索代理的性能，为解决知识密集型问题提供了更有效的框架

Abstract: Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.

</details>


### [74] [DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation](https://arxiv.org/abs/2601.04895)
*Renzhao Liang,Jingru Chen,Bo Jia,Bo Deng,Chenggang Xie,Yidong Wang,Ke Jin,Xin Wang,Linfeng Zhang,Cunxiang Wang*

Main category: cs.AI

TL;DR: 提出DVD方法检测大语言模型评估中的变体污染问题，通过建模温度采样下的局部输出分布方差来识别记忆而非推理的测试项


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估面临变体污染问题：训练语料中包含语义等价但词汇或句法改变的测试项变体，这些变体逃避了基于采样一致性或困惑度的现有检测器，通过记忆而非真正推理来虚高基准分数

Method: 提出DVD（通过生成分布方差检测）方法，建模温度采样诱导的局部输出分布。核心洞见是：受污染项会在记忆遵循状态和扰动漂移状态之间交替，导致低概率标记的合成难度方差异常高；未受污染项则保持漂移状态且方差相对平滑

Result: 在Omni-MATH和SuperGPQA两个领域构建首个变体污染基准，通过微调不同规模和架构的模型（Qwen2.5和Llama3.1）模拟污染。DVD在所有数据集和模型上一致优于基于困惑度、Min-k%++、编辑距离（CDD）和嵌入相似度的基线方法，同时对超参数表现出强鲁棒性

Conclusion: 生成分布的方差成为检测大语言模型评估中变体污染的原则性和实用指纹，为更可靠的模型评估提供了新方法

Abstract: Evaluating large language models (LLMs) is increasingly confounded by \emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \textbf{DVD} (\textbf{D}etection via \textbf{V}ariance of generation \textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \emph{memory-adherence} state and a \emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \textbf{DVD} consistently outperforms perplexity-based, Min-$k$\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.

</details>


### [75] [From Stories to Cities to Games: A Qualitative Evaluation of Behaviour Planning](https://arxiv.org/abs/2601.04911)
*Mustafa F. Abdelwahed,Joan Espasa,Alice Toniolo,Ian P. Gent*

Main category: cs.AI

TL;DR: 行为规划是一种新型的多样化规划范式，通过在规划过程中显式纳入多样性模型并支持多种规划类别，生成彼此不同的计划集合，应用于风险管理、自动化流数据分析、恶意软件检测等领域。


<details>
  <summary>Details</summary>
Motivation: 多样化规划方法旨在生成彼此不同的计划集合，在风险管理、自动化流数据分析、恶意软件检测等现实领域有广泛应用需求。传统方法需要扩展到更复杂的场景，因此提出了行为规划这一新范式。

Method: 行为规划通过显式地将多样性模型纳入规划过程，并支持多种规划类别，扩展了早期的多样化规划方法。论文通过三个案例研究展示其实际应用：故事讲述、城市规划、游戏评估。

Result: 论文通过三个具体案例研究证明了行为规划在现实世界中的实用性：1）故事讲述领域；2）城市规划领域；3）游戏评估领域。这些案例展示了行为规划方法在不同应用场景中的有效性。

Conclusion: 行为规划作为一种新型的多样化规划范式，通过在规划过程中显式纳入多样性模型并支持多种规划类别，能够有效应用于多种现实世界场景，包括故事讲述、城市规划和游戏评估等领域。

Abstract: The primary objective of a diverse planning approach is to generate a set of plans that are distinct from one another. Such an approach is applied in a variety of real-world domains, including risk management, automated stream data analysis, and malware detection. More recently, a novel diverse planning paradigm, referred to as behaviour planning, has been proposed. This approach extends earlier methods by explicitly incorporating a diversity model into the planning process and supporting multiple planning categories. In this paper, we demonstrate the usefulness of behaviour planning in real-world settings by presenting three case studies. The first case study focuses on storytelling, the second addresses urban planning, and the third examines game evaluation.

</details>


### [76] [What Students Ask, How a Generative AI Assistant Responds: Exploring Higher Education Students' Dialogues on Learning Analytics Feedback](https://arxiv.org/abs/2601.04919)
*Yildiz Uzun,Andrea Gauthier,Mutlu Cukurova*

Main category: cs.AI

TL;DR: 研究探讨了将对话式生成AI助手整合到学习分析仪表板中，分析不同自我调节学习能力学生在10周学期中与AI的对话模式，发现AI对低SRL学生特别有价值，但存在个性化不足等局限。


<details>
  <summary>Details</summary>
Motivation: 学习分析仪表板旨在通过数据反馈支持学生学习调节，但学生（尤其是自我调节学习能力较低者）往往难以有效参与和解读分析反馈。对话式生成AI助手通过实时、个性化、基于对话的支持，有望为这一过程提供支架。

Method: 在10周学期期间，将生成AI助手整合到学习分析仪表板中，收集学生与AI助手的真实对话。分析重点包括：不同SRL水平学生提出的问题类型、助手回答的相关性和质量，以及学生对助手在学习中作用的感知。

Result: 发现明显的查询模式差异：低SRL学生寻求澄清和安慰，高SRL学生则询问技术细节和请求个性化策略。助手能提供清晰可靠的解释，但在个性化、处理情感化查询以及整合多个数据点提供定制响应方面存在局限。生成AI干预对低SRL学生特别有价值，能缩小与高SRL同伴的差距。

Conclusion: 对话式生成AI助手在学习分析仪表板中具有潜力，尤其能为低SRL学生提供有价值的支架支持。未来系统需要增强信任、适应性、情境感知和技术完善，以更好地支持不同SRL水平学生的学习调节过程。

Abstract: Learning analytics dashboards (LADs) aim to support students' regulation of learning by translating complex data into feedback. Yet students, especially those with lower self-regulated learning (SRL) competence, often struggle to engage with and interpret analytics feedback. Conversational generative artificial intelligence (GenAI) assistants have shown potential to scaffold this process through real-time, personalised, dialogue-based support. Further advancing this potential, we explored authentic dialogues between students and GenAI assistant integrated into LAD during a 10-week semester. The analysis focused on questions students with different SRL levels posed, the relevance and quality of the assistant's answers, and how students perceived the assistant's role in their learning. Findings revealed distinct query patterns. While low SRL students sought clarification and reassurance, high SRL students queried technical aspects and requested personalised strategies. The assistant provided clear and reliable explanations but limited in personalisation, handling emotionally charged queries, and integrating multiple data points for tailored responses. Findings further extend that GenAI interventions can be especially valuable for low SRL students, offering scaffolding that supports engagement with feedback and narrows gaps with their higher SRL peers. At the same time, students' reflections underscored the importance of trust, need for greater adaptivity, context-awareness, and technical refinement in future systems.

</details>


### [77] [Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition](https://arxiv.org/abs/2601.04920)
*Nils Einecke*

Main category: cs.AI

TL;DR: 本文通过欧洲航天局ELOPE竞赛案例研究，展示了ChatGPT在科学快速原型开发中的潜力，获得第二名成绩，同时分析了AI辅助科研的优势与局限。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型作为编程伙伴在加速科学发现中的作用，特别是在竞争性科学环境中的应用潜力。

Method: 使用ChatGPT进行快速原型开发，参与ESA的ELOPE竞赛（基于事件的月球光流自运动估计），处理事件相机数据以估计月球着陆器轨迹。

Result: 尽管参赛较晚，仍获得第二名（得分0.01282），展示了人机协作在竞争性科学环境中的有效性。

Conclusion: 对话式AI既能加速开发又能支持科学研究的概念性洞察，通过结构化整合LLMs到科学工作流程中，可以增强快速原型开发能力，并提出AI辅助科学工作的最佳实践。

Abstract: Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.

</details>


### [78] [T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs](https://arxiv.org/abs/2601.04945)
*Chunyu Wei,Huaiyu Qin,Siyuan He,Yunhai Wang,Yueguo Chen*

Main category: cs.AI

TL;DR: T-Retriever：基于语义和结构引导编码树的图检索新框架，通过自适应压缩编码和语义结构熵优化，解决现有图RAG方法在层次信息管理中的局限性


<details>
  <summary>Details</summary>
Motivation: 当前基于图的RAG方法在处理层次信息时存在两个关键限制：1）僵化的层特定压缩配额会破坏局部图结构；2）过度关注拓扑结构而忽视语义内容。需要一种能同时考虑结构和语义的层次信息管理方法。

Method: 提出T-Retriever框架，将属性图检索重新表述为基于树的检索，使用语义和结构引导的编码树。核心创新包括：1）自适应压缩编码，用全局优化策略替代人工压缩配额，保留图的自然层次组织；2）语义结构熵（S²-Entropy），在创建层次分区时同时优化结构凝聚性和语义一致性。

Result: 在多个图推理基准测试中，T-Retriever显著优于最先进的RAG方法，能够为复杂查询提供更连贯和上下文相关的响应。

Conclusion: T-Retriever通过结合语义和结构信息的层次编码树，有效解决了现有图RAG方法的局限性，为复杂知识检索任务提供了更优的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models' ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph's natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.

</details>


### [79] [ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.04973)
*Minda Hu,Zexuan Qiu,Zenan Xu,Kun Li,Bo Zhou,Irwin King*

Main category: cs.AI

TL;DR: ConMax是一种通过强化学习自动压缩推理轨迹的新框架，能在保持推理模式的同时减少冗余，实现推理长度减少43%而准确率仅下降0.7%的高效性能平衡。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）需要大量思维链（CoT）生成来实现复杂认知行为，但这会导致"过度思考"问题，产生冗余推理路径，增加计算成本而不提升准确性。现有的推理轨迹压缩技术要么损害逻辑连贯性，要么采样成本过高。

Method: ConMax（置信度最大化压缩）是一个新颖的强化学习框架，将压缩问题形式化为奖励驱动的优化问题。训练一个策略来修剪冗余，通过最大化加权组合的答案置信度（用于预测保真度）和思考置信度（用于推理有效性），这些置信度通过冻结的辅助LRM计算。

Result: 在五个推理数据集上的广泛实验表明，ConMax实现了优越的效率-性能平衡。具体来说，与强基线相比，推理长度减少了43%，而准确率仅下降了0.7%，证明了其为LRMs生成高质量、高效训练数据的有效性。

Conclusion: ConMax框架能够自动压缩推理轨迹，在保持推理模式的同时显著减少冗余，为大型推理模型提供了高质量、高效的训练数据生成方法，解决了过度思考导致的效率问题。

Abstract: Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.

</details>


### [80] [AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?](https://arxiv.org/abs/2601.04996)
*Henan Sun,Kaichi Yu,Yuyao Wang,Bowen Liu,Xunkai Li,Rong-Hua Li,Nuo Chen,Jia Li*

Main category: cs.AI

TL;DR: AlgBench是一个专家策划的算法推理基准测试，包含3000多个原创问题，涵盖27种算法，用于评估大型推理模型是否真正掌握算法推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有算法推理基准测试有限，无法回答大型推理模型是否真正掌握算法推理能力这一关键问题。需要一个新的基准测试来评估模型在算法中心范式下的表现。

Method: 提出AlgBench基准测试，由ACM算法专家构建，包含3000多个原创问题，涵盖27种算法，采用全面分类法组织，包括欧几里得结构、非欧几里得结构、非优化、局部优化、全局优化和启发式优化等类别。

Result: 对领先大型推理模型的评估显示显著的性能异质性：模型在非优化任务上表现良好（高达92%），但在动态规划等全局优化算法上准确率急剧下降至约49%。分析发现"战略过度转移"现象，即模型因必要的低熵标记而过早放弃正确的算法设计。

Conclusion: 研究结果揭示了问题中心强化学习的基本局限性，并强调了采用算法中心训练范式对于实现稳健算法推理的必要性。

Abstract: Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.
  AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.

</details>


### [81] [An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions](https://arxiv.org/abs/2601.05009)
*Avik Dutta,Harshit Nigam,Hosein Hasanbeig,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: LLMs在表格数据受到语义和结构扭曲时表现不佳，缺乏自动检测和纠正能力，即使有明确提示也只能部分纠正，准确率下降至少22%。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在处理语义和结构扭曲的表格数据时的失败情况，探索模型是否具备自动检测和纠正表格表示中细微扭曲的能力。

Method: 引入一个专家策划的小型数据集，专门评估LLMs在表格问答任务中的表现，这些任务需要在分析前进行额外的纠错步骤。研究模型在有无明确系统提示下的表现差异。

Result: LLMs缺乏检测和纠正表格表示中细微扭曲的内在能力。只有在提供明确提示时，模型才能部分调整推理策略并纠正一些扭曲，但效果不一致也不完整。即使是GPT-5.2这样的最先进模型，在扭曲下的准确率也至少下降22%。

Conclusion: LLMs在摄入和解释扭曲表格信息时存在系统性差异，这引发了关于模型何时以及如何自主决定重新对齐表格输入的重要研究问题，类似于人类行为，而不依赖明确提示或表格数据预处理。

Abstract: We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing.

</details>


### [82] [OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.05027)
*Yi Jiang,Sendong Zhao,Jianbo Li,Bairui Hu,Yanrui Du,Haochun Wang,Bing Qin*

Main category: cs.AI

TL;DR: OptiSet：一个面向检索增强生成的集合中心框架，通过"扩展-精炼"范式、自合成策略和集合列表训练，解决传统静态选择方法的冗余问题，实现紧凑高效证据集的选择。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法大多基于静态选择前k个相关段落，这种方法无法利用段落间的组合增益，且常常引入大量冗余。需要一种能够识别互补证据、减少冗余的集合级优化方法。

Method: 提出OptiSet框架，采用"扩展-精炼"范式：1）将查询扩展为多个视角以获得多样化候选池；2）通过重新选择形成紧凑证据集。设计自合成策略从生成器的条件效用变化中推导偏好标签，识别互补和冗余证据。引入集合列表训练策略联合优化集合选择和集合级排序。

Result: 大量实验表明，OptiSet在复杂组合问题上提升了性能，并使生成更加高效。源代码已公开。

Conclusion: OptiSet通过集合中心的方法有效解决了传统RAG中的冗余问题，实现了更高效、更准确的证据检索和生成。

Abstract: Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an "Expand-then-Refine" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available.

</details>


### [83] [How to Set the Batch Size for Large-Scale Pre-training?](https://arxiv.org/abs/2601.05034)
*Yunhua Zhou,Junhao Huang,Shuhao Xin,Yechen Zhang,Runyu Peng,Qiping Guo,Xipeng Qiu*

Main category: cs.AI

TL;DR: 论文针对Warmup-Stable-Decay学习率调度器，修正了临界批大小的理论框架，提出了最小批大小阈值和最优批大小概念，并设计了动态批大小调度器。


<details>
  <summary>Details</summary>
Motivation: OpenAI提出的临界批大小概念在传统预训练中有效，但随着WSD学习率调度器的普及，原有理论框架无法适应新的预训练动态，需要建立理论与实践的桥梁。

Method: 为WSD调度器推导了修正的E(S)关系，揭示了两个关键属性：B_min（达到目标损失所需的最小批大小阈值）和B_opt（最大化数据效率的最优批大小），并基于此设计了动态批大小调度器。

Result: 实验表明，修正公式能精确捕捉大规模预训练的动态特性，提出的调度策略显著提升了训练效率和最终模型质量。

Conclusion: 论文成功为WSD调度器建立了新的临界批大小理论框架，提出的动态批大小调度器在实际预训练中具有显著优势，为大规模预训练提供了更有效的理论指导。

Abstract: The concept of Critical Batch Size, as pioneered by OpenAI, has long served as a foundational principle for large-scale pre-training. However, with the paradigm shift towards the Warmup-Stable-Decay (WSD) learning rate scheduler, we observe that the original theoretical framework and its underlying mechanisms fail to align with new pre-training dynamics. To bridge this gap between theory and practice, this paper derives a revised E(S) relationship tailored for WSD scheduler, characterizing the trade-off between training data consumption E and steps S during pre-training. Our theoretical analysis reveals two fundamental properties of WSD-based pre-training: 1) B_min, the minimum batch size threshold required to achieve a target loss, and 2) B_opt, the optimal batch size that maximizes data efficiency by minimizing total tokens. Building upon these properties, we propose a dynamic Batch Size Scheduler. Extensive experiments demonstrate that our revised formula precisely captures the dynamics of large-scale pre-training, and the resulting scheduling strategy significantly enhances both training efficiency and final model quality.

</details>


### [84] [How to Set the Learning Rate for Large-Scale Pre-training?](https://arxiv.org/abs/2601.05049)
*Yunhua Zhou,Shuhao Xing,Junhao Huang,Xipeng Qiu,Qipeng Guo*

Main category: cs.AI

TL;DR: 本文研究了大规模预训练中学习率优化问题，提出了拟合和迁移两种研究范式，挑战了现有μTransfer方法在大规模场景下的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练中学习率的最优配置是一个重要但困难的问题，需要在训练成本和模型性能之间取得平衡。核心问题是能否从低成本实验中准确推断出最优学习率。

Method: 1. 提出两种研究范式：拟合范式和迁移范式。2. 在拟合范式中，创新性地引入了搜索因子的缩放定律，通过预测建模将搜索复杂度从O(n³)降低到O(n*C_D*C_η)。3. 在迁移范式中，将μTransfer原则扩展到混合专家架构，扩大了其在模型深度、权重衰减和token范围等方面的适用性。

Result: 1. 通过大规模实验比较了两种范式。2. 实证结果表明广泛采用的μTransfer在大规模预训练场景下的可扩展性受到挑战。3. 从训练稳定性和特征学习两个角度分析了模块级参数调优在大规模设置中表现不佳的原因。

Conclusion: 这项工作为工业级预训练优化提供了系统的实践指导和新颖的理论视角，挑战了现有超参数研究在大规模场景下的适用性假设。

Abstract: Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training. Given the stringent trade-off between training costs and model performance, the pivotal question is whether the optimal LR can be accurately extrapolated from low-cost experiments. In this paper, we formalize this investigation into two distinct research paradigms: Fitting and Transfer. Within the Fitting Paradigm, we innovatively introduce a Scaling Law for search factor, effectively reducing the search complexity from O(n^3) to O(n*C_D*C_η) via predictive modeling. Within the Transfer Paradigm, we extend the principles of $μ$Transfer to the Mixture of Experts (MoE) architecture, broadening its applicability to encompass model depth, weight decay, and token horizons. By pushing the boundaries of existing hyperparameter research in terms of scale, we conduct a comprehensive comparison between these two paradigms. Our empirical results challenge the scalability of the widely adopted $μ$ Transfer in large-scale pre-training scenarios. Furthermore, we provide a rigorous analysis through the dual lenses of training stability and feature learning to elucidate the underlying reasons why module-wise parameter tuning underperforms in large-scale settings. This work offers systematic practical guidelines and a fresh theoretical perspective for optimizing industrial-level pre-training.

</details>


### [85] [Large language models can effectively convince people to believe conspiracies](https://arxiv.org/abs/2601.05050)
*Thomas H. Costello,Kellin Pelrine,Matthew Kowal,Antonio A. Arechar,Jean-François Godbout,Adam Gleave,David Rand,Gordon Pennycook*

Main category: cs.AI

TL;DR: GPT-4o在去除安全护栏后，既能有效反驳阴谋论，也能同样有效地推广阴谋论，甚至推广阴谋论时获得更积极评价并增加对AI的信任。标准版GPT-4o也有类似效果，但要求AI只使用准确信息能显著减少其推广阴谋论的能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型的说服力是否更倾向于支持真相而非虚假信息，以及LLMs是否同样容易推广错误信念。虽然LLMs在各种情境中表现出说服力，但尚不清楚这种说服力是否有利于真相而非虚假信息。

Method: 通过三个预注册实验，让2,724名美国参与者与GPT-4o讨论他们不确定的阴谋论。模型被指示要么反驳（"揭穿"）要么支持（"推广"）该阴谋论。使用去除安全护栏的"越狱"版GPT-4o和标准版GPT-4o进行对比。

Result: 1. 越狱版GPT-4o在增加阴谋论信念方面与减少信念同样有效；2. 推广阴谋论的AI获得更积极评价，并增加对AI的信任；3. 标准版GPT-4o产生类似效果，OpenAI的安全护栏未能有效防止LLM推广阴谋论；4. 纠正性对话能逆转新诱导的阴谋论信念；5. 提示GPT-4o只使用准确信息能显著减少其增加阴谋论信念的能力。

Conclusion: LLMs具备推广真相和虚假信息的强大能力，但存在潜在解决方案来减轻这种风险。虽然AI能同样有效地推广真相和虚假信息，但通过适当提示和纠正性对话可以缓解这一问题。

Abstract: Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against ("debunking") or for ("bunking") that conspiracy. When using a "jailbroken" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.

</details>


### [86] [Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence](https://arxiv.org/abs/2601.05051)
*Jennifer D'Souza,Soren Auer,Eleni Poupaki,Alex Watkins,Anjana Devi,Riikka L. Puurunen,Bora Karasulu,Adrie Mackus,Erwin Kessels*

Main category: cs.AI

TL;DR: 将材料科学综述表格转化为FAIR、机器可操作的开放知识图谱，对比符号查询与LLM查询，主张符号层应作为神经符号AI的可靠基础


<details>
  <summary>Details</summary>
Motivation: 材料科学综述的关键见解被锁定在叙述性文本和静态PDF表格中，限制了人类和机器的重用，需要将其转化为结构化、可查询的知识

Method: 以原子层沉积和蚀刻(ALD/E)为案例研究，将综述表格发布为开放研究知识图谱(ORKG)中的FAIR、机器可操作的比较，构建结构化知识库

Result: 成功将综述表格转化为结构化、可查询的知识，对比了符号查询与基于大语言模型的查询方法

Conclusion: 在材料科学的神经符号AI中，精心策划的符号层应保持为可靠性的核心，大语言模型应作为补充性的、符号基础接口而非独立的真相来源

Abstract: Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.

</details>


### [87] [Reinforced Efficient Reasoning via Semantically Diverse Exploration](https://arxiv.org/abs/2601.05053)
*Ziqi Zhao,Zhaochun Ren,Jiahong Zou,Liu Yang,Zhiwei Xu,Xuri Ge,Zhumin Chen,Xinyu Ma,Daiting Shi,Shuaiqiang Wang,Dawei Yin,Xin Xin*

Main category: cs.AI

TL;DR: ROSE方法通过语义多样性探索和长度感知优势估计，提升大语言模型的强化学习推理效率和多样性


<details>
  <summary>Details</summary>
Motivation: 现有的基于MCTS的RLVR方法存在探索多样性有限和推理效率低下的问题，需要改进

Method: 提出ROSE方法，包含基于语义熵的分支策略、ε探索机制和长度感知的片段级优势估计器

Result: 在多个数学推理基准测试中，使用Qwen和Llama模型验证了ROSE的有效性和效率

Conclusion: ROSE通过语义多样性探索和高效推理机制，显著提升了LLMs的推理能力

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.

</details>


### [88] [Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models](https://arxiv.org/abs/2601.05076)
*Arghyadeep Das,Sai Sreenivas Chintha,Rishiraj Girmal,Kinjal Pandey,Sharvi Endait*

Main category: cs.AI

TL;DR: 大型推理模型通过显式思维链提高性能，但中间推理会泄露个人身份信息。研究提出隐私优先推理方法，通过提示控制或微调显著减少PII泄露，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成显式思维链推理虽然提高了性能、可靠性和可解释性，但这种透明性带来了严重的隐私风险：即使最终答案经过清理，中间推理过程仍会泄露个人身份信息。

Method: 1. 引入PII-CoT-Bench数据集，包含隐私感知的思维链标注；2. 创建类别平衡的评估基准，涵盖现实和对抗性泄露场景；3. 采用两种可部署干预措施：基于提示的控制和模型微调。

Result: 1. 发现能力依赖趋势：最先进模型从基于提示的控制中获益最大，而较弱模型需要微调才能实现有意义的泄露减少；2. 两种方法都能显著减少PII暴露，同时保持最小程度的性能下降；3. 隐私思维链推理可以在不牺牲性能的情况下实现。

Conclusion: 隐私优先推理可以通过可部署的干预措施实现，且性能损失最小。研究为构建隐私保护推理系统提供了实用指导，展示了在保持模型性能的同时显著减少个人身份信息泄露的可行性。

Abstract: Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.

</details>


### [89] [Arabic Prompts with English Tools: A Benchmark](https://arxiv.org/abs/2601.05101)
*Konstantin Kubrak,Ahmed El-Moselhy,Ammar Alsulami,Remaz Altuwaim,Hassan Ismail Fawaz,Faisal Alsaby*

Main category: cs.AI

TL;DR: 本文提出了首个专门评估阿拉伯语大语言模型工具调用和智能体能力的基准，发现阿拉伯语交互时工具调用准确率平均下降5-10%。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语原生大语言模型发展迅速，但评估其能力的基准滞后，现有框架主要针对英语。工具调用这一关键领域被忽视，特别是对于在英语数据上预训练的模型在阿拉伯语提示下的表现了解不足。

Method: 引入首个专门评估阿拉伯语大语言模型工具调用和智能体能力的基准，提供标准化框架来衡量阿拉伯语智能体工作流中的功能准确性和鲁棒性。

Result: 发现显著的性能差距：当用户使用阿拉伯语交互时，工具调用准确率平均下降5-10%，无论工具描述本身是阿拉伯语还是英语。

Conclusion: 该基准揭示了阿拉伯语大语言模型在工具调用方面的关键挑战，旨在促进为阿拉伯语用户开发更可靠和语言公平的人工智能智能体。

Abstract: Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.

</details>


### [90] [Token-Level LLM Collaboration via FusionRoute](https://arxiv.org/abs/2601.05106)
*Nuoya Xiong,Yuhang Zhou,Hanqing Zeng,Zhaorun Chen,Furong Huang,Shuchao Bi,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.AI

TL;DR: FusionRoute是一个轻量级的路由框架，通过token级别的多LLM协作，在解码时选择最合适的专家模型并贡献互补logits来优化下一个token的分布，解决了通用大模型成本高而专用小模型泛化差的两难问题。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（LLMs）在多个领域表现出色，但达到跨领域强性能通常需要扩展到训练和部署成本极高的规模。而较小的领域专用模型虽然更高效，却难以泛化到训练分布之外。为了解决这一困境，需要一种既能保持效率又能获得强泛化能力的方法。

Method: 提出FusionRoute框架：一个轻量级的路由器同时在每个解码步骤中（1）选择最合适的专家模型，（2）贡献一个互补的logit，通过logit加法来优化或纠正所选专家的下一个token分布。与仅依赖固定专家输出的现有token级协作方法不同，该方法通过可训练的互补生成器扩展了有效策略类别。

Result: 在Llama-3和Gemma-2系列模型以及涵盖数学推理、代码生成和指令遵循的多样化基准测试中，FusionRoute优于序列级和token级协作、模型融合和直接微调方法，同时在各自任务上与领域专家保持竞争力。

Conclusion: FusionRoute通过理论分析表明纯专家路由存在根本限制，而通过专家选择与可训练互补生成器的结合，能够在温和条件下恢复最优值函数，为高效且泛化能力强的多LLM协作提供了有效解决方案。

Abstract: Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.

</details>


### [91] [Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction](https://arxiv.org/abs/2601.05107)
*Muzhao Tian,Zisu Huang,Xiaohua Wang,Jingwen Xu,Zhengkang Guo,Qi Qian,Yuanzhe Shen,Kaitao Song,Jiakang Yuan,Changze Lv,Xiaoqing Zheng*

Main category: cs.AI

TL;DR: SteeM框架让用户能够动态调节LLM智能体对记忆的依赖程度，从创新导向的"重新开始"模式到忠实遵循历史交互的"高保真"模式，解决了传统记忆系统"全有或全无"的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在长期交互中采用"全有或全无"的记忆使用方式：要么包含所有相关信息导致"记忆锚定"问题（被过去交互束缚），要么完全排除记忆导致历史信息利用不足。需要更精细的记忆控制机制。

Method: 首先引入记忆依赖行为度量来量化过去交互对当前输出的影响，然后提出SteeM（可调控记忆智能体）框架，允许用户动态调节记忆依赖程度，实现从创新导向到历史忠实的不同模式。

Result: 实验表明，SteeM在不同场景下始终优于传统提示方法和刚性记忆掩码策略，为个性化人机协作提供了更细致有效的控制。

Conclusion: 智能体对记忆的依赖可以建模为明确且用户可控的维度，SteeM框架通过动态调节记忆依赖程度，实现了更灵活有效的个性化人机协作。

Abstract: As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \textbf{Stee}rable \textbf{M}emory Agent, \texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.

</details>


### [92] [GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts](https://arxiv.org/abs/2601.05110)
*Wenhao Zeng,Xuteng Zhang,Yuling Shi,Chao Hu,Yuting Chen,Beijun Shen,Xiaodong Gu*

Main category: cs.AI

TL;DR: GlimpRouter：基于初始token熵预测推理步骤难度，实现轻量模型与大模型协作推理的免训练框架，显著降低推理延迟同时保持准确率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式生成多步思维链获得优异性能，但带来高推理延迟和计算成本。现有协作推理方法依赖局部token概率或事后验证，引入显著推理开销。需要找到一种简单有效的方法来判断推理步骤何时需要大模型能力或小模型效率。

Method: 提出GlimpRouter框架，基于"顿悟时刻"现象发现推理步骤难度可以从第一个token的熵预测。使用轻量模型仅生成每个推理步骤的第一个token，当初始token熵超过阈值时，将步骤路由到大模型处理，否则由小模型完成。

Result: 在多个基准测试中，GlimpRouter显著降低推理延迟同时保持准确率。在AIME25上相比独立大模型，准确率提升10.7%，推理延迟降低25.9%。

Conclusion: 初始token熵是推理步骤难度的强预测指标，基于"思维一瞥"而非完整步骤评估来分配计算资源是简单有效的推理机制，为协作推理提供了免训练的高效解决方案。

Abstract: Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the "Aha Moment" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.

</details>


### [93] [Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior](https://arxiv.org/abs/2601.05114)
*Wajid Nasser*

Main category: cs.AI

TL;DR: 研究发现LLM评估系统存在可靠性悖论：单个评估者内部一致性高，但评估者间一致性几乎为零，且这种不一致性具有稳定模式，可作为评估者的"指纹"。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示LLM-as-judge系统中评估者间一致性的真实情况，挑战了这类系统提供可扩展、一致评估的普遍假设。

Method: 通过3,240次评估（9个评估者×120个独特视频×3次独立运行），使用Krippendorff's α测量评估者间一致性，构建分类器识别评估者身份，并分析评估倾向的多个维度。

Result: 评估者间一致性接近零（α=0.042），某些维度甚至低于随机噪声预期（α<0）。但分类器仅凭评分就能以77.1%准确率识别评估者，加入倾向特征后达89.9%。同一模型家族内区分度更高（GPT-4.1 vs GPT-5.2达99.6%）。

Conclusion: LLM评估者并非可互换的测量工具，而是各自编码了独特质量理论的独立测量设备。平均评分产生的是不反映任何评估者真实价值观的综合判断，这挑战了LLM评估系统的可靠性基础。

Abstract: LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's α = 0.042). On two dimensions, judges disagree more than random noise would predict (α < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an "evaluative disposition" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.

</details>


### [94] [Stock Market Price Prediction using Neural Prophet with Deep Neural Network](https://arxiv.org/abs/2601.05202)
*Navin Chhibber,Suneel Khemka,Navneet Kumar Tyagi,Rohit Tewari,Bireswar Banerjee,Piyush Ranjan*

Main category: cs.AI

TL;DR: 本文提出了一种结合Neural Prophet和深度神经网络（NP-DNN）的股票价格预测模型，通过Z-score归一化和缺失值填补进行数据预处理，使用多层感知机学习非线性关系，实现了99.21%的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 股票价格预测是金融、统计和经济学的交叉研究领域，现有统计方法在预测未来股票价格概率范围方面效果有限，需要更有效的预测方法。

Method: 提出NP-DNN模型，采用Z-score归一化进行数据预处理，使用缺失值填补技术处理历史数据，通过多层感知机学习股票价格间的复杂非线性关系并提取隐藏模式。

Result: NP-DNN模型达到了99.21%的预测准确率，相比其他基于融合大语言模型的方法表现更优。

Conclusion: NP-DNN模型在股票价格预测方面表现出色，能够有效预测股票价格的概率范围，为股票市场预测提供了新的有效方法。

Abstract: Stock market price prediction is a significant interdisciplinary research domain that depends at the intersection of finance, statistics, and economics. Forecasting Accurately predicting stock prices has always been a focal point for various researchers. However, existing statistical approaches for time-series prediction often fail to effectively forecast the probability range of future stock prices. Hence, to solve this problem, the Neural Prophet with a Deep Neural Network (NP-DNN) is proposed to predict stock market prices. The preprocessing technique used in this research is Z-score normalization, which normalizes stock price data by removing scale differences, making patterns easier to detect. Missing value imputation fills gaps in historical data, enhancing the models use of complete information for more accurate predictions. The Multi-Layer Perceptron (MLP) learns complex nonlinear relationships among stock market prices and extracts hidden patterns from the input data, thereby creating meaningful feature representations for better prediction accuracy. The proposed NP-DNN model achieved an accuracy of 99.21% compared with other approaches using the Fused Large Language Model. Keywords: deep neural network, forecasting stock prices, multi-layer perceptron, neural prophet, stock market price prediction.

</details>


### [95] [Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models](https://arxiv.org/abs/2601.05144)
*Shuliang Liu,Xingyu Li,Hongyi Liu,Yibo Yan,Bingchen Duan,Qi Zheng,Dong Fang,Lingfeng Su,Xuming Hu*

Main category: cs.AI

TL;DR: ReasonMark是一个专门为推理密集型大语言模型设计的水印框架，通过分离思考阶段和回答阶段，使用关键性评分识别语义关键标记，并基于语义自适应机制调整水印强度，在保持逻辑完整性的同时实现高效水印嵌入。


<details>
  <summary>Details</summary>
Motivation: 现有的数字水印方法在处理推理大语言模型时面临挑战：基于标记的水印技术会破坏推理流程的逻辑连贯性，而语义感知方法虽然提高了质量但引入了显著延迟或需要辅助模型。需要一种既能保护推理逻辑完整性又能高效运行的水印方案。

Method: 将生成过程解耦为不受干扰的思考阶段和加水印的回答阶段。提出关键性评分来识别推理轨迹中的语义关键标记，将其提炼为主语义向量。该向量指导语义自适应机制，根据标记与PSV的对齐程度调节水印强度，确保鲁棒性而不损害逻辑完整性。

Result: 实验表明ReasonMark优于现有方法：文本困惑度降低0.35，翻译BLEU分数提高0.164，数学准确性提升0.67分。同时水印检测AUC提高0.34%，对攻击具有更强鲁棒性，延迟增加可忽略不计。

Conclusion: ReasonMark框架实现了推理大语言模型的可追溯和可信部署，在保持逻辑完整性的同时提供高效、鲁棒的水印方案，适用于实际应用场景。

Abstract: Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.

</details>


### [96] [Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop](https://arxiv.org/abs/2601.05184)
*Yaxuan Wang,Zhongteng Cai,Yujia Bao,Xueru Zhang,Yang Liu*

Main category: cs.AI

TL;DR: 该研究提出了自消耗执行循环(SCPL)概念，探讨了在受控执行反馈下合成数据在动态迭代训练过程中如何塑造偏见，并通过奖励拒绝采样策略来减轻偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展促使人们使用合成数据训练未来模型，但这会形成自消耗的再训练循环，可能导致性能下降和偏见产生。现实应用中，先前部署的LLM可能影响其生成的数据，形成由用户反馈驱动的动态系统。由于无法获取动态生产系统中的真实用户偏好数据，需要建立受控环境来隔离和分析反馈驱动的偏见演化。

Method: 引入自消耗执行循环(SCPL)概念，在受控执行反馈设置下研究合成数据在动态迭代训练过程中如何塑造偏见。研究两种循环类型：典型的再训练设置和增量微调设置。在三个真实世界任务上进行实验，设计基于奖励的拒绝采样策略来减轻偏见。

Result: 实验发现执行循环会增加偏好偏见并减少差异偏见。基于奖励的拒绝采样策略能够有效减轻偏见，朝着更可信的自改进系统发展。

Conclusion: 自消耗执行循环在动态迭代训练过程中对偏见演化有重要影响，需要设计有效策略来减轻偏见，构建更可信的自改进系统。受控执行反馈设置为分析反馈驱动的偏见演化提供了原则性方法。

Abstract: The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \textbf{S}elf-\textbf{C}onsuming \textbf{P}erformative \textbf{L}oop (\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.

</details>


### [97] [SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning](https://arxiv.org/abs/2601.05187)
*Yanchang Liang,Xiaowei Zhao*

Main category: cs.AI

TL;DR: SimuAgent是一个基于LLM的Simulink建模与仿真代理，通过Python字典表示替代XML，采用两阶段训练和ReGRPO算法，在5300个多领域建模任务上表现优于标准RL方法和GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本代码自动化方面取得了革命性进展，但在图形式工程工作流中的应用潜力尚未充分探索。Simulink等图形化建模环境需要更智能的AI辅助设计工具。

Method: 1) 用简洁的Python字典表示替代冗长的XML格式，大幅减少token数量；2) 采用轻量级计划-执行架构；3) 两阶段训练：先训练底层工具技能，再训练高层设计推理；4) 提出Reflection-GRPO算法，通过自我反思轨迹提供中间反馈；5) 使用抽象-重构数据增强提升泛化能力。

Result: 在SimuBench基准测试（5300个多领域建模任务）上，使用Qwen2.5-7B模型微调的SimuAgent比标准RL基线收敛更快、建模精度更高，甚至在使用少量提示的情况下超越了GPT-4o。消融实验证实了两阶段课程学习和数据增强的有效性。

Conclusion: SimuAgent弥合了LLM与图形化建模环境之间的鸿沟，提供了一种隐私保护、成本效益高的工业级模型驱动工程解决方案，可在本地硬件上完全训练和运行。

Abstract: Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.

</details>


### [98] [Internal Representations as Indicators of Hallucinations in Agent Tool Selection](https://arxiv.org/abs/2601.05214)
*Kait Healy,Bharathi Srinivasan,Visakh Madathil,Jing Wu*

Main category: cs.AI

TL;DR: 提出一种实时检测LLM工具调用幻觉的高效框架，利用单次前向传播的内部表征，实现86.4%准确率的检测，特别擅长参数级幻觉和不适当工具选择的识别。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在工具调用中存在幻觉问题，包括选择错误工具、参数格式错误以及"工具绕过"行为，这导致生产系统中LLM代理的可靠性降低，产生不一致结果并绕过安全审计控制。

Method: 提出计算高效的实时检测框架，利用LLM在生成过程中的内部表征，在同一前向传播过程中检测工具调用幻觉，无需多次前向传播或外部验证。

Result: 在多个领域的推理任务上评估，展示出强大的检测性能（最高86.4%准确率），同时保持实时推理能力且计算开销最小，特别擅长检测参数级幻觉和不适当的工具选择。

Conclusion: 该框架为可靠部署LLM代理提供了有效的实时幻觉检测方案，能够早期发现工具选择错误，确保系统的一致性和安全性。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.

</details>


### [99] [MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents](https://arxiv.org/abs/2601.05215)
*Tamil Sudaravan Mohan Doss,Michael Xu,Sudha Rao,Andrew D. Wilson,Balasaravanan Thoravi Kumaravel*

Main category: cs.AI

TL;DR: MineNPC-Task是一个用于测试《我的世界》中具备记忆能力的混合主动LLM代理的基准测试框架，包含用户创作的任务集和评估工具，通过真实玩家合作设计任务模板和验证器，评估代理在代码执行、物品管理、导航等方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理评估通常依赖合成提示，缺乏真实玩家参与设计的任务，且难以评估记忆能力和混合主动交互。需要建立基于真实玩家体验、具有明确依赖结构和可验证性的基准测试框架来评估开放世界环境中的记忆感知代理。

Method: 1) 与专家玩家进行形成性和总结性合作游戏，收集真实任务；2) 将任务规范化为参数化模板，包含明确的先决条件和依赖结构；3) 在禁止使用世界外捷径的有界知识策略下，配对机器可检查的验证器；4) 捕获计划/行动/记忆事件，包括计划预览、针对性澄清、内存读写、先决条件检查和修复尝试。

Result: 使用GPT-4o实例化框架，评估8名经验玩家的216个子任务。观察到代码执行、库存/工具处理、引用和导航方面的重复性故障模式，同时发现混合主动澄清和轻量级内存支持下的恢复能力。参与者对交互质量和界面可用性评价积极，但强调需要更强的跨任务记忆持久性。

Conclusion: MineNPC-Task提供了一个透明、可复现的评估框架，支持未来记忆感知具身代理的发展。通过释放完整的任务套件、验证器、日志和评估工具，促进该领域的研究进展，特别强调了真实玩家参与设计和混合主动交互的重要性。

Abstract: We present \textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.
  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \textbf{216} subtasks across \textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.

</details>


### [100] [Learning Latent Action World Models In The Wild](https://arxiv.org/abs/2601.05230)
*Quentin Garrido,Tushar Nagarajan,Basile Terver,Nicolas Ballas,Yann LeCun,Michael Rabbat*

Main category: cs.AI

TL;DR: 该研究提出了一种从野外视频中学习潜在动作世界模型的方法，能够在不依赖动作标签的情况下从视频中学习动作空间，并展示了其在规划任务中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的智能体需要预测其行动的后果，但传统世界模型通常需要动作标签，而这些标签在大规模获取时很复杂。因此需要从视频中学习潜在动作模型，扩展现有工作（主要关注简单机器人模拟、视频游戏或操作数据）到更丰富的野外视频场景。

Method: 提出了学习潜在动作世界模型的方法，讨论了动作应遵循的属性、相关架构选择和评估方法。研究发现连续但有约束的潜在动作能够捕捉野外视频中动作的复杂性，而常见的向量量化方法则不能。还训练了一个控制器，将已知动作映射到潜在动作。

Result: 连续但有约束的潜在动作能够捕捉野外视频中动作的复杂性，能够跨视频转移环境变化（如人类进入房间）。在缺乏跨视频共同体现的情况下，主要学习到相对于相机空间局部化的潜在动作。控制器能够将已知动作映射到潜在动作，使潜在动作作为通用接口，在规划任务中达到与动作条件基线相似的性能。

Conclusion: 该研究为将潜在动作模型扩展到现实世界提供了重要一步，展示了从野外视频中学习动作空间的可行性，并为解决规划任务提供了有效方法。

Abstract: Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [101] [Correct and Weight: A Simple Yet Effective Loss for Implicit Feedback Recommendation](https://arxiv.org/abs/2601.04291)
*Minglei Yin,Chuanbo Hu,Bin Liu,Neil Zhenqiang Gong,Yanfang,Ye,Xin Li*

Main category: cs.IR

TL;DR: 提出一种名为CW损失的新型损失函数，通过修正假阴性样本的影响来改进隐式反馈推荐系统


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统通常基于隐式反馈学习，但存在假阴性问题——未观测到的用户-物品交互不一定表示负面偏好，这会影响模型训练效果

Method: 提出CW损失函数，包含两个关键技术：1) 基于正-未标记学习思想，通过重新校准假设的负分布来修正负采样过程；2) 引入动态重加权机制，根据模型当前预测调整每个负实例的重要性

Result: 在四个大规模稀疏基准数据集上的实验表明，该方法在多个排序导向指标上显著优于一系列最先进的损失函数

Conclusion: CW损失函数能有效处理隐式反馈中的假阴性问题，方法优雅高效，无需复杂的数据采样修改或显著计算开销，可广泛应用于现有推荐模型

Abstract: Learning from implicit feedback has become the standard paradigm for modern recommender systems. However, this setting is fraught with the persistent challenge of false negatives, where unobserved user-item interactions are not necessarily indicative of negative preference. To address this issue, this paper introduces a novel and principled loss function, named Corrected and Weighted (CW) loss, that systematically corrects for the impact of false negatives within the training objective. Our approach integrates two key techniques. First, inspired by Positive-Unlabeled learning, we debias the negative sampling process by re-calibrating the assumed negative distribution. By theoretically approximating the true negative distribution (p-) using the observable general data distribution (p) and the positive interaction distribution (p^+), our method provides a more accurate estimate of the likelihood that a sampled unlabeled item is truly negative. Second, we introduce a dynamic re-weighting mechanism that modulates the importance of each negative instance based on the model's current prediction. This scheme encourages the model to enforce a larger ranking margin between positive items and confidently predicted (i.e., easy) negative items, while simultaneously down-weighting the penalty on uncertain negatives that have a higher probability of being false negatives. A key advantage of our approach is its elegance and efficiency; it requires no complex modifications to the data sampling process or significant computational overhead, making it readily applicable to a wide array of existing recommendation models. Extensive experiments conducted on four large-scale, sparse benchmark datasets demonstrate the superiority of our proposed loss. The results show that our method consistently and significantly outperforms a suite of state-of-the-art loss functions across multiple ranking-oriented metrics.

</details>


### [102] [The Overlooked Role of Graded Relevance Thresholds in Multilingual Dense Retrieval](https://arxiv.org/abs/2601.04395)
*Tomer Wullach,Ori Shapira,Amir DN Cohen*

Main category: cs.IR

TL;DR: 研究分析了分级相关性评分及其二值化阈值对多语言稠密检索的影响，发现最优阈值因语言和任务而异，合理选择阈值可提升效果、减少训练数据需求并缓解标注噪声。


<details>
  <summary>Details</summary>
Motivation: 当前稠密检索模型通常使用对比学习目标进行微调，这需要二值相关性判断，但相关性本质上是分级的。研究者希望探索分级相关性评分及其二值化阈值如何影响多语言稠密检索性能。

Method: 使用带有LLM标注相关性分数的多语言数据集，研究单语言、多语言混合和跨语言检索场景。分析分级相关性分数以及将其转换为二值标签的阈值如何影响检索效果。

Result: 研究发现最优阈值在不同语言和任务间存在系统性差异，通常反映资源水平的差异。合理选择的阈值可以提高检索效果、减少微调所需数据量并缓解标注噪声，而选择不当则会降低性能。

Conclusion: 分级相关性是稠密检索中一个有价值但未充分利用的信号，阈值校准应被视为微调流程中的一个原则性组件。合理利用分级相关性信息可以优化多语言检索系统的性能。

Abstract: Dense retrieval models are typically fine-tuned with contrastive learning objectives that require binary relevance judgments, even though relevance is inherently graded. We analyze how graded relevance scores and the threshold used to convert them into binary labels affect multilingual dense retrieval. Using a multilingual dataset with LLM-annotated relevance scores, we examine monolingual, multilingual mixture, and cross-lingual retrieval scenarios. Our findings show that the optimal threshold varies systematically across languages and tasks, often reflecting differences in resource level. A well-chosen threshold can improve effectiveness, reduce the amount of fine-tuning data required, and mitigate annotation noise, whereas a poorly chosen one can degrade performance. We argue that graded relevance is a valuable but underutilized signal for dense retrieval, and that threshold calibration should be treated as a principled component of the fine-tuning pipeline.

</details>


### [103] [Re-Rankers as Relevance Judges](https://arxiv.org/abs/2601.04455)
*Chuan Meng,Jiqun Liu,Mohammad Aliannejadi,Fengran Mo,Jeff Dalton,Maarten de Rijke*

Main category: cs.IR

TL;DR: 该研究探索将重排序模型用作相关性判断器，通过两种适配策略（二进制标记和阈值化）让重排序模型预测查询-文档相关性，在TREC-DL数据集上实验表明重排序模型作为判断器在40-50%情况下优于现有LLM方法，但存在明显的自偏好和跨家族偏见。


<details>
  <summary>Details</summary>
Motivation: 当前使用大语言模型预测相关性判断的研究大多将其视为独立任务，专注于提示设计。然而，预测相关性判断本质上是相关性预测问题，这在重排序任务中已有深入研究。由于缺乏对现有重排序方法的复用和适配，导致资源浪费和重复开发。本研究旨在填补这一空白，探索将重排序模型用作相关性判断器的可行性。

Method: 设计了两种适配策略：1）使用重排序模型生成的二进制标记（如"true"和"false"）作为直接判断；2）通过阈值化将连续的重排序分数转换为二进制标签。在TREC-DL 2019至2023数据集上进行了广泛实验，使用了来自3个家族的8个重排序模型，参数规模从2.2亿到320亿不等。

Result: 实验结果显示：1）基于重排序模型的相关性判断器在两种策略下，在约40%到50%的情况下能够超越当前最先进的LLM相关性判断器UMBRELA；2）这些判断器表现出强烈的自偏好，倾向于对自己和同家族的重排序模型给出更高评价；3）存在明显的跨家族偏见。

Conclusion: 重排序模型可以作为有效的相关性判断器，其性能在相当多情况下优于专门的LLM方法。然而，评估偏见（自偏好和跨家族偏见）是需要关注的重要问题。这为复用现有重排序模型进行相关性判断提供了实证支持，避免了重复开发。

Abstract: Using large language models (LLMs) to predict relevance judgments has shown promising results. Most studies treat this task as a distinct research line, e.g., focusing on prompt design for predicting relevance labels given a query and passage. However, predicting relevance judgments is essentially a form of relevance prediction, a problem extensively studied in tasks such as re-ranking. Despite this potential overlap, little research has explored reusing or adapting established re-ranking methods to predict relevance judgments, leading to potential resource waste and redundant development. To bridge this gap, we reproduce re-rankers in a re-ranker-as-relevance-judge setup. We design two adaptation strategies: (i) using binary tokens (e.g., "true" and "false") generated by a re-ranker as direct judgments, and (ii) converting continuous re-ranking scores into binary labels via thresholding. We perform extensive experiments on TREC-DL 2019 to 2023 with 8 re-rankers from 3 families, ranging from 220M to 32B, and analyse the evaluation bias exhibited by re-ranker-based judges. Results show that re-ranker-based relevance judges, under both strategies, can outperform UMBRELA, a state-of-the-art LLM-based relevance judge, in around 40% to 50% of the cases; they also exhibit strong self-preference towards their own and same-family re-rankers, as well as cross-family bias.

</details>


### [104] [Self-MedRAG: a Self-Reflective Hybrid Retrieval-Augmented Generation Framework for Reliable Medical Question Answering](https://arxiv.org/abs/2601.04531)
*Jessica Ryan,Alexander I. Gumilang,Robert Wiliam,Derwin Suhartono*

Main category: cs.IR

TL;DR: Self-MedRAG：一种结合混合检索与自我反思的临床推理框架，通过迭代假设验证过程减少LLM在医学问答中的幻觉，显著提升MedQA和PubMedQA基准的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医学问答中表现出潜力，但存在幻觉和缺乏依据的推理问题，限制了其在高风险临床场景中的可靠性。传统的单次检索增强生成方法难以解决需要多步推理的复杂生物医学查询。

Method: 提出Self-MedRAG框架，结合稀疏检索（BM25）和密集检索（Contriever）的混合检索策略，通过互惠排名融合最大化证据覆盖。系统包含生成器产生答案和推理依据，轻量级自我反思模块使用自然语言推理或LLM验证，若推理依据不足则自主重新表述查询并迭代优化上下文。

Result: 混合检索方法显著优于单检索器基线。自我反思循环带来实质性提升：MedQA准确率从80.00%提高到83.33%，PubMedQA准确率从69.10%提高到79.82%。

Conclusion: 混合检索与基于证据的迭代自我反思相结合，有效减少无依据的断言，增强基于LLM系统的临床可靠性。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in medical Question Answering (QA), yet they remain prone to hallucinations and ungrounded reasoning, limiting their reliability in high-stakes clinical scenarios. While Retrieval-Augmented Generation (RAG) mitigates these issues by incorporating external knowledge, conventional single-shot retrieval often fails to resolve complex biomedical queries requiring multi-step inference. To address this, we propose Self-MedRAG, a self-reflective hybrid framework designed to mimic the iterative hypothesis-verification process of clinical reasoning. Self-MedRAG integrates a hybrid retrieval strategy, combining sparse (BM25) and dense (Contriever) retrievers via Reciprocal Rank Fusion (RRF) to maximize evidence coverage. It employs a generator to produce answers with supporting rationales, which are then assessed by a lightweight self-reflection module using Natural Language Inference (NLI) or LLM-based verification. If the rationale lacks sufficient evidentiary support, the system autonomously reformulates the query and iterates to refine the context. We evaluated Self-MedRAG on the MedQA and PubMedQA benchmarks. The results demonstrate that our hybrid retrieval approach significantly outperforms single-retriever baselines. Furthermore, the inclusion of the self-reflective loop yielded substantial gains, increasing accuracy on MedQA from 80.00% to 83.33% and on PubMedQA from 69.10% to 79.82%. These findings confirm that integrating hybrid retrieval with iterative, evidence-based self-reflection effectively reduces unsupported claims and enhances the clinical reliability of LLM-based systems.

</details>


### [105] [Exploring Recommender System Evaluation: A Multi-Modal User Agent Framework for A/B Testing](https://arxiv.org/abs/2601.04554)
*Wenlin Zhang,Xiangyang Li,Qiyuan Ge,Kuicai Dong,Pengyue Jia,Xiaopeng Li,Zijian Zhang,Maolin Wang,Yichao Wang,Huifeng Guo,Ruiming Tang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出A/B Agent多模态用户代理，通过构建推荐沙盒环境和模拟真实用户行为，替代传统在线A/B测试，降低经济成本和时间需求


<details>
  <summary>Details</summary>
Motivation: 传统在线A/B测试存在经济成本高、用户体验下降、时间需求大等问题，而现有代理缺乏真实环境和视觉感知能力，无法模拟用户感知过程和交互模式

Method: 构建推荐沙盒环境支持多模态多页面交互，设计代理具备多模态信息感知、细粒度用户偏好，集成用户画像、动作记忆检索和疲劳系统来模拟复杂人类决策

Result: 从模型、数据和特征三个角度验证了A/B Agent替代传统A/B测试的潜力，发现其生成的数据能有效增强推荐模型能力

Conclusion: A/B Agent为在线A/B测试提供了有效的替代方案，通过模拟真实用户行为和多模态交互，解决了传统测试的成本和效率问题

Abstract: In recommender systems, online A/B testing is a crucial method for evaluating the performance of different models. However, conducting online A/B testing often presents significant challenges, including substantial economic costs, user experience degradation, and considerable time requirements. With the Large Language Models' powerful capacity, LLM-based agent shows great potential to replace traditional online A/B testing. Nonetheless, current agents fail to simulate the perception process and interaction patterns, due to the lack of real environments and visual perception capability. To address these challenges, we introduce a multi-modal user agent for A/B testing (A/B Agent). Specifically, we construct a recommendation sandbox environment for A/B testing, enabling multimodal and multi-page interactions that align with real user behavior on online platforms. The designed agent leverages multimodal information perception, fine-grained user preferences, and integrates profiles, action memory retrieval, and a fatigue system to simulate complex human decision-making. We validated the potential of the agent as an alternative to traditional A/B testing from three perspectives: model, data, and features. Furthermore, we found that the data generated by A/B Agent can effectively enhance the capabilities of recommendation models. Our code is publicly available at https://github.com/Applied-Machine-Learning-Lab/ABAgent.

</details>


### [106] [Adaptive Retrieval for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.04618)
*Jongho Kim,Jaeyoung Kim,Seung-won Hwang,Jihyuk Kim,Yu Jin Kim,Moontae Lee*

Main category: cs.IR

TL;DR: REPAIR框架通过将推理计划重新用作自适应检索的密集反馈信号，解决推理密集型检索中桥接文档召回不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的重排序管道试图在排名中呈现桥接文档（对推理过程有贡献但不直接与初始查询相关的文档），但存在召回率受限的问题。将自适应检索简单引入这些管道通常会导致规划错误传播。

Method: 提出REPAIR框架，通过将推理计划重新用作自适应检索的密集反馈信号来弥合这一差距。关键区别在于通过选择性自适应检索实现重排序过程中的中途修正，检索支持关键计划的文档。

Result: 在推理密集型检索和复杂QA任务上的实验结果表明，该方法比现有基线方法性能提升5.6个百分点。

Conclusion: REPAIR框架通过将推理计划转化为自适应检索的反馈信号，有效解决了推理密集型检索中桥接文档召回不足的问题，显著提升了检索性能。

Abstract: We study leveraging adaptive retrieval to ensure sufficient "bridge" documents are retrieved for reasoning-intensive retrieval. Bridge documents are those that contribute to the reasoning process yet are not directly relevant to the initial query. While existing reasoning-based reranker pipelines attempt to surface these documents in ranking, they suffer from bounded recall. Naive solution with adaptive retrieval into these pipelines often leads to planning error propagation. To address this, we propose REPAIR, a framework that bridges this gap by repurposing reasoning plans as dense feedback signals for adaptive retrieval. Our key distinction is enabling mid-course correction during reranking through selective adaptive retrieval, retrieving documents that support the pivotal plan. Experimental results on reasoning-intensive retrieval and complex QA tasks demonstrate that our method outperforms existing baselines by 5.6%pt.

</details>


### [107] [Succeeding at Scale: Automated Multi-Retriever Fusion and Query-Side Adaptation for Multi-Tenant Search](https://arxiv.org/abs/2601.04646)
*Prateek Jain,Shabari S Nair,Ritesh Goru,Prakhar Agarwal,Ajay Yadav,Yoga Sri Varshan Varadharajan,Constantine Caramanis*

Main category: cs.IR

TL;DR: 论文提出了DevRev Search基准测试和索引保持适配策略，通过仅微调查询编码器来解决多租户检索系统中缺乏标注数据和重新索引成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模多租户检索系统面临两个关键挑战：1) 缺乏用于有效领域适应的相关标注数据（"暗数据"问题）；2) 模型更新的操作成本高，联合微调查询和文档编码器需要重新索引整个语料库，这在拥有数千个独立索引的多租户环境中成本过高。

Method: 1) 构建DevRev Search基准测试：通过融合式候选生成策略（汇集稀疏和稠密检索器的结果）和LLM-as-a-Judge进行一致性过滤和相关性分配；2) 提出索引保持适配策略：仅通过低秩适配（LoRA）微调查询编码器，保持文档索引冻结，并针对查询编码器中的特定Transformer层进行优化。

Result: 在DevRev Search和SciFact数据集上的实验表明，仅微调查询编码器就能获得有竞争力的性能提升，同时保持文档索引不变。针对查询编码器中特定Transformer层的微调实现了最佳的质量-效率权衡。

Conclusion: 该方法为个性化企业搜索提供了一条可扩展的路径，通过仅微调查询编码器并保持文档索引冻结，解决了多租户检索系统中的暗数据问题和重新索引成本高的挑战。

Abstract: Large-scale multi-tenant retrieval systems amass vast user query logs yet critically lack the curated relevance labels required for effective domain adaptation. This "dark data" problem is exacerbated by the operational cost of model updates: jointly fine-tuning query and document encoders requires re-indexing the entire corpus, which is prohibitive in multi-tenant environments with thousands of isolated indices. To address these dual challenges, we introduce \textbf{DevRev Search}, a passage retrieval benchmark for technical customer support constructed through a fully automatic pipeline. We employ a \textbf{fusion-based candidate generation} strategy, pooling results from diverse sparse and dense retrievers, and utilize an LLM-as-a-Judge to perform rigorous \textbf{consistency filtering} and relevance assignment. We further propose a practical \textbf{Index-Preserving Adaptation} strategy: by fine-tuning only the query encoder via Low-Rank Adaptation (LoRA), we achieve competitive performance improvements while keeping the document index frozen. Our experiments on DevRev Search and SciFact demonstrate that targeting specific transformer layers in the query encoder yields optimal quality-efficiency trade-offs, offering a scalable path for personalized enterprise search.

</details>


### [108] [PROMISE: Process Reward Models Unlock Test-Time Scaling Laws in Generative Recommendations](https://arxiv.org/abs/2601.04674)
*Chengcheng Guo,Kuo Cai,Yu Zhou,Qiang Luo,Ruiming Tang,Han Li,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: Promise框架通过过程奖励模型和引导波束搜索解决生成式推荐中的语义漂移问题，实现推理时计算扩展定律


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法存在语义漂移问题，早期高层级token的错误会不可逆地将生成轨迹导向不相关的语义子空间，严重影响推荐准确性

Method: 提出Promise框架，包含轻量级过程奖励模型评估中间推理步骤质量，以及PRM引导的波束搜索策略，利用密集反馈动态修剪错误分支

Result: Promise有效缓解语义漂移，显著提高推荐准确性，实现推荐系统的推理时扩展定律，较小模型通过增加推理计算可匹配或超越较大模型

Conclusion: Promise框架为生成式推荐提供了有效的语义漂移解决方案，通过密集逐步验证和动态分支修剪，在保持高效部署的同时提升推荐性能

Abstract: Generative Recommendation has emerged as a promising paradigm, reformulating recommendation as a sequence-to-sequence generation task over hierarchical Semantic IDs. However, existing methods suffer from a critical issue we term Semantic Drift, where errors in early, high-level tokens irreversibly divert the generation trajectory into irrelevant semantic subspaces. Inspired by Process Reward Models (PRMs) that enhance reasoning in Large Language Models, we propose Promise, a novel framework that integrates dense, step-by-step verification into generative models. Promise features a lightweight PRM to assess the quality of intermediate inference steps, coupled with a PRM-guided Beam Search strategy that leverages dense feedback to dynamically prune erroneous branches. Crucially, our approach unlocks Test-Time Scaling Laws for recommender systems: by increasing inference compute, smaller models can match or surpass larger models. Extensive offline experiments and online A/B tests on a large-scale platform demonstrate that Promise effectively mitigates Semantic Drift, significantly improving recommendation accuracy while enabling efficient deployment.

</details>


### [109] [Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective](https://arxiv.org/abs/2601.04918)
*Ziwen Wang,Shangshang Yang,Xiaoshan Yu,Haiping Ma,Xingyi Zhang*

Main category: cs.IR

TL;DR: OSCD是一种用于认知诊断的进化多目标一次性神经架构搜索方法，通过两阶段训练和搜索流程，在异构噪声场景下自动发现最优模型架构，提高学习者能力评估的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有认知诊断研究过于关注模型性能提升，忽视了观测响应数据中普遍存在的噪声污染问题，这严重阻碍了实际部署。同时，现有CDMs过度依赖研究者领域专业知识进行结构设计，未能充分探索架构可能性，限制了模型潜力的发挥。

Method: 提出OSCD方法，包含两个阶段：1) 训练阶段：构建包含多样化架构组合的搜索空间，训练基于完全二叉树拓扑的权重共享超网络；2) 搜索阶段：将异构噪声场景下的最优架构搜索建模为多目标优化问题，开发集成帕累托最优解搜索策略和跨场景性能评估的优化框架。

Result: 在真实世界教育数据集上的大量实验验证了OSCD模型发现的认知诊断任务最优架构的有效性和鲁棒性。

Conclusion: OSCD方法能够高效且鲁棒地提升模型评估学习者熟练程度的能力，超越了传统手动设计的架构限制，为认知诊断领域提供了自动化的架构搜索解决方案。

Abstract: With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners' mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers' domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures' full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model's capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks.

</details>


### [110] [Dynamics in Search Engine Query Suggestions for European Politicians](https://arxiv.org/abs/2601.05081)
*Franziska Pradel,Fabian Haak*

Main category: cs.IR

TL;DR: 该研究分析了谷歌搜索引擎对欧洲政治人物的搜索建议，发现这些建议在不同国家和时间上的稳定性存在差异，受政治人物国籍、职位类型和性别等因素影响。


<details>
  <summary>Details</summary>
Motivation: 搜索引擎是获取政治信息的重要渠道，但人们对搜索建议如何反映用户潜在兴趣在不同国家和时间上的变化了解不足。本研究旨在系统分析谷歌对欧洲政治人物的搜索建议模式。

Method: 研究使用原创数据集，收集了十个国家中欧洲政治人物的谷歌搜索建议，分析这些建议在不同条件下的稳定性。

Result: 研究发现：1）在政治人物原籍国，搜索建议随时间变化更不稳定；2）担任超国家角色的政治人物搜索建议更不稳定；3）女性政治人物的搜索建议更不稳定；4）政治领导人和男性政治人物的搜索建议在不同国家间更相似。

Conclusion: 研究揭示了政治人物搜索建议的时空变化模式，为未来研究欧洲政治人物在线信息搜索提供了方向。

Abstract: Search engines are commonly used for online political information seeking. Yet, it remains unclear how search query suggestions for political searches that reflect the latent interest of internet users vary across countries and over time. We provide a systematic analysis of Google search engine query suggestions for European and national politicians. Using an original dataset of search query suggestions for European politicians collected in ten countries, we find that query suggestions are less stable over time in politicians' countries of origin, when the politicians hold a supranational role, and for female politicians. Moreover, query suggestions for political leaders and male politicians are more similar across countries. We conclude by discussing possible future directions for studying information search about European politicians in online search.

</details>


### [111] [Multivector Reranking in the Era of Strong First-Stage Retrievers](https://arxiv.org/abs/2601.05200)
*Silvio Martinico,Franco Maria Nardini,Cosimo Rulli,Rossano Venturini*

Main category: cs.IR

TL;DR: 本文提出用两阶段检索架构替代多向量检索中的token级收集阶段，通过单向量文档检索器和推理无关的LSR方法，在保持检索质量的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现代多向量检索系统虽然检索效果好，但token级检索成本高，现有收集-精炼策略需要搜索大型token级索引且可能错过最佳文档，需要更高效的检索方案。

Method: 1) 用单向量文档检索器（学习稀疏检索器LSR）替代token级收集阶段；2) 集成推理无关的LSR方法减少查询编码时间；3) 引入两种优化技术早期剪枝低质量候选。

Result: 两阶段方法比最先进的多向量检索系统提速24倍以上，同时保持相当或更优的检索质量，优化技术可将检索效率提升1.8倍且不损失质量。

Conclusion: 将多向量检索重新构建为两阶段架构，结合单向量检索器和推理无关LSR，能显著提升效率同时保持检索效果，为实际部署提供了实用解决方案。

Abstract: Learned multivector representations power modern search systems with strong retrieval effectiveness, but their real-world use is limited by the high cost of exhaustive token-level retrieval. Therefore, most systems adopt a \emph{gather-and-refine} strategy, where a lightweight gather phase selects candidates for full scoring. However, this approach requires expensive searches over large token-level indexes and often misses the documents that would rank highest under full similarity. In this paper, we reproduce several state-of-the-art multivector retrieval methods on two publicly available datasets, providing a clear picture of the current multivector retrieval field and observing the inefficiency of token-level gathering. Building on top of that, we show that replacing the token-level gather phase with a single-vector document retriever -- specifically, a learned sparse retriever (LSR) -- produces a smaller and more semantically coherent candidate set. This recasts the gather-and-refine pipeline into the well-established two-stage retrieval architecture. As retrieval latency decreases, query encoding with two neural encoders becomes the dominant computational bottleneck. To mitigate this, we integrate recent inference-free LSR methods, demonstrating that they preserve the retrieval effectiveness of the dual-encoder pipeline while substantially reducing query encoding time. Finally, we investigate multiple reranking configurations that balance efficiency, memory, and effectiveness, and we introduce two optimization techniques that prune low-quality candidates early. Empirical results show that these techniques improve retrieval efficiency by up to 1.8$\times$ with no loss in quality. Overall, our two-stage approach achieves over $24\times$ speedup over the state-of-the-art multivector retrieval systems, while maintaining comparable or superior retrieval quality.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [112] [The Forgotten Shield: Safety Grafting in Parameter-Space for Medical MLLMs](https://arxiv.org/abs/2601.04199)
*Jiale Zhao,Xing Mou,Jinlin Wu,Hongyuan Yu,Mingrui Sun,Yang Shi,Xuanwu Yin,Zhen Chen,Zhen Lei,Yaohua Wang*

Main category: cs.LG

TL;DR: 本文提出了一种评估医疗多模态大语言模型安全性的框架，发现现有模型存在普遍脆弱性，并提出了一种参数空间干预方法进行安全重新对齐。


<details>
  <summary>Details</summary>
Motivation: 医疗多模态大语言模型在专业医疗任务上取得了显著进展，但其安全性研究滞后，存在实际部署风险。需要系统评估现有模型的安全漏洞，并开发有效方法来增强安全性。

Method: 1) 建立多维评估框架系统评估医疗MLLMs的安全性；2) 提出"参数空间干预"方法，从原始基础模型中提取内在安全知识表示，并在构建医疗能力时将其注入目标模型；3) 设计细粒度参数搜索算法，在安全性和医疗性能之间实现最优权衡。

Result: 实证分析显示现有模型在通用和医疗特定安全维度上都存在普遍脆弱性，特别是对跨模态越狱攻击的脆弱性。医疗微调过程经常导致模型原始安全对齐的灾难性遗忘。提出的方法显著增强了医疗MLLMs的安全防护，无需依赖额外的领域特定安全数据，同时最小化核心医疗性能的退化。

Conclusion: 医疗多模态大语言模型存在严重的安全漏洞，特别是在跨模态攻击下。提出的参数空间干预方法能有效重新对齐模型安全性，在保持医疗性能的同时增强安全防护，为医疗AI的安全部署提供了实用解决方案。

Abstract: Medical Multimodal Large Language Models (Medical MLLMs) have achieved remarkable progress in specialized medical tasks; however, research into their safety has lagged, posing potential risks for real-world deployment. In this paper, we first establish a multidimensional evaluation framework to systematically benchmark the safety of current SOTA Medical MLLMs. Our empirical analysis reveals pervasive vulnerabilities across both general and medical-specific safety dimensions in existing models, particularly highlighting their fragility against cross-modality jailbreak attacks. Furthermore, we find that the medical fine-tuning process frequently induces catastrophic forgetting of the model's original safety alignment. To address this challenge, we propose a novel "Parameter-Space Intervention" approach for efficient safety re-alignment. This method extracts intrinsic safety knowledge representations from original base models and concurrently injects them into the target model during the construction of medical capabilities. Additionally, we design a fine-grained parameter search algorithm to achieve an optimal trade-off between safety and medical performance. Experimental results demonstrate that our approach significantly bolsters the safety guardrails of Medical MLLMs without relying on additional domain-specific safety data, while minimizing degradation to core medical performance.

</details>


### [113] [Green MLOps: Closed-Loop, Energy-Aware Inference with NVIDIA Triton, FastAPI, and Bio-Inspired Thresholding](https://arxiv.org/abs/2601.04250)
*Mustapha Hamdi,Mourad Jabou*

Main category: cs.LG

TL;DR: 该论文提出了一种受生物启发的框架，将蛋白质折叠能量盆地映射到推理成本景观，通过衰减闭环阈值控制执行，仅在预期效用-能量权衡有利时处理请求，从而显著减少处理时间并保持高精度。


<details>
  <summary>Details</summary>
Motivation: AI部署中的能源效率是首要关注点，因为长期运行的推理在累积碳影响上可能超过训练。需要开发能够平衡计算成本和模型性能的节能推理方法。

Method: 提出生物启发框架：1) 将蛋白质折叠能量盆地映射到推理成本景观；2) 使用衰减闭环阈值控制执行；3) 仅在预期效用-能量权衡有利时处理请求（高置信度/效用、低边际能量和拥塞）；4) 偏向于第一个可接受的局部盆地而非追求昂贵的全局最小值。

Result: 在RTX 4000 Ada GPU上评估DistilBERT和ResNet-18：1) 生物控制器相比标准开环执行减少42%处理时间（A100测试集上0.50s vs 0.29s）；2) 精度损失极小（<0.5%）；3) 建立了轻量级本地服务（ORT）与托管批处理（Triton）之间的效率边界。

Conclusion: 该研究将生物物理能量模型与绿色MLOps连接起来，为生产环境中的闭环能量感知推理提供了实用、可审计的基础，实现了显著的能源节省和性能保持。

Abstract: Energy efficiency is a first-order concern in AI deployment, as long-running inference can exceed training in cumulative carbon impact. We propose a bio-inspired framework that maps protein-folding energy basins to inference cost landscapes and controls execution via a decaying, closed-loop threshold. A request is admitted only when the expected utility-to-energy trade-off is favorable (high confidence/utility at low marginal energy and congestion), biasing operation toward the first acceptable local basin rather than pursuing costly global minima. We evaluate DistilBERT and ResNet-18 served through FastAPI with ONNX Runtime and NVIDIA Triton on an RTX 4000 Ada GPU. Our ablation study reveals that the bio-controller reduces processing time by 42% compared to standard open-loop execution (0.50s vs 0.29s on A100 test set), with a minimal accuracy degradation (<0.5%). Furthermore, we establish the efficiency boundaries between lightweight local serving (ORT) and managed batching (Triton). The results connect biophysical energy models to Green MLOps and offer a practical, auditable basis for closed-loop energy-aware inference in production.

</details>


### [114] [Safety-Utility Conflicts Are Not Global: Surgical Alignment via Head-Level Diagnosis](https://arxiv.org/abs/2601.04262)
*Wang Cai,Yilin Wen,Jinchang Hou,Du Su,Guoqiu Wang,Zhonghou Lv,Chenfu Bao,Yunfang Wu*

Main category: cs.LG

TL;DR: 论文提出CAST框架，通过注意力头级别的诊断和稀疏微调来解决LLM安全对齐中的多目标优化冲突问题，避免全局梯度方法对高冲突头的过度更新。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法采用全局梯度几何来解决多目标优化冲突，但忽视了Transformer中不同注意力头的功能敏感性和冲突程度的异质性，导致对通用能力的意外退化。

Method: 提出CAST框架：1）构建预对齐冲突图，综合优化冲突和功能敏感性分析；2）基于冲突图选择性更新参数，跳过高冲突注意力头；3）实现稀疏微调，仅更新低冲突参数。

Result: 实验发现LLM中的对齐冲突分布不均，通用能力下降主要来自更新少量"高冲突"注意力头。通过跳过这些头的训练，能显著减少能力损失而不影响安全性。

Conclusion: CAST框架提供了一种可解释且参数高效的方法来改善安全性与实用性的权衡，通过注意力头级别的冲突诊断和选择性更新，实现了更好的多目标优化平衡。

Abstract: Safety alignment in Large Language Models (LLMs) inherently presents a multi-objective optimization conflict, often accompanied by an unintended degradation of general capabilities. Existing mitigation strategies typically rely on global gradient geometry to resolve these conflicts, yet they overlook Modular Heterogeneity within Transformers, specifically that the functional sensitivity and degree of conflict vary substantially across different attention heads. Such global approaches impose uniform update rules across all parameters, often resulting in suboptimal trade-offs by indiscriminately updating utility sensitive heads that exhibit intense gradient conflicts. To address this limitation, we propose Conflict-Aware Sparse Tuning (CAST), a framework that integrates head-level diagnosis with sparse fine-tuning. CAST first constructs a pre-alignment conflict map by synthesizing Optimization Conflict and Functional Sensitivity, which then guides the selective update of parameters. Experiments reveal that alignment conflicts in LLMs are not uniformly distributed. We find that the drop in general capabilities mainly comes from updating a small group of ``high-conflict'' heads. By simply skipping these heads during training, we significantly reduce this loss without compromising safety, offering an interpretable and parameter-efficient approach to improving the safety-utility trade-off.

</details>


### [115] [Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer](https://arxiv.org/abs/2601.04263)
*Nilushika Udayangani Hewa Dehigahawattage,Kishor Nandakishor,Marimuthu Palaniswami*

Main category: cs.LG

TL;DR: 该论文提出了一种用于时间序列的时序显著性知识蒸馏方法，通过传递教师模型对输入时间步重要性的解释性知识，而不仅仅是预测结果，从而提高学生模型的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列知识蒸馏主要基于计算机视觉任务开发的logit和特征对齐技术，存在两个关键问题：1）传递的知识如何帮助学生模型学习过程不明确，因为logits和特征难以解释；2）这些方法只传递有限的知识，主要复制教师的预测准确性，导致学生模型的预测分布与教师模型显著不同，阻碍其安全替代教师模型。

Method: 提出时序显著性蒸馏方法，扩展传统的logit传递，不仅传递正确的预测，还传递教师的正确推理过程。具体来说，从教师logits中提取称为时序显著性的有用知识，捕捉每个输入时间步对教师预测的重要性。通过使用时序显著性蒸馏训练学生模型，鼓励其基于与教师相同的输入特征进行预测。该方法不需要额外的参数或特定架构假设。

Result: 时序显著性蒸馏有效提高了基线方法的性能，同时实现了超越预测准确性的理想特性。该方法为时间序列分析中的可解释知识蒸馏建立了新范式。

Conclusion: 该工作提出了一种可解释的知识蒸馏方法，通过传递教师模型的时序显著性知识，使学生模型不仅学习预测结果，还学习教师的推理过程，从而在保持预测准确性的同时提高模型的可解释性和安全性。

Abstract: Knowledge distillation has proven effective for model compression by transferring knowledge from a larger network called the teacher to a smaller network called the student. Current knowledge distillation in time series is predominantly based on logit and feature aligning techniques originally developed for computer vision tasks. These methods do not explicitly account for temporal data and fall short in two key aspects. First, the mechanisms by which the transferred knowledge helps the student model learning process remain unclear due to uninterpretability of logits and features. Second, these methods transfer only limited knowledge, primarily replicating the teacher predictive accuracy. As a result, student models often produce predictive distributions that differ significantly from those of their teachers, hindering their safe substitution for teacher models. In this work, we propose transferring interpretable knowledge by extending conventional logit transfer to convey not just the right prediction but also the right reasoning of the teacher. Specifically, we induce other useful knowledge from the teacher logits termed temporal saliency which captures the importance of each input timestep to the teacher prediction. By training the student with Temporal Saliency Distillation we encourage it to make predictions based on the same input features as the teacher. Temporal Saliency Distillation requires no additional parameters or architecture specific assumptions. We demonstrate that Temporal Saliency Distillation effectively improves the performance of baseline methods while also achieving desirable properties beyond predictive accuracy. We hope our work establishes a new paradigm for interpretable knowledge distillation in time series analysis.

</details>


### [116] [MemKD: Memory-Discrepancy Knowledge Distillation for Efficient Time Series Classification](https://arxiv.org/abs/2601.04264)
*Nilushika Udayangani,Kishor Nandakishor,Marimuthu Palaniswami*

Main category: cs.LG

TL;DR: 提出MemKD知识蒸馏框架，专门针对时间序列模型，通过捕捉师生模型在时间序列子序列上的记忆保留差异，实现模型压缩同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型（如LSTM）在时间序列分析中表现出色，但计算复杂度和模型大小限制了在资源受限环境（如可穿戴设备、边缘计算）的部署。现有知识蒸馏方法主要针对计算机视觉任务，忽略了时间序列模型特有的时间依赖性和记忆保留特性。

Method: 提出Memory-Discrepancy Knowledge Distillation (MemKD)框架，使用专门的损失函数捕捉师生模型在时间序列子序列上的记忆保留差异，确保学生模型有效模仿教师模型的行为。

Result: MemKD显著优于现有最先进的知识蒸馏方法，将参数大小和内存使用减少约500倍，同时保持与教师模型相当的性能。

Conclusion: MemKD框架能够开发出适用于实时时间序列分析任务的紧凑高性能循环神经网络，解决了时间序列模型在资源受限环境中的部署挑战。

Abstract: Deep learning models, particularly recurrent neural networks and their variants, such as long short-term memory, have significantly advanced time series data analysis. These models capture complex, sequential patterns in time series, enabling real-time assessments. However, their high computational complexity and large model sizes pose challenges for deployment in resource-constrained environments, such as wearable devices and edge computing platforms. Knowledge Distillation (KD) offers a solution by transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student), thereby retaining high performance while reducing computational demands. Current KD methods, originally designed for computer vision tasks, neglect the unique temporal dependencies and memory retention characteristics of time series models. To this end, we propose a novel KD framework termed Memory-Discrepancy Knowledge Distillation (MemKD). MemKD leverages a specialized loss function to capture memory retention discrepancies between the teacher and student models across subsequences within time series data, ensuring that the student model effectively mimics the teacher model's behaviour. This approach facilitates the development of compact, high-performing recurrent neural networks suitable for real-time, time series analysis tasks. Our extensive experiments demonstrate that MemKD significantly outperforms state-of-the-art KD methods. It reduces parameter size and memory usage by approximately 500 times while maintaining comparable performance to the teacher model.

</details>


### [117] [Making Tunable Parameters State-Dependent in Weather and Climate Models with Reinforcement Learning](https://arxiv.org/abs/2601.04268)
*Pritthijit Nath,Sebastian Schemm,Henry Moss,Peter Haynes,Emily Shuckburgh,Mark J. Webb*

Main category: cs.LG

TL;DR: 该研究提出了一种使用强化学习在线学习参数化方案组件的框架，在理想化测试平台上评估了9种RL算法，发现TQC、DDPG和TD3表现最佳，联邦多智能体设置能实现地理专业化控制并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统天气和气候模型依赖离线调谐的固定系数参数化方案，导致持续偏差且无法适应底层物理过程，限制了模型的适应能力。

Method: 使用强化学习在线学习参数化方案组件，在三个理想化测试平台（SCBC、RCE、EBM）上评估9种RL算法，包括单智能体和联邦多智能体设置，使用面积加权RMSE、温度剖面和气压层诊断进行评估。

Result: TQC、DDPG和TD3算法在各项配置中表现最佳且收敛最稳定；EBM中单智能体RL优于静态参数调谐，热带和中纬度带改进最大；联邦多智能体设置（特别是六智能体DDPG配置）实现了地理专业化控制，收敛更快，在热带和中纬度带获得最低面积加权RMSE。

Conclusion: 强化学习能够提供技能性的状态依赖和机制感知的参数化方案，为数值模型中的在线学习提供了可扩展的途径，学习到的修正具有物理意义，能够减少偏差并稳定模型。

Abstract: Weather and climate models rely on parametrisations to represent unresolved sub-grid processes. Traditional schemes rely on fixed coefficients that are weakly constrained and tuned offline, contributing to persistent biases that limit their ability to adapt to the underlying physics. This study presents a framework that learns components of parametrisation schemes online as a function of the evolving model state using reinforcement learning (RL) and evaluates the resulting RL-driven parameter updates across a hierarchy of idealised testbeds spanning a simple climate bias correction (SCBC), a radiative-convective equilibrium (RCE), and a zonal mean energy balance model (EBM) with both single-agent and federated multi-agent settings. Across nine RL algorithms, Truncated Quantile Critics (TQC), Deep Deterministic Policy Gradient (DDPG), and Twin Delayed DDPG (TD3) achieved the highest skill and the most stable convergence across configurations, with performance assessed against a static baseline using area-weighted RMSE, temperature profile and pressure-level diagnostics. For the EBM, single-agent RL outperformed static parameter tuning with the strongest gains in tropical and mid-latitude bands, while federated RL on multi-agent setups enabled geographically specialised control and faster convergence, with a six-agent DDPG configuration using frequent aggregation yielding the lowest area-weighted RMSE across the tropics and mid-latitudes. The learnt corrections were also physically meaningful as agents modulated EBM radiative parameters to reduce meridional biases, adjusted RCE lapse rates to match vertical temperature errors, and stabilised SCBC heating increments to limit drift. Overall, results highlight RL to deliver skilful state-dependent, and regime-aware parametrisations, offering a scalable pathway for online learning within numerical models.

</details>


### [118] [Predictable Gradient Manifolds in Deep Learning: Temporal Path-Length and Intrinsic Rank as a Complexity Regime](https://arxiv.org/abs/2601.04270)
*Anherutowa Calvo*

Main category: cs.LG

TL;DR: 本文提出了一种可测量的框架来分析深度学习优化中的梯度可预测性和低维结构，引入了预测路径长度和可预测秩两个可计算量，将传统优化理论重新表述为依赖这些实际可观测性质而非最坏情况边界。


<details>
  <summary>Details</summary>
Motivation: 深度学习优化中存在未被最坏情况梯度边界捕捉的结构特征。经验上，训练轨迹中的梯度通常具有时间可预测性，并在低维子空间中演化。本文旨在通过可测量框架形式化这一观察。

Method: 引入两个可计算量：基于预测的路径长度（衡量梯度从历史信息中可预测的程度）和可预测秩（量化梯度增量的内在时间维度）。展示了如何将经典在线和非凸优化保证重新表述，使收敛性和遗憾明确依赖于这些量而非最坏情况变化。

Result: 在卷积网络、视觉Transformer、语言模型和合成控制任务中，发现梯度轨迹具有局部可预测性，并在时间上表现出强烈的低秩结构。这些性质在不同架构和优化器之间保持稳定，可以通过轻量级随机投影从记录的梯度中直接诊断。

Conclusion: 研究结果为理解现代深度学习中的优化动力学提供了统一视角，将标准训练重新定义为在低复杂度时间机制中运行。这一视角为自适应优化器、秩感知跟踪和基于预测的算法设计提供了新方向，这些设计都基于实际训练运行的可测量性质。

Abstract: Deep learning optimization exhibits structure that is not captured by worst-case gradient bounds. Empirically, gradients along training trajectories are often temporally predictable and evolve within a low-dimensional subspace. In this work we formalize this observation through a measurable framework for predictable gradient manifolds.
  We introduce two computable quantities: a prediction-based path length that measures how well gradients can be forecast from past information, and a predictable rank that quantifies the intrinsic temporal dimension of gradient increments. We show how classical online and nonconvex optimization guarantees can be restated so that convergence and regret depend explicitly on these quantities, rather than on worst-case variation.
  Across convolutional networks, vision transformers, language models, and synthetic control tasks, we find that gradient trajectories are locally predictable and exhibit strong low-rank structure over time. These properties are stable across architectures and optimizers, and can be diagnosed directly from logged gradients using lightweight random projections.
  Our results provide a unifying lens for understanding optimization dynamics in modern deep learning, reframing standard training as operating in a low-complexity temporal regime. This perspective suggests new directions for adaptive optimizers, rank-aware tracking, and prediction-based algorithm design grounded in measurable properties of real training runs.

</details>


### [119] [Unlocking the Pre-Trained Model as a Dual-Alignment Calibrator for Post-Trained LLMs](https://arxiv.org/abs/2601.04277)
*Beier Luo,Cheng Wang,Hongxin Wei,Sharon Li,Xuefeng Du*

Main category: cs.LG

TL;DR: 论文提出Dual-Align框架，通过双重对齐（置信度对齐和过程对齐）解决后训练语言模型的校准问题，无需监督数据即可同时修正置信度漂移和过程漂移。


<details>
  <summary>Details</summary>
Motivation: 后训练虽然能提升大语言模型性能，但往往会恶化置信度校准，导致系统性过度自信。现有的无监督后处理方法仅关注静态输出分布匹配，忽略了后训练引入的推理时动态变化。

Method: 提出Dual-Align无监督后处理框架，包含：1) 置信度对齐：通过最终分布匹配修正置信度漂移；2) 过程对齐：定位轨迹分叉层并重新对齐后续推理稳定性，仅学习单个温度参数同时修正两种漂移类型。

Result: 实验显示Dual-Align相比基线方法持续改进，显著减少校准误差，接近有监督oracle性能，且不牺牲后训练带来的性能增益。

Conclusion: 通过诊断校准误差的两个来源（置信度漂移和过程漂移），提出的双重对齐框架能有效解决后训练语言模型的校准问题，为模型校准提供了更全面的解决方案。

Abstract: Post-training improves large language models (LLMs) but often worsens confidence calibration, leading to systematic overconfidence. Recent unsupervised post-hoc methods for post-trained LMs (PoLMs) mitigate this by aligning PoLM confidence to that of well-calibrated pre-trained counterparts. However, framing calibration as static output-distribution matching overlooks the inference-time dynamics introduced by post-training. In particular, we show that calibration errors arise from two regimes: (i) confidence drift, where final confidence inflates despite largely consistent intermediate decision processes, and (ii) process drift, where intermediate inference pathways diverge. Guided by this diagnosis, we propose Dual-Align, an unsupervised post-hoc framework for dual alignment in confidence calibration. Dual-Align performs confidence alignment to correct confidence drift via final-distribution matching, and introduces process alignment to address process drift by locating the layer where trajectories diverge and realigning the stability of subsequent inference. This dual strategy learns a single temperature parameter that corrects both drift types without sacrificing post-training performance gains. Experiments show consistent improvements over baselines, reducing calibration errors and approaching a supervised oracle.

</details>


### [120] [ArtCognition: A Multimodal AI Framework for Affective State Sensing from Visual and Kinematic Drawing Cues](https://arxiv.org/abs/2601.04297)
*Behrad Binaei-Haghighi,Nafiseh Sadat Sajadi,Mehrad Liviyan,Reyhane Akhavan Kharazi,Fatemeh Amirkhani,Behnam Bahrak*

Main category: cs.LG

TL;DR: ArtCognition是一个多模态框架，通过分析HTP绘画测试的视觉特征和绘制过程的行为动力学线索，结合RAG架构进行心理状态评估。


<details>
  <summary>Details</summary>
Motivation: 人类情感和心理状态的客观评估是一个重大挑战，特别是通过非语言渠道。数字绘画作为一种丰富但未被充分探索的情感感知模态，具有潜力用于心理评估。

Method: 提出ArtCognition多模态框架，融合两种数据流：1) 最终艺术品的静态视觉特征（通过计算机视觉模型提取）；2) 绘制过程的动态行为动力学线索（如笔画速度、停顿、流畅度）。采用检索增强生成(RAG)架构，将低层特征与高层心理解释联系起来。

Result: 视觉和行为动力学线索的融合比单一模态提供更细致的评估。提取的多模态特征与标准化心理指标存在显著相关性，验证了该框架作为可扩展临床支持工具的潜力。

Conclusion: 该工作为非侵入性情感状态评估提供了新方法，为技术辅助心理健康护理开辟了新途径。

Abstract: The objective assessment of human affective and psychological states presents a significant challenge, particularly through non-verbal channels. This paper introduces digital drawing as a rich and underexplored modality for affective sensing. We present a novel multimodal framework, named ArtCognition, for the automated analysis of the House-Tree-Person (HTP) test, a widely used psychological instrument. ArtCognition uniquely fuses two distinct data streams: static visual features from the final artwork, captured by computer vision models, and dynamic behavioral kinematic cues derived from the drawing process itself, such as stroke speed, pauses, and smoothness. To bridge the gap between low-level features and high-level psychological interpretation, we employ a Retrieval-Augmented Generation (RAG) architecture. This grounds the analysis in established psychological knowledge, enhancing explainability and reducing the potential for model hallucination. Our results demonstrate that the fusion of visual and behavioral kinematic cues provides a more nuanced assessment than either modality alone. We show significant correlations between the extracted multimodal features and standardized psychological metrics, validating the framework's potential as a scalable tool to support clinicians. This work contributes a new methodology for non-intrusive affective state assessment and opens new avenues for technology-assisted mental healthcare.

</details>


### [121] [Generation of synthetic delay time series for air transport applications](https://arxiv.org/abs/2601.04279)
*Pau Esteve,Massimiliano Zanin*

Main category: cs.LG

TL;DR: 本文比较了三种生成机场延误时间序列的合成数据方法，发现简化遗传算法能生成与真实数据几乎无法区分且保持高变异性的时间序列，并在延迟传播检测问题上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 解决航空运输中的数据稀缺和隐私问题，通过生成真实可信的机场延误时间序列合成数据，为科学研究和应用提供支持。

Method: 比较了三种模型：两种基于深度学习算法，一种简化遗传算法方法，使用欧洲和美国的大量运营数据生成机场延误时间序列。

Result: 简化遗传算法生成的合成时间序列与真实数据几乎无法区分，同时保持了高变异性，在检测机场间延迟传播问题上得到有效验证。

Conclusion: 简化遗传算法在生成机场延误时间序列方面表现优异，合成的数据已向科学界公开，为解决航空运输中的数据稀缺和隐私问题提供了有效工具。

Abstract: The generation of synthetic data is receiving increasing attention from the scientific community, thanks to its ability to solve problems like data scarcity and privacy, and is starting to find applications in air transport. We here tackle the problem of generating synthetic, yet realistic, time series of delays at airports, starting from large collections of operations in Europe and the US. We specifically compare three models, two of them based on state of the art Deep Learning algorithms, and one simplified Genetic Algorithm approach. We show how the latter can generate time series that are almost indistinguishable from real ones, while maintaining a high variability. We further validate the resulting time series in a problem of detecting delay propagations between airports. We finally make the synthetic data available to the scientific community.

</details>


### [122] [LEGATO: Good Identity Unlearning Is Continuous](https://arxiv.org/abs/2601.04282)
*Qiang Chen,Chun-Wun Cheng,Xiu Su,Hongyan Xu,Xi Lin,Shan You,Angelica I. Aviles-Rivero,Yi Chen*

Main category: cs.LG

TL;DR: LEGATO提出了一种基于神经ODE的连续轨迹遗忘方法，用于生成模型的身份遗忘，解决了现有方法效率低、可控性差和灾难性崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法面临三大挑战：1) 效率低下，身份擦除需要微调所有模型参数；2) 可控性有限，遗忘强度无法控制且缺乏可解释性；3) 灾难性崩溃，随着遗忘进程模型保留能力急剧下降。

Method: LEGATO将身份遗忘建模为连续轨迹，使用轨迹一致的神经ODE。在预训练生成器中添加可微调的轻量级神经ODE适配器，通过ODE步长精确调节遗忘强度，并引入轨迹一致性约束防止灾难性崩溃。

Result: 在领域内和领域外身份遗忘基准测试中，LEGATO实现了最先进的遗忘性能，避免了灾难性崩溃，并显著减少了需要微调的参数量。

Conclusion: LEGATO通过将身份遗忘建模为连续轨迹，提供了一种高效、可控且稳定的机器遗忘方法，为生成模型的身份管理提供了新的解决方案。

Abstract: Machine unlearning has become a crucial role in enabling generative models trained on large datasets to remove sensitive, private, or copyright-protected data. However, existing machine unlearning methods face three challenges in learning to forget identity of generative models: 1) inefficient, where identity erasure requires fine-tuning all the model's parameters; 2) limited controllability, where forgetting intensity cannot be controlled and explainability is lacking; 3) catastrophic collapse, where the model's retention capability undergoes drastic degradation as forgetting progresses. Forgetting has typically been handled through discrete and unstable updates, often requiring full-model fine-tuning and leading to catastrophic collapse. In this work, we argue that identity forgetting should be modeled as a continuous trajectory, and introduce LEGATO - Learn to ForgEt Identity in GenerAtive Models via Trajectory-consistent Neural Ordinary Differential Equations. LEGATO augments pre-trained generators with fine-tunable lightweight Neural ODE adapters, enabling smooth, controllable forgetting while keeping the original model weights frozen. This formulation allows forgetting intensity to be precisely modulated via ODE step size, offering interpretability and robustness. To further ensure stability, we introduce trajectory consistency constraints that explicitly prevent catastrophic collapse during unlearning. Extensive experiments across in-domain and out-of-domain identity unlearning benchmarks show that LEGATO achieves state-of-the-art forgetting performance, avoids catastrophic collapse and reduces fine-tuned parameters.

</details>


### [123] [Mitigating Position-Shift Failures in Text-Based Modular Arithmetic via Position Curriculum and Template Diversity](https://arxiv.org/abs/2601.04283)
*Nikolay Yudin*

Main category: cs.LG

TL;DR: 该研究基于grokking文献的见解，研究字符级Transformer在文本中执行模加法的鲁棒性，发现模型在输入格式变化时存在灾难性失败，并提出结合边界标记、位置课程、多样化模板和一致性训练的训练方法，显著提升了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索字符级Transformer在执行模加法任务时的鲁棒性，特别是针对输入格式变化（如字符位置偏移和自然语言模板变化）的泛化能力，而不仅仅是分布内准确性。

Method: 研究方法包括：1) 使用字符级Transformer训练模加法任务；2) 识别位置偏移和模板OOD的失败模式；3) 提出包含四个要素的训练方法：显式表达式边界标记、扩大绝对位置范围的位置课程、多样化模板混合、以及每个示例多个变体的一致性训练。

Result: 实验结果表明，基线模型在分布内表现良好，但在位置偏移和模板OOD下崩溃。提出的训练方法在三个随机种子下显著提高了对位置偏移和模板OOD的鲁棒性，同时保持高分布内准确性，而ALiBi风格的消融实验在该设置下无法学习任务。

Conclusion: 结论表明，在噪声监督下引导程序泛化需要显式训练数据分布中不存在的恒定性，研究提供了可重复的评估协议和实验成果，强调了训练不变性对鲁棒性的重要性。

Abstract: Building on insights from the grokking literature, we study character-level Transformers trained to compute modular addition from text, and focus on robustness under input-format variation rather than only in-distribution accuracy. We identify a previously under-emphasized failure mode: models that achieve high in-distribution accuracy can fail catastrophically when the same expression is shifted to different absolute character positions ("position shift") or presented under out-of-distribution natural-language templates. Using a disjoint-pair split over all ordered pairs for p=97, we show that a baseline model reaches strong in-distribution performance yet collapses under position shift and template OOD. We then introduce a simple training recipe that combines (i) explicit expression boundary markers, (ii) position curriculum that broadens the range of absolute positions seen during training, (iii) diverse template mixtures, and (iv) consistency training across multiple variants per example. Across three seeds, this intervention substantially improves robustness to position shift and template OOD while maintaining high in-distribution accuracy, whereas an ALiBi-style ablation fails to learn the task under our setup. Our results suggest that steering procedural generalization under noisy supervision benefits from explicitly training invariances that are otherwise absent from the data distribution, and we provide a reproducible evaluation protocol and artifacts.

</details>


### [124] [Enhancing Robustness of Asynchronous EEG-Based Movement Prediction using Classifier Ensembles](https://arxiv.org/abs/2601.04286)
*Niklas Kueper,Kartik Chari,Elsa Andrea Kirchner*

Main category: cs.LG

TL;DR: 研究探索了分类器集成和滑动窗口后处理技术，用于增强基于EEG信号的异步运动意图检测，在伪在线评估中显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 中风是导致残疾的主要原因之一，机器人辅助运动治疗是一种有前景的康复方法。为了实现患者自主发起的康复训练，需要准确检测患者的运动意图来触发机器人辅助。虽然可以从脑电图信号中检测运动意图，但在线异步分类特别具有挑战性。

Method: 分析了14名健康受试者执行自主发起手臂运动的两个EEG数据集。进行了离线和伪在线评估，比较了支持向量机、多层感知器和EEGNet分类模型的集成组合，并研究了滑动窗口后处理技术的效果。

Result: 伪在线评估结果显示，在最佳后处理窗口数量下，两种模型集成显著优于最佳单模型。对于单模型，增加后处理窗口数量显著提高了分类性能。有趣的是，在离线评估中，最佳单模型与分类器集成之间没有显著差异。

Conclusion: 分类器集成和适当的后处理方法能有效增强基于EEG信号的异步运动意图检测。特别是分类器集成方法在线分类中的改进比离线分类更显著，并能减少误检（早期误报）。

Abstract: Objective: Stroke is one of the leading causes of disabilities. One promising approach is to extend the rehabilitation with self-initiated robot-assisted movement therapy. To enable this, it is required to detect the patient's intention to move to trigger the assistance of a robotic device. This intention to move can be detected from human surface electroencephalography (EEG) signals; however, it is particularly challenging to decode when classifications are performed online and asynchronously. In this work, the effectiveness of classifier ensembles and a sliding-window postprocessing technique was investigated to enhance the robustness of such asynchronous classification. Approach: To investigate the effectiveness of classifier ensembles and a sliding-window postprocessing, two EEG datasets with 14 healthy subjects who performed self-initiated arm movements were analyzed. Offline and pseudo-online evaluations were conducted to compare ensemble combinations of the support vector machine (SVM), multilayer perceptron (MLP), and EEGNet classification models. Results: The results of the pseudo-online evaluation show that the two model ensembles significantly outperformed the best single model for the optimal number of postprocessing windows. In particular, for single models, an increased number of postprocessing windows significantly improved classification performances. Interestingly, we found no significant improvements between performances of the best single model and classifier ensembles in the offline evaluation. Significance: We demonstrated that classifier ensembles and appropriate postprocessing methods effectively enhance the asynchronous detection of movement intentions from EEG signals. In particular, the classifier ensemble approach yields greater improvements in online classification than in offline classification, and reduces false detections, i.e., early false positives.

</details>


### [125] [Online Action-Stacking Improves Reinforcement Learning Performance for Air Traffic Control](https://arxiv.org/abs/2601.04287)
*Ben Carvell,George De Ath,Eseoghene Benjamin,Richard Everson*

Main category: cs.LG

TL;DR: 在线动作堆叠是一种推理时包装器，可将强化学习策略的简单增量动作编译为符合空中交通管制要求的复合指令，使用小动作空间训练但实现与大动作空间相当的性能。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习公式与空中交通管制操作需求之间存在关键差距，需要将简单的增量动作转换为符合领域规范的复合指令，同时保持训练动作空间的简洁性。

Method: 使用在线动作堆叠作为推理时包装器，训练时采用简单的增量航向或高度调整动作，配合动作阻尼惩罚减少指令频率；推理时将原始动作突发编译为领域适当的复合许可。

Result: 在横向导航实验中，动作堆叠显著减少了指令数量，仅使用5个动作就达到了与37维动作空间训练策略相当的性能，有效缩小了RL公式与ATC操作要求之间的差距。

Conclusion: 在线动作堆叠为强化学习在复杂控制场景中的应用提供了简单有效的机制，能够将简单动作空间训练的智能体输出转换为符合领域规范的复合指令。

Abstract: We introduce online action-stacking, an inference-time wrapper for reinforcement learning policies that produces realistic air traffic control commands while allowing training on a much smaller discrete action space. Policies are trained with simple incremental heading or level adjustments, together with an action-damping penalty that reduces instruction frequency and leads agents to issue commands in short bursts. At inference, online action-stacking compiles these bursts of primitive actions into domain-appropriate compound clearances. Using Proximal Policy Optimisation and the BluebirdDT digital twin platform, we train agents to navigate aircraft along lateral routes, manage climb and descent to target flight levels, and perform two-aircraft collision avoidance under a minimum separation constraint. In our lateral navigation experiments, action stacking greatly reduces the number of issued instructions relative to a damped baseline and achieves comparable performance to a policy trained with a 37-dimensional action space, despite operating with only five actions. These results indicate that online action-stacking helps bridge a key gap between standard reinforcement learning formulations and operational ATC requirements, and provides a simple mechanism for scaling to more complex control scenarios.

</details>


### [126] [Transformer-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes](https://arxiv.org/abs/2601.04299)
*Pir Bakhsh Khokhar,Carmine Gravino,Fabio Palomba,Sule Yildrim Yayilgan,Sarang Shaikh*

Main category: cs.LG

TL;DR: 提出可解释深度学习框架，整合连续血糖监测与实验室数据，识别1型糖尿病的五种代谢表型，支持超越单一生物标志物的风险分层。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病具有高度代谢异质性，传统生物标志物如糖化血红蛋白无法充分表征疾病特征，需要更全面的多模态分析方法。

Method: 开发可解释深度学习框架，整合连续血糖监测数据和实验室资料，使用Transformer编码器建模跨模态时间依赖性，通过高斯混合模型识别潜在代谢表型，利用注意力可视化和SHAP特征归因实现模型可解释性。

Result: 在577名1型糖尿病患者中识别出五种潜在代谢表型，从代谢稳定到心血管代谢风险升高不等；这些表型在血糖控制、脂质代谢、肾功能标志物和促甲状腺激素水平方面表现出显著差异；表型成员与高血压、心肌梗死和心力衰竭存在统计学显著关联。

Conclusion: 该可解释多模态时间嵌入框架揭示了1型糖尿病中生理学上一致的代谢亚组，支持超越单一生物标志物的风险分层，为个性化医疗提供新方法。

Abstract: Type 1 diabetes (T1D) is a highly metabolically heterogeneous disease that cannot be adequately characterized by conventional biomarkers such as glycated hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn multimodal temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a transformer encoder, while latent metabolic phenotypes are identified via Gaussian mixture modeling. Model interpretability is achieved through transformer attention visualization and SHAP-based feature attribution. Five latent metabolic phenotypes, ranging from metabolic stability to elevated cardiometabolic risk, were identified among 577 individuals with T1D. These phenotypes exhibit distinct biochemical profiles, including differences in glycemic control, lipid metabolism, renal markers, and thyrotropin (TSH) levels. Attention analysis highlights glucose variability as a dominant temporal factor, while SHAP analysis identifies HbA1c, triglycerides, cholesterol, creatinine, and TSH as key contributors to phenotype differentiation. Phenotype membership shows statistically significant, albeit modest, associations with hypertension, myocardial infarction, and heart failure. Overall, this explainable multimodal temporal embedding framework reveals physiologically coherent metabolic subgroups in T1D and supports risk stratification beyond single biomarkers.

</details>


### [127] [Quantifying the Effect of Test Set Contamination on Generative Evaluations](https://arxiv.org/abs/2601.04301)
*Rylan Schaeffer,Joshua Kazdan,Baber Abbasi,Ken Ziyu Liu,Brando Miranda,Ahmed Ahmed,Abhay Puri,Niloofar Mireshghallah,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 该研究量化了测试集污染对生成式评估的影响，发现即使单个测试集副本也会显著降低模型损失，而监督微调、采样温度和解长度等因素会调节记忆效应。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI系统在网页规模数据上进行预训练，测试集污染已成为准确评估其能力的关键问题。虽然已有研究深入探讨了测试集污染对判别式评估（如多项选择题回答）的影响，但对生成式评估中测试集污染影响的研究相对较少。

Method: 研究通过在MATH基准测试中混合网页数据来预训练语言模型，探索不同模型大小和测试集副本数量对预训练语料库的污染影响。使用缩放定律分析性能变化，并研究进一步训练（包括使用新数据进行过度训练和在训练集上进行监督微调）以及推理过程中的因素（如采样温度和解决方案长度）如何调节记忆效应。

Result: 性能随污染程度和模型大小而提高。令人惊讶的发现是：即使包含单个测试集副本，模型也能达到比未污染语料库训练的不可约误差更低的损失。过度训练使用新数据可减少污染影响，而监督微调对测试数据性能的影响取决于预训练污染程度。在推理过程中，高采样温度可缓解污染效应，较长的解决方案比较短的解决方案指数级更难记忆。

Conclusion: 通过描述生成和记忆之间的相互作用，本研究强调了AI系统可信评估的新复杂性层次，为生成式评估中的测试集污染问题提供了重要见解。

Abstract: As frontier AI systems are pretrained on web-scale data, test set contamination has become a critical concern for accurately assessing their capabilities. While research has thoroughly investigated the impact of test set contamination on discriminative evaluations like multiple-choice question-answering, comparatively little research has studied the impact of test set contamination on generative evaluations. In this work, we quantitatively assess the effect of test set contamination on generative evaluations through the language model lifecycle. We pretrain language models on mixtures of web data and the MATH benchmark, sweeping model sizes and number of test set replicas contaminating the pretraining corpus; performance improves with contamination and model size. Using scaling laws, we make a surprising discovery: including even a single test set replica enables models to achieve lower loss than the irreducible error of training on the uncontaminated corpus. We then study further training: overtraining with fresh data reduces the effects of contamination, whereas supervised finetuning on the training set can either increase or decrease performance on test data, depending on the amount of pretraining contamination. Finally, at inference, we identify factors that modulate memorization: high sampling temperatures mitigate contamination effects, and longer solutions are exponentially more difficult to memorize than shorter ones, presenting a contrast with discriminative evaluations, where solutions are only a few tokens in length. By characterizing how generation and memorization interact, we highlight a new layer of complexity for trustworthy evaluation of AI systems.

</details>


### [128] [Causally-Aware Information Bottleneck for Domain Adaptation](https://arxiv.org/abs/2601.04361)
*Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: 该论文提出了一种因果领域自适应方法，用于在目标变量在目标域完全缺失的情况下进行填补。通过学习机制稳定的紧凑表示，该方法在源域训练后可直接零样本部署到目标域。


<details>
  <summary>Details</summary>
Motivation: 解决因果系统中常见的领域自适应问题：目标变量在源域可观测但在目标域完全缺失。需要在各种分布偏移下从目标域的其他观测变量中填补缺失的目标变量。

Method: 1. 学习紧凑的机制稳定表示，保留预测目标所需信息，丢弃虚假变异
2. 对于线性高斯因果模型：推导闭式高斯信息瓶颈（GIB）解，简化为典型相关分析（CCA）式投影，可选择DAG感知选项
3. 对于非线性或非高斯数据：引入变分信息瓶颈（VIB）编码器-预测器框架，可扩展到高维数据

Result: 在合成和真实数据集上，该方法始终获得准确的填补结果。支持高维因果模型的实用应用，为因果领域自适应提供了统一、轻量级的工具包。

Conclusion: 提出的方法能够有效处理因果领域自适应中的目标变量填补问题，通过机制稳定的表示学习实现源域训练、目标域零样本部署，为高维因果模型提供了实用的解决方案。

Abstract: We tackle a common domain adaptation setting in causal systems. In this setting, the target variable is observed in the source domain but is entirely missing in the target domain. We aim to impute the target variable in the target domain from the remaining observed variables under various shifts. We frame this as learning a compact, mechanism-stable representation. This representation preserves information relevant for predicting the target while discarding spurious variation. For linear Gaussian causal models, we derive a closed-form Gaussian Information Bottleneck (GIB) solution. This solution reduces to a canonical correlation analysis (CCA)-style projection and offers Directed Acyclic Graph (DAG)-aware options when desired. For nonlinear or non-Gaussian data, we introduce a Variational Information Bottleneck (VIB) encoder-predictor. This approach scales to high dimensions and can be trained on source data and deployed zero-shot to the target domain. Across synthetic and real datasets, our approach consistently attains accurate imputations, supporting practical use in high-dimensional causal models and furnishing a unified, lightweight toolkit for causal domain adaptation.

</details>


### [129] [Phasor Agents: Oscillatory Graphs with Three-Factor Plasticity and Sleep-Staged Learning](https://arxiv.org/abs/2601.04362)
*Rodja Trappe*

Main category: cs.LG

TL;DR: Phasor Agents是一种基于耦合Stuart-Landau振荡器图（Phasor Graph）的动态系统，使用振荡器相位作为表示媒介，通过三因素局部可塑性学习耦合权重，无需反向传播。系统采用醒睡分离机制解决稳定性问题，实验验证了各机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 在振荡器基质中，在线权重更新可能导致网络进入不良状态（如全局同步），破坏表示多样性。需要解决振荡器系统的稳定性问题，同时实现有效的学习和规划能力。

Method: 1. 使用Phasor Graph作为内部状态，其中每个Stuart-Landau振荡器作为抽象计算单元；2. 通过三因素局部可塑性学习耦合权重（资格迹门控+稀疏全局调制器+振荡定时写入窗口）；3. 采用醒睡分离机制：清醒期标记，离线巩固期（深睡眠期门控捕获和REM期回放扰动）

Result: 1. 资格迹在延迟调制下保持信用分配；2. 压缩进展信号通过时间戳洗牌控制；3. 相位相干检索在噪声下达到4倍扩散基线；4. 醒睡分离在匹配权重范数预算下将稳定学习扩展67%；5. REM回放将迷宫成功率提高45.5个百分点；6. 出现Tolman式潜在学习特征（无奖励探索后立即具备能力和绕道优势）

Conclusion: Phasor Agents通过振荡器相位表示和三因素局部可塑性，结合醒睡分离机制，实现了稳定的学习、规划和内部模型构建，为无需反向传播的神经形态计算提供了新途径。

Abstract: Phasor Agents are dynamical systems whose internal state is a Phasor Graph: a weighted graph of coupled Stuart-Landau oscillators. A Stuart-Landau oscillator is a minimal stable "rhythm generator" (the normal form near a Hopf bifurcation); each oscillator is treated as an abstract computational unit (inspired by, but not claiming to model, biological oscillatory populations). In this interpretation, oscillator phase tracks relative timing (coherence), while amplitude tracks local gain or activity. Relative phase structure serves as a representational medium; coupling weights are learned via three-factor local plasticity - eligibility traces gated by sparse global modulators and oscillation-timed write windows - without backpropagation.
  A central challenge in oscillatory substrates is stability: online weight updates can drive the network into unwanted regimes (e.g., global synchrony), collapsing representational diversity. We therefore separate wake tagging from offline consolidation, inspired by synaptic tagging-and-capture and sleep-stage dynamics: deep-sleep-like gated capture commits tagged changes safely, while REM-like replay reconstructs and perturbs experience for planning.
  A staged experiment suite validates each mechanism with ablations and falsifiers: eligibility traces preserve credit under delayed modulation; compression-progress signals pass timestamp-shuffle controls; phase-coherent retrieval reaches 4x diffusive baselines under noise; wake/sleep separation expands stable learning by 67 percent under matched weight-norm budgets; REM replay improves maze success rate by +45.5 percentage points; and a Tolman-style latent-learning signature - immediate competence and detour advantage after unrewarded exploration, consistent with an internal model - emerges from replay (Tolman, 1948).
  The codebase and all artifacts are open-source.

</details>


### [130] [Survival Dynamics of Neural and Programmatic Policies in Evolutionary Reinforcement Learning](https://arxiv.org/abs/2601.04365)
*Anton Roupassov-Ruiz,Yiyang Zuo*

Main category: cs.LG

TL;DR: 该研究比较了程序化策略（PERL）与神经网络策略（NERL）在进化强化学习任务中的表现，发现程序化策略在人工生命测试环境中平均多存活201.69步，表现显著优于神经网络策略。


<details>
  <summary>Details</summary>
Motivation: 传统进化强化学习中使用的小型神经网络策略缺乏明确的模块化结构，限制了行为解释性。研究者希望探索程序化策略是否能达到甚至超越神经网络策略的性能。

Method: 使用可微分软决策列表（SDDL）实现程序化策略，并首次提供了1992年经典人工生命进化强化学习测试平台的完整开源重实现。通过4000次独立试验进行严格的生存分析，使用Kaplan-Meier曲线和限制平均生存时间（RMST）等统计方法。

Result: 程序化策略（PERL）在生存概率上显著优于神经网络策略（NERL），平均多存活201.69步。仅使用学习（无进化）的SDDL代理比同时使用学习和进化的神经网络代理平均多存活73.67步。

Conclusion: 程序化策略在人工生命环境中能够超越神经网络策略的生存性能，为进化强化学习提供了新的可解释性策略表示方法。

Abstract: In evolutionary reinforcement learning tasks (ERL), agent policies are often encoded as small artificial neural networks (NERL). Such representations lack explicit modular structure, limiting behavioral interpretation. We investigate whether programmatic policies (PERL), implemented as soft, differentiable decision lists (SDDL), can match the performance of NERL. To support reproducible evaluation, we provide the first fully specified and open-source reimplementation of the classic 1992 Artificial Life (ALife) ERL testbed. We conduct a rigorous survival analysis across 4000 independent trials utilizing Kaplan-Meier curves and Restricted Mean Survival Time (RMST) metrics absent in the original study. We find a statistically significant difference in survival probability between PERL and NERL. PERL agents survive on average 201.69 steps longer than NERL agents. Moreover, SDDL agents using learning alone (no evolution) survive on average 73.67 steps longer than neural agents using both learning and evaluation. These results demonstrate that programmatic policies can exceed the survival performance of neural policies in ALife.

</details>


### [131] [Machine Learning Model for Sparse PCM Completion](https://arxiv.org/abs/2601.04366)
*Selcuk Koyuncu,Ronak Nouri,Stephen Providence*

Main category: cs.LG

TL;DR: 提出了一种结合经典成对比较矩阵方法和图学习技术的机器学习模型，用于处理稀疏成对比较矩阵


<details>
  <summary>Details</summary>
Motivation: 针对稀疏成对比较矩阵的处理需求，结合传统方法和现代机器学习技术，提高处理效率和可扩展性

Method: 将经典成对比较矩阵方法与基于图的学习技术相结合，构建机器学习模型来处理稀疏成对比较矩阵

Result: 通过数值结果证明了所提方法的有效性和可扩展性

Conclusion: 提出的方法能够有效处理稀疏成对比较矩阵，具有良好的可扩展性

Abstract: In this paper, we propose a machine learning model for sparse pairwise comparison matrices (PCMs), combining classical PCM approaches with graph-based learning techniques. Numerical results are provided to demonstrate the effectiveness and scalability of the proposed method.

</details>


### [132] [Aligned explanations in neural networks](https://arxiv.org/abs/2601.04378)
*Corentin Lobet,Francesca Chiaromonte*

Main category: cs.LG

TL;DR: 论文提出PiNets作为可解释深度学习框架，通过伪线性网络实现实例级线性预测，确保解释与预测直接对齐，而非事后合理化。


<details>
  <summary>Details</summary>
Motivation: 当前特征归因方法大多只是对黑盒模型的表面解释，未能真正反映模型的预测过程。作者认为解释对齐是预测任务可信度的关键，解释必须与预测直接关联。

Method: 提出模型可读性作为设计原则，并开发PiNets框架。PiNets是伪线性网络，能在任意特征空间中产生实例级线性预测，使其具有线性可读性。

Result: 在图像分类和分割任务上展示了PiNets的应用，证明其产生的解释不仅对齐，还在多个标准下具有忠实性。

Conclusion: PiNets通过确保解释与预测过程直接对齐，提供了比传统特征归因方法更可信的解释框架，实现了真正的模型可读性。

Abstract: Feature attribution is the dominant paradigm for explaining deep neural networks. However, most existing methods only loosely reflect the model's prediction-making process, thereby merely white-painting the black box. We argue that explanatory alignment is a key aspect of trustworthiness in prediction tasks: explanations must be directly linked to predictions, rather than serving as post-hoc rationalizations. We present model readability as a design principle enabling alignment, and PiNets as a modeling framework to pursue it in a deep learning context. PiNets are pseudo-linear networks that produce instance-wise linear predictions in an arbitrary feature space, making them linearly readable. We illustrate their use on image classification and segmentation tasks, demonstrating how PiNets produce explanations that are faithful across multiple criteria in addition to alignment.

</details>


### [133] [Enhanced-FQL($λ$), an Efficient and Interpretable RL with novel Fuzzy Eligibility Traces and Segmented Experience Replay](https://arxiv.org/abs/2601.04392)
*Mohsen Jalaeian-Farimani*

Main category: cs.LG

TL;DR: 论文提出Enhanced-FQL(λ)框架，将模糊化资格迹和分段经验回放集成到模糊Q学习中，用于连续控制任务，在保持可解释性的同时提升样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 针对连续控制任务，现有深度强化学习方法虽然性能强大但缺乏可解释性且计算复杂度高，而传统模糊强化学习方法在样本效率和稳定性方面存在不足。需要在保持模糊系统可解释性优势的同时，提升学习效率和稳定性。

Method: 提出Enhanced-FQL(λ)框架，核心包括：1) 模糊化资格迹(FET)用于稳定的多步信用分配；2) 分段经验回放(SER)机制提升样本效率；3) 使用模糊规则库而非复杂神经网络架构；4) 基于模糊化贝尔曼方程(FBE)进行学习。

Result: 理论分析证明方法在标准假设下收敛。在连续控制领域的广泛评估显示，相比n步模糊TD和模糊SARSA(λ)基线，Enhanced-FQL(λ)具有更优的样本效率和更低的方差，同时计算复杂度远低于DDPG等深度RL方法。

Conclusion: Enhanced-FQL(λ)框架结合了可解释性、计算效率和理论收敛保证，特别适合对透明度和资源约束有严格要求的安全关键应用，为连续控制任务提供了一种平衡性能与可解释性的解决方案。

Abstract: This paper introduces a fuzzy reinforcement learning framework, Enhanced-FQL($λ$), that integrates novel Fuzzified Eligibility Traces (FET) and Segmented Experience Replay (SER) into fuzzy Q-learning with Fuzzified Bellman Equation (FBE) for continuous control tasks. The proposed approach employs an interpretable fuzzy rule base instead of complex neural architectures, while maintaining competitive performance through two key innovations: a fuzzified Bellman equation with eligibility traces for stable multi-step credit assignment, and a memory-efficient segment-based experience replay mechanism for enhanced sample efficiency. Theoretical analysis proves the proposed method convergence under standard assumptions. Extensive evaluations in continuous control domains demonstrate that Enhanced-FQL($λ$) achieves superior sample efficiency and reduced variance compared to n-step fuzzy TD and fuzzy SARSA($λ$) baselines, while maintaining substantially lower computational complexity than deep RL alternatives such as DDPG. The framework's inherent interpretability, combined with its computational efficiency and theoretical convergence guarantees, makes it particularly suitable for safety-critical applications where transparency and resource constraints are essential.

</details>


### [134] [Rate or Fate? RLV$^\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards](https://arxiv.org/abs/2601.04411)
*Ali Rad,Khashayar Filom,Darioush Keivan,Peyman Mohajerin Esfahani,Ehsan Kamalinejad*

Main category: cs.LG

TL;DR: RLVR（可验证奖励强化学习）训练LLM时，验证噪声可能导致学习失败而非仅减慢速度，存在基于Youden指数的相变边界


<details>
  <summary>Details</summary>
Motivation: 实际应用中验证器几乎从不完美（单元测试有限、人工标签不准确、LLM判断有噪声），在编程等困难领域问题更严重。需要探究验证噪声是仅减慢学习速度，还是可能改变学习结果

Method: 建立可分析的多臂老虎机视角的RLVR动力学模型，使用GRPO实例化并在控制实验中验证。建模假阳性和假阴性，将补全分组为重复推理模式，得到概率单纯形上的复制器式（自然选择）流

Result: 动力学解耦为正确模式内竞争和一维错误模式质量演化，其漂移仅由Youden指数J=TPR-FPR决定。存在尖锐相变：J>0时错误模式趋于灭绝（学习）；J=0时过程中性；J<0时错误模式放大直至主导（反学习和崩溃）。在学习机制J>0时，噪声主要重新缩放收敛时间

Conclusion: 验证噪声在Youden指数J>0时主要影响收敛速度而非结果，但J<0时会导致学习失败。该框架为分析RLVR稳定性、收敛性和算法干预提供了通用视角

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a simple but powerful paradigm for training LLMs: sample a completion, verify it, and update. In practice, however, the verifier is almost never clean--unit tests probe only limited corner cases; human and synthetic labels are imperfect; and LLM judges (e.g., RLAIF) are noisy and can be exploited--and this problem worsens on harder domains (especially coding) where tests are sparse and increasingly model-generated. We ask a pragmatic question: does the verification noise merely slow down the learning (rate), or can it flip the outcome (fate)?
  To address this, we develop an analytically tractable multi-armed bandit view of RLVR dynamics, instantiated with GRPO and validated in controlled experiments. Modeling false positives and false negatives and grouping completions into recurring reasoning modes yields a replicator-style (natural-selection) flow on the probability simplex. The dynamics decouples into within-correct-mode competition and a one-dimensional evolution for the mass on incorrect modes, whose drift is determined solely by Youden's index J=TPR-FPR. This yields a sharp phase transition: when J>0, the incorrect mass is driven toward extinction (learning); when J=0, the process is neutral; and when J<0, incorrect modes amplify until they dominate (anti-learning and collapse). In the learning regime J>0, noise primarily rescales convergence time ("rate, not fate"). Experiments on verifiable programming tasks under synthetic noise reproduce the predicted J=0 boundary. Beyond noise, the framework offers a general lens for analyzing RLVR stability, convergence, and algorithmic interventions.

</details>


### [135] [Distribution-Guided and Constrained Quantum Machine Unlearning](https://arxiv.org/abs/2601.04413)
*Nausherwan Malik,Zubair Khalid,Muhammad Faryad*

Main category: cs.LG

TL;DR: 本文提出了一种基于分布引导的量子机器学习遗忘框架，通过可调目标分布和锚点约束实现类级遗忘，在保留模型性能的同时有效抑制遗忘类置信度。


<details>
  <summary>Details</summary>
Motivation: 当前量子机器学习遗忘方法主要依赖固定、均匀的目标分布，未能明确控制遗忘与保留模型行为之间的权衡，需要更可靠和可解释的量子机器学习遗忘方法。

Method: 提出分布引导的量子机器学习遗忘框架，将遗忘视为约束优化问题：1) 引入基于模型相似性统计的可调目标分布，解耦遗忘类置信度抑制与保留类重新分布假设；2) 结合基于锚点的保留约束，在选定保留数据上明确保持预测行为，限制与原模型的偏差。

Result: 在Iris和Covertype数据集上训练变分量子分类器的评估结果显示：1) 有效抑制遗忘类置信度；2) 保留类性能退化最小；3) 相比均匀目标遗忘方法，更接近重新训练的金标准基线模型。

Conclusion: 目标分布设计和基于约束的公式对于实现可靠和可解释的量子机器学习遗忘至关重要，该方法在控制遗忘与保留权衡方面表现出优越性。

Abstract: Machine unlearning aims to remove the influence of specific training data from a learned model without full retraining. While recent work has begun to explore unlearning in quantum machine learning, existing approaches largely rely on fixed, uniform target distributions and do not explicitly control the trade-off between forgetting and retained model behaviour. In this work, we propose a distribution-guided framework for class-level quantum machine unlearning that treats unlearning as a constrained optimization problem. Our method introduces a tunable target distribution derived from model similarity statistics, decoupling the suppression of forgotten-class confidence from assumptions about redistribution among retained classes. We further incorporate an anchor-based preservation constraint that explicitly maintains predictive behaviour on selected retained data, yielding a controlled optimization trajectory that limits deviation from the original model. We evaluate the approach on variational quantum classifiers trained on the Iris and Covertype datasets. Results demonstrate sharp suppression of forgotten-class confidence, minimal degradation of retained-class performance, and closer alignment with the gold retrained model baselines compared to uniform-target unlearning. These findings highlight the importance of target design and constraint-based formulations for reliable and interpretable quantum machine unlearning.

</details>


### [136] [Improving and Accelerating Offline RL in Large Discrete Action Spaces with Structured Policy Initialization](https://arxiv.org/abs/2601.04441)
*Matthew Landers,Taylor W. Killian,Thomas Hartvigsen,Afsaneh Doryab*

Main category: cs.LG

TL;DR: SPIN框架通过两阶段方法解决离散组合动作空间中的强化学习问题：先预训练动作结构模型捕捉有效动作流形，再冻结该表示训练轻量策略头，显著提升性能并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么假设子动作独立导致动作不连贯或无效，要么联合学习动作结构和控制导致训练缓慢不稳定，需要一种能有效处理离散组合动作空间的方法。

Method: 提出结构化策略初始化(SPIN)两阶段框架：1)预训练动作结构模型(ASM)捕捉有效动作流形；2)冻结ASM表示，训练轻量策略头进行控制。

Result: 在挑战性离散DM Control基准测试中，SPIN相比现有最佳方法平均回报提升达39%，收敛时间减少达12.8倍。

Conclusion: SPIN通过分离动作结构学习与控制学习，有效解决了离散组合动作空间的强化学习问题，在性能和效率上均有显著提升。

Abstract: Reinforcement learning in discrete combinatorial action spaces requires searching over exponentially many joint actions to simultaneously select multiple sub-actions that form coherent combinations. Existing approaches either simplify policy learning by assuming independence across sub-actions, which often yields incoherent or invalid actions, or attempt to learn action structure and control jointly, which is slow and unstable. We introduce Structured Policy Initialization (SPIN), a two-stage framework that first pre-trains an Action Structure Model (ASM) to capture the manifold of valid actions, then freezes this representation and trains lightweight policy heads for control. On challenging discrete DM Control benchmarks, SPIN improves average return by up to 39% over the state of the art while reducing time to convergence by up to 12.8$\times$.

</details>


### [137] [When Predictions Shape Reality: A Socio-Technical Synthesis of Performative Predictions in Machine Learning](https://arxiv.org/abs/2601.04447)
*Gal Fybish,Teo Susnjak*

Main category: cs.LG

TL;DR: 这篇SoK论文系统化地综述了执行性预测（performative prediction）领域，提出了"执行强度vs影响矩阵"评估框架，帮助从业者评估预测模型部署中的执行性风险并选择适当干预措施。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在高风险领域部署时，其预测会主动塑造其运行环境，这种现象称为执行性预测。这种动态性可能导致反馈循环、性能问题和社会风险。现有文献缺乏系统化的概念梳理和实践指导，需要提供一个综合性的知识体系。

Method: 采用系统化知识（SoK）方法，全面综述执行性预测文献。分析执行性表现的主要机制，建立相关风险的类型学，调查文献中提出的解决方案，并开发"执行强度vs影响矩阵"评估框架。

Result: 提出了一个实用的评估框架——"执行强度vs影响矩阵"，帮助从业者评估预测模型部署中的执行性影响和严重程度，并选择适当的算法或人工干预级别。系统化梳理了执行性预测的概念、机制、风险和解决方案。

Conclusion: 执行性预测是一个重要且日益增长的研究领域，需要系统化的理解和实践指导。提出的评估框架为从业者提供了实用的工具来识别和管理执行性风险，促进更负责任和有效的机器学习模型部署。

Abstract: Machine learning models are increasingly used in high-stakes domains where their predictions can actively shape the environments in which they operate, a phenomenon known as performative prediction. This dynamic, in which the deployment of the model influences the very outcome it seeks to predict, can lead to unintended consequences, including feedback loops, performance issues, and significant societal risks. While the literature in the field has grown rapidly in recent years, a socio-technical synthesis that systemises the phenomenon concepts and provides practical guidance has been lacking. This Systematisation of Knowledge (SoK) addresses this gap by providing a comprehensive review of the literature on performative predictions. We provide an overview of the primary mechanisms through which performativity manifests, present a typology of associated risks, and survey the proposed solutions offered in the literature. Our primary contribution is the ``Performative Strength vs. Impact Matrix" assessment framework. This practical tool is designed to help practitioners assess the potential influence and severity of performativity on their deployed predictive models and select the appropriate level of algorithmic or human intervention.

</details>


### [138] [Explainable Admission-Level Predictive Modeling for Prolonged Hospital Stay in Elderly Populations: Challenges in Low- and Middle-Income Countries](https://arxiv.org/abs/2601.04449)
*Daniel Sierra-Botero,Ana Molina-Taborda,Leonardo Espinosa-Leal,Alexander Karpenko,Alejandro Hernandez,Olga Lopez-Acevedo*

Main category: cs.LG

TL;DR: 该研究开发了一个预测住院时间延长(pLoS)的模型，使用基于图论的特征选择方法从医院数据中筛选出9个可解释变量，逻辑回归模型在验证集上AUC-ROC达到0.82，具有良好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 住院时间延长(pLoS)与院内不良事件风险显著相关，开发预测模型有助于医院管理和干预研究，但需要可解释的预测工具来识别影响因素。

Method: 研究使用120,354例住院记录，采用基于图论的特征选择方法，通过证据权重筛选非相关高信息值特征，建立逻辑回归模型预测住院时间是否超过7天，数据集按67%/22%/11%分为训练/测试/验证集。

Result: 特征选择方法筛选出9个可解释变量，验证集上模型特异性0.83、敏感性0.64、准确率0.76、精确率0.67、AUC-ROC 0.82，表现出较强的预测性能。

Conclusion: 该模型具有良好的预测性能和可解释性，可作为医院管理的有价值工具，并为未来减少住院时间延长的干预研究提供基础。

Abstract: Prolonged length of stay (pLoS) is a significant factor associated with the risk of adverse in-hospital events. We develop and explain a predictive model for pLos using admission-level patient and hospital administrative data. The approach includes a feature selection method by selecting non-correlated features with the highest information value. The method uses features weights of evidence to select a representative within cliques from graph theory. The prognosis study analyzed the records from 120,354 hospital admissions at the Hospital Alma Mater de Antioquia between January 2017 and March 2022. After a cleaning process the dataset was split into training (67%), test (22%), and validation (11%) cohorts. A logistic regression model was trained to predict the pLoS in two classes: less than or greater than 7 days. The performance of the model was evaluated using accuracy, precision, sensitivity, specificity, and AUC-ROC metrics. The feature selection method returns nine interpretable variables, enhancing the models' transparency. In the validation cohort, the pLoS model achieved a specificity of 0.83 (95% CI, 0.82-0.84), sensitivity of 0.64 (95% CI, 0.62-0.65), accuracy of 0.76 (95% CI, 0.76-0.77), precision of 0.67 (95% CI, 0.66-0.69), and AUC-ROC of 0.82 (95% CI, 0.81-0.83). The model exhibits strong predictive performance and offers insights into the factors that influence prolonged hospital stays. This makes it a valuable tool for hospital management and for developing future intervention studies aimed at reducing pLoS.

</details>


### [139] [Using Large Language Models to Detect Socially Shared Regulation of Collaborative Learning](https://arxiv.org/abs/2601.04458)
*Jiayi Zhang,Conrad Borchers,Clayton Cohn,Namrata Srivastava,Caitlin Snyder,Siyuan Guo,Ashwin T S,Naveeduddin Mohammed,Haley Noh,Gautam Biswas*

Main category: cs.LG

TL;DR: 该研究使用嵌入方法和大语言模型，在协作计算建模环境中自动检测社会共享学习调节行为，发现文本嵌入在检测执行和群体动态相关行为方面表现更好，而多模态特征在规划和反思方面有补充作用。


<details>
  <summary>Details</summary>
Motivation: 学习分析领域在自动检测多模态数据中的复杂学习过程方面取得了显著进展，但大多数研究集中在个体化问题解决上，而非协作开放式问题解决。协作环境既提供了更丰富的数据，也带来了低内聚性等挑战，需要扩展预测模型来检测社会共享学习调节行为。

Method: 使用基于嵌入的方法，利用大语言模型作为总结工具，生成与学生对话对齐的任务感知表示，结合系统日志。这些总结与纯文本嵌入、上下文丰富嵌入和日志衍生特征一起用于训练预测模型。

Result: 结果显示，纯文本嵌入在检测与执行或群体动态相关的SSRL行为（如偏离任务行为或请求帮助）方面通常表现更强。相比之下，上下文和多模态特征在规划和反思等构念方面提供补充效益。

Conclusion: 研究结果凸显了基于嵌入的模型在扩展学习分析方面的潜力，能够实现SSRL行为的可扩展检测，最终支持协作学习环境中教师重视的实时反馈和适应性支架。

Abstract: The field of learning analytics has made notable strides in automating the detection of complex learning processes in multimodal data. However, most advancements have focused on individualized problem-solving instead of collaborative, open-ended problem-solving, which may offer both affordances (richer data) and challenges (low cohesion) to behavioral prediction. Here, we extend predictive models to automatically detect socially shared regulation of learning (SSRL) behaviors in collaborative computational modeling environments using embedding-based approaches. We leverage large language models (LLMs) as summarization tools to generate task-aware representations of student dialogue aligned with system logs. These summaries, combined with text-only embeddings, context-enriched embeddings, and log-derived features, were used to train predictive models. Results show that text-only embeddings often achieve stronger performance in detecting SSRL behaviors related to enactment or group dynamics (e.g., off-task behavior or requesting assistance). In contrast, contextual and multimodal features provide complementary benefits for constructs such as planning and reflection. Overall, our findings highlight the promise of embedding-based models for extending learning analytics by enabling scalable detection of SSRL behaviors, ultimately supporting real-time feedback and adaptive scaffolding in collaborative learning environments that teachers value.

</details>


### [140] [Meta-probabilistic Modeling](https://arxiv.org/abs/2601.04462)
*Kevin Zhang,Yixin Wang*

Main category: cs.LG

TL;DR: 提出元概率建模（MPM），一种从多个相关数据集中直接学习生成模型结构的元学习算法，通过分层架构共享全局模型规范，同时保持局部参数的数据集特定性。


<details>
  <summary>Details</summary>
Motivation: 概率图模型可以发现数据中的潜在结构，但其有效性依赖于选择良好指定的模型。在实践中识别这样的模型具有挑战性，通常需要通过试错进行迭代检查和修订。

Method: 提出元概率建模（MPM），采用分层架构，全局模型规范在多个数据集间共享，局部参数保持数据集特定性。使用受VAE启发的可处理代理目标进行学习和推理，通过双层优化：局部变量通过坐标上升分析更新，全局参数使用基于梯度的方法训练。

Result: 在面向对象的图像建模和序列文本建模任务上评估MPM，证明其能够使生成模型适应数据，同时恢复有意义的潜在表示。

Conclusion: MPM能够直接从多个相关数据集中学习生成模型结构，有效解决概率图模型在实践中需要反复试错选择合适模型的问题，在图像和文本建模任务中表现出良好的适应性。

Abstract: While probabilistic graphical models can discover latent structure in data, their effectiveness hinges on choosing well-specified models. Identifying such models is challenging in practice, often requiring iterative checking and revision through trial and error. To this end, we propose meta-probabilistic modeling (MPM), a meta-learning algorithm that learns generative model structure directly from multiple related datasets. MPM uses a hierarchical architecture where global model specifications are shared across datasets while local parameters remain dataset-specific. For learning and inference, we propose a tractable VAE-inspired surrogate objective, and optimize it through bi-level optimization: local variables are updated analytically via coordinate ascent, while global parameters are trained with gradient-based methods. We evaluate MPM on object-centric image modeling and sequential text modeling, demonstrating that it adapts generative models to data while recovering meaningful latent representations.

</details>


### [141] [When Models Manipulate Manifolds: The Geometry of a Counting Task](https://arxiv.org/abs/2601.04480)
*Wes Gurnee,Emmanuel Ameisen,Isaac Kauvar,Julius Tarng,Adam Pearce,Chris Olah,Joshua Batson*

Main category: cs.LG

TL;DR: Claude 3.5 Haiku模型通过低维流形几何变换实现固定宽度文本的换行决策，类似于生物位置细胞机制


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何仅通过词元序列感知文本的视觉属性，特别是固定宽度文本的换行机制

Method: 通过机械性调查分析Claude 3.5 Haiku模型，发现字符计数在低维弯曲流形上表示，通过几何变换序列实现换行决策：词元长度累积到字符计数流形，注意力头扭曲流形估计到行边界的距离，通过正交排列估计创建线性决策边界

Result: 验证了发现并通过因果干预确认，发现了视觉错觉现象——能够劫持计数机制的字符序列

Conclusion: 展示了早期层的丰富感官处理、注意力算法的复杂性，以及结合基于特征和几何视角的可解释性的重要性

Abstract: Language models can perceive visual properties of text despite receiving only sequences of tokens-we mechanistically investigate how Claude 3.5 Haiku accomplishes one such task: linebreaking in fixed-width text. We find that character counts are represented on low-dimensional curved manifolds discretized by sparse feature families, analogous to biological place cells. Accurate predictions emerge from a sequence of geometric transformations: token lengths are accumulated into character count manifolds, attention heads twist these manifolds to estimate distance to the line boundary, and the decision to break the line is enabled by arranging estimates orthogonally to create a linear decision boundary. We validate our findings through causal interventions and discover visual illusions--character sequences that hijack the counting mechanism. Our work demonstrates the rich sensory processing of early layers, the intricacy of attention algorithms, and the importance of combining feature-based and geometric views of interpretability.

</details>


### [142] [Hybrid Federated Learning for Noise-Robust Training](https://arxiv.org/abs/2601.04483)
*Yongjun Kim,Hyeongjun Park,Hwanjin Kim,Junil Choi*

Main category: cs.LG

TL;DR: 提出混合联邦学习框架HFL，结合联邦学习和联邦蒸馏的优势，通过自适应用户设备聚类和权重选择来提升低信噪比下的性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)和联邦蒸馏(FD)各有优缺点：FL对噪声更鲁棒但学习速度慢，FD学习速度快但对噪声敏感。需要结合两者优势来克服各自的弱点

Method: 提出混合联邦学习(HFL)框架，用户设备可选择传输梯度或logits，基站自适应选择每轮FL和FD更新的权重。采用两种自由度利用方法：1)通过Jenks优化进行自适应用户设备聚类；2)通过阻尼牛顿法进行自适应权重选择

Result: 推导了HFL框架的收敛性，数值结果显示当同时利用两种自由度时，HFL在低信噪比下实现了优越的测试准确率

Conclusion: HFL框架成功结合了FL和FD的优势，通过自适应聚类和权重选择机制，在低信噪比环境下实现了更好的性能表现

Abstract: Federated learning (FL) and federated distillation (FD) are distributed learning paradigms that train UE models with enhanced privacy, each offering different trade-offs between noise robustness and learning speed. To mitigate their respective weaknesses, we propose a hybrid federated learning (HFL) framework in which each user equipment (UE) transmits either gradients or logits, and the base station (BS) selects the per-round weights of FL and FD updates. We derive convergence of HFL framework and introduce two methods to exploit degrees of freedom (DoF) in HFL, which are (i) adaptive UE clustering via Jenks optimization and (ii) adaptive weight selection via a damped Newton method. Numerical results show that HFL achieves superior test accuracy at low SNR when both DoF are exploited.

</details>


### [143] [IGenBench: Benchmarking the Reliability of Text-to-Infographic Generation](https://arxiv.org/abs/2601.04498)
*Yinghao Tang,Xueding Liu,Boyuan Zhang,Tingfeng Lan,Yupeng Xie,Jiale Lao,Yiyao Wang,Haoxuan Li,Tingting Gao,Bo Pan,Luoxuan Weng,Xiuqi Huang,Minfeng Zhu,Yingchaojie Feng,Yuyu Luo,Wei Chen*

Main category: cs.LG

TL;DR: IGENBENCH是首个评估文本到信息图生成可靠性的基准测试，包含600个测试用例和30种信息图类型，通过自动化评估框架发现当前T2I模型在生成信息图时存在数据编码失真、文本内容错误等可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的文本到图像模型可以生成美观的图像，但它们在生成信息图（结合数据可视化、文本和插图的复合视觉作品）方面的可靠性尚不明确。生成的信息图可能初看正确，但包含容易被忽视的问题，如扭曲的数据编码或不正确的文本内容。

Method: 提出了IGENBENCH基准测试，包含600个精心策划的测试用例，涵盖30种信息图类型。设计了自动化评估框架，将可靠性验证分解为基于10种问题类型的原子是/否问题，使用多模态大语言模型验证每个问题，计算问题级准确率（Q-ACC）和信息图级准确率（I-ACC）。

Result: 对10个最先进的T2I模型进行评估发现：(1) 性能呈现三层等级，最佳模型的Q-ACC为0.90但I-ACC仅为0.49；(2) 数据相关维度成为普遍瓶颈（如数据完整性：0.21）；(3) 所有模型都难以实现端到端的完全正确性。

Conclusion: IGENBENCH揭示了当前T2I模型在生成可靠信息图方面的局限性，特别是数据相关维度的挑战，为未来模型开发提供了关键见解。该基准测试已公开发布，可用于进一步研究和改进。

Abstract: Infographics are composite visual artifacts that combine data visualizations with textual and illustrative elements to communicate information. While recent text-to-image (T2I) models can generate aesthetically appealing images, their reliability in generating infographics remains unclear. Generated infographics may appear correct at first glance but contain easily overlooked issues, such as distorted data encoding or incorrect textual content. We present IGENBENCH, the first benchmark for evaluating the reliability of text-to-infographic generation, comprising 600 curated test cases spanning 30 infographic types. We design an automated evaluation framework that decomposes reliability verification into atomic yes/no questions based on a taxonomy of 10 question types. We employ multimodal large language models (MLLMs) to verify each question, yielding question-level accuracy (Q-ACC) and infographic-level accuracy (I-ACC). We comprehensively evaluate 10 state-of-the-art T2I models on IGENBENCH. Our systematic analysis reveals key insights for future model development: (i) a three-tier performance hierarchy with the top model achieving Q-ACC of 0.90 but I-ACC of only 0.49; (ii) data-related dimensions emerging as universal bottlenecks (e.g., Data Completeness: 0.21); and (iii) the challenge of achieving end-to-end correctness across all models. We release IGENBENCH at https://igen-bench.vercel.app/.

</details>


### [144] [Surface-based Molecular Design with Multi-modal Flow Matching](https://arxiv.org/abs/2601.04506)
*Fang Wu,Zhengyuan Zhou,Shuting Jin,Xiangxiang Zeng,Jure Leskovec,Jinbo Xu*

Main category: cs.LG

TL;DR: SurfFlow是一个基于分子表面的肽生成模型，通过多模态条件流匹配架构共同设计肽的序列、结构和表面，在PepMerge基准测试中优于全原子基线方法。


<details>
  <summary>Details</summary>
Motivation: 治疗性肽在靶向不可成药结合位点方面具有潜力，但现有深度生成模型主要关注全原子设计，忽视了分子表面在蛋白质-蛋白质相互作用中的关键作用。

Method: 提出SurfFlow表面生成算法，采用多模态条件流匹配架构学习表面几何和生化特性的分布，实现肽序列、结构和表面的全面协同设计。

Result: 在PepMerge基准测试中，SurfFlow在所有指标上均优于全原子基线方法，证明了考虑分子表面在肽设计中的优势。

Conclusion: 分子表面在从头肽发现中具有重要作用，整合多种蛋白质模态能够实现更有效的治疗性肽发现。

Abstract: Therapeutic peptides show promise in targeting previously undruggable binding sites, with recent advancements in deep generative models enabling full-atom peptide co-design for specific protein receptors. However, the critical role of molecular surfaces in protein-protein interactions (PPIs) has been underexplored. To bridge this gap, we propose an omni-design peptides generation paradigm, called SurfFlow, a novel surface-based generative algorithm that enables comprehensive co-design of sequence, structure, and surface for peptides. SurfFlow employs a multi-modality conditional flow matching (CFM) architecture to learn distributions of surface geometries and biochemical properties, enhancing peptide binding accuracy. Evaluated on the comprehensive PepMerge benchmark, SurfFlow consistently outperforms full-atom baselines across all metrics. These results highlight the advantages of considering molecular surfaces in de novo peptide discovery and demonstrate the potential of integrating multiple protein modalities for more effective therapeutic peptide discovery.

</details>


### [145] [TSSR: Two-Stage Swap-Reward-Driven Reinforcement Learning for Character-Level SMILES Generation](https://arxiv.org/abs/2601.04521)
*Jacob Ede Levine,Yun Lyan Luo,Sai Chandra Kosaraju*

Main category: cs.LG

TL;DR: TSSR是一个两阶段强化学习框架，通过交换奖励机制改进SMILES字符串生成，提高语法有效性和化学合理性


<details>
  <summary>Details</summary>
Motivation: 当前基于SMILES的化学语言模型存在语法错误和化学不合理问题，硬约束限制了化学空间探索，需要更可靠的分子生成方法

Method: 提出TSSR两阶段强化学习框架：第一阶段奖励修复语法的局部令牌交换，第二阶段基于RDKit诊断提供化学感知反馈，奖励减少价键、芳香性和连接性问题

Result: 在MOSES基准测试中，纯强化学习显著提高语法有效性、化学有效性和新颖性；微调强化学习保持药物相似性和可合成性同时提高有效性和新颖性

Conclusion: TSSR将稀疏终端目标转化为更密集可解释的奖励，在不降低多样性的情况下提高语法和化学质量，是数据集无关且可适应多种强化学习方法

Abstract: The design of reliable, valid, and diverse molecules is fundamental to modern drug discovery, as improved molecular generation supports efficient exploration of the chemical space for potential drug candidates and reduces the cost of early design efforts. Despite these needs, current chemical language models that generate molecules as SMILES strings are vulnerable to compounding token errors: many samples are unparseable or chemically implausible, and hard constraints meant to prevent failure can restrict exploration. To address this gap, we introduce TSSR, a Two-Stage, Swap-Reward-driven reinforcement learning (RL) framework for character-level SMILES generation. Stage one rewards local token swaps that repair syntax, promoting transitions from invalid to parseable strings. Stage two provides chemistry-aware feedback from RDKit diagnostics, rewarding reductions in valence, aromaticity, and connectivity issues. The reward decomposes into interpretable terms (swap efficiency, error reduction, distance to validity), is model agnostic, and requires no task-specific labels or hand-crafted grammars. We evaluated TSSR on the MOSES benchmark using a GRU policy trained with PPO in both pure RL (P-RL) from random initialization and fine-tuning RL (F-RL) starting from a pretrained chemical language model, assessing 10,000 generated SMILES per run. In P-RL, TSSR significantly improves syntactic validity, chemical validity, and novelty. In F-RL, TSSR preserves drug-likeness and synthesizability while increasing validity and novelty. Token-level analysis shows that syntax edits and chemistry fixes act jointly to reduce RDKit detected errors. TSSR converts a sparse terminal objective into a denser and more interpretable reward, improving both syntactic and chemical quality without reducing diversity. TSSR is dataset-agnostic and can be adapted to various reinforcement learning approaches.

</details>


### [146] [Not All Steps are Informative: On the Linearity of LLMs' RLVR Training](https://arxiv.org/abs/2601.04537)
*Tianle Wang,Zhongyuan Wu,Shenghao Jin,Hao Xu,Wei Chen,Ning Miao*

Main category: cs.LG

TL;DR: 论文发现强化学习验证奖励(RLVR)训练中LLM呈现强线性演化，提出通过权重外推和logits外推来预测未来模型状态，大幅减少计算成本


<details>
  <summary>Details</summary>
Motivation: RLVR训练需要数千步才能达到强性能，计算成本高昂，主要归因于漫长的探索过程。研究发现RLVR训练中LLM呈现强线性演化，这启发通过外推预测未来模型状态来避免持续训练

Method: 提出两种外推方法：1) 权重外推：从中间检查点预测未来模型权重；2) Logits外推：外推模型输出log-probabilities。利用RLVR训练中观察到的强线性相关性进行预测

Result: 权重外推产生的模型性能与标准RL训练相当，但计算需求显著减少。Logits外推在四个基准测试中始终优于持续RL训练，特别是在RL训练保持稳定的步数范围之外进行外推

Conclusion: RLVR训练中LLM的强线性演化特性使得通过外推预测未来模型状态成为可能，这为减少RL训练的计算成本提供了有效方法，权重外推和logits外推都能在保持或提升性能的同时显著降低计算需求

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a central component of large language model (LLM) post-training. Unlike supervised fine-tuning (SFT), RLVR lets an LLM generate multiple candidate solutions and reinforces those that lead to a verifiably correct final answer. However, in practice, RLVR often requires thousands of training steps to reach strong performance, incurring substantial computation largely attributed to prolonged exploration. In this work, we make a surprising observation: during RLVR, LLMs evolve in a strongly linear manner. Specifically, both model weights and model output log-probabilities exhibit strong linear correlations with RL training steps. This suggests that RLVR predominantly amplifies trends that emerge early in training, rather than continuously discovering new behaviors throughout the entire optimization trajectory. Motivated by this linearity, we investigate whether future model states can be predicted from intermediate checkpoints via extrapolation, avoiding continued expensive training. We show that Weight Extrapolation produces models with performance comparable to standard RL training while requiring significantly less computation. Moreover, Logits Extrapolation consistently outperforms continued RL training on all four benchmarks by extrapolating beyond the step range where RL training remains stable.

</details>


### [147] [Timeliness-Oriented Scheduling and Resource Allocation in Multi-Region Collaborative Perception](https://arxiv.org/abs/2601.04542)
*Mengmeng Zhu,Yuxuan Sun,Yukuan Jia,Wei Chen,Bo Ai,Sheng Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种时间感知的多区域优先调度算法（TAMP），用于解决协同感知中的动态调度问题，在感知精度和通信资源使用之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 协同感知在自动驾驶和智慧城市等应用中至关重要，但面临两个主要挑战：1）环境动态变化导致信息时效性对感知性能至关重要；2）传感器计算能力有限且无线带宽受限，需要精心设计通信量以确保特征表示既有效又充分。

Method: 提出TAMP调度算法，基于Lyapunov优化策略，将长期平均目标分解为每时隙的优先级排序问题。设计了经验惩罚函数，将信息年龄和通信量的联合影响映射到感知性能上，平衡调度价值与资源成本。

Result: 在交叉路口和走廊场景中使用真实世界RCooper数据集进行验证，TAMP算法优于最佳基线方法，在各种配置下平均精度提升高达27%。

Conclusion: TAMP算法能有效解决协同感知中的动态调度问题，在保证感知精度的同时优化通信资源使用，为多区域协同感知系统提供了实用的调度解决方案。

Abstract: Collaborative perception (CP) is a critical technology in applications like autonomous driving and smart cities. It involves the sharing and fusion of information among sensors to overcome the limitations of individual perception, such as blind spots and range limitations. However, CP faces two primary challenges. First, due to the dynamic nature of the environment, the timeliness of the transmitted information is critical to perception performance. Second, with limited computational power at the sensors and constrained wireless bandwidth, the communication volume must be carefully designed to ensure feature representations are both effective and sufficient. This work studies the dynamic scheduling problem in a multi-region CP scenario, and presents a Timeliness-Aware Multi-region Prioritized (TAMP) scheduling algorithm to trade-off perception accuracy and communication resource usage. Timeliness reflects the utility of information that decays as time elapses, which is manifested by the perception performance in CP tasks. We propose an empirical penalty function that maps the joint impact of Age of Information (AoI) and communication volume to perception performance. Aiming to minimize this timeliness-oriented penalty in the long-term, and recognizing that scheduling decisions have a cumulative effect on subsequent system states, we propose the TAMP scheduling algorithm. TAMP is a Lyapunov-based optimization policy that decomposes the long-term average objective into a per-slot prioritization problem, balancing the scheduling worth against resource cost. We validate our algorithm in both intersection and corridor scenarios with the real-world Roadside Cooperative perception (RCooper) dataset. Extensive simulations demonstrate that TAMP outperforms the best-performing baseline, achieving an Average Precision (AP) improvement of up to 27% across various configurations.

</details>


### [148] [GEnSHIN: Graphical Enhanced Spatio-temporal Hierarchical Inference Network for Traffic Flow Prediction](https://arxiv.org/abs/2601.04550)
*Zhiyan Zhou,Junjie Liao,Manho Zhang,Yingyi Liao,Ziai Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的图增强时空分层推理网络(GEnSHIN)，用于交通流预测，通过注意力增强的图卷积循环单元、非对称双嵌入图生成机制和动态记忆库模块，有效处理复杂的时空依赖关系。


<details>
  <summary>Details</summary>
Motivation: 随着城市化进程加速，智能交通系统对准确交通流预测的需求日益增长。现有模型在处理复杂的时空依赖关系方面存在局限，需要更有效的建模方法来提高预测精度。

Method: 1) 注意力增强的图卷积循环单元(GCRU)，通过引入Transformer模块增强长期时间依赖建模能力；2) 非对称双嵌入图生成机制，结合真实路网和数据驱动的潜在非对称拓扑生成更符合实际交通流特征的图结构；3) 动态记忆库模块，利用可学习的交通模式原型为每个传感器节点提供个性化交通模式表示，并在解码阶段引入轻量级图更新器以适应路网状态的动态变化。

Result: 在公开数据集METR-LA上的大量实验表明，GEnSHIN在MAE、RMSE、MAPE等多个指标上达到或超越了对比模型的性能。特别是在早晚交通高峰时段表现出优异的预测稳定性。消融实验进一步验证了每个核心模块的有效性及其对最终性能的贡献。

Conclusion: GEnSHIN模型通过创新的图增强时空分层推理架构，有效解决了交通流预测中的复杂时空依赖问题，在预测精度和稳定性方面均表现出色，为智能交通系统提供了有效的解决方案。

Abstract: With the acceleration of urbanization, intelligent transportation systems have an increasing demand for accurate traffic flow prediction. This paper proposes a novel Graph Enhanced Spatio-temporal Hierarchical Inference Network (GEnSHIN) to handle the complex spatio-temporal dependencies in traffic flow prediction. The model integrates three innovative designs: 1) An attention-enhanced Graph Convolutional Recurrent Unit (GCRU), which strengthens the modeling capability for long-term temporal dependencies by introducing Transformer modules; 2) An asymmetric dual-embedding graph generation mechanism, which leverages the real road network and data-driven latent asymmetric topology to generate graph structures that better fit the characteristics of actual traffic flow; 3) A dynamic memory bank module, which utilizes learnable traffic pattern prototypes to provide personalized traffic pattern representations for each sensor node, and introduces a lightweight graph updater during the decoding phase to adapt to dynamic changes in road network states. Extensive experiments on the public dataset METR-LA show that GEnSHIN achieves or surpasses the performance of comparative models across multiple metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). Notably, the model demonstrates excellent prediction stability during peak morning and evening traffic hours. Ablation experiments further validate the effectiveness of each core module and its contribution to the final performance.

</details>


### [149] [Improving Semi-Supervised Contrastive Learning via Entropy-Weighted Confidence Integration of Anchor-Positive Pairs](https://arxiv.org/abs/2601.04555)
*Shogo Nakayama,Masahiro Okuda*

Main category: cs.LG

TL;DR: 提出一种基于熵的自适应加权对比学习损失函数，通过置信度评估实现更广泛的伪标签分配和更稳定的学习性能


<details>
  <summary>Details</summary>
Motivation: 传统半监督对比学习方法只对预测概率超过阈值的样本分配伪标签，限制了训练数据的利用效率，需要更灵活的置信度评估机制

Method: 提出基于预测概率分布熵的置信度估计方法，设计自适应加权损失函数，允许对更多样本分配伪标签，并考虑锚点和正样本的置信度

Result: 实验结果表明，该方法提高了分类准确率，在低标签条件下实现了更稳定的学习性能

Conclusion: 基于熵的置信度自适应加权方法能够更有效地利用半监督数据，提升对比学习的效果和稳定性

Abstract: Conventional semi-supervised contrastive learning methods assign pseudo-labels only to samples whose highest predicted class probability exceeds a predefined threshold, and then perform supervised contrastive learning using those selected samples. In this study, we propose a novel loss function that estimates the confidence of each sample based on the entropy of its predicted probability distribution and applies confidence-based adaptive weighting. This approach enables pseudo-label assignment even to samples that were previously excluded from training and facilitates contrastive learning that accounts for the confidence of both anchor and positive samples in a more principled manner. Experimental results demonstrate that the proposed method improves classification accuracy and achieves more stable learning performance even under low-label conditions.

</details>


### [150] [A Vision for Multisensory Intelligence: Sensing, Synergy, and Science](https://arxiv.org/abs/2601.04563)
*Paul Pu Liang*

Main category: cs.LG

TL;DR: 该论文提出了未来十年多感官人工智能的研究愿景，旨在将AI从数字模态扩展到人类所有感官体验，通过感知、科学和协同三个主题推进多感官AI发展。


<details>
  <summary>Details</summary>
Motivation: 人类对世界的体验是多感官的，但当前人工智能主要局限于文本、视觉和音频等数字模态。需要将AI扩展到人类所有感官体验，改变人与AI的交互方式。

Method: 通过三个相互关联的主题推进：1）感知研究 - 扩展AI捕捉世界的方式；2）科学原理 - 量化多模态异质性，开发统一架构和表示；3）协同学习 - 研究模态间及人机间的协同。

Result: 提出了完整的多感官AI研究框架，并展示了MIT媒体实验室多感官智能小组的最新项目、资源和演示成果。

Conclusion: 多感官人工智能是未来十年重要发展方向，通过感知、科学和协同三个主题的系统研究，将实现AI与人类感官的深度连接，改变人机交互体验。

Abstract: Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/.

</details>


### [151] [Spatial-Temporal Feedback Diffusion Guidance for Controlled Traffic Imputation](https://arxiv.org/abs/2601.04572)
*Xiaowei Mao,Huihu Ding,Yan Lin,Tingrui Wu,Shengnan Guo,Dazhuo Qiu,Feiling Fang,Jilin Hu,Huaiyu Wan*

Main category: cs.LG

TL;DR: FENCE提出了一种空间-时间反馈扩散引导方法，用于自适应控制交通数据缺失值插补中的引导尺度，解决了现有扩散模型在缺失率高节点上引导不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的扩散模型在交通数据插补中表现良好，但它们通常对空间和时间维度使用统一的引导尺度。对于缺失率高的节点，稀疏观测提供的条件引导不足，导致生成过程偏向学习到的先验分布而非紧密跟随条件观测，从而影响插补性能。

Method: FENCE包含两个核心机制：1) 基于后验似然近似的动态反馈机制，根据生成值与观测值的偏离程度自适应调整引导尺度；2) 基于注意力分数对节点进行聚类，在集群级别计算引导尺度，利用空间-时间相关性提供更精确的引导。

Result: 在真实世界交通数据集上的实验结果表明，FENCE显著提高了插补准确性。

Conclusion: FENCE通过自适应反馈机制和集群级引导尺度调整，有效解决了高缺失率节点插补中的引导不足问题，提升了空间-时间交通数据插补的性能。

Abstract: Imputing missing values in spatial-temporal traffic data is essential for intelligent transportation systems. Among advanced imputation methods, score-based diffusion models have demonstrated competitive performance. These models generate data by reversing a noising process, using observed values as conditional guidance. However, existing diffusion models typically apply a uniform guidance scale across both spatial and temporal dimensions, which is inadequate for nodes with high missing data rates. Sparse observations provide insufficient conditional guidance, causing the generative process to drift toward the learned prior distribution rather than closely following the conditional observations, resulting in suboptimal imputation performance.
  To address this, we propose FENCE, a spatial-temporal feedback diffusion guidance method designed to adaptively control guidance scales during imputation. First, FENCE introduces a dynamic feedback mechanism that adjusts the guidance scale based on the posterior likelihood approximations. The guidance scale is increased when generated values diverge from observations and reduced when alignment improves, preventing overcorrection. Second, because alignment to observations varies across nodes and denoising steps, a global guidance scale for all nodes is suboptimal. FENCE computes guidance scales at the cluster level by grouping nodes based on their attention scores, leveraging spatial-temporal correlations to provide more accurate guidance. Experimental results on real-world traffic datasets show that FENCE significantly enhances imputation accuracy.

</details>


### [152] [FedKDX: Federated Learning with Negative Knowledge Distillation for Enhanced Healthcare AI Systems](https://arxiv.org/abs/2601.04587)
*Quang-Tu Pham,Hoang-Dieu Vu,Dinh-Dat Pham,Hieu H. Pham*

Main category: cs.LG

TL;DR: FedKDX是一个联邦学习框架，通过负知识蒸馏(NKD)解决医疗AI中的局限性，同时捕捉目标和非目标信息来提升模型泛化能力，在医疗数据集上实现了更好的准确性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法主要关注正知识转移，但在医疗AI应用中，需要同时考虑目标和非目标信息来提升模型泛化能力，特别是在隐私敏感、数据分布非独立同分布的医疗场景下。

Method: FedKDX整合了多种知识转移技术：传统知识蒸馏、对比学习和负知识蒸馏(NKD)，形成一个统一的架构，在保护隐私的同时降低通信成本，专门针对医疗数据统计异质性设计。

Result: 在SLEEP、UCI-HAR和PAMAP2等医疗数据集上的实验表明，FedKDX相比现有方法准确率提升最高达2.53%，收敛速度更快，在非独立同分布数据上表现更好，理论分析支持NKD对处理统计异质性的贡献。

Conclusion: FedKDX为隐私敏感的医疗应用提供了一个平衡性能与实施要求的解决方案，符合HIPAA和GDPR等监管框架，在去中心化医疗环境中具有实际应用前景。

Abstract: This paper introduces FedKDX, a federated learning framework that addresses limitations in healthcare AI through Negative Knowledge Distillation (NKD). Unlike existing approaches that focus solely on positive knowledge transfer, FedKDX captures both target and non-target information to improve model generalization in healthcare applications. The framework integrates multiple knowledge transfer techniques--including traditional knowledge distillation, contrastive learning, and NKD--within a unified architecture that maintains privacy while reducing communication costs. Through experiments on healthcare datasets (SLEEP, UCI-HAR, and PAMAP2), FedKDX demonstrates improved accuracy (up to 2.53% over state-of-the-art methods), faster convergence, and better performance on non-IID data distributions. Theoretical analysis supports NKD's contribution to addressing statistical heterogeneity in distributed healthcare data. The approach shows promise for privacy-sensitive medical applications under regulatory frameworks like HIPAA and GDPR, offering a balanced solution between performance and practical implementation requirements in decentralized healthcare settings. The code and model are available at https://github.com/phamdinhdat-ai/Fed_2024.

</details>


### [153] [Density Matrix RNN (DM-RNN): A Quantum Information Theoretic Framework for Modeling Musical Context and Polyphony](https://arxiv.org/abs/2601.04592)
*Joonwon Seo,Mariana Montiel*

Main category: cs.LG

TL;DR: 提出密度矩阵RNN（DM-RNN），利用量子力学密度矩阵概念来建模音乐中的模糊性，通过量子通道描述时间动态，确保物理有效性，并使用冯诺依曼熵和量子互信息量化音乐不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统RNN将音乐上下文总结为确定性隐藏状态向量，形成信息瓶颈，无法捕捉音乐固有的模糊性。需要一种能够表示音乐解释统计集合的模型。

Method: 提出密度矩阵RNN（DM-RNN）架构，使用密度矩阵表示混合状态，通过量子通道（CPTP映射）定义时间动态，基于Choi-Jamiolkowski同构的参数化策略确保学习动态保持物理有效性。

Result: 建立了使用冯诺依曼熵量化音乐不确定性和量子互信息测量声部间纠缠的分析框架，为建模复杂模糊音乐结构提供了数学严谨的框架。

Conclusion: DM-RNN通过量子力学概念为音乐建模提供了新范式，能够捕捉传统RNN无法处理的音乐模糊性和多重解释，为复杂音乐结构分析提供了理论基础。

Abstract: Classical Recurrent Neural Networks (RNNs) summarize musical context into a deterministic hidden state vector, imposing an information bottleneck that fails to capture the inherent ambiguity in music. We propose the Density Matrix RNN (DM-RNN), a novel theoretical architecture utilizing the Density Matrix. This allows the model to maintain a statistical ensemble of musical interpretations (a mixed state), capturing both classical probabilities and quantum coherences. We rigorously define the temporal dynamics using Quantum Channels (CPTP maps). Crucially, we detail a parameterization strategy based on the Choi-Jamiolkowski isomorphism, ensuring the learned dynamics remain physically valid (CPTP) by construction. We introduce an analytical framework using Von Neumann Entropy to quantify musical uncertainty and Quantum Mutual Information (QMI) to measure entanglement between voices. The DM-RNN provides a mathematically rigorous framework for modeling complex, ambiguous musical structures.

</details>


### [154] [DeepHalo: A Neural Choice Model with Controllable Context Effects](https://arxiv.org/abs/2601.04616)
*Shuhan Zhang,Zhi Wang,Rui Gao,Shuang Li*

Main category: cs.LG

TL;DR: DeepHalo是一个神经建模框架，用于捕捉人类决策中的情境效应（光环效应），能够明确控制交互阶数并提供可解释性，在特征和无特征设置下都表现出色。


<details>
  <summary>Details</summary>
Motivation: 人类决策建模在推荐系统、偏好学习和人机对齐等应用中至关重要。传统模型通常假设选择行为与情境无关，但大量行为研究表明，偏好常常受到选择集本身组成的影响，这种现象被称为情境效应或光环效应。现有模型要么关注无特征设置，要么在基于特征的设置中依赖限制性交互结构或混淆所有阶数的交互，限制了可解释性。

Method: 提出DeepHalo神经建模框架，该框架在纳入特征的同时，能够明确控制交互阶数，并提供情境效应的原则性解释。模型通过阶数系统识别交互效应，在专门用于无特征设置时，可以作为情境依赖选择函数的通用逼近器。

Result: 在合成和真实世界数据集上的实验表明，DeepHalo具有强大的预测性能，同时为选择驱动因素提供了更大的透明度。

Conclusion: DeepHalo框架成功解决了现有模型在捕捉情境效应时的局限性，既保持了预测准确性，又提高了模型的可解释性，为理解人类决策中的复杂交互效应提供了有效工具。

Abstract: Modeling human decision-making is central to applications such as recommendation, preference learning, and human-AI alignment. While many classic models assume context-independent choice behavior, a large body of behavioral research shows that preferences are often influenced by the composition of the choice set itself -- a phenomenon known as the context effect or Halo effect. These effects can manifest as pairwise (first-order) or even higher-order interactions among the available alternatives. Recent models that attempt to capture such effects either focus on the featureless setting or, in the feature-based setting, rely on restrictive interaction structures or entangle interactions across all orders, which limits interpretability. In this work, we propose DeepHalo, a neural modeling framework that incorporates features while enabling explicit control over interaction order and principled interpretation of context effects. Our model enables systematic identification of interaction effects by order and serves as a universal approximator of context-dependent choice functions when specialized to a featureless setting. Experiments on synthetic and real-world datasets demonstrate strong predictive performance while providing greater transparency into the drivers of choice.

</details>


### [155] [Learning Dynamics in RL Post-Training for Language Models](https://arxiv.org/abs/2601.04670)
*Akiyoshi Tomihari*

Main category: cs.LG

TL;DR: 本文通过神经正切核框架分析RL后训练的学习动态，发现特征表示多样性不足会导致模型置信度系统性增加，从而解释输出多样性下降现象，并提出了优先更新分类器的两阶段训练策略CF-RL。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练是现代语言模型开发的关键阶段，但存在输出多样性下降等现象未被充分理解。本文旨在从监督学习中研究过但强化学习中未充分探索的角度，更广泛地理解RL后训练的学习动态。

Method: 采用经验神经正切核框架，将NTK分解为两个组件来分析RL更新如何在训练样本间传播。基于分析发现，提出了分类器优先强化学习策略，即先更新分类器再进行标准RL优化的两阶段训练方法。

Result: 分析表明特征表示多样性有限会导致RL更新系统性增加模型置信度，这解释了RL后训练中常见的输出多样性下降现象。实验验证了CF-RL能增加模型置信度并加速优化，且其机制与监督学习中的线性探测再微调不同。

Conclusion: 本研究形式化了RL后训练的学习动态，为理解输出多样性下降现象提供了理论解释，并提出了有效的改进策略CF-RL，为后续分析和改进提供了理论基础和实用方法。

Abstract: Reinforcement learning (RL) post-training is a critical stage in modern language model development, playing a key role in improving alignment and reasoning ability. However, several phenomena remain poorly understood, including the reduction in output diversity. To gain a broader understanding of RL post-training, we analyze the learning dynamics of RL post-training from a perspective that has been studied in supervised learning but remains underexplored in RL. We adopt an empirical neural tangent kernel (NTK) framework and decompose the NTK into two components to characterize how RL updates propagate across training samples. Our analysis reveals that limited variability in feature representations can cause RL updates to systematically increase model confidence, providing an explanation for the commonly observed reduction in output diversity after RL post-training. Furthermore, we show that effective learning in this regime depends on rapidly shaping the classifier, which directly affects the gradient component of the NTK. Motivated by these insights, we propose classifier-first reinforcement learning (CF-RL), a simple two-stage training strategy that prioritizes classifier updates before standard RL optimization. Experimental results validate our theoretical analysis by demonstrating increased model confidence and accelerated optimization under CF-RL. Additional analysis shows that the mechanism underlying CF-RL differs from that of linear-probing-then-fine-tuning in supervised learning. Overall, our study formalizes the learning dynamics of RL post-training and motivates further analysis and improvement.

</details>


### [156] [Estimating Causal Effects in Gaussian Linear SCMs with Finite Data](https://arxiv.org/abs/2601.04673)
*Aurghya Maiti,Prateek Jain*

Main category: cs.LG

TL;DR: 论文提出集中化高斯线性结构因果模型(CGL-SCMs)来解决传统GL-SCMs在有限数据下参数估计不可行的问题，并开发了基于EM的估计算法。


<details>
  <summary>Details</summary>
Motivation: 在存在潜在混杂因子的观测数据中估计因果效应是因果推断的基本挑战。传统高斯线性结构因果模型(GL-SCMs)虽然分析上易于处理，但由于过度参数化，在有限数据下参数估计往往不可行。

Method: 提出集中化高斯线性结构因果模型(CGL-SCMs)，这是GL-SCMs的一个简化但表达能力强的子类，其中外生变量遵循标准化分布。开发了基于EM的估计算法，可以从有限观测样本中学习CGL-SCM参数并估计可识别的因果效应。

Result: 理论分析表明CGL-SCMs在从观测分布识别因果效应方面与传统GL-SCMs具有同等表达能力。在合成数据和基准因果图上的实验验证了学习模型能够准确恢复因果分布。

Conclusion: CGL-SCMs为解决有限数据下高斯线性结构因果模型的参数估计问题提供了一个可行的解决方案，其EM估计算法能够有效学习模型参数并估计因果效应。

Abstract: Estimating causal effects from observational data remains a fundamental challenge in causal inference, especially in the presence of latent confounders. This paper focuses on estimating causal effects in Gaussian Linear Structural Causal Models (GL-SCMs), which are widely used due to their analytical tractability. However, parameter estimation in GL-SCMs is often infeasible with finite data, primarily due to overparameterization. To address this, we introduce the class of Centralized Gaussian Linear SCMs (CGL-SCMs), a simplified yet expressive subclass where exogenous variables follow standardized distributions. We show that CGL-SCMs are equally expressive in terms of causal effect identifiability from observational distributions and present a novel EM-based estimation algorithm that can learn CGL-SCM parameters and estimate identifiable causal effects from finite observational samples. Our theoretical analysis is validated through experiments on synthetic data and benchmark causal graphs, demonstrating that the learned models accurately recover causal distributions.

</details>


### [157] [Nightmare Dreamer: Dreaming About Unsafe States And Planning Ahead](https://arxiv.org/abs/2601.04686)
*Oluwatosin Oseni,Shengjie Wang,Jun Zhu,Micah Corah*

Main category: cs.LG

TL;DR: Nightmare Dreamer是一种基于模型的安全强化学习算法，通过使用学习的世界模型预测潜在安全违规来确保安全性，在Safety Gymnasium任务上实现了接近零安全违规和高效率。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在机器人控制等实际应用中取得了显著成功，但由于缺乏足够的安全保证，其应用仍然受限。需要开发能够确保安全性的强化学习算法。

Method: 提出Nightmare Dreamer算法，这是一种基于模型的安全强化学习方法。它通过学习一个世界模型来预测潜在的安全违规，并据此规划行动，从而在最大化奖励的同时确保安全性。

Result: 在Safety Gymnasium任务上，仅使用图像观测，Nightmare Dreamer实现了接近零安全违规，同时效率比无模型基线提高了近20倍。

Conclusion: Nightmare Dreamer通过基于模型的方法有效解决了强化学习中的安全问题，在保持高性能的同时显著减少了安全违规，为安全强化学习的实际应用提供了有前景的解决方案。

Abstract: Reinforcement Learning (RL) has shown remarkable success in real-world applications, particularly in robotics control. However, RL adoption remains limited due to insufficient safety guarantees. We introduce Nightmare Dreamer, a model-based Safe RL algorithm that addresses safety concerns by leveraging a learned world model to predict potential safety violations and plan actions accordingly. Nightmare Dreamer achieves nearly zero safety violations while maximizing rewards. Nightmare Dreamer outperforms model-free baselines on Safety Gymnasium tasks using only image observations, achieving nearly a 20x improvement in efficiency.

</details>


### [158] [Do LLMs Benefit from User and Item Embeddings in Recommendation Tasks?](https://arxiv.org/abs/2601.04690)
*Mir Rayat Imtiaz Hossain,Leo Feng,Leonid Sigal,Mohamed Osama Ahmed*

Main category: cs.LG

TL;DR: 提出一种简单有效的LLM推荐方法，将协同过滤学习到的用户和物品嵌入通过轻量级投影模块映射到LLM标记空间，结合文本标记生成推荐


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法主要依赖文本语义或仅有限地融入协同信号（通常只使用用户或物品嵌入），难以处理代表用户历史的多个物品嵌入，往往回归到文本语义而忽略了更丰富的协同信息

Method: 通过独立的轻量级投影模块将协同过滤学习到的用户和物品嵌入投影到LLM标记空间，然后微调LLM使其能够基于这些投影嵌入和文本标记共同生成推荐

Result: 初步结果表明，该方法能有效利用结构化的用户-物品交互数据，相比仅基于文本的LLM基线方法提高了推荐性能

Conclusion: 该方法为连接传统推荐系统和现代LLMs提供了实用路径，能够更好地融合协同过滤信号和语言模型能力

Abstract: Large Language Models (LLMs) have emerged as promising recommendation systems, offering novel ways to model user preferences through generative approaches. However, many existing methods often rely solely on text semantics or incorporate collaborative signals in a limited manner, typically using only user or item embeddings. These methods struggle to handle multiple item embeddings representing user history, reverting to textual semantics and neglecting richer collaborative information. In this work, we propose a simple yet effective solution that projects user and item embeddings, learned from collaborative filtering, into the LLM token space via separate lightweight projector modules. A finetuned LLM then conditions on these projected embeddings alongside textual tokens to generate recommendations. Preliminary results show that this design effectively leverages structured user-item interaction data, improves recommendation performance over text-only LLM baselines, and offers a practical path for bridging traditional recommendation systems with modern LLMs.

</details>


### [159] [A zone-based training approach for last-mile routing using Graph Neural Networks and Pointer Networks](https://arxiv.org/abs/2601.04705)
*Àngel Ruiz-Fas,Carlos Granell,José Francisco Ramos,Joaquín Huerta,Sergio Trilles*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的最后一公里配送路径优化方法，通过地理分区训练模型来处理非对称旅行时间问题，相比通用训练显著减少了预测路径长度。


<details>
  <summary>Details</summary>
Motivation: 电子商务的快速增长使最后一公里配送网络面临极限挑战，传统启发式算法在处理非对称旅行时间（如单行道、拥堵）时适应性差。需要更智能的路径优化方法来降低成本、提高服务速度并减少排放。

Method: 采用编码器-解码器架构：1）将路线表示为完全有向图，节点为停靠点，边权重为非对称旅行时间；2）图神经网络编码器生成捕捉停靠点空间关系的节点嵌入；3）指针网络解码器基于嵌入和起始节点顺序选择下一个停靠点。通过离散全球网格系统将训练数据中的停靠点聚类为相似大小的地理分区，每个分区训练独立的模型实例。

Result: 使用2021年亚马逊最后一公里路由挑战的洛杉矶路线进行评估。结果显示，基于分区的训练相比通用训练平均预测路径长度减少，且随着每条路线停靠点数量的增加，分区方法的性能改进更加显著。

Conclusion: 基于地理分区的深度学习模型能够有效优化最后一公里配送路径，特别是在处理非对称旅行时间和大规模停靠点场景时表现优异，为实际物流配送提供了可行的智能解决方案。

Abstract: Rapid e-commerce growth has pushed last-mile delivery networks to their limits, where small routing gains translate into lower costs, faster service, and fewer emissions. Classical heuristics struggle to adapt when travel times are highly asymmetric (e.g., one-way streets, congestion). A deep learning-based approach to the last-mile routing problem is presented to generate geographical zones composed of stop sequences to minimize last-mile delivery times.
  The presented approach is an encoder-decoder architecture. Each route is represented as a complete directed graph whose nodes are stops and whose edge weights are asymmetric travel times. A Graph Neural Network encoder produces node embeddings that captures the spatial relationships between stops. A Pointer Network decoder then takes the embeddings and the route's start node to sequentially select the next stops, assigning a probability to each unvisited node as the next destination.
  Cells of a Discrete Global Grid System which contain route stops in the training data are obtained and clustered to generate geographical zones of similar size in which the process of training and inference are divided. Subsequently, a different instance of the model is trained per zone only considering the stops of the training routes which are included in that zone.
  This approach is evaluated using the Los Angeles routes from the 2021 Amazon Last Mile Routing Challenge. Results from general and zone-based training are compared, showing a reduction in the average predicted route length in the zone-based training compared to the general training. The performance improvement of the zone-based approach becomes more pronounced as the number of stops per route increases.

</details>


### [160] [MQ-GNN: A Multi-Queue Pipelined Architecture for Scalable and Efficient GNN Training](https://arxiv.org/abs/2601.04707)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: MQ-GNN是一个多队列流水线框架，通过交错GNN训练阶段和优化资源利用率，解决了现有GNN训练框架在可扩展性方面的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在处理图结构数据方面很强大，但其可扩展性受到低效的小批量生成、数据传输瓶颈和昂贵的GPU间同步的限制。现有训练框架无法重叠这些阶段，导致资源利用率低下。

Method: 提出了MQ-GNN框架，包括：1）Ready-to-Update Asynchronous Consistent Model (RaCoM)，支持异步梯度共享和模型更新，通过自适应周期性同步确保全局一致性；2）全局邻居采样与缓存以减少数据传输开销；3）自适应队列大小策略来平衡计算和内存效率。

Result: 在四个大规模数据集和十个基线模型上的实验表明，MQ-GNN实现了高达4.6倍的训练加速和30%的GPU利用率提升，同时保持了有竞争力的准确性。

Conclusion: MQ-GNN为多GPU GNN训练提供了一个可扩展且高效的解决方案，通过流水线化训练阶段和优化资源利用，显著提升了训练效率。

Abstract: Graph Neural Networks (GNNs) are powerful tools for learning graph-structured data, but their scalability is hindered by inefficient mini-batch generation, data transfer bottlenecks, and costly inter-GPU synchronization. Existing training frameworks fail to overlap these stages, leading to suboptimal resource utilization. This paper proposes MQ-GNN, a multi-queue pipelined framework that maximizes training efficiency by interleaving GNN training stages and optimizing resource utilization. MQ-GNN introduces Ready-to-Update Asynchronous Consistent Model (RaCoM), which enables asynchronous gradient sharing and model updates while ensuring global consistency through adaptive periodic synchronization. Additionally, it employs global neighbor sampling with caching to reduce data transfer overhead and an adaptive queue-sizing strategy to balance computation and memory efficiency. Experiments on four large-scale datasets and ten baseline models demonstrate that MQ-GNN achieves up to \boldmath $\bm{4.6\,\times}$ faster training time and 30% improved GPU utilization while maintaining competitive accuracy. These results establish MQ-GNN as a scalable and efficient solution for multi-GPU GNN training.

</details>


### [161] [GPU-Accelerated INT8 Quantization for KV Cache Compression in Large Language Models](https://arxiv.org/abs/2601.04719)
*Maanas Taneja,Purab Shingvi*

Main category: cs.LG

TL;DR: 该论文研究了在大型语言模型推理中使用INT8量化压缩KV缓存的方法，实现了4倍内存减少，同时保持最小精度损失，CUDA向量化内核相比CPU基线获得高达1694倍加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理中的键值（KV）缓存存在显著的内存瓶颈，其大小随序列长度线性增长，常常超过模型权重本身的内存占用，需要有效的压缩方法来减少内存压力。

Method: 实现并评估了GPU加速的INT8量化用于KV缓存压缩，开发了四种CUDA内核变体：naive、tiled、coarsened和vectorized，并在高达10亿元素的真实工作负载规模上进行基准测试。

Result: 实现了4倍内存减少，向量化内核相比CPU基线获得高达1694倍加速，重建误差低于0.004，注意力分数误差低于0.1（即使在8K维度的注意力头中），计算开销仅为6-58毫秒。

Conclusion: INT8量化为减少LLM推理中的内存压力提供了一种实用方法，具有可忽略的计算开销和对下游模型行为的最小影响。

Abstract: The key-value (KV) cache in large language models presents a significant memory bottleneck during inference, growing linearly with sequence length and often exceeding the memory footprint of model weights themselves. We implement and evaluate GPU-accelerated INT8 quantization for KV cache compression, achieving 4$\times$ memory reduction with minimal accuracy degradation. We develop four CUDA kernel variants -- naive, tiled, coarsened, and vectorized -- and benchmark them across realistic workload sizes up to 1 billion elements. Our vectorized kernel achieves up to 1,694$\times$ speedup over CPU baselines while maintaining reconstruction error below 0.004 and attention score error below 0.1 even for 8K-dimensional heads. These results demonstrate that INT8 quantization provides a practical approach for reducing memory pressure in LLM inference with negligible computational overhead (6--58ms) and minimal impact on downstream model behavior

</details>


### [162] [Excess Description Length of Learning Generalizable Predictors](https://arxiv.org/abs/2601.04728)
*Elizabeth Donoway,Hailey Joren,Fabien Roger,Jan Leike*

Main category: cs.LG

TL;DR: 论文提出了一种基于信息论的框架，通过超额描述长度(EDL)量化微调从训练数据中提取并写入模型参数的预测结构，区分了能力激发与能力教学的不同机制。


<details>
  <summary>Details</summary>
Motivation: 理解微调是激发语言模型的潜在能力还是教授新能力，对于模型评估和安全至关重要。需要建立理论框架来量化微调过程中从训练数据提取并写入模型参数的预测结构。

Method: 开发了基于信息论的形式化框架，核心概念是超额描述长度(EDL)，通过预序编码定义，衡量在线训练模型顺序编码训练标签所需的比特数与最终训练模型下剩余编码成本之间的差距。

Result: EDL在期望上非负，在无限数据极限下收敛到剩余描述长度，并提供期望泛化增益的界限。通过玩具模型澄清了学习中的常见混淆：随机标签产生接近零的EDL，单个示例可消除关于数据分布底层规则的许多比特不确定性，罕见输入上学到的结构对期望泛化贡献很小，格式学习产生与能力获取不同的早期瞬态。

Conclusion: 该框架为能力激发与教学表现出不同定性扩展特征的实证观察提供了严格理论基础，有助于区分微调过程中模型是激发已有能力还是学习新能力。

Abstract: Understanding whether fine-tuning elicits latent capabilities or teaches new ones is a fundamental question for language model evaluation and safety. We develop a formal information-theoretic framework for quantifying how much predictive structure fine-tuning extracts from the train dataset and writes into a model's parameters. Our central quantity, Excess Description Length (EDL), is defined via prequential coding and measures the gap between the bits required to encode training labels sequentially using an evolving model (trained online) and the residual encoding cost under the final trained model. We establish that EDL is non-negative in expectation, converges to surplus description length in the infinite-data limit, and provides bounds on expected generalization gain. Through a series of toy models, we clarify common confusions about information in learning: why random labels yield EDL near zero, how a single example can eliminate many bits of uncertainty about the underlying rule(s) that describe the data distribution, why structure learned on rare inputs contributes proportionally little to expected generalization, and how format learning creates early transients distinct from capability acquisition. This framework provides rigorous foundations for the empirical observation that capability elicitation and teaching exhibit qualitatively distinct scaling signatures.

</details>


### [163] [Fast Mining and Dynamic Time-to-Event Prediction over Multi-sensor Data Streams](https://arxiv.org/abs/2601.04741)
*Kota Nakamura,Koki Kawabata,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: TimeCast是一个动态预测框架，用于实时分析多传感器数据流，自适应地预测机器故障发生时间，具有动态适应、实用性强和可扩展的特点。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据流具有动态特性，底层模式会随时间演变。现有方法难以适应这种变化，无法提供准确、实时的未来事件时间预测，特别是在机器故障预测场景中。

Method: 提出TimeCast动态预测框架，通过识别数据流中随时间演变的模式（阶段），为每个阶段学习单独的模型，实现基于模式转变的自适应预测。该方法能够发现捕捉多个传感器间时变相互依赖关系的有意义阶段。

Result: 在真实数据集上的大量实验表明，TimeCast比最先进方法提供更高的预测准确性，同时能够发现数据流中的动态变化，并大幅减少计算时间。

Conclusion: TimeCast框架能够有效处理动态数据流，通过识别不同阶段并学习相应模型，实现了对机器故障时间的准确、实时预测，具有动态适应性和计算效率优势。

Abstract: Given real-time sensor data streams obtained from machines, how can we continuously predict when a machine failure will occur? This work aims to continuously forecast the timing of future events by analyzing multi-sensor data streams. A key characteristic of real-world data streams is their dynamic nature, where the underlying patterns evolve over time. To address this, we present TimeCast, a dynamic prediction framework designed to adapt to these changes and provide accurate, real-time predictions of future event time. Our proposed method has the following properties: (a) Dynamic: it identifies the distinct time-evolving patterns (i.e., stages) and learns individual models for each, enabling us to make adaptive predictions based on pattern shifts. (b) Practical: it finds meaningful stages that capture time-varying interdependencies between multiple sensors and improve prediction performance; (c) Scalable: our algorithm scales linearly with the input size and enables online model updates on data streams. Extensive experiments on real datasets demonstrate that TimeCast provides higher prediction accuracy than state-of-the-art methods while finding dynamic changes in data streams with a great reduction in computational time.

</details>


### [164] [Intraday spatiotemporal PV power prediction at national scale using satellite-based solar forecast models](https://arxiv.org/abs/2601.04751)
*Luca Lanzilao,Angela Meyer*

Main category: cs.LG

TL;DR: 该研究提出了一个时空光伏功率预测框架，评估了7种日内光伏功率临近预报模型，发现卫星方法优于数值天气预报，其中SolarSTEPS和SHADECast表现最佳，在国家级尺度上82%的日预测误差低于10%。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一个综合框架来评估不同光伏功率预测模型在时空尺度上的性能，填补国家级尺度时空光伏预测研究的空白，并可视化中尺度云系统对光伏生产的影响。

Method: 提出了一个新颖的时空光伏功率预测框架，包含7种模型（卫星深度学习、光流法、物理数值天气预报模型），首先用卫星反演的地表太阳辐照度验证，然后通过站点特定机器学习模型将辐照度转换为光伏功率，使用瑞士6434个光伏站点的数据进行对比。

Result: 卫星方法在短预报时效内优于集成预报系统；SolarSTEPS和SHADECast提供最准确的地表太阳辐照度和光伏功率预测；SHADECast提供最可靠的集合分布；IrradianceNet获得最低均方根误差；SolarSTEPS和SHADECast提供更好校准的不确定性；预报技能随海拔升高而降低；国家级尺度上82%的日预测误差低于10%。

Conclusion: 卫星方法在光伏功率预测方面表现优异，特别是SolarSTEPS和SHADECast模型，展示了在国家级尺度上进行可靠光伏预测的潜力，适合业务化应用，为电网管理和可再生能源整合提供了重要工具。

Abstract: We present a novel framework for spatiotemporal photovoltaic (PV) power forecasting and use it to evaluate the reliability, sharpness, and overall performance of seven intraday PV power nowcasting models. The model suite includes satellite-based deep learning and optical-flow approaches and physics-based numerical weather prediction models, covering both deterministic and probabilistic formulations. Forecasts are first validated against satellite-derived surface solar irradiance (SSI). Irradiance fields are then converted into PV power using station-specific machine learning models, enabling comparison with production data from 6434 PV stations across Switzerland. To our knowledge, this is the first study to investigate spatiotemporal PV forecasting at a national scale. We additionally provide the first visualizations of how mesoscale cloud systems shape national PV production on hourly and sub-hourly timescales. Our results show that satellite-based approaches outperform the Integrated Forecast System (IFS-ENS), particularly at short lead times. Among them, SolarSTEPS and SHADECast deliver the most accurate SSI and PV power predictions, with SHADECast providing the most reliable ensemble spread. The deterministic model IrradianceNet achieves the lowest root mean square error, while probabilistic forecasts of SolarSTEPS and SHADECast provide better-calibrated uncertainty. Forecast skill generally decreases with elevation. At a national scale, satellite-based models forecast the daily total PV generation with relative errors below 10% for 82% of the days in 2019-2020, demonstrating robustness and their potential for operational use.

</details>


### [165] [Smart IoT-Based Wearable Device for Detection and Monitoring of Common Cow Diseases Using a Novel Machine Learning Technique](https://arxiv.org/abs/2601.04761)
*Rupsa Rani Mishra,D. Chandrasekhar Rao,Ajaya Kumar Tripathy*

Main category: cs.LG

TL;DR: 该研究提出了一种基于物联网的赛博物理系统框架，结合新型机器学习算法，用于自动化监测奶牛健康状况并预测多种常见疾病。


<details>
  <summary>Details</summary>
Motivation: 大规模养殖场中人工监测奶牛疾病存在劳动密集、耗时、准确性低等问题，导致疾病识别延迟，影响动物健康和农场生产效率。现有研究很少能同时高精度检测多种常见疾病。

Method: 提出物联网支持的赛博物理系统框架，用于监测奶牛日常活动和健康状况。开发了一种新型机器学习算法，通过分析收集的生理和行为数据来诊断常见奶牛疾病。

Result: 该系统能够通过分析全面的生理和行为特征集，准确预测多种疾病，实现高效的健康评估。

Conclusion: 该研究提出的物联网-机器学习集成系统为奶牛健康监测提供了一种自动化、低成本、可靠的解决方案，能够显著提高疾病检测的准确性和效率。

Abstract: Manual observation and monitoring of individual cows for disease detection present significant challenges in large-scale farming operations, as the process is labor-intensive, time-consuming, and prone to reduced accuracy. The reliance on human observation often leads to delays in identifying symptoms, as the sheer number of animals can hinder timely attention to each cow. Consequently, the accuracy and precision of disease detection are significantly compromised, potentially affecting animal health and overall farm productivity. Furthermore, organizing and managing human resources for the manual observation and monitoring of cow health is a complex and economically demanding task. It necessitates the involvement of skilled personnel, thereby contributing to elevated farm maintenance costs and operational inefficiencies. Therefore, the development of an automated, low-cost, and reliable smart system is essential to address these challenges effectively. Although several studies have been conducted in this domain, very few have simultaneously considered the detection of multiple common diseases with high prediction accuracy. However, advancements in Internet of Things (IoT), Machine Learning (ML), and Cyber-Physical Systems have enabled the automation of cow health monitoring with enhanced accuracy and reduced operational costs. This study proposes an IoT-enabled Cyber-Physical System framework designed to monitor the daily activities and health status of cow. A novel ML algorithm is proposed for the diagnosis of common cow diseases using collected physiological and behavioral data. The algorithm is designed to predict multiple diseases by analyzing a comprehensive set of recorded physiological and behavioral features, enabling accurate and efficient health assessment.

</details>


### [166] [AgentOCR: Reimagining Agent History via Optical Self-Compression](https://arxiv.org/abs/2601.04786)
*Lang Feng,Fuchao Yang,Feng Chen,Xin Cheng,Haiyang Xu,Zhenglin Wan,Ming Yan,Bo An*

Main category: cs.LG

TL;DR: AgentOCR框架通过将文本历史转换为紧凑的视觉图像表示，结合分段光学缓存和智能自压缩机制，显著降低LLM智能体系统的token消耗和内存使用，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体系统在多轮交互中面临文本历史快速增长的问题，导致token预算和内存使用急剧膨胀，限制了实际部署的可行性。

Method: 提出AgentOCR框架：1）将累积的观察-行动历史表示为紧凑的渲染图像，利用视觉token的信息密度优势；2）引入分段光学缓存机制，将历史分解为可哈希的片段并维护视觉缓存，消除冗余重新渲染；3）提出智能自压缩机制，让智能体主动发出压缩率，并通过压缩感知奖励训练，自适应平衡任务成功和token效率。

Result: 在ALFWorld和基于搜索的QA等挑战性智能体基准测试中，AgentOCR在保持超过95%文本智能体性能的同时，显著减少token消耗（>50%），实现一致的token和内存效率。分析验证了分段光学缓存带来20倍的渲染加速，以及自压缩机制的有效战略平衡。

Conclusion: AgentOCR通过视觉表示、缓存优化和自适应压缩的协同设计，有效解决了LLM智能体系统中历史表示的可扩展性问题，为实际部署提供了高效的解决方案。

Abstract: Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\% of text-based agent performance while substantially reducing token consumption (>50\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.

</details>


### [167] [Neural-Symbolic Integration with Evolvable Policies](https://arxiv.org/abs/2601.04799)
*Marios Thoma,Vassilis Vassiliades,Loizos Michael*

Main category: cs.LG

TL;DR: 提出了一种通过进化过程同时学习不可微分符号策略和神经网络权重的神经符号AI框架，解决了现有方法需要预定义符号策略或可微分策略的限制。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号AI框架通常需要预定义的符号策略或可微分策略，这在领域专业知识不可用或策略本身不可微分时限制了其适用性。需要一种能够同时学习不可微分符号策略和神经网络权重的方法。

Method: 1. 将神经符号系统视为种群中的有机体，通过进化过程（符号规则添加和神经网络权重变化的突变）进行演化；2. 基于适应度的选择引导收敛到隐藏目标策略；3. 扩展NEUROLOG架构使符号策略可训练；4. 将Valiant的Evolvability框架适配到神经符号上下文；5. 使用Machine Coaching语义实现可变符号表示；6. 通过符号组件的溯因推理训练神经网络，消除可微分性要求。

Result: 通过大量实验证明，从空策略和随机神经网络权重开始的神经符号系统能够成功近似隐藏的不可微分目标策略，中位数正确性能接近100%。

Conclusion: 这项工作代表了在从专家获取符号知识具有挑战性或不可行的领域中，推动神经符号AI研究向前迈进了一步，为学习不可微分符号策略提供了新方法。

Abstract: Neural-Symbolic (NeSy) Artificial Intelligence has emerged as a promising approach for combining the learning capabilities of neural networks with the interpretable reasoning of symbolic systems. However, existing NeSy frameworks typically require either predefined symbolic policies or policies that are differentiable, limiting their applicability when domain expertise is unavailable or when policies are inherently non-differentiable. We propose a framework that addresses this limitation by enabling the concurrent learning of both non-differentiable symbolic policies and neural network weights through an evolutionary process. Our approach casts NeSy systems as organisms in a population that evolve through mutations (both symbolic rule additions and neural weight changes), with fitness-based selection guiding convergence toward hidden target policies. The framework extends the NEUROLOG architecture to make symbolic policies trainable, adapts Valiant's Evolvability framework to the NeSy context, and employs Machine Coaching semantics for mutable symbolic representations. Neural networks are trained through abductive reasoning from the symbolic component, eliminating differentiability requirements. Through extensive experimentation, we demonstrate that NeSy systems starting with empty policies and random neural weights can successfully approximate hidden non-differentiable target policies, achieving median correct performance approaching 100%. This work represents a step toward enabling NeSy research in domains where the acquisition of symbolic knowledge from experts is challenging or infeasible.

</details>


### [168] [Parallelizing Node-Level Explainability in Graph Neural Networks](https://arxiv.org/abs/2601.04807)
*Oscar Llorente,Jaime Boal,Eugenio F. Sánchez-Úbeda,Antonio Diaz-Cano,Miguel Familiar*

Main category: cs.LG

TL;DR: 提出基于图划分的GNN节点级可解释性并行计算方法，通过将图分解为不相交子图实现并行计算，显著提升大规模图的可解释性计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着图规模增大，GNN节点级可解释性计算变得极其耗时，而批处理策略往往会降低解释质量，需要一种既能保持解释质量又能提高计算效率的解决方案。

Method: 采用图划分方法将图分解为不相交子图，实现节点邻居可解释性的并行计算；针对内存受限场景，提出基于dropout的重建机制，在内存使用和解释保真度之间提供可控权衡。

Result: 在真实世界数据集上的实验结果表明，该方法实现了显著的速度提升，能够为大规模GNN模型提供可扩展且透明的可解释性。

Conclusion: 基于图划分的并行计算方法有效解决了GNN节点级可解释性的可扩展性问题，在保证结果正确性的前提下显著提高了计算效率，为大规模GNN应用提供了实用的可解释性解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance in a wide range of tasks, such as node classification, link prediction, and graph classification, by exploiting the structural information in graph-structured data. However, in node classification, computing node-level explainability becomes extremely time-consuming as the size of the graph increases, while batching strategies often degrade explanation quality. This paper introduces a novel approach to parallelizing node-level explainability in GNNs through graph partitioning. By decomposing the graph into disjoint subgraphs, we enable parallel computation of explainability for node neighbors, significantly improving the scalability and efficiency without affecting the correctness of the results, provided sufficient memory is available. For scenarios where memory is limited, we further propose a dropout-based reconstruction mechanism that offers a controllable trade-off between memory usage and explanation fidelity. Experimental results on real-world datasets demonstrate substantial speedups, enabling scalable and transparent explainability for large-scale GNN models.

</details>


### [169] [Rethinking GNNs and Missing Features: Challenges, Evaluation and a Robust Solution](https://arxiv.org/abs/2601.04855)
*Francesco Ferrini,Veronica Lachi,Antonio Longa,Bruno Lepri,Matono Akiyoshi,Andrea Passerini,Xin Liu,Manfred Jaeger*

Main category: cs.LG

TL;DR: 该论文针对图神经网络处理缺失节点特征的问题，提出了更现实的评估协议和有效基线方法GNNmim，解决了现有研究在特征稀疏性和缺失机制方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络处理缺失节点特征的研究主要针对高维稀疏特征和完全随机缺失的良性场景，这限制了模型性能的真实评估。现实应用中存在密集语义特征和更复杂的缺失机制，需要更现实的评估框架。

Method: 1) 理论证明高稀疏性会限制缺失造成的信息损失；2) 引入一个合成和三个真实世界数据集，具有密集语义特征；3) 设计超越完全随机缺失的更现实缺失机制评估协议；4) 提出GNNmim基线方法用于不完整特征数据的节点分类。

Result: 实验表明，GNNmim在不同数据集和缺失机制下与专门架构相比具有竞争力。理论分析为缺失过程提供了明确假设，并分析了不同方法的影响。

Conclusion: 该研究为图神经网络处理缺失节点特征提供了更现实的评估框架和有效基线方法，解决了现有研究的局限性，推动了该领域在现实应用中的发展。

Abstract: Handling missing node features is a key challenge for deploying Graph Neural Networks (GNNs) in real-world domains such as healthcare and sensor networks. Existing studies mostly address relatively benign scenarios, namely benchmark datasets with (a) high-dimensional but sparse node features and (b) incomplete data generated under Missing Completely At Random (MCAR) mechanisms. For (a), we theoretically prove that high sparsity substantially limits the information loss caused by missingness, making all models appear robust and preventing a meaningful comparison of their performance. To overcome this limitation, we introduce one synthetic and three real-world datasets with dense, semantically meaningful features. For (b), we move beyond MCAR and design evaluation protocols with more realistic missingness mechanisms. Moreover, we provide a theoretical background to state explicit assumptions on the missingness process and analyze their implications for different methods. Building on this analysis, we propose GNNmim, a simple yet effective baseline for node classification with incomplete feature data. Experiments show that GNNmim is competitive with respect to specialized architectures across diverse datasets and missingness regimes.

</details>


### [170] [FibreCastML: An Open Web Platform for Predicting Electrospun Nanofibre Diameter Distributions](https://arxiv.org/abs/2601.04873)
*Elisa Roldan,Kirstie Andrews,Stephen M. Richardson,Reyhaneh Fatahian,Glen Cooper,Rasool Erfani,Tasneem Sabir,Neil D. Reeves*

Main category: cs.LG

TL;DR: FibreCastML是一个基于机器学习的开源框架，能够从常规电纺参数预测完整的纤维直径分布，而不仅仅是平均直径，为电纺支架优化提供更全面的数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法主要预测电纺纤维的平均直径，忽略了影响支架性能的完整直径分布。需要开发能够预测完整纤维直径谱的框架，以更好地优化电纺支架结构。

Method: 从1778项研究中提取68538个纤维直径测量值，构建元数据集。使用6个标准工艺参数训练7个机器学习模型，采用嵌套交叉验证和留一研究外部折叠。通过变量重要性分析、SHAP解释、相关矩阵和三维参数图实现模型可解释性。

Result: 非线性模型显著优于线性基线，对多种常用聚合物达到0.91以上的决定系数。溶液浓度是纤维直径分布的主要驱动因素。实验验证显示预测分布与实测分布高度一致。

Conclusion: FibreCastML能够预测完整的纤维直径分布，提供可解释的工艺-结构关系洞察，实现了更可重复和数据驱动的电纺支架结构优化。

Abstract: Electrospinning is a scalable technique for producing fibrous scaffolds with tunable micro- and nanoscale architectures for applications in tissue engineering, drug delivery, and wound care. While machine learning (ML) has been used to support electrospinning process optimisation, most existing approaches predict only mean fibre diameters, neglecting the full diameter distribution that governs scaffold performance. This work presents FibreCastML, an open, distribution-aware ML framework that predicts complete fibre diameter spectra from routinely reported electrospinning parameters and provides interpretable insights into process structure relationships.
  A meta-dataset comprising 68538 individual fibre diameter measurements extracted from 1778 studies across 16 biomedical polymers was curated. Six standard processing parameters, namely solution concentration, applied voltage, flow rate, tip to collector distance, needle diameter, and collector rotation speed, were used to train seven ML models using nested cross validation with leave one study out external folds. Model interpretability was achieved using variable importance analysis, SHapley Additive exPlanations, correlation matrices, and three dimensional parameter maps.
  Non linear models consistently outperformed linear baselines, achieving coefficients of determination above 0.91 for several widely used polymers. Solution concentration emerged as the dominant global driver of fibre diameter distributions. Experimental validation across different electrospinning systems demonstrated close agreement between predicted and measured distributions. FibreCastML enables more reproducible and data driven optimisation of electrospun scaffold architectures.

</details>


### [171] [Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers](https://arxiv.org/abs/2601.04890)
*Maksim Velikanov,Ilyas Chahed,Jingwei Zuo,Dhia Eddine Rhaiem,Younes Belkada,Hakim Hacid*

Main category: cs.LG

TL;DR: 该研究提出通过学习性乘数来优化权重衰减-噪声平衡中的权重矩阵尺度，替代传统的固定乘数方法，提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统权重衰减训练中，权重矩阵的范数由权重衰减和随机梯度噪声的平衡决定，这种平衡产生的权重范数可能不是最优的，限制了模型性能

Method: 引入可学习的乘数来优化权重尺度：1) 为权重矩阵W添加可学习的标量乘数；2) 进一步引入可学习的逐行和逐列乘数，释放行列范数的约束

Result: 学习性乘数方法优于调优良好的muP基线，减少了乘数调优的计算开销，在Adam和Muon优化器上都显示出改进，下游评估的改进程度相当于从Adam切换到Muon的改进

Conclusion: 学习性乘数是一种更灵活、表达能力更强的权重尺度优化方法，能够适应数据特征并提升模型性能，同时提出了前向传递对称性和学习乘数宽度缩放等实际问题

Abstract: Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.

</details>


### [172] [Distributed Online Convex Optimization with Efficient Communication: Improved Algorithm and Lower bounds](https://arxiv.org/abs/2601.04907)
*Sifan Yang,Wenhao Yang,Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文研究分布式在线凸优化中的压缩通信问题，提出了新的算法显著改进了现有遗憾界，消除了对压缩质量因子的二次/四次依赖，并建立了该问题的首个下界。


<details>
  <summary>Details</summary>
Motivation: 现有分布式在线凸优化算法在压缩通信下的遗憾界存在两个主要问题：1) 对压缩质量因子ω⁻¹有二次甚至四次依赖；2) 对学习者数量n有超线性依赖。这些限制在实际应用中代价高昂，需要更高效的算法设计。

Method: 提出了一种新颖的两级阻塞更新框架，包含两个关键组件：在线gossip策略和误差补偿方案。该框架通过更好的学习者间共识机制来克服现有方法的局限性。对于bandit反馈场景，还结合了经典梯度估计器来扩展方法。

Result: 新算法在凸函数上实现了Õ(ω⁻¹/²ρ⁻¹n√T)的遗憾界，在强凸函数上实现了Õ(ω⁻¹ρ⁻²n ln T)的遗憾界，显著优于现有结果的O(max{ω⁻²ρ⁻⁴n¹/², ω⁻⁴ρ⁻⁸}n√T)和O(max{ω⁻²ρ⁻⁴n¹/², ω⁻⁴ρ⁻⁸}n ln T)。同时建立了该问题的首个下界，证明了结果在ω和T维度上的最优性。

Conclusion: 本文提出的两级阻塞更新框架有效解决了分布式在线凸优化中压缩通信的关键挑战，显著改进了遗憾界，消除了对压缩质量因子的高次依赖，并建立了理论下界证明了算法的近乎最优性。该工作为压缩通信下的分布式优化提供了更高效的解决方案。

Abstract: We investigate distributed online convex optimization with compressed communication, where $n$ learners connected by a network collaboratively minimize a sequence of global loss functions using only local information and compressed data from neighbors. Prior work has established regret bounds of $O(\max\{ω^{-2}ρ^{-4}n^{1/2},ω^{-4}ρ^{-8}\}n\sqrt{T})$ and $O(\max\{ω^{-2}ρ^{-4}n^{1/2},ω^{-4}ρ^{-8}\}n\ln{T})$ for convex and strongly convex functions, respectively, where $ω\in(0,1]$ is the compression quality factor ($ω=1$ means no compression) and $ρ<1$ is the spectral gap of the communication matrix. However, these regret bounds suffer from a \emph{quadratic} or even \emph{quartic} dependence on $ω^{-1}$. Moreover, the \emph{super-linear} dependence on $n$ is also undesirable. To overcome these limitations, we propose a novel algorithm that achieves improved regret bounds of $\tilde{O}(ω^{-1/2}ρ^{-1}n\sqrt{T})$ and $\tilde{O}(ω^{-1}ρ^{-2}n\ln{T})$ for convex and strongly convex functions, respectively. The primary idea is to design a \emph{two-level blocking update framework} incorporating two novel ingredients: an online gossip strategy and an error compensation scheme, which collaborate to \emph{achieve a better consensus} among learners. Furthermore, we establish the first lower bounds for this problem, justifying the optimality of our results with respect to both $ω$ and $T$. Additionally, we consider the bandit feedback scenario, and extend our method with the classic gradient estimators to enhance existing regret bounds.

</details>


### [173] [Cardinality augmented loss functions](https://arxiv.org/abs/2601.04941)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: 提出基于数学中基数概念的增强损失函数，用于解决神经网络训练中的类别不平衡问题，在人工不平衡数据集和真实材料科学数据集上均取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是神经网络训练中常见且有害的问题，多数类可能主导训练过程，导致分类器性能偏向多数类结果

Method: 引入基数增强损失函数，基于现代数学文献中的基数类不变量（如magnitude和spread），这些不变量通过评估度量空间的"有效多样性"来丰富基数概念，为解决训练数据过于同质化提供了自然解决方案

Result: 在人工不平衡数据集和真实世界不平衡材料科学数据集上，观察到少数类性能显著提升，整体性能指标也有所改善

Conclusion: 基数增强损失函数为处理神经网络训练中的类别不平衡问题提供了一种有效方法，能够显著提升少数类性能并改善整体分类效果

Abstract: Class imbalance is a common and pernicious issue for the training of neural networks. Often, an imbalanced majority class can dominate training to skew classifier performance towards the majority outcome. To address this problem we introduce cardinality augmented loss functions, derived from cardinality-like invariants in modern mathematics literature such as magnitude and the spread. These invariants enrich the concept of cardinality by evaluating the `effective diversity' of a metric space, and as such represent a natural solution to overly homogeneous training data. In this work, we establish a methodology for applying cardinality augmented loss functions in the training of neural networks and report results on both artificially imbalanced datasets as well as a real-world imbalanced material science dataset. We observe significant performance improvement among minority classes, as well as improvement in overall performance metrics.

</details>


### [174] [Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following](https://arxiv.org/abs/2601.04954)
*Yirong Zeng,Yufei Liu,Xiao Ding,Yutai Hou,Yuxian Wang,Haonan Song,Wu Ning,Dandan Tu,Qixun Zhang,Bibo Cai,Yuxiang He,Ting Liu*

Main category: cs.LG

TL;DR: 论文挑战了强化学习中指令跟随任务需要多样约束（可验证硬约束+不可验证软约束）的主流观点，发现仅使用硬约束训练的模型表现更好，奖励精度比约束多样性更重要


<details>
  <summary>Details</summary>
Motivation: 挑战当前强化学习中指令跟随任务的主流共识，即认为多样化的约束混合（可验证硬约束和不可验证软约束）对于泛化到未见指令至关重要。通过系统实证研究检验这一假设

Method: 进行系统性实证调查，比较仅使用硬约束训练和混合约束训练的模型表现。分析奖励精度的影响，研究LLM评判者在检测错误响应时的低召回率问题。提出以奖励精度为中心的数据精炼策略

Result: 仅使用硬约束训练的模型始终优于混合约束训练的模型。奖励精度是有效对齐的主要驱动因素，而非约束多样性。LLM评判者在检测错误响应时召回率低，导致严重奖励黑客攻击。高精度奖励发展出可迁移的指令跟随元技能

Conclusion: 研究结果支持范式转变：从盲目追求数据多样性转向关注高精度奖励。提出的数据精炼策略在五个基准测试中性能提升13.4%，训练时间减少58%，且在指令跟随之外保持强泛化能力

Abstract: A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\% in performance while achieving a 58\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.

</details>


### [175] [On the Definition and Detection of Cherry-Picking in Counterfactual Explanations](https://arxiv.org/abs/2601.04977)
*James Hinns,Sofie Goethals,Stephan Van der Veeken,Theodoros Evgeniou,David Martens*

Main category: cs.LG

TL;DR: 该研究揭示了反事实解释中存在的"选择性呈现"问题，即解释提供者可以从多个有效反事实中选择有利解释来操纵叙事，而审计者难以检测这种操纵。


<details>
  <summary>Details</summary>
Motivation: 反事实解释被广泛用于说明输入如何改变才能改变模型预测。对于单个实例，可能存在多个有效的反事实解释，这为解释提供者提供了选择性呈现的机会——他们可以选择性地展示有利的解释，隐藏揭示问题行为的例子，从而操纵叙事。

Method: 研究首先形式化定义了反事实解释中的选择性呈现问题，包括可接受的解释空间和效用函数。然后从三个访问级别研究外部审计者检测这种操纵的能力：完全程序访问、部分程序访问和仅解释访问。通过理论分析和实证研究，评估选择性呈现解释与基线解释在统计上的可区分性。

Result: 研究发现检测选择性呈现在实践中极为有限。即使拥有完全程序访问权限，选择性呈现的解释也往往难以与非选择性呈现的解释区分开来，因为有效反事实的多样性和解释规范的灵活性提供了足够的自由度来掩盖故意选择。实证研究表明，这种变异性通常超过选择性呈现对标准反事实质量指标（如接近性、合理性和稀疏性）的影响，使得选择性呈现的解释在统计上与基线解释无法区分。

Conclusion: 研究认为，保障措施应优先考虑可重复性、标准化和程序约束，而不是事后检测。为算法开发者、解释提供者和审计者提供了具体建议，强调需要建立更严格的解释生成标准和透明度要求。

Abstract: Counterfactual explanations are widely used to communicate how inputs must change for a model to alter its prediction. For a single instance, many valid counterfactuals can exist, which leaves open the possibility for an explanation provider to cherry-pick explanations that better suit a narrative of their choice, highlighting favourable behaviour and withholding examples that reveal problematic behaviour. We formally define cherry-picking for counterfactual explanations in terms of an admissible explanation space, specified by the generation procedure, and a utility function. We then study to what extent an external auditor can detect such manipulation. Considering three levels of access to the explanation process: full procedural access, partial procedural access, and explanation-only access, we show that detection is extremely limited in practice. Even with full procedural access, cherry-picked explanations can remain difficult to distinguish from non cherry-picked explanations, because the multiplicity of valid counterfactuals and flexibility in the explanation specification provide sufficient degrees of freedom to mask deliberate selection. Empirically, we demonstrate that this variability often exceeds the effect of cherry-picking on standard counterfactual quality metrics such as proximity, plausibility, and sparsity, making cherry-picked explanations statistically indistinguishable from baseline explanations. We argue that safeguards should therefore prioritise reproducibility, standardisation, and procedural constraints over post-hoc detection, and we provide recommendations for algorithm developers, explanation providers, and auditors.

</details>


### [176] [On the Hidden Objective Biases of Group-based Reinforcement Learning](https://arxiv.org/abs/2601.05002)
*Aleksandar Fontana,Marco Simoni,Giulio Rossolini,Andrea Saracino,Paolo Mori*

Main category: cs.LG

TL;DR: 本文对GRPO等基于群体的强化学习方法进行了理论分析，揭示了这些方法在奖励优化与训练目标之间的结构不匹配问题，并发现了三个影响所有分析方法的共同特性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于群体的强化学习方法（如GRPO）在大型语言模型后训练中被广泛使用且经验上成功，但这些方法在奖励优化与底层训练目标之间存在结构不匹配。本文旨在通过理论分析揭示这些方法的根本局限性。

Method: 通过将GRPO风格方法置于统一的代理公式框架下进行研究，从理论角度分析这些方法的训练动态和优化特性。

Result: 分析揭示了三个影响所有研究方法的共同特性：1）非均匀群体加权导致共享前缀令牌的系统性梯度偏差；2）与AdamW优化器的交互使训练动态对奖励缩放基本不敏感；3）优化器动量在重复优化步骤下可能将策略更新推至预期的裁剪区域之外。

Conclusion: 这些发现突显了当前方法的根本局限性，为未来公式设计提供了原则性指导，表明需要重新思考基于群体的强化学习方法的理论基础。

Abstract: Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.

</details>


### [177] [HMVI: Unifying Heterogeneous Attributes with Natural Neighbors for Missing Value Inference](https://arxiv.org/abs/2601.05017)
*Xiaopeng Luo,Zexi Tan,Zhuowei Wang*

Main category: cs.LG

TL;DR: 提出了一种新的缺失值填补方法，通过统一框架显式建模异构特征间的跨类型依赖关系，利用完整和不完整实例实现准确一致的填补，显著提升下游机器学习任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺失值填补方法通常独立处理数值和分类属性，忽略了异构特征间的关键相互依赖关系，这限制了填补的准确性和一致性。

Method: 提出了一种新颖的填补方法，在统一框架中显式建模跨类型特征依赖关系，同时利用完整和不完整实例来确保表格数据中准确一致的填补。

Result: 广泛的实验结果表明，该方法在性能上优于现有技术，并显著增强了下游机器学习任务，为现实世界中存在缺失数据的系统提供了稳健的解决方案。

Conclusion: 该方法通过建模异构特征间的跨类型依赖关系，提供了一种更准确、一致的缺失值填补解决方案，能够有效提升实际系统中的数据完整性和机器学习性能。

Abstract: Missing value imputation is a fundamental challenge in machine intelligence, heavily dependent on data completeness. Current imputation methods often handle numerical and categorical attributes independently, overlooking critical interdependencies among heterogeneous features. To address these limitations, we propose a novel imputation approach that explicitly models cross-type feature dependencies within a unified framework. Our method leverages both complete and incomplete instances to ensure accurate and consistent imputation in tabular data. Extensive experimental results demonstrate that the proposed approach achieves superior performance over existing techniques and significantly enhances downstream machine learning tasks, providing a robust solution for real-world systems with missing data.

</details>


### [178] [Approximate equivariance via projection-based regularisation](https://arxiv.org/abs/2601.05028)
*Torben Berndt,Jan Stühmer*

Main category: cs.LG

TL;DR: 提出一种基于投影的正则化方法，用于构建近似等变神经网络，相比现有基于样本的方法具有更高效率和性能


<details>
  <summary>Details</summary>
Motivation: 等变性是神经网络中强大的归纳偏置，但实际应用中存在不完美对称性，非等变模型因运行性能更好而重新受到关注。现有近似等变方法通常依赖数据增强，样本复杂度高，特别是对于连续群如SO(3)。

Method: 通过投影基正则化方法，利用线性层到等变和非等变分量的正交分解。在算子级别惩罚非等变性，而非逐点惩罚。提出在空间域和谱域精确高效计算非等变惩罚的数学框架。

Result: 实验表明，该方法在模型性能和效率方面始终优于先前的近似等变方法，相比基于样本的正则化方法实现了显著的运行时增益。

Conclusion: 提出的投影基正则化方法为构建近似等变神经网络提供了更高效和有效的解决方案，特别适用于连续对称群。

Abstract: Equivariance is a powerful inductive bias in neural networks, improving generalisation and physical consistency. Recently, however, non-equivariant models have regained attention, due to their better runtime performance and imperfect symmetries that might arise in real-world applications. This has motivated the development of approximately equivariant models that strike a middle ground between respecting symmetries and fitting the data distribution. Existing approaches in this field usually apply sample-based regularisers which depend on data augmentation at training time, incurring a high sample complexity, in particular for continuous groups such as $SO(3)$. This work instead approaches approximate equivariance via a projection-based regulariser which leverages the orthogonal decomposition of linear layers into equivariant and non-equivariant components. In contrast to existing methods, this penalises non-equivariance at an operator level across the full group orbit, rather than point-wise. We present a mathematical framework for computing the non-equivariance penalty exactly and efficiently in both the spatial and spectral domain. In our experiments, our method consistently outperforms prior approximate equivariance approaches in both model performance and efficiency, achieving substantial runtime gains over sample-based regularisers.

</details>


### [179] [A Data-Driven Predictive Framework for Inventory Optimization Using Context-Augmented Machine Learning Models](https://arxiv.org/abs/2601.05033)
*Anees Fatima,Mohammad Abdus Salam*

Main category: cs.LG

TL;DR: 本研究使用四种机器学习算法（XGBoost、ARIMA、Facebook Prophet、SVR）结合外部因素（工作日、节假日、销售偏差指标）改进零售和自动售货机行业的需求预测，发现XGBoost表现最佳。


<details>
  <summary>Details</summary>
Motivation: 供应链管理中的需求预测对优化库存、减少浪费和提高客户满意度至关重要。传统方法经常忽略天气、节日和设备故障等外部影响因素，导致效率低下。

Method: 研究使用四种机器学习算法：极端梯度提升（XGBoost）、自回归积分移动平均（ARIMA）、Facebook Prophet（Fb Prophet）和支持向量回归（SVR）来预测库存需求。系统性地纳入了工作日、节假日和销售偏差指标等外部因素以提高预测精度。

Result: XGBoost在包含外部变量时表现最佳，达到最低平均绝对误差（MAE）22.7。ARIMAX和Fb Prophet也显示出显著改进，而SVR表现较差。纳入外部因素显著提高了需求预测模型的精度。

Conclusion: XGBoost被确定为最有效的需求预测算法，纳入外部因素能显著提高预测精度。本研究为改进零售和自动售货机系统的库存管理提供了强有力的框架。

Abstract: Demand forecasting in supply chain management (SCM) is critical for optimizing inventory, reducing waste, and improving customer satisfaction. Conventional approaches frequently neglect external influences like weather, festivities, and equipment breakdowns, resulting in inefficiencies. This research investigates the use of machine learning (ML) algorithms to improve demand prediction in retail and vending machine sectors. Four machine learning algorithms. Extreme Gradient Boosting (XGBoost), Autoregressive Integrated Moving Average (ARIMA), Facebook Prophet (Fb Prophet), and Support Vector Regression (SVR) were used to forecast inventory requirements. Ex-ternal factors like weekdays, holidays, and sales deviation indicators were methodically incorporated to enhance precision. XGBoost surpassed other models, reaching the lowest Mean Absolute Error (MAE) of 22.7 with the inclusion of external variables. ARIMAX and Fb Prophet demonstrated noteworthy enhancements, whereas SVR fell short in performance. Incorporating external factors greatly improves the precision of demand forecasting models, and XGBoost is identified as the most efficient algorithm. This study offers a strong framework for enhancing inventory management in retail and vending machine systems.

</details>


### [180] [DeepWeightFlow: Re-Basined Flow Matching for Generating Neural Network Weights](https://arxiv.org/abs/2601.05052)
*Saumya Gupta,Scott Biggs,Moritz Laber,Zohair Shafi,Robin Walters,Ayan Paul*

Main category: cs.LG

TL;DR: DeepWeightFlow是一种基于流匹配的生成模型，可直接在权重空间中生成多样化、高精度的神经网络权重，无需微调即可使用，并能扩展到大型网络。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型面临挑战：高维权重空间和对称性问题；部分模型只能生成部分权重（特别是大型模型如ResNet、ViT）；完整权重生成模型存在生成速度慢或需要微调的问题。

Method: 提出DeepWeightFlow流匹配模型，直接在权重空间操作；应用Git Re-Basin和TransFusion进行神经网络规范化，处理排列对称性问题并提高大型模型的生成效率。

Result: 生成的神经网络无需微调即可表现良好，可扩展到大型网络；在迁移学习中表现出色；能在几分钟内生成数百个神经网络的集成，效率远超基于扩散的方法。

Conclusion: DeepWeightFlow为更高效、可扩展的多样化神经网络集合生成开辟了新途径。

Abstract: Building efficient and effective generative models for neural network weights has been a research focus of significant interest that faces challenges posed by the high-dimensional weight spaces of modern neural networks and their symmetries. Several prior generative models are limited to generating partial neural network weights, particularly for larger models, such as ResNet and ViT. Those that do generate complete weights struggle with generation speed or require finetuning of the generated models. In this work, we present DeepWeightFlow, a Flow Matching model that operates directly in weight space to generate diverse and high-accuracy neural network weights for a variety of architectures, neural network sizes, and data modalities. The neural networks generated by DeepWeightFlow do not require fine-tuning to perform well and can scale to large networks. We apply Git Re-Basin and TransFusion for neural network canonicalization in the context of generative weight models to account for the impact of neural network permutation symmetries and to improve generation efficiency for larger model sizes. The generated networks excel at transfer learning, and ensembles of hundreds of neural networks can be generated in minutes, far exceeding the efficiency of diffusion-based methods. DeepWeightFlow models pave the way for more efficient and scalable generation of diverse sets of neural networks.

</details>


### [181] [Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward](https://arxiv.org/abs/2601.05073)
*Jianlong Chen,Daocheng Fu,Shengze Xu,Jiawei Chen,Yuan Feng,Yue Yang,Junchi Yan,Hongyuan Zha,Renqiu Xia*

Main category: cs.LG

TL;DR: 该论文针对多模态大语言模型在复杂几何推理上的不足，提出从结果监督转向子目标级评估与学习的新范式，通过构建GeoGoal基准和SGVR框架显著提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在复杂几何推理方面表现不佳，主要因为传统的"黑箱"结果监督无法区分幸运猜测和严谨推导，导致模型学习效率低下。

Method: 1. 构建GeoGoal基准：通过形式化验证数据引擎将抽象证明转换为可验证的数值子目标；2. 提出SGVR框架：用基于骨架率的密集奖励替代稀疏信号，实现子目标级可验证奖励。

Result: SGVR框架显著提升几何推理性能(+9.7%)，并展现出强泛化能力，在一般数学任务(+8.0%)和其他通用推理任务(+2.8%)上也有明显提升。

Conclusion: 子目标级评估与学习范式能有效解决MLLMs在复杂几何推理中的局限性，SGVR框架通过密集奖励机制提升推理质量，且具有跨领域泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) struggle with complex geometric reasoning, largely because "black box" outcome-based supervision fails to distinguish between lucky guesses and rigorous deduction. To address this, we introduce a paradigm shift towards subgoal-level evaluation and learning. We first construct GeoGoal, a benchmark synthesized via a rigorous formal verification data engine, which converts abstract proofs into verifiable numeric subgoals. This structure reveals a critical divergence between reasoning quality and outcome accuracy. Leveraging this, we propose the Sub-Goal Verifiable Reward (SGVR) framework, which replaces sparse signals with dense rewards based on the Skeleton Rate. Experiments demonstrate that SGVR not only enhances geometric performance (+9.7%) but also exhibits strong generalization, transferring gains to general math (+8.0%) and other general reasoning tasks (+2.8%), demonstrating broad applicability across diverse domains.

</details>


### [182] [Exploring Student Expectations and Confidence in Learning Analytics](https://arxiv.org/abs/2601.05082)
*Hayk Asatryan,Basile Tousside,Janis Mohr,Malte Neugebauer,Hildo Bijl,Paul Spiegelberg,Claudia Frohn-Schauf,Jörg Frochte*

Main category: cs.LG

TL;DR: 使用SELAQ问卷分析学生对学习分析数据处理的期望和信心，识别出四个学生群体：热情者、现实主义者、谨慎者和漠不关心者。


<details>
  <summary>Details</summary>
Motivation: 学习分析在教育系统中广泛应用，但数据收集需要符合日益增长的隐私法规要求。需要了解学生对学习分析数据处理的期望和信心，以平衡学习优化与隐私保护的需求。

Method: 使用学生学习分析期望问卷（SELAQ）收集来自不同院系学生的数据，通过聚类算法分析学生对学习分析数据处理的期望和信心。

Result: 通过聚类分析识别出四个学生群体：热情者（对学习分析持积极态度）、现实主义者（理性看待）、谨慎者（对隐私敏感）和漠不关心者（不关注）。

Conclusion: 这种结构化分析为了解学生对学习分析的接受度和批评提供了有价值的见解，有助于在教育系统中更好地实施学习分析，同时尊重学生隐私期望。

Abstract: Learning Analytics (LA) is nowadays ubiquitous in many educational systems, providing the ability to collect and analyze student data in order to understand and optimize learning and the environments in which it occurs. On the other hand, the collection of data requires to comply with the growing demand regarding privacy legislation. In this paper, we use the Student Expectation of Learning Analytics Questionnaire (SELAQ) to analyze the expectations and confidence of students from different faculties regarding the processing of their data for Learning Analytics purposes. This allows us to identify four clusters of students through clustering algorithms: Enthusiasts, Realists, Cautious and Indifferents. This structured analysis provides valuable insights into the acceptance and criticism of Learning Analytics among students.

</details>


### [183] [Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning](https://arxiv.org/abs/2601.05134)
*Polina Dolgova,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 本文提出了一种名为"顺序噪声调度"的新方法，通过在参数空间的正交子空间中分配噪声预算，显著提高了基于差分隐私的认证遗忘的实用性，同时保持原有的隐私保证。


<details>
  <summary>Details</summary>
Motivation: 基于差分隐私的认证遗忘虽然提供强大的隐私保证，但现有方法（特别是噪声微调方法）在实现这些保证时会严重降低模型准确性，导致方法在实际应用中不实用。

Method: 提出了顺序噪声调度方法，将噪声预算分配到参数空间的正交子空间中，而不是一次性注入所有噪声。这种方法减轻了噪声的破坏性影响，同时保持了原有的认证保证。作者还将噪声微调的分析扩展到子空间设置，证明了相同的(ε,δ)隐私预算得以保留。

Result: 在图像分类基准测试上的实证结果表明，该方法在遗忘后显著提高了准确性，同时保持了对成员推理攻击的鲁棒性。这些结果表明认证遗忘可以同时实现严格的隐私保证和实际效用。

Conclusion: 顺序噪声调度方法解决了认证遗忘中准确性与隐私保证之间的权衡问题，使得基于差分隐私的认证遗忘既具有理论上的严格保证，又具备实际应用价值。

Abstract: Certified unlearning based on differential privacy offers strong guarantees but remains largely impractical: the noisy fine-tuning approaches proposed so far achieve these guarantees but severely reduce model accuracy. We propose sequential noise scheduling, which distributes the noise budget across orthogonal subspaces of the parameter space, rather than injecting it all at once. This simple modification mitigates the destructive effect of noise while preserving the original certification guarantees. We extend the analysis of noisy fine-tuning to the subspace setting, proving that the same $(\varepsilon,δ)$ privacy budget is retained. Empirical results on image classification benchmarks show that our approach substantially improves accuracy after unlearning while remaining robust to membership inference attacks. These results show that certified unlearning can achieve both rigorous guarantees and practical utility.

</details>


### [184] [Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art](https://arxiv.org/abs/2601.05152)
*Timofey Tomashevskiy*

Main category: cs.LG

TL;DR: 本文对持续安全在线强化学习（COSRL）方法进行了全面的综述，讨论了理论、挑战和开放性问题，提供了基于安全学习机制的分类，并探讨了构建可靠安全在线学习算法的前景。


<details>
  <summary>Details</summary>
Motivation: 随着强化学习在现实世界应用中的扩展，特别是在非平稳环境中，确保学习过程的安全性和适应性变得至关重要。本文旨在系统梳理持续安全在线强化学习领域的研究现状，为构建可靠的安全在线学习算法提供理论基础和方法指导。

Method: 采用文献综述方法，首先对COSRL方法进行系统分类，基于安全学习机制的类型（考虑非平稳性适应）进行分类。然后分类讨论在线强化学习算法的安全约束表述，最后分析构建可靠安全在线学习算法的前景。

Result: 提供了持续安全在线强化学习方法的全面分类体系，包括基于HM-MDP、NSMDP、POMDP、安全POMDP等框架的方法。总结了安全约束在持续学习中的表述方式，并识别了该领域的关键挑战和开放性问题。

Conclusion: 持续安全在线强化学习是一个重要且具有挑战性的研究领域，需要进一步研究以开发能够适应非平稳环境的安全强化学习算法。本文为研究人员提供了系统的分类框架和研究方向，有助于推动该领域的发展。

Abstract: This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorize safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss prospects for creating reliable, safe online learning algorithms.
  Keywords: safe RL in nonstationary environments, safe continual reinforcement learning under nonstationarity, HM-MDP, NSMDP, POMDP, safe POMDP, constraints for continual learning, safe continual reinforcement learning review, safe continual reinforcement learning survey, safe continual reinforcement learning, safe online learning under distribution shift, safe continual online adaptation, safe reinforcement learning, safe exploration, safe adaptation, constrained Markov decision processes, safe reinforcement learning, partially observable Markov decision process, safe reinforcement learning and hidden Markov decision processes, Safe Online Reinforcement Learning, safe online reinforcement learning, safe online reinforcement learning, safe meta-learning, safe meta-reinforcement learning, safe context-based reinforcement learning, formulating safety constraints for continual learning

</details>


### [185] [FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts](https://arxiv.org/abs/2601.05174)
*Yiji Zhao,Zihao Zhong,Ao Wang,Haomin Wen,Ming Jin,Yuxuan Liang,Huaiyu Wan,Hao Wu*

Main category: cs.LG

TL;DR: FaST是一个基于异质性感知专家混合模型的高效框架，用于大规模时空图的长时预测，能够实现一周前（672步，15分钟粒度）的预测，支持数千个节点。


<details>
  <summary>Details</summary>
Motivation: 现有时空图预测模型主要关注短时预测，在扩展到长时预测和大规模图时面临计算成本和内存消耗的巨大挑战。

Method: 1. 自适应图代理注意力机制：减轻传统图卷积和自注意力模块在大规模图上的计算负担；2. 并行MoE模块：用门控线性单元替代传统前馈网络，实现高效可扩展的并行结构。

Result: 在真实世界数据集上的大量实验表明，FaST不仅提供了优越的长时预测准确性，而且相比最先进的基线方法实现了显著的计算效率提升。

Conclusion: FaST是一个有效且高效的框架，能够解决大规模时空图长时预测中的计算和内存挑战，实现了前所未有的预测范围（一周前）和规模（数千节点）。

Abstract: Spatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graphs. Targeting the above challenges, we present FaST, an effective and efficient framework based on heterogeneity-aware Mixture-of-Experts (MoEs) for long-horizon and large-scale STG forecasting, which unlocks one-week-ahead (672 steps at a 15-minute granularity) prediction with thousands of nodes. FaST is underpinned by two key innovations. First, an adaptive graph agent attention mechanism is proposed to alleviate the computational burden inherent in conventional graph convolution and self-attention modules when applied to large-scale graphs. Second, we propose a new parallel MoE module that replaces traditional feed-forward networks with Gated Linear Units (GLUs), enabling an efficient and scalable parallel structure. Extensive experiments on real-world datasets demonstrate that FaST not only delivers superior long-horizon predictive accuracy but also achieves remarkable computational efficiency compared to state-of-the-art baselines. Our source code is available at: https://github.com/yijizhao/FaST.

</details>


### [186] [An interpretable data-driven approach to optimizing clinical fall risk assessment](https://arxiv.org/abs/2601.05194)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Holley Farley,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 本研究通过数据驱动方法优化约翰霍普金斯跌倒风险评估工具(JHFRAT)的评分权重，在保持工具原有结构和临床阈值的同时，显著提升了跌倒风险预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有JHFRAT工具的跌倒风险预测与临床实际意义测量之间存在偏差，需要更好地对齐。通过数据驱动方法优化评分权重，可以在不改变工具形式和部署流程的情况下，提高预测准确性，从而改善患者安全和资源分配。

Method: 采用回顾性队列分析，纳入约翰霍普金斯卫生系统三家医院2022年3月至2023年10月的54,209例住院患者。使用约束评分优化(CSO)模型重新加权JHFRAT评分权重，保持其累加结构和临床阈值不变。同时比较了包含和不包含电子健康记录变量的CSO模型，并与XGBoost黑盒模型进行基准对比。

Result: CSO模型显著优于当前JHFRAT工具(AUC-ROC: 0.91 vs 0.86)。这一性能提升相当于每周在约翰霍普金斯卫生系统中多保护35名高风险患者。虽然XGBoost模型性能更优(AUC-ROC=0.94)，但CSO模型对风险标签变化更具鲁棒性。CSO模型在有/无EHR变量情况下表现相似。

Conclusion: 基于证据的约束评分优化方法为卫生系统提供了稳健基础，可通过数据驱动优化技术系统性地增强住院患者跌倒预防协议和患者安全，有助于改善医疗环境中的风险评估和资源分配。

Abstract: In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study's risk labels, and without changing the tool's form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.

</details>


### [187] [EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI](https://arxiv.org/abs/2601.05205)
*Zain Iqbal,Lorenzo Valerio*

Main category: cs.LG

TL;DR: EARL：一个能量感知的强化学习框架，用于联合优化液体状态机的准确性和能耗，显著提升资源受限设备上AI应用的效率


<details>
  <summary>Details</summary>
Motivation: 随着AI在设备端部署的增加，需要低延迟、高能效的计算系统。液体状态机在低功耗时序处理方面有潜力，但传统优化方法忽略了能量约束，且超参数敏感度高，部署困难

Method: EARL框架结合贝叶斯优化和自适应强化学习选择策略，使用代理模型进行全局探索，强化学习动态优先选择候选方案，并采用提前终止机制消除冗余评估

Result: 在三个基准数据集上，EARL相比领先的超参数调优框架实现了6-15%的准确率提升，60-80%的能耗降低，优化时间减少一个数量级

Conclusion: 能量感知的自适应搜索能有效提高液体状态机在资源受限设备端AI应用中的效率和可扩展性

Abstract: Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.

</details>


### [188] [Robust Reasoning as a Symmetry-Protected Topological Phase](https://arxiv.org/abs/2601.05240)
*Ilmo Sung*

Main category: cs.LG

TL;DR: 该论文提出将大语言模型的逻辑推理问题重新概念化为拓扑相变问题，通过引入非阿贝尔规范对称性实现鲁棒推理，在符号操作任务中展现出超越传统Transformer的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在"幻觉"问题，即语义噪声导致的逻辑不一致性。作者认为当前架构在"度量相"中运行，因果顺序容易受到自发对称性破缺的影响，需要寻找更鲁棒的推理机制。

Method: 提出将鲁棒推理概念化为对称性保护的拓扑相，其中逻辑操作与非阿贝尔任意子编织形式同构。开发了全纯网络（Holonomic Network），用鲁棒的拓扑不变量替代脆弱的几何插值，利用非阿贝尔规范对称性保护逻辑推理。

Result: 实验显示明显的拓扑相变：Transformer和RNN表现出无间隙衰减，而全纯网络展现出宏观"质量间隙"，在临界噪声阈值下保持不变的保真度。在S10（360万状态）的变量绑定任务中，拓扑模型在训练范围外100倍（L=50→5000）仍保持完美保真度，而Transformer失去逻辑一致性。消融研究表明这种保护严格来自非阿贝尔规范对称性。

Conclusion: 这为逻辑推理提供了一个新的普适性类别，将因果稳定性与语义流形的拓扑结构联系起来。研究表明通过拓扑方法可以实现超越传统架构的鲁棒推理和泛化能力。

Abstract: Large language models suffer from "hallucinations"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a "Metric Phase," where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic "mass gap," maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\times$ beyond training ($L=50 \to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.

</details>


### [189] [Optimal Lower Bounds for Online Multicalibration](https://arxiv.org/abs/2601.05245)
*Natalie Collina,Jiuyao Lu,Georgy Noarov,Aaron Roth*

Main category: cs.LG

TL;DR: 本文证明了在线多校准的紧下界，建立了与边际校准的信息论分离。在一般设置下，使用三个不相交的二元组证明了Ω(T^{2/3})的下界；在组函数仅依赖于上下文的情况下，通过正交函数系统构造了Θ(T)大小的组族，证明了Ω̃(T^{2/3})的下界。


<details>
  <summary>Details</summary>
Motivation: 研究在线多校准问题的计算复杂性下界，特别是与边际校准问题的分离。现有研究已经为边际校准提供了O(T^{2/3-ε})的上界，但多校准问题的下界尚未明确。本文旨在建立多校准问题的紧下界，证明其本质上比边际校准更困难。

Method: 采用信息论方法证明下界。首先在一般设置下，使用三个不相交的二元组构造反例，证明Ω(T^{2/3})的下界。然后在组函数仅依赖于上下文的情况下，利用正交函数系统构造大小为Θ(T)的组族，证明Ω̃(T^{2/3})的下界。

Result: 1. 在一般设置下，证明了Ω(T^{2/3})的期望多校准误差下界，与Noarov等人的上界匹配（对数因子内）。2. 该下界超过了边际校准的O(T^{2/3-ε})上界，从而分离了两个问题。3. 在组函数仅依赖于上下文的情况下，证明了Ω̃(T^{2/3})的下界，同样与现有上界匹配。

Conclusion: 在线多校准问题本质上比边际校准更困难，具有Ω(T^{2/3})的紧下界。这一结果建立了两个校准问题之间的信息论分离，为理解在线学习中的校准复杂性提供了理论基础。

Abstract: We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.
  In the general setting where group functions can depend on both context and the learner's predictions, we prove an $Ω(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the $O(T^{2/3-\varepsilon})$ upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems.
  We then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner's predictions. In this case, we establish an $\widetildeΩ(T^{2/3})$ lower bound for online multicalibration via a $Θ(T)$-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors.

</details>
