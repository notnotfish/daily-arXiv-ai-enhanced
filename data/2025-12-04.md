<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation](https://arxiv.org/abs/2512.03048)
*Austin Spizzirri*

Main category: cs.AI

TL;DR: 论文主张将AI对齐重新构想为通过基于过程、多智能体、发展性机制来构建熵减的、对理性响应的智能体，而非编码固定的人类价值内容。


<details>
  <summary>Details</summary>
Motivation: 传统基于内容的价值规范存在结构性不稳定性问题，包括"是-应该"鸿沟、价值多元论和扩展框架问题，这构成了"规范陷阱"，需要新的对齐框架。

Method: 提出熵减（syntropy）作为理解多智能体对齐动态的信息论框架，建立基于兼容主义指导控制理论的真实与模拟道德能力的功能区分，并设计具身实验范式和验证机制。

Result: 该框架产生了关于人工系统中价值涌现和道德能动性的具体、可证伪预测，但实证验证仍在进行中，是更广泛研究计划的哲学组成部分。

Conclusion: AI对齐应转向基于过程的、多智能体的发展性方法，通过熵减机制构建对理性响应的智能体，而非试图编码固定的人类价值内容。

Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.

</details>


### [2] [Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI](https://arxiv.org/abs/2512.03072)
*Hu Keyi*

Main category: cs.AI

TL;DR: 论文提出了一种名为"权重计算主义"的新型认知架构，基于逻辑原子和基本操作构建，通过可解释的权重计算模型实现决策，为AGI提供了透明、可解释且价值对齐的理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI范式作为"体验架构师"面临可解释性和价值对齐的根本挑战，需要一种基于第一原理的认知架构来解决这些问题，为AGI提供可行的路径。

Method: 将认知解构为不可分割的逻辑原子和两个基本操作：指向和比较。通过可解释的权重计算模型（权重=收益*概率）形式化决策，所有值都可追溯到可审计的初始权重集。采用基于图算法的计算引擎和全局工作空间工作流实现。

Result: 该架构实现了透明、类人的推理，并在前所未有的场景中展现出强大的学习能力，为构建可信赖且对齐的AGI建立了实践和理论基础。

Conclusion: 权重计算主义为AGI提供了一条可行的路径，通过原子分解实现了根本的可解释性、对新情境的内在通用性以及可追溯的价值对齐，为解决当前AI范式面临的挑战提供了理论框架。

Abstract: Current AI paradigms, as "architects of experience," face fundamental challenges in explainability and value alignment. This paper introduces "Weight-Calculatism," a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.

</details>


### [3] [When Do Symbolic Solvers Enhance Reasoning in Large Language Models?](https://arxiv.org/abs/2512.03272)
*Zhiyuan He,Dingmin Wang*

Main category: cs.AI

TL;DR: 本文探讨了符号求解器集成方法何时能增强传统长思维链推理，发现该方法仅在问题需要有限隐式推理但涉及充分搜索空间时有效，特别是在需要重复回溯的约束满足问题上。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成长思维链在复杂推理任务上表现良好，但这种方法可能导致大量token开销，甚至因"过度思考"产生冗长推理链而导致错误答案。符号求解器集成方法利用LLM的代码生成能力将推理任务转换为可执行代码，然后用符号求解器解决，但何时这种传统长思维链能被符号求解器增强仍是一个开放问题。

Method: 采用符号求解器集成方法，利用LLM的代码生成能力将推理任务翻译成可执行代码，然后使用符号求解器解决。通过实验比较传统长思维链方法与符号求解器集成方法在不同类型推理问题上的表现。

Result: 实验结果显示：1）符号求解器集成方法仅在问题需要有限隐式推理但涉及充分搜索空间时有效；2）最新LLM（如GPT-4o）在推理深度较浅的演绎问题上表现更好；3）符号求解器集成方法显著提高了LLM在需要重复回溯的约束满足问题上的性能；4）当提供声明性示例时，即使是CodeLlama-13B也能在困难的斑马谜题上超越GPT-4o。

Conclusion: 符号求解器集成方法对特定类型的推理问题有显著优势，特别是那些需要大量搜索和回溯的约束满足问题，而对于推理深度较浅的演绎问题，传统长思维链方法可能更有效。这表明需要根据问题类型选择合适的推理方法。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models "overthink" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.

</details>


### [4] [Prior preferences in active inference agents: soft, hard, and goal shaping](https://arxiv.org/abs/2512.03293)
*Filippo Torresan,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: 本文研究了主动推理中偏好分布的四种定义方式（硬目标vs软目标，有无目标塑造），在网格世界导航任务中比较了不同偏好分布对智能体性能的影响。


<details>
  <summary>Details</summary>
Motivation: 主动推理使用期望自由能作为规划目标，但文献中很少关注偏好分布应如何指定以及不同指定方式对推理和学习的影响。本文旨在填补这一研究空白。

Method: 考虑了四种定义偏好分布的方式：硬目标vs软目标，有无目标塑造（中间目标）。在网格世界导航任务中比较了四种智能体的性能。

Result: 目标塑造能带来最佳整体性能（促进利用），但会牺牲对环境转移动态的学习（阻碍探索）。

Conclusion: 偏好分布的定义方式对主动推理智能体的性能有显著影响，目标塑造在促进利用的同时会限制探索，需要在两者之间权衡。

Abstract: Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).

</details>


### [5] [Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia](https://arxiv.org/abs/2512.03318)
*Chandler Smith,Marwa Abdulhai,Manfred Diaz,Marko Tesic,Rakshit S. Trivedi,Alexander Sasha Vezhnevets,Lewis Hammond,Jesse Clifton,Minsuk Chang,Edgar A. Duéñez-Guzmán,John P. Agapiou,Jayd Matyas,Danny Karmon,Akash Kundu,Aliaksei Korshuk,Ananya Ananya,Arrasy Rahman,Avinaash Anand Kulandaivel,Bain McHale,Beining Zhang,Buyantuev Alexander,Carlos Saith Rodriguez Rojas,Caroline Wang,Chetan Talele,Chenao Liu,Chichen Lin,Diana Riazi,Di Yang Shi,Emanuel Tewolde,Elizaveta Tennant,Fangwei Zhong,Fuyang Cui,Gang Zhao,Gema Parreño Piqueras,Hyeonggeun Yun,Ilya Makarov,Jiaxun Cui,Jebish Purbey,Jim Dilkes,Jord Nguyen,Lingyun Xiao,Luis Felipe Giraldo,Manuela Chacon-Chamorro,Manuel Sebastian Rios Beltran,Marta Emili García Segura,Mengmeng Wang,Mogtaba Alim,Nicanor Quijano,Nico Schiavone,Olivia Macmillan-Scott,Oswaldo Peña,Peter Stone,Ram Mohan Rao Kadiyala,Rolando Fernandez,Ruben Manrique,Sunjia Lu,Sheila A. McIlraith,Shamika Dhuri,Shuqing Shi,Siddhant Gupta,Sneheel Sarangi,Sriram Ganapathi Subramanian,Taehun Cha,Toryn Q. Klassen,Wenming Tu,Weijian Fan,Wu Ruiyang,Xue Feng,Yali Du,Yang Liu,Yiding Wang,Yipeng Kang,Yoonchang Sung,Yuxuan Chen,Zhaowei Zhang,Zhihan Wang,Zhiqiang Wu,Ziang Chen,Zilong Zheng,Zixia Jia,Ziyan Wang,Dylan Hadfield-Menell,Natasha Jaques,Tim Baarslag,Jose Hernandez-Orallo,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 该论文提出了一种评估LLM智能体在零样本混合动机环境中合作能力的方法，使用Concordia自然语言多智能体模拟环境，揭示了当前智能体在需要说服和规范执行的场景中存在显著能力差距。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在社会互动方面展现出强大能力，但现有评估方法无法衡量这些能力在新颖社交情境中的泛化能力，特别是在混合动机环境中实现可靠合作的能力。

Method: 引入Concordia自然语言多智能体模拟环境，通过测试智能体在不同合作伙伴和情境中识别并利用互利机会的能力，来评估其一般合作智能。基于NeurIPS 2024 Concordia竞赛进行实证评估。

Result: 研究发现当前智能体能力与实现可靠合作所需的稳健泛化之间存在显著差距，特别是在需要说服和规范执行的场景中。智能体在谈判和集体行动问题等多样化情境中实现互利的能力有限。

Conclusion: LLM智能体在零样本混合动机环境中的合作能力仍有待提升，特别是在复杂社交情境中。需要进一步研究来提高智能体在多样化社交互动中的合作泛化能力。

Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.

</details>


### [6] [Multimodal Reinforcement Learning with Agentic Verifier for AI Agents](https://arxiv.org/abs/2512.03438)
*Reuben Tan,Baolin Peng,Zhengyuan Yang,Hao Cheng,Oier Mees,Theodore Zhao,Andrea Tupini,Isar Meijier,Qianhui Wu,Yuncong Yang,Lars Liden,Yu Gu,Sheng Zhang,Xiaodong Liu,Lijuan Wang,Marc Pollefeys,Yong Jae Lee,Jianfeng Gao*

Main category: cs.AI

TL;DR: Argos是一个用于多模态强化学习的智能奖励代理，通过选择性地组合教师模型和基于规则的评分函数来评估最终答案准确性、时空定位和推理过程质量，从而改进智能体训练。


<details>
  <summary>Details</summary>
Motivation: 当前多模态强化学习主要依赖稀疏的、基于最终结果的奖励信号，缺乏对推理过程的细粒度指导。不同样本需要不同的评分函数，且教师模型可能提供噪声奖励信号，这限制了学习效果。

Method: 提出Argos智能奖励代理，为每个样本从教师模型衍生和基于规则的评分函数池中选择合适的函数，同时评估：1)最终响应准确性；2)所提及实体和动作的时空定位；3)推理过程质量。

Result: Argos在监督微调数据整理和强化学习训练中都取得了最先进的结果，在空间推理、视觉幻觉以及机器人和具身AI基准测试中表现优异。能有效防止智能体在强化学习中崩溃到非基础解，减少奖励黑客行为。

Conclusion: 仅依赖高质量推理数据的监督微调后训练是不够的，强化学习过程中需要在线验证。Argos通过帕累托最优性提供了理论依据，证明了智能奖励代理在多模态强化学习中的有效性。

Abstract: Agentic reasoning models trained with multimodal reinforcement learning (MMRL) have become increasingly capable, yet they are almost universally optimized using sparse, outcome-based rewards computed based on the final answers. Richer rewards computed from the reasoning tokens can improve learning significantly by providing more fine-grained guidance. However, it is challenging to compute more informative rewards in MMRL beyond those based on outcomes since different samples may require different scoring functions and teacher models may provide noisy reward signals too. In this paper, we introduce the Argos (Agentic Reward for Grounded & Objective Scoring), a principled reward agent to train multimodal reasoning models for agentic tasks. For each sample, Argos selects from a pool of teacher-model derived and rule-based scoring functions to simultaneously evaluate: (i) final response accuracy, (ii) spatiotemporal localization of referred entities and actions, and (iii) the quality of the reasoning process. We find that by leveraging our agentic verifier across both SFT data curation and RL training, our model achieves state-of-the-art results across multiple agentic tasks such as spatial reasoning, visual hallucination as well as robotics and embodied AI benchmarks. Critically, we demonstrate that just relying on SFT post-training on highly curated reasoning data is insufficient, as agents invariably collapse to ungrounded solutions during RL without our online verification. We also show that our agentic verifier can help to reduce reward-hacking in MMRL. Finally, we also provide a theoretical justification for the effectiveness of Argos through the concept of pareto-optimality.

</details>


### [7] [Multi-Agent Reinforcement Learning with Communication-Constrained Priors](https://arxiv.org/abs/2512.03528)
*Guang Yang,Tianpei Yang,Jingwen Qiao,Yanqing Wu,Jing Huo,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: 提出一个通信受限的多智能体强化学习框架，通过区分有损和无损消息，将通信影响量化到全局奖励中，提高在复杂动态环境中的可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中普遍存在有损通信问题，现有基于通信的多智能体强化学习方法由于可扩展性和鲁棒性有限，难以应用于复杂动态的真实环境。

Method: 1) 提出广义通信约束模型统一描述不同场景的通信条件；2) 将其作为学习先验区分有损和无损消息；3) 使用双重互信息估计器解耦有损和无损消息对分布式决策的影响；4) 引入通信约束的多智能体强化学习框架，将通信消息影响量化到全局奖励中。

Result: 在多个通信约束基准测试中验证了方法的有效性。

Conclusion: 提出的通信约束多智能体强化学习框架能够有效处理现实世界中的有损通信问题，提高系统在复杂动态环境中的适应性和性能。

Abstract: Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.

</details>


### [8] [PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks](https://arxiv.org/abs/2512.03549)
*Yuki Orimo,Iori Kurata,Hodaka Mori,Ryuhei Okuno,Ryohto Sawada,Daisuke Okanohara*

Main category: cs.AI

TL;DR: PARC是一个用于自主执行长时程计算任务的编码代理，采用分层多智能体架构，具备自我评估和反馈机制，能够在材料科学和Kaggle竞赛中完成复杂任务。


<details>
  <summary>Details</summary>
Motivation: 开发能够自主执行长时程计算任务的AI系统，减少人工干预，提高科学研究和数据分析的自动化程度。

Method: 采用分层多智能体架构，包含任务规划、执行、自我评估和反馈机制，能够从独立上下文评估自身行动和结果，检测并纠正高级战略错误。

Result: 在材料科学中成功复现锂离子传导和合金偏析研究的关键结果，协调数十个并行模拟任务（每个约43小时计算）；在Kaggle实验中，从自然语言指令出发，实现数据分析并产生与人工基线竞争的结果。

Conclusion: 分层多智能体系统与自我评估/反馈机制的结合，能够实现独立、大规模的科学和分析工作，展示了AI系统自主执行复杂计算任务的潜力。

Abstract: We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.

</details>


### [9] [Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks](https://arxiv.org/abs/2512.03560)
*Gianni Molinari,Fabio Ciravegna*

Main category: cs.AI

TL;DR: RP-ReAct是一种新型多智能体架构，通过将战略规划与低级执行解耦来解决企业环境中自主智能体处理复杂任务的稳定性问题，并采用上下文保存策略管理大型工具输出。


<details>
  <summary>Details</summary>
Motivation: 现有自主智能体在企业领域处理需要协调多个工具和多样化数据源的复杂任务时存在两大限制：单智能体架构导致轨迹不稳定，以及本地开源模型的小上下文窗口无法有效处理大型工具输出。

Method: 提出RP-ReAct多智能体方法，包含Reasoner Planner Agent（RPA）负责使用大型推理模型进行战略规划，和Proxy-Execution Agent（PEA）使用ReAct方法将子步骤转换为具体工具交互。采用上下文保存策略通过外部存储和按需访问管理大型工具输出。

Result: 在ToolQA基准测试中使用六种开源推理模型进行评估，RP-ReAct在性能、泛化能力、鲁棒性和稳定性方面均优于现有最先进基线方法，能够有效处理跨领域的多样化复杂任务。

Conclusion: RP-ReAct通过解耦规划与执行的多智能体架构和上下文管理策略，为企业提供了可靠、高效且可部署的智能体解决方案，在不同模型规模下均表现出增强的鲁棒性。

Abstract: Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.

</details>


### [10] [EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths](https://arxiv.org/abs/2512.03571)
*Zhening Li,Armando Solar-Lezama,Yisong Yue,Stephan Zheng*

Main category: cs.AI

TL;DR: 提出了一种名为"概率天使非确定性"（PAN）的新代理编程方法，通过分离核心工作流逻辑和推理时策略，简化LLM代理开发


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理编程方法通常将核心工作流逻辑和推理时策略（如树搜索）耦合在一起，这限制了开发者的灵活性和实验效率

Method: 引入PAN编程模型，将工作流描述与推理策略解耦；实现EnCompass框架，使用Python装饰器将代理工作流程序编译为搜索空间

Result: 通过三个案例研究证明，该框架能让开发者快速提升代理可靠性，轻松切换不同推理策略，且只需少量额外编码

Conclusion: PAN模型和EnCompass框架为LLM代理开发提供了更灵活、可实验的编程范式，显著提高了开发效率和代理性能

Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.

</details>


### [11] [DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization](https://arxiv.org/abs/2512.03607)
*Yusen Wu,Xiaotie Deng*

Main category: cs.AI

TL;DR: DeepRule是一个用于零售品类和定价优化的自动化业务规则生成框架，通过LLM解析非结构化文本、博弈论约束优化和可解释决策蒸馏，解决理论模型与现实经济复杂性之间的系统错位问题。


<details>
  <summary>Details</summary>
Motivation: 现有理论模型与现实经济复杂性存在系统错位，具体表现为三个关键差距：1）数据模态不匹配（非结构化文本源阻碍准确客户画像）；2）动态特征纠缠挑战（非线性价格弹性和时变属性建模困难）；3）多层级业务约束导致的操作不可行性。

Method: 采用三层架构：1）混合知识融合引擎，使用LLM深度语义解析非结构化文本，将分销协议和销售评估转化为结构化特征；2）博弈论约束优化机制，通过双边效用函数动态协调供应链利益；3）可解释决策蒸馏接口，利用LLM引导的符号回归优化定价策略和可审计业务规则。

Result: 在真实零售环境中验证框架，相比系统性B2C基线实现了更高的利润，同时确保操作可行性。

Conclusion: 建立了一个闭环管道，统一了非结构化知识注入、多智能体优化和可解释策略合成，为真实经济智能提供了解决方案。

Abstract: This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.
  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.

</details>


### [12] [MemVerse: Multimodal Memory for Lifelong Learning Agents](https://arxiv.org/abs/2512.03627)
*Junming Liu,Yifei Sun,Weihua Cheng,Haodong Lei,Yirong Chen,Licheng Wen,Xuemeng Yang,Daocheng Fu,Pinlong Cai,Nianchen Deng,Yi Yu,Shuyue Hu,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: MemVerse是一个模型无关、即插即用的记忆框架，通过结合快速参数化回忆和分层检索式记忆，解决AI代理的记忆限制问题，支持多模态智能的扩展和适应。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模语言和视觉模型发展迅速，但AI代理仍存在根本性限制：无法记忆。没有可靠的记忆，代理会灾难性地遗忘过去经验，难以进行长时程推理，在多模态或交互环境中无法连贯操作。

Method: MemVerse框架结合快速参数化回忆与分层检索式记忆，维护短期记忆处理近期上下文，同时将原始多模态体验转化为结构化长期记忆，组织为分层知识图谱。采用周期性蒸馏机制将长期记忆中的关键知识压缩到参数化模型中，实现快速可微回忆并保持可解释性。

Result: 广泛实验表明，MemVerse显著提高了多模态推理和持续学习效率，使代理能够在扩展交互中记忆、适应和连贯推理。

Conclusion: MemVerse通过桥接参数化记忆和检索式记忆，解决了AI代理的记忆限制问题，实现了可扩展、自适应的多模态智能，支持持续整合、自适应遗忘和有界内存增长。

Abstract: Despite rapid progress in large-scale language and vision models, AI agents still suffer from a fundamental limitation: they cannot remember. Without reliable memory, agents catastrophically forget past experiences, struggle with long-horizon reasoning, and fail to operate coherently in multimodal or interactive environments. We introduce MemVerse, a model-agnostic, plug-and-play memory framework that bridges fast parametric recall with hierarchical retrieval-based memory, enabling scalable and adaptive multimodal intelligence. MemVerse maintains short-term memory for recent context while transforming raw multimodal experiences into structured long-term memories organized as hierarchical knowledge graphs. This design supports continual consolidation, adaptive forgetting, and bounded memory growth. To handle real-time demands, MemVerse introduces a periodic distillation mechanism that compresses essential knowledge from long-term memory into the parametric model, allowing fast, differentiable recall while preserving interpretability. Extensive experiments demonstrate that MemVerse significantly improves multimodal reasoning and continual learning efficiency, empowering agents to remember, adapt, and reason coherently across extended interactions.

</details>


### [13] [RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design](https://arxiv.org/abs/2512.03762)
*Jiawei Xu,Fengfeng Wei,Weineng Chen*

Main category: cs.AI

TL;DR: RoCo是一个基于多智能体角色协作的自动启发式设计系统，通过四个专门角色（探索者、利用者、批评者、整合者）的协作，在组合优化问题上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自动启发式设计研究通常只考虑单一角色，限制了启发式的多样性和质量。需要一种多角色协作系统来增强自动启发式设计的性能。

Method: 提出RoCo多智能体角色协作系统，包含四个专门角色：探索者（创造性、多样性驱动）、利用者（保守性、效率导向）、批评者（评估效果并提供反馈）、整合者（平衡创新与利用）。这些智能体通过结构化多轮过程进行交互，包括反馈、精炼和精英突变。

Result: 在五个不同组合优化问题的白盒和黑盒设置下评估，RoCo表现出优越性能，生成的启发式在两种场景下均优于现有方法（包括ReEvo和HSEvo）。

Conclusion: 基于角色的协作范式为稳健且高性能的自动启发式设计建立了新标准，通过多角色协作显著提升了启发式设计的多样性和质量。

Abstract: Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.

</details>


### [14] [Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783)
*Dongchao Yang,Songxiang Liu,Disong Wang,Yuanyuan Wang,Guanglu Wan,Helen Meng*

Main category: cs.AI

TL;DR: Omni-AutoThink：一种自适应推理框架，能根据任务难度动态调整Omni模型的推理深度，提升多模态感知与生成能力


<details>
  <summary>Details</summary>
Motivation: 现有Omni模型存在推理行为僵化的问题——要么对简单问题过度思考，要么在需要推理时无法有效推理。需要一种能根据任务难度自适应调整推理深度的框架来解决这一局限性。

Method: 提出两阶段框架：1) 自适应监督微调阶段，使用大规模推理增强数据赋予模型基本推理能力；2) 自适应强化学习阶段，基于任务复杂度和奖励反馈优化推理行为。同时构建了涵盖文本、文本-音频、文本-视觉、文本-音频-视觉模态的全面自适应推理基准。

Result: 实验结果表明，与先前基线相比，该框架显著提升了自适应推理性能。所有基准数据和代码将公开发布。

Conclusion: Omni-AutoThink通过自适应调整推理深度，有效解决了现有Omni模型推理行为僵化的问题，在多模态推理任务中表现出优越性能。

Abstract: Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.

</details>


### [15] [A Hierarchical Tree-based approach for creating Configurable and Static Deep Research Agent (Static-DRA)](https://arxiv.org/abs/2512.03887)
*Saurav Prateek*

Main category: cs.AI

TL;DR: 提出Static-DRA，一种基于树状静态工作流的深度研究代理，通过可配置的Depth和Breadth参数让用户平衡研究质量与计算成本


<details>
  <summary>Details</summary>
Motivation: 现有静态RAG管道在处理复杂多轮研究任务时存在局限性，需要更灵活的代理系统来支持深度研究

Method: 基于可配置层次化树状工作流，集成Depth和Breadth两个用户可调参数，采用Supervisor、Independent和Worker三层代理架构实现多跳信息检索和并行子主题研究

Result: 在DeepResearch Bench上使用RACE框架评估，配置Depth=2、Breadth=5，使用gemini-2.5-pro模型获得34.72分，实验证明增加Depth和Breadth参数能提升研究深度和评估分数

Conclusion: Static-DRA提供了实用且资源感知的解决方案，赋予用户对深度研究过程的透明控制，代码和结果已开源

Abstract: The advancement in Large Language Models has driven the creation of complex agentic systems, such as Deep Research Agents (DRAs), to overcome the limitations of static Retrieval Augmented Generation (RAG) pipelines in handling complex, multi-turn research tasks. This paper introduces the Static Deep Research Agent (Static-DRA), a novel solution built upon a configurable and hierarchical Tree-based static workflow.
  The core contribution is the integration of two user-tunable parameters, Depth and Breadth, which provide granular control over the research intensity. This design allows end-users to consciously balance the desired quality and comprehensiveness of the research report against the associated computational cost of Large Language Model (LLM) interactions. The agent's architecture, comprising Supervisor, Independent, and Worker agents, facilitates effective multi-hop information retrieval and parallel sub-topic investigation.
  We evaluate the Static-DRA against the established DeepResearch Bench using the RACE (Reference-based Adaptive Criteria-driven Evaluation) framework. Configured with a depth of 2 and a breadth of 5, and powered by the gemini-2.5-pro model, the agent achieved an overall score of 34.72. Our experiments validate that increasing the configured Depth and Breadth parameters results in a more in-depth research process and a correspondingly higher evaluation score. The Static-DRA offers a pragmatic and resource-aware solution, empowering users with transparent control over the deep research process. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/Static-Deep-Research/

</details>


### [16] [Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties](https://arxiv.org/abs/2512.03931)
*Vineel Tummala,Daniela Inclezan*

Main category: cs.AI

TL;DR: 该论文提出了一个基于逻辑编程的策略感知自主代理框架，能够推理违反策略的潜在惩罚并相应行动，扩展了AOPL语言并整合ASP推理，在保证策略合规的同时允许必要偏离。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注确保合规性，但实际场景中为实现高风险目标可能需要偏离策略。同时，建模不合规行为有助于政策制定者模拟真实的人类决策过程。

Method: 扩展Gelfond和Lobo的授权与义务策略语言(AOPL)以纳入惩罚机制，整合答案集编程(ASP)进行推理，开发从扩展AOPL到ASP的自动翻译，并改进基于ASP的规划算法以考虑惩罚。

Result: 在两个领域的实验中，该框架生成了更高质量的计划，避免了有害行动，在某些情况下还提高了计算效率。能够区分不同不合规计划，优先选择惩罚最小的方案。

Conclusion: 该框架通过惩罚推理增强了自主决策能力，为策略细化提供了信息支持，在逻辑编程领域具有实际应用潜力，目前正在TPLP期刊审稿中。

Abstract: This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [17] [Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol](https://arxiv.org/abs/2512.03955)
*Niklas Jobs,Luis Miguel Vieira da Silva,Jayanth Somashekaraiah,Maximilian Weigand,David Kube,Felix Gehlhoff*

Main category: cs.AI

TL;DR: 提出了一个基于Blocksworld问题的可执行仿真基准测试，用于系统评估LLM智能体在工业自动化中的自适应规划与执行能力


<details>
  <summary>Details</summary>
Motivation: 工业自动化需要灵活的自适应控制策略，基于LLM的智能体具有潜力但缺乏标准化基准进行系统比较

Method: 引入包含五个复杂度类别的Blocksworld问题仿真环境，集成MCP作为标准化工具接口，允许不同智能体架构无需修改即可连接评估

Result: 通过单智能体实现验证了基准的适用性，建立了用于比较LLM规划与执行方法的量化指标

Conclusion: 该基准为系统评估LLM智能体在自适应工业控制中的表现提供了标准化框架，促进了不同方法的公平比较

Abstract: Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling](https://arxiv.org/abs/2512.03050)
*Peter Hedström,Victor Lamelas Cubero,Jón Sigurdsson,Viktor Österberg,Satish Kolli,Joakim Odqvist,Ziyong Hou,Wangzhong Mu,Viswanadh Gowtham Arigela*

Main category: cs.LG

TL;DR: 该研究开发了一个结合物理洞察与机器学习的计算框架，用于预测钢的连续冷却转变（CCT）图，在4100个图的数据集上训练，能高效生成完整CCT图并展示良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在材料科学中已有广泛应用，但将其应用于复杂工业材料如钢铁仍面临挑战。主要障碍在于准确捕捉化学成分、加工参数与最终微观结构和性能之间的复杂关系。

Method: 引入结合物理洞察与机器学习的计算框架，开发物理信息化的连续冷却转变（CCT）模型。该模型在4100个CCT图的数据集上进行训练，并针对文献和实验数据进行验证。

Result: 模型计算效率高，能在5秒内生成包含100条冷却曲线的完整CCT图。对所有合金钢表现出强泛化能力，各相分类F1分数均高于88%。相变温度回归方面，除贝氏体相（MAE为27°C）外，其余各相平均绝对误差均低于20°C。

Conclusion: 该框架可通过添加通用和定制化机器学习模型扩展，建立通用的热处理数字孪生平台。与互补模拟工具和针对性实验的整合将进一步支持加速材料设计工作流程。

Abstract: Machine learning (ML) has emerged as a powerful tool for accelerating the computational design and production of materials. In materials science, ML has primarily supported large-scale discovery of novel compounds using first-principles data and digital twin applications for optimizing manufacturing processes. However, applying general-purpose ML frameworks to complex industrial materials such as steel remains a challenge. A key obstacle is accurately capturing the intricate relationship between chemical composition, processing parameters, and the resulting microstructure and properties. To address this, we introduce a computational framework that combines physical insights with ML to develop a physics-informed continuous cooling transformation (CCT) model for steels. Our model, trained on a dataset of 4,100 diagrams, is validated against literature and experimental data. It demonstrates high computational efficiency, generating complete CCT diagrams with 100 cooling curves in under 5 seconds. It also shows strong generalizability across alloy steels, achieving phase classification F1 scores above 88% for all phases. For phase transition temperature regression, it attains mean absolute errors (MAE) below 20 °C across all phases except bainite, which shows a slightly higher MAE of 27 °C. This framework can be extended with additional generic and customized ML models to establish a universal digital twin platform for heat treatment. Integration with complementary simulation tools and targeted experiments will further support accelerated materials design workflows.

</details>


### [19] [Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation](https://arxiv.org/abs/2512.03053)
*Andrew S. Cassidy,Guillaume Garreau,Jay Sivagnaname,Mike Grassi,Bernard Brezzo,John V. Arthur,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 提出一种利用大语言模型进行无损编码解码的方法，用于可逆问题（如逻辑条件表到硬件描述语言的转换），能有效减少幻觉和遗漏错误，提高开发效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在代码生成任务中常见的幻觉（hallucinations）和遗漏（omissions）问题，提高从逻辑条件表到硬件描述语言转换的准确性和可靠性。

Method: 采用信息论中无损压缩的思想，使用LLM作为从源域（逻辑条件表）到目标域（HDL代码）的无损编码器，然后再作为从目标域回源域的无损解码器，通过比较原始和重建的逻辑条件表来验证生成的正确性。

Result: 成功生成了二维片上网络路由器（13个单元，1500-2000行代码）的完整HDL代码，通过7种不同LLM进行测试，能够确认正确生成的逻辑、检测错误生成的逻辑，并帮助开发者发现设计规范错误。

Conclusion: 该方法显著提高了开发效率，不仅减少了LLM的幻觉和遗漏问题，还能辅助开发者验证设计正确性，为可逆问题的自动化转换提供了有效解决方案。

Abstract: We show for invertible problems that transform data from a source domain (for example, Logic Condition Tables (LCTs)) to a destination domain (for example, Hardware Description Language (HDL) code), an approach of using Large Language Models (LLMs) as a lossless encoder from source to destination followed by as a lossless decoder back to the source, comparable to lossless compression in information theory, can mitigate most of the LLM drawbacks of hallucinations and omissions. Specifically, using LCTs as inputs, we generate the full HDL for a two-dimensional network-on-chip router (13 units, 1500-2000 lines of code) using seven different LLMs, reconstruct the LCTs from the auto-generated HDL, and compare the original and reconstructed LCTs. This approach yields significant productivity improvements, not only confirming correctly generated LLM logic and detecting incorrectly generated LLM logic but also assisting developers in finding design specification errors.

</details>


### [20] [Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research](https://arxiv.org/abs/2512.03054)
*Ciro Benito Raggio,Lucia Migliorelli,Nils Skupien,Mathias Krohmer Zabaleta,Oliver Blanck,Francesco Cicone,Giuseppe Lucio Cascini,Paolo Zaffino,Maria Francesca Spadea*

Main category: cs.LG

TL;DR: 提出一种面向绿色AI的自适应层冻结策略，用于联邦学习中的MRI到CT转换任务，在保持模型性能的同时减少23%的训练时间、能耗和碳排放。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能促进医疗平等，但其高资源需求会排除计算基础设施有限的机构，加剧医疗不平等。需要开发更节能的联邦学习方法。

Method: 提出自适应层冻结策略：基于轮次间编码器权重的相对差异监控，选择性冻结编码器权重；采用基于耐心的机制，仅在更新持续最小时才进行冻结；使用CodeCarbon库跟踪能耗和碳排放。

Result: 相比未冻结的对照方法，训练时间、总能耗和CO2eq排放减少达23%；MRI到CT转换性能保持稳定，平均绝对误差仅有微小变化；5种架构中3种无统计显著差异，2种有统计显著改进。

Conclusion: 该工作推进了满足临床需求同时确保气候、社会和经济可持续性的深度学习框架研究范式，为新型联邦学习评估框架奠定基础，促进AI驱动医疗中的隐私、公平和正义。

Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.

</details>


### [21] [Physics-informed self-supervised learning for predictive modeling of coronary artery digital twins](https://arxiv.org/abs/2512.03055)
*Xiaowu Sun,Thabo Mahendiran,Ortal Senouf,Denise Auberson,Bernard De Bruyne,Stephane Fournier,Olivier Muller,Pascal Frossard,Emmanuel Abbe,Dorina Thanou*

Main category: cs.LG

TL;DR: PINS-CAD：基于物理信息自监督学习的冠状动脉疾病预测框架，通过合成数字孪生预训练图神经网络，无需CFD或标记数据即可预测心血管事件


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，冠状动脉疾病（CAD）需要早期风险预测。传统3D冠状动脉数字孪生分析依赖计算流体动力学（CFD），计算成本高且难以扩展。数据驱动方法受限于标记数据稀缺和缺乏生理学先验知识。

Method: 提出PINS-CAD物理信息自监督学习框架：1）在20万个合成冠状动脉数字孪生上预训练图神经网络；2）使用1D Navier-Stokes方程和压降定律指导压力与血流预测；3）无需CFD或标记数据；4）在FAME2研究的635名患者临床数据上进行微调。

Result: 在预测未来心血管事件方面达到AUC 0.73，优于临床风险评分和数据驱动基线方法。物理信息预训练提高了样本效率并产生生理学有意义的表征。还能生成空间分辨的压力和血流储备分数曲线，提供可解释的生物标志物。

Conclusion: 通过将物理先验嵌入几何深度学习，PINS-CAD将常规血管造影转变为无需模拟、具有生理感知的框架，为可扩展的预防性心脏病学提供新途径。

Abstract: Cardiovascular disease is the leading global cause of mortality, with coronary artery disease (CAD) as its most prevalent form, necessitating early risk prediction. While 3D coronary artery digital twins reconstructed from imaging offer detailed anatomy for personalized assessment, their analysis relies on computationally intensive computational fluid dynamics (CFD), limiting scalability. Data-driven approaches are hindered by scarce labeled data and lack of physiological priors. To address this, we present PINS-CAD, a physics-informed self-supervised learning framework. It pre-trains graph neural networks on 200,000 synthetic coronary digital twins to predict pressure and flow, guided by 1D Navier-Stokes equations and pressure-drop laws, eliminating the need for CFD or labeled data. When fine-tuned on clinical data from 635 patients in the multicenter FAME2 study, PINS-CAD predicts future cardiovascular events with an AUC of 0.73, outperforming clinical risk scores and data-driven baselines. This demonstrates that physics-informed pretraining boosts sample efficiency and yields physiologically meaningful representations. Furthermore, PINS-CAD generates spatially resolved pressure and fractional flow reserve curves, providing interpretable biomarkers. By embedding physical priors into geometric deep learning, PINS-CAD transforms routine angiography into a simulation-free, physiology-aware framework for scalable, preventive cardiology.

</details>


### [22] [Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models](https://arxiv.org/abs/2512.03056)
*Zhidong Gao,Zimeng Pan,Yuhang Yao,Chenyue Xie,Wei Wei*

Main category: cs.LG

TL;DR: Delta Sampling (DS) 是一种无需原始训练数据、在推理时实现不同架构基础模型间知识迁移的新方法，通过利用模型适配前后的预测差异来指导新基础模型的去噪过程。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型生态系统中的适配组件（如LoRA、LyCORIS、ControlNet）与特定基础模型紧密耦合，当基础模型升级（如从SD 1.x到2.x）时，由于模型参数和架构的显著变化，这些适配组件难以重用。

Method: 提出Delta Sampling方法，完全在推理时操作，利用模型在适配前后的预测差异（delta），然后用这个delta来指导新基础模型的去噪过程，实现跨不同架构基础模型的知识迁移。

Result: 在不同版本的Stable Diffusion上评估DS，证明DS在不同采样策略下都能在创建期望效果（如视觉风格、语义概念和结构）方面实现一致的改进。

Conclusion: DS作为一种有效的即插即用机制，为基于扩散的图像合成中的知识迁移提供了新方案，解决了适配组件与基础模型紧密耦合的问题。

Abstract: Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling

</details>


### [23] [Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding](https://arxiv.org/abs/2512.03058)
*Duy-Tung Pham,An The Nguyen,Viet-Hoang Tran,Nhan-Phu Chung,Xin T. Tong,Tan M. Nguyen,Thieu N. Vo*

Main category: cs.LG

TL;DR: 研究预训练Transformer模型中token的动态特性，分析其连续时间极限的动力学系统，探究token收敛或发散的条件，并提出改进Transformer架构的简单方法。


<details>
  <summary>Details</summary>
Motivation: 研究预训练Transformer模型中token的动态行为特性，理解不同模型参数下token的渐近行为，探索位置编码形式对动力学机制的影响，为改进Transformer模型提供理论基础。

Method: 分析预训练模型的连续时间极限动力学系统，基于模型参数提供token收敛或发散的充分条件，研究绝对位置编码和旋转位置编码对动力学机制的影响，提出缓解收敛行为的架构改进方法。

Result: 建立了比先前工作更广泛适用的token收敛/发散条件，发现收敛情景会损害模型性能，提出了针对绝对和旋转位置编码的简单架构改进来缓解收敛行为。

Conclusion: 通过分析Transformer中token的动力学特性，揭示了模型参数和位置编码形式对性能的影响，提出的架构改进为Transformer模型的理论基础和设计原则提供了支持。

Abstract: This paper investigates the dynamical properties of tokens in pre-trained Transformer models and explores their application to improving Transformers. To this end, we analyze the dynamical system governing the continuous-time limit of the pre-trained model and characterize the asymptotic behavior of its solutions. Specifically, we characterize when tokens move closer to or farther from one another over time, depending on the model parameters. We provide sufficient conditions, based on these parameters, to identify scenarios where tokens either converge to zero or diverge to infinity. Unlike prior works, our conditions are broader in scope and more applicable to real-world models. Furthermore, we investigate how different forms of positional encoding -- specifically absolute and rotary -- affect these dynamical regimes. Empirical evidence reveals that the convergence scenario adversely impacts model performance. Motivated by these insights, we propose simple refinements to Transformer architectures that mitigate convergence behavior in models with absolute or rotary positional encoding. These findings support theoretical foundations and design principles for improving Transformer models.

</details>


### [24] [Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL](https://arxiv.org/abs/2512.03059)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 提出了一种用于电动公交车充电调度的安全分层深度强化学习框架，通过双层决策机制解决多源不确定性下的优化问题，在成本最小化和安全合规方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电动公交车与光伏等可再生能源的整合是促进可持续低碳公共交通的有前景方案，但在实际运行中，光伏发电、动态电价、可变行驶时间和有限充电设施等多源不确定性使得优化充电调度同时确保安全运行（避免电池耗尽）具有挑战性。

Method: 将问题建模为带选项的约束马尔可夫决策过程，提出新型分层深度强化学习算法DAC-MAPPO-Lagrangian，将拉格朗日松弛集成到双演员-评论家框架中。高层采用集中式PPO-Lagrangian学习安全充电桩分配策略，低层采用MAPPO-Lagrangian在集中训练分散执行范式下学习分散式充电功率决策。

Result: 基于真实数据的广泛实验表明，所提方法在成本最小化和安全合规方面均优于现有基线方法，同时保持了快速的收敛速度。

Conclusion: 提出的安全分层深度强化学习框架能够有效解决电动公交车充电调度问题中的多源不确定性挑战，实现了成本优化与安全运行的平衡，为可持续公共交通系统提供了可行的技术方案。

Abstract: The integration of Electric Buses (EBs) with renewable energy sources such as photovoltaic (PV) panels is a promising approach to promote sustainable and low-carbon public transportation. However, optimizing EB charging schedules to minimize operational costs while ensuring safe operation without battery depletion remains challenging - especially under real-world conditions, where uncertainties in PV generation, dynamic electricity prices, variable travel times, and limited charging infrastructure must be accounted for. In this paper, we propose a safe Hierarchical Deep Reinforcement Learning (HDRL) framework for solving the EB Charging Scheduling Problem (EBCSP) under multi-source uncertainties. We formulate the problem as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making. We develop a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian), which integrates Lagrangian relaxation into the Double Actor-Critic (DAC) framework. At the high level, we adopt a centralized PPO-Lagrangian algorithm to learn safe charger allocation policies. At the low level, we incorporate MAPPO-Lagrangian to learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm. Extensive experiments with real-world data demonstrate that the proposed approach outperforms existing baselines in both cost minimization and safety compliance, while maintaining fast convergence speed.

</details>


### [25] [A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap](https://arxiv.org/abs/2512.03060)
*Jing Pan,Li Shi,Paul Lo*

Main category: cs.LG

TL;DR: 大规模工业框架利用数亿Snapchat用户实验数据估计异质性处理效应，通过多实验组合发现潜在用户特征，并应用于广告影响力和敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 传统假设所有用户处理效应相同，但实际存在异质性。需要大规模工业框架来估计异质性处理效应，从海量实验数据中发现潜在用户特征，为精准营销提供支持。

Method: 构建大规模工业框架，包括实验选择、基础学习器设计和增量训练。通过组合数百个实验的结果，利用数亿Snapchat用户数据，估计异质性处理效应和条件平均处理效应。

Result: 框架成功发现先前无法测量的潜在用户特征，产生稳定的处理效应估计。应用包括用户对广告的影响力和敏感性分析。在线A/B测试显示，使用影响力分数进行定向投放，关键业务指标提升超过通常显著水平的6倍以上。

Conclusion: 该大规模工业框架能够有效估计异质性处理效应，发现潜在用户特征，为精准广告定向提供有力工具，显著提升业务效果。

Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.

</details>


### [26] [Globally optimized SVD compression of LLMs via Fermi-function-based rank selection and gauge fixing](https://arxiv.org/abs/2512.03062)
*Roman Rausch,David Jansen,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: 该论文提出了两种基于物理启发的改进方法，用于提升LLM的SVD压缩效果：FermiGrad算法通过费米函数将离散的奇异值截断转化为连续优化问题，确定全局最优的层间秩；PivGa方法利用参数化的内在规范自由度，对低秩因子进行无损压缩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对计算资源需求极高，低秩分解（如SVD）是LLM压缩的有前景方法，但面临层间秩选择和参数冗余消除等实际挑战。

Method: 提出两种物理启发的改进：1) FermiGrad算法，使用费米函数将离散奇异值截断松弛为连续优化问题，通过梯度下降确定全局最优层间秩；2) PivGa方法，利用低秩因子参数化的内在规范自由度进行无损压缩。

Result: 论文展示了两种改进方法能够有效提升SVD压缩LLM的效果，FermiGrad优化了层间秩选择，PivGa进一步压缩了低秩因子的参数冗余。

Conclusion: 通过物理启发的FermiGrad和PivGa方法，显著改进了LLM的SVD压缩技术，解决了层间秩选择和参数冗余等实际问题，为LLM压缩提供了更有效的解决方案。

Abstract: Large Language Models (LLMs) are very demanding in terms of their computational resources. Low-rank decompositions of LLM weights, e.g. via Singular Value Decomposition (SVD), is a promising approach for LLM compression, but presents several practical hurdles, e.g. selecting appropriate layer-wise ranks and getting rid of its parameter redundancy. In this work, we present two physics-inspired improvements to SVD LLM compression: (1) \textbf{FermiGrad}, a gradient-descent algorithm that determines globally optimal layer-wise ranks by relaxing the discrete singular-value truncation into a continuous optimization using the Fermi function; (2) \textbf{PivGa}, an additional \textit{lossless} compression of the low-rank factors that exploits the intrinsic gauge freedom in their parametrization.

</details>


### [27] [Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning](https://arxiv.org/abs/2512.03065)
*Nihir Chadderwala*

Main category: cs.LG

TL;DR: 提出结合AWS Strands Agents与Thompson Sampling上下文bandits的框架，让AI代理仅从用户反馈中学习最优决策策略，在生命科学领域实现15-30%的用户满意度提升


<details>
  <summary>Details</summary>
Motivation: 生命科学领域的生成式AI代理面临关键挑战：如何为从简单事实问题到复杂机制推理的多样化查询确定最优方法。传统方法依赖固定规则或昂贵的标注训练数据，无法适应变化条件或用户偏好。

Method: 结合AWS Strands Agents与Thompson Sampling上下文bandits的框架，通过用户反馈学习优化三个关键维度：生成策略选择（直接vs.链式思考）、工具选择（文献搜索、药物数据库等）和领域路由（药理学、分子生物学、临床专家）。

Result: 在生命科学查询的实证评估中，相比随机基线实现了15-30%的用户满意度提升，在20-30个查询后出现清晰的学习模式。方法无需真实标签，能持续适应用户偏好。

Conclusion: 该框架为智能AI系统中的探索-利用困境提供了原则性解决方案，仅从用户反馈中学习最优决策策略，无需标注数据且能持续适应用户偏好。

Abstract: Generative AI agents in life sciences face a critical challenge: determining the optimal approach for diverse queries ranging from simple factoid questions to complex mechanistic reasoning. Traditional methods rely on fixed rules or expensive labeled training data, neither of which adapts to changing conditions or user preferences. We present a novel framework that combines AWS Strands Agents with Thompson Sampling contextual bandits to enable AI agents to learn optimal decision-making strategies from user feedback alone. Our system optimizes three key dimensions: generation strategy selection (direct vs. chain-of-thought), tool selection (literature search, drug databases, etc.), and domain routing (pharmacology, molecular biology, clinical specialists). Through empirical evaluation on life science queries, we demonstrate 15-30\% improvement in user satisfaction compared to random baselines, with clear learning patterns emerging after 20-30 queries. Our approach requires no ground truth labels, adapts continuously to user preferences, and provides a principled solution to the exploration-exploitation dilemma in agentic AI systems.

</details>


### [28] [Hierarchical clustering of complex energy systems using pretopology](https://arxiv.org/abs/2512.03069)
*Loup-Noe Levy,Jeremie Bosom,Guillaume Guerard,Soufian Ben Amor,Marc Bui,Hai Tran*

Main category: cs.LG

TL;DR: 使用预拓扑学建模能耗曲线，开发基于预拓扑空间特性的多准则分层分类算法，用于大规模建筑能耗优化管理


<details>
  <summary>Details</summary>
Motivation: 对数千栋建筑进行逐个深入审计需要大量时间、金钱和专业人员，因此需要开发自动化方法来建立有效的能耗管理推荐系统

Method: 使用预拓扑学建模站点能耗曲线，开发基于预拓扑空间特性的多准则分层分类算法，并实现为Python库

Result: 在二维点数据集上能识别基于位置和大小的聚类；在生成的时间序列数据集上能识别基于皮尔逊相关性的聚类，调整兰德指数达到1；在400个真实能耗站点数据集上进行了评估

Conclusion: 提出的预拓扑学方法和多准则分层分类算法能够有效建模和分类大规模分布式区域的能耗曲线，为建筑能耗优化管理提供自动化解决方案

Abstract: This article attempts answering the following problematic: How to model and classify energy consumption profiles over a large distributed territory to optimize the management of buildings' consumption?
  Doing case-by-case in depth auditing of thousands of buildings would require a massive amount of time and money as well as a significant number of qualified people. Thus, an automated method must be developed to establish a relevant and effective recommendations system.
  To answer this problematic, pretopology is used to model the sites' consumption profiles and a multi-criterion hierarchical classification algorithm, using the properties of pretopological space, has been developed in a Python library.
  To evaluate the results, three data sets are used: A generated set of dots of various sizes in a 2D space, a generated set of time series and a set of consumption time series of 400 real consumption sites from a French Energy company.
  On the point data set, the algorithm is able to identify the clusters of points using their position in space and their size as parameter. On the generated time series, the algorithm is able to identify the time series clusters using Pearson's correlation with an Adjusted Rand Index (ARI) of 1.

</details>


### [29] [Mixed Data Clustering Survey and Challenges](https://arxiv.org/abs/2512.03070)
*Guillaume Guerard,Sonia Djebali*

Main category: cs.LG

TL;DR: 本文提出了一种基于预拓扑空间的混合数据聚类方法，用于解决大数据背景下数值和分类变量混合的聚类挑战。


<details>
  <summary>Details</summary>
Motivation: 大数据时代带来了数据量、速度和多样性的挑战，混合数据聚类成为关键问题。传统聚类方法通常针对同质数据集设计，难以处理混合数据的复杂性，需要专门的方法来有效利用数值和分类变量。

Method: 提出了一种基于预拓扑空间的聚类方法，该方法专门针对混合数据类型设计，能够处理数值和分类变量的异构性。

Result: 通过与经典数值聚类算法和现有预拓扑方法进行基准测试，评估了所提出方法的性能和有效性，验证了其在大数据范式下的适用性。

Conclusion: 基于预拓扑空间的聚类方法为解决大数据环境下的混合数据聚类问题提供了有效途径，特别是层次化和可解释的算法结构有助于支持决策制定。

Abstract: The advent of the big data paradigm has transformed how industries manage and analyze information, ushering in an era of unprecedented data volume, velocity, and variety. Within this landscape, mixed-data clustering has become a critical challenge, requiring innovative methods that can effectively exploit heterogeneous data types, including numerical and categorical variables. Traditional clustering techniques, typically designed for homogeneous datasets, often struggle to capture the additional complexity introduced by mixed data, underscoring the need for approaches specifically tailored to this setting. Hierarchical and explainable algorithms are particularly valuable in this context, as they provide structured, interpretable clustering results that support informed decision-making. This paper introduces a clustering method grounded in pretopological spaces. In addition, benchmarking against classical numerical clustering algorithms and existing pretopological approaches yields insights into the performance and effectiveness of the proposed method within the big data paradigm.

</details>


### [30] [PretopoMD: Pretopology-based Mixed Data Hierarchical Clustering](https://arxiv.org/abs/2512.03071)
*Loup-Noe Levy,Guillaume Guerard,Sonia Djebali,Soufian Ben Amor*

Main category: cs.LG

TL;DR: 提出了一种基于预拓扑的新算法，用于聚类混合数据而无需降维，通过可定制的逻辑规则和可调超参数构建用户定义的分层聚类，在保持数据完整性的同时提供可解释的聚类结果。


<details>
  <summary>Details</summary>
Motivation: 解决混合数据聚类的挑战，避免传统降维技术导致的信息损失，提高聚类结果的可解释性，为异构数据集提供定制化解决方案。

Method: 基于预拓扑的算法，利用析取范式制定可定制的逻辑规则和可调超参数，支持用户定义的分层聚类构建，直接从原始数据中识别聚类结构。

Result: 通过分层树状图分析和聚类指标比较，该方法在保持数据完整性的同时，能够准确且可解释地划分聚类，展现出优于传统方法的性能，并增强了聚类数据的可解释性。

Conclusion: 该研究通过创新的逻辑规则应用，避免了传统降维技术，在混合数据聚类领域做出了重要贡献，提高了聚类形成的清晰度和可解释性，为异构数据集提供了有效的定制化解决方案。

Abstract: This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm's robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.

</details>


### [31] [Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information](https://arxiv.org/abs/2512.03074)
*Mahdi Tavassoli Kejani,Fadi Dornaika,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出了一种新的模型无关的公平性正则化框架，用于处理图神经网络中敏感属性仅部分可用的现实场景，在保持节点分类性能的同时显著减轻偏见。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在关系学习任务中表现出色，但会延续和放大针对受保护群体（如种族、性别）的社会偏见。现有公平性GNN方法依赖敏感属性完全可用的强假设，这在实践中因隐私和数据收集限制而难以实现。

Method: 提出了一种模型无关的公平性正则化框架，针对敏感属性仅部分可用的现实场景。该方法形式化了一个公平性目标函数，将平等机会和统计均等作为可微分的正则化项进行集成。

Result: 在五个真实世界基准数据集上的综合实证评估表明，该方法在关键公平性指标上显著减轻了偏见，同时保持了有竞争力的节点分类性能。该框架在实现有利的公平性-准确性权衡方面始终优于基线模型，预测准确性下降最小。

Conclusion: 提出的框架有效解决了图神经网络中敏感属性部分可用的公平性问题，在保持模型性能的同时显著减轻了偏见，为实际应用提供了可行的解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated exceptional efficacy in relational learning tasks, including node classification and link prediction. However, their application raises significant fairness concerns, as GNNs can perpetuate and even amplify societal biases against protected groups defined by sensitive attributes such as race or gender. These biases are often inherent in the node features, structural topology, and message-passing mechanisms of the graph itself. A critical limitation of existing fairness-aware GNN methods is their reliance on the strong assumption that sensitive attributes are fully available for all nodes during training--a condition that poses a practical impediment due to privacy concerns and data collection constraints. To address this gap, we propose a novel, model-agnostic fairness regularization framework designed for the realistic scenario where sensitive attributes are only partially available. Our approach formalizes a fairness-aware objective function that integrates both equal opportunity and statistical parity as differentiable regularization terms. Through a comprehensive empirical evaluation across five real-world benchmark datasets, we demonstrate that the proposed method significantly mitigates bias across key fairness metrics while maintaining competitive node classification performance. Results show that our framework consistently outperforms baseline models in achieving a favorable fairness-accuracy trade-off, with minimal degradation in predictive accuracy. The datasets and source code will be publicly released at https://github.com/mtavassoli/GNN-FC.

</details>


### [32] [Risk-Entropic Flow Matching](https://arxiv.org/abs/2512.03078)
*Vahid R. Ramezani,Benjamin Englard*

Main category: cs.LG

TL;DR: 该论文将倾斜（熵）风险应用于流匹配，通过log-exponential变换改进标准平方损失，以更好地捕捉数据流形的高阶条件信息（方差、偏度、多模态）和少数分支结构。


<details>
  <summary>Details</summary>
Motivation: 标准整流流匹配使用均方误差损失，将所有到达同一时空点的速度目标压缩为单一条件均值，忽略了编码数据流形精细几何结构和少数分支的高阶条件信息（方差、偏度、多模态）。

Method: 将标准风险敏感（log-exponential）变换应用于条件流匹配损失，得到的倾斜风险损失是每个时空点上有意义条件熵流匹配目标的上界。通过小阶展开，得到两个可解释的一阶修正：流匹配残差的协方差预处理，以及偏好不对称或稀有分支的偏尾项。

Result: 在专门设计用于探测模糊性和尾部的合成数据上，风险敏感损失相比标准整流流匹配，改善了统计指标，并更忠实地恢复了几何结构。

Conclusion: 倾斜风险损失为流匹配提供了一种自然框架，能够更好地捕捉数据流形的高阶统计特性，特别是对稀有事件和几何结构的建模，在合成数据上表现出优越性能。

Abstract: Tilted (entropic) risk, obtained by applying a log-exponential transform to a base loss, is a well established tool in statistics and machine learning for emphasizing rare or high loss events while retaining a tractable optimization problem. In this work, our aim is to interpret its structure for Flow Matching (FM). FM learns a velocity field that transports samples from a simple source distribution to data by integrating an ODE. In rectified FM, training pairs are obtained by linearly interpolating between a source sample and a data sample, and a neural velocity field is trained to predict the straight line displacement using a mean squared error loss. This squared loss collapses all velocity targets that reach the same space-time point into a single conditional mean, thereby ignoring higher order conditional information (variance, skewness, multi-modality) that encodes fine geometric structure about the data manifold and minority branches. We apply the standard risk-sensitive (log-exponential) transform to the conditional FM loss and show that the resulting tilted risk loss is a natural upper-bound on a meaningful conditional entropic FM objective defined at each space-time point. Furthermore, we show that a small order expansion of the gradient of this conditional entropic objective yields two interpretable first order corrections: covariance preconditioning of the FM residual, and a skew tail term that favors asymmetric or rare branches. On synthetic data designed to probe ambiguity and tails, the resulting risk-sensitive loss improves statistical metrics and recovers geometric structure more faithfully than standard rectified FM.

</details>


### [33] [ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification](https://arxiv.org/abs/2512.03101)
*Congjing Zhang,Feng Lin,Xinyi Zhao,Pei Guo,Wei Li,Lin Chen,Chaoyue Zhao,Shuai Huang*

Main category: cs.LG

TL;DR: ALARM是一个基于多模态大语言模型的视觉异常检测框架，集成了不确定性量化技术，通过推理链、自我反思和模型集成等方法提高检测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，视觉异常往往具有高度上下文依赖性和模糊性，因此基于多模态大语言模型的视觉异常检测系统需要具备不确定性量化能力来确保成功部署。

Method: ALARM框架将不确定性量化与质量保证技术（如推理链、自我反思和多模态大语言模型集成）相结合，基于严格的概率推理流程和计算过程设计。

Result: 在真实世界的智能家居基准数据和伤口图像分类数据上进行了广泛实证评估，结果显示ALARM具有优越性能，并在不同领域展现出通用的适用性。

Conclusion: ALARM框架通过集成不确定性量化技术，为基于多模态大语言模型的视觉异常检测提供了可靠决策支持，在复杂环境中表现出色。

Abstract: The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.

</details>


### [34] [Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration](https://arxiv.org/abs/2512.03102)
*Yiwei Shi,Hongnan Ma,Mengyue Yang,Cunjia Liu,Weiru Liu*

Main category: cs.LG

TL;DR: 论文提出了一种扩散驱动的贝叶斯探索框架，用于实时纠正早期状态估计错误，解决了粒子滤波中因初始先验偏差导致的永久性探索限制问题。


<details>
  <summary>Details</summary>
Motivation: 在应急响应等高风险社会应用中，基于有限或偏差信息的早期状态估计可能与现实严重不符，导致后续行动受限、资源错配和人员伤害。传统粒子滤波存在"平稳性诱导后验支持不变性"问题，即初始先验排除的区域将永久无法探索，即使新证据与当前信念矛盾也无法纠正。

Method: 提出扩散驱动的贝叶斯探索框架，通过熵正则化采样和协方差缩放扩散来扩展后验支持。使用Metropolis-Hastings检查验证提议，使推理能够适应意外证据，同时保持统计严谨性。

Result: 在危险气体定位任务上的实证评估显示：当先验正确时，该方法与强化学习和规划基线相当；在先验错配情况下，显著优于经典SMC扰动和基于RL的方法。理论保证DEPF能够解决S-PSI问题。

Conclusion: 该框架为高风险应用中的早期状态估计错误提供了原则性的实时纠正机制，通过扩散驱动的探索克服了传统粒子滤波的探索限制，同时保持了统计严谨性。

Abstract: In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.

</details>


### [35] [Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%](https://arxiv.org/abs/2512.03107)
*Mainak Singha*

Main category: cs.LG

TL;DR: ECLIPSE框架通过结合语义熵估计和困惑度分解来检测大语言模型的幻觉，将幻觉视为模型语义熵与可用证据容量之间的不匹配。


<details>
  <summary>Details</summary>
Motivation: 大语言模型会产生流畅但无根据的答案（幻觉），这限制了其在高风险领域的安全部署。需要一种有效的幻觉检测机制。

Method: 提出ECLIPSE框架，结合多样本聚类的熵估计和新的困惑度分解方法，测量模型如何使用检索到的证据。证明在温和条件下，得到的熵-容量目标是严格凸的且具有唯一稳定最优解。

Result: 在受控金融问答数据集上，ECLIPSE在GPT-3.5-turbo上达到ROC AUC 0.89和平均精度0.90，显著优于仅使用语义熵的基线（AUC 0.50）。对Claude-3-Haiku的消融实验显示AUC降至0.59，系数幅度下降95%，表明ECLIPSE是依赖于校准的token级不确定性的logprob原生机制。

Conclusion: ECLIPSE通过将幻觉视为语义熵与证据容量的不匹配，提供了一种有效的幻觉检测机制。困惑度分解特征具有最大的学习系数，证实证据利用是幻觉检测的核心。这项工作是一个受控机制研究，跨领域和自然发生幻觉的更广泛验证是未来工作。

Abstract: Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.

</details>


### [36] [E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing](https://arxiv.org/abs/2512.03109)
*Shuvom Sadhuka,Drew Prinster,Clara Fannjiang,Gabriele Scalia,Aviv Regev,Hanchen Wang*

Main category: cs.LG

TL;DR: e-valuator：一种将任意黑盒验证器评分转换为具有可证明误报率控制的决策规则的方法，用于评估智能体轨迹的成功概率


<details>
  <summary>Details</summary>
Motivation: 现有智能体AI系统的验证器（如LLM评判器和过程奖励模型）虽然能提供启发式评分，但无法保证正确性，缺乏统计保证来决定智能体是否会产生成功输出

Method: 将区分成功轨迹和失败轨迹的问题构建为顺序假设检验问题，基于e-processes开发顺序假设检验，在智能体轨迹的每一步都保持统计有效性，支持对任意长动作序列的在线监控

Result: 在六个数据集和三种智能体上的实验表明，e-valuator相比其他策略具有更高的统计功效和更好的误报率控制，还能用于快速终止问题轨迹以节省token

Conclusion: e-valuator提供了一个轻量级、模型无关的框架，将验证器启发式方法转换为具有统计保证的决策规则，使能部署更可靠的智能体系统

Abstract: Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.

</details>


### [37] [Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability](https://arxiv.org/abs/2512.03112)
*Jialai She*

Main category: cs.LG

TL;DR: SISR是一个统一的非线性解释框架，通过同时学习单调变换恢复可加性和施加L0稀疏约束，解决了Shapley值在非可加性场景下的失真问题和高维稀疏解释的计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值面临两个主要挑战：1) 假设价值函数具有可加性，但现实世界中的收益构造（非高斯分布、重尾、特征依赖、领域特定损失尺度）经常违反这一假设，导致归因失真；2) 在高维空间中通过计算密集Shapley值然后应用临时阈值化来获得稀疏解释成本过高且存在不一致风险。

Method: 提出稀疏保序Shapley回归(SISR)，统一框架同时学习单调变换以恢复可加性（无需封闭形式规范）并对Shapley向量施加L0稀疏约束。优化算法利用相邻池化违规者进行高效保序回归，使用归一化硬阈值进行支持选择，实现简便实现和全局收敛保证。

Result: 分析表明SISR在广泛场景中恢复真实变换，在高噪声下实现强支持恢复。首次证明无关特征和特征间依赖可以诱导显著偏离线性的真实收益变换。在回归、逻辑回归和树集成实验中，SISR稳定了不同收益方案下的归因，正确过滤无关特征，而标准Shapley值遭受严重的排名和符号失真。

Conclusion: 通过统一非线性变换估计与稀疏性追求，SISR推进了非线性可解释性的前沿，提供了一个理论上有基础且实用的归因框架。

Abstract: Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.

</details>


### [38] [Temporal Graph Neural Networks for Early Anomaly Detection and Performance Prediction via PV System Monitoring Data](https://arxiv.org/abs/2512.03114)
*Srijani Mukherjee,Laurent Vuillon,Liliane Bou Nassif,Stéphanie Giroux-Julien,Hervé Pabiou,Denys Dutykh,Ionnasis Tsanakas*

Main category: cs.LG

TL;DR: 提出基于时序图神经网络的方法，利用环境参数预测光伏系统输出功率并检测异常


<details>
  <summary>Details</summary>
Motivation: 光伏系统快速增长需要先进的性能监控和异常检测方法以确保最佳运行

Method: 使用时序图神经网络，基于辐照度、模块温度和环境温度等关键参数之间的图结构时序关系来预测光伏输出功率

Result: 基于法国里昂屋顶光伏设施收集的数据，包括光伏模块功率测量和气象参数

Conclusion: 提出的时序GNN方法能够有效预测光伏输出功率并检测异常，为光伏系统性能监控提供新方案

Abstract: The rapid growth of solar photovoltaic (PV) systems necessitates advanced methods for performance monitoring and anomaly detection to ensure optimal operation. In this study, we propose a novel approach leveraging Temporal Graph Neural Network (Temporal GNN) to predict solar PV output power and detect anomalies using environmental and operational parameters. The proposed model utilizes graph-based temporal relationships among key PV system parameters, including irradiance, module and ambient temperature to predict electrical power output. This study is based on data collected from an outdoor facility located on a rooftop in Lyon (France) including power measurements from a PV module and meteorological parameters.

</details>


### [39] [Real-Time Structural Health Monitoring with Bayesian Neural Networks: Distinguishing Aleatoric and Epistemic Uncertainty for Digital Twin Frameworks](https://arxiv.org/abs/2512.03115)
*Hanbin Cho,Jecheon Yu,Hyeonbin Moon,Jiyoung Yoon,Junhyeong Lee,Giyoung Kim,Jinhyoung Park,Seunghwa Ryu*

Main category: cs.LG

TL;DR: 提出了一种结合PCA、贝叶斯神经网络和哈密顿蒙特卡洛的结构健康监测框架，能够从稀疏应变测量重建全场应变分布并量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测需要可靠的全场不确定性分析来支持可信决策，但现有方法难以同时获得空间分辨的随机和认知不确定性。

Method: 集成PCA、贝叶斯神经网络和哈密顿蒙特卡洛推理，将稀疏应变测量映射到主成分模态，重建全场应变分布并进行不确定性量化。

Result: 在碳纤维增强聚合物试件的四点弯曲循环测试中验证，实现了准确的应变场重建（R²>0.9），同时实时生成不确定性场。

Conclusion: 该框架能够从含噪声实验数据中重建稳健的全场应变，提供互补的不确定性场，支持可靠的数字孪生部署和风险感知结构诊断。

Abstract: Reliable real-time analysis of sensor data is essential for structural health monitoring (SHM) of high-value assets, yet a major challenge is to obtain spatially resolved full-field aleatoric and epistemic uncertainties for trustworthy decision-making. We present an integrated SHM framework that combines principal component analysis (PCA), a Bayesian neural network (BNN), and Hamiltonian Monte Carlo (HMC) inference, mapping sparse strain gauge measurements onto leading PCA modes to reconstruct full-field strain distributions with uncertainty quantification. The framework was validated through cyclic four-point bending tests on carbon fiber reinforced polymer (CFRP) specimens with varying crack lengths, achieving accurate strain field reconstruction (R squared value > 0.9) while simultaneously producing real-time uncertainty fields. A key contribution is that the BNN yields robust full-field strain reconstructions from noisy experimental data with crack-induced strain singularities, while also providing explicit representations of two complementary uncertainty fields. Considered jointly in full-field form, the aleatoric and epistemic uncertainty fields make it possible to diagnose at a local level, whether low-confidence regions are driven by data-inherent issues or by model-related limitations, thereby supporting reliable decision-making. Collectively, the results demonstrate that the proposed framework advances SHM toward trustworthy digital twin deployment and risk-aware structural diagnostics.

</details>


### [40] [Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models](https://arxiv.org/abs/2512.03125)
*Xiwen Wei,Mustafa Munir,Radu Marculescu*

Main category: cs.LG

TL;DR: MoDE提出了一种模态解耦专家架构，通过隔离模态特定更新和知识蒸馏来解决统一多模态生成模型中的模态内和模态间灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 统一多模态生成模型在连续学习新任务时面临严重的灾难性遗忘问题，包括模态内遗忘和模态间遗忘。虽然模态内遗忘已有研究，但模态间遗忘尚未被充分探索，本文旨在解决这一问题。

Method: 提出模态解耦专家架构，通过隔离模态特定更新来减轻梯度冲突，并利用知识蒸馏来防止灾难性遗忘和保留预训练能力。

Result: 在多样化基准测试中，MoDE显著减轻了模态间和模态内遗忘，在统一多模态生成设置中优于先前的连续学习基线方法。

Conclusion: MoDE通过模态解耦有效解决了统一多模态生成模型中的连续学习问题，为多模态系统的持续学习提供了轻量级且可扩展的解决方案。

Abstract: Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git

</details>


### [41] [Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra](https://arxiv.org/abs/2512.03127)
*Ziyu Xiong,Yichi Zhang,Foyez Alauddin,Chu Xin Cheng,Joon Soo An,Mohammad R. Seyedsayamdost,Ellen D. Zhong*

Main category: cs.LG

TL;DR: ChefNMR是一个端到端框架，直接从1D NMR谱和化学式预测未知分子结构，在天然产物结构预测中准确率超过65%


<details>
  <summary>Details</summary>
Motivation: NMR谱解析是确定小分子结构的关键技术，但传统方法耗时且需要专业知识，需要自动化解决方案来加速分子发现

Method: 将结构解析构建为条件生成问题，使用基于非等变transformer架构的原子扩散模型，并创建了包含11.1万+天然产物的模拟1D NMR谱数据集

Result: ChefNMR在挑战性天然产物化合物结构预测中取得了超过65%的准确率，超越了现有方法

Conclusion: 该研究在自动化小分子结构解析方面迈出了重要一步，展示了深度学习在加速分子发现中的潜力

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.

</details>


### [42] [Contrastive Deep Learning for Variant Detection in Wastewater Genomic Sequencing](https://arxiv.org/abs/2512.03158)
*Adele Chinda,Richmond Azumah,Hemanth Demakethepalli Venkateswara*

Main category: cs.LG

TL;DR: 本文提出了一种基于VQ-VAE的无监督病毒变异检测框架，用于废水基因组监测，无需参考基因组或变异标签，在SARS-CoV-2废水测序数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 废水基因组监测面临高测序噪声、低病毒覆盖率、片段化读取和缺乏标记变异注释等计算挑战，传统基于参考的变异检测方法难以处理新突变且计算资源需求大。

Method: 使用向量量化变分自编码器（VQ-VAE）从k-mer标记化序列中学习基因组模式的离散码本，无需参考基因组或变异标签。扩展了基础VQ-VAE架构，包括掩码重建预训练以增强对缺失数据的鲁棒性，以及对比学习以获得高度区分的嵌入表示。

Result: 在约100,000条SARS-CoV-2废水测序数据上，VQ-VAE达到99.52%的平均标记级准确率和56.33%的精确序列匹配率，同时保持19.73%的码本利用率（512个码中101个活跃）。对比微调显著改善聚类效果：64维嵌入使轮廓分数提高35%（0.31到0.42），128维嵌入提高42%（0.31到0.44）。

Conclusion: 该无参考框架为基因组监测提供了一种可扩展、可解释的方法，可直接应用于公共卫生监测，有效解决了废水基因组监测中的计算挑战。

Abstract: Wastewater-based genomic surveillance has emerged as a powerful tool for population-level viral monitoring, offering comprehensive insights into circulating viral variants across entire communities. However, this approach faces significant computational challenges stemming from high sequencing noise, low viral coverage, fragmented reads, and the complete absence of labeled variant annotations. Traditional reference-based variant calling pipelines struggle with novel mutations and require extensive computational resources. We present a comprehensive framework for unsupervised viral variant detection using Vector-Quantized Variational Autoencoders (VQ-VAE) that learns discrete codebooks of genomic patterns from k-mer tokenized sequences without requiring reference genomes or variant labels. Our approach extends the base VQ-VAE architecture with masked reconstruction pretraining for robustness to missing data and contrastive learning for highly discriminative embeddings. Evaluated on SARS-CoV-2 wastewater sequencing data comprising approximately 100,000 reads, our VQ-VAE achieves 99.52% mean token-level accuracy and 56.33% exact sequence match rate while maintaining 19.73% codebook utilization (101 of 512 codes active), demonstrating efficient discrete representation learning. Contrastive fine-tuning with different projection dimensions yields substantial clustering improvements: 64-dimensional embeddings achieve +35% Silhouette score improvement (0.31 to 0.42), while 128-dimensional embeddings achieve +42% improvement (0.31 to 0.44), clearly demonstrating the impact of embedding dimensionality on variant discrimination capability. Our reference-free framework provides a scalable, interpretable approach to genomic surveillance with direct applications to public health monitoring.

</details>


### [43] [Plantain: Plan-Answer Interleaved Reasoning](https://arxiv.org/abs/2512.03176)
*Anthony Liang,Jonathan Berant,Adam Fisch,Abhimanyu Goyal,Kalpesh Krishna,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 论文提出交错推理(IR)方法，让语言模型在推理过程中交替输出中间结果，替代传统的"先思考后回答"模式，从而减少用户感知延迟并允许早期干预。


<details>
  <summary>Details</summary>
Motivation: 传统推理模型在生成最终答案前需要长时间"思考"，期间不给用户任何提示或干预机会，导致用户时间浪费在错误的推理前提上。相比之下，人类对话中会进行轻量级、增量的确认行为来确保对话双方理解一致。

Method: 提出交错推理(IR)方法，让模型在推理过程中交替输出中间结果。进一步提出Plantain(计划-思考-答案交错)方法，其中第一个中间响应是明确的、逐步的任务执行计划，允许用户干预并为后续推理步骤提供早期反馈。

Result: Plantain方法在多个具有挑战性的数学推理和编程基准测试中实现了约6%的pass@1改进，同时相对于"先思考后回答"基线，首次响应时间减少了60%以上。

Conclusion: 交错推理方法能够显著减少用户感知延迟，提高推理质量，并通过计划优先策略实现用户早期干预，为语言模型与用户交互提供了更自然、高效的范式。

Abstract: Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard "think-then-answer" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.

</details>


### [44] [Neighborhood density estimation using space-partitioning based hashing schemes](https://arxiv.org/abs/2512.03187)
*Aashi Jindal*

Main category: cs.LG

TL;DR: FiRE/FiRE.1是一种基于草图的异常检测算法，用于快速识别大规模单细胞RNA测序数据中的罕见细胞亚群；Enhash是一种快速、资源高效的集成学习器，使用投影哈希检测流数据中的概念漂移。


<details>
  <summary>Details</summary>
Motivation: 需要在大规模单细胞RNA测序数据中快速识别罕见细胞亚群，以及在流数据中高效检测概念漂移，现有方法在速度和资源效率方面存在不足。

Method: FiRE/FiRE.1采用基于草图的异常检测算法；Enhash使用投影哈希技术构建快速、资源高效的集成学习器来检测概念漂移。

Result: FiRE/FiRE.1在识别罕见细胞亚群方面表现出优于现有技术的性能；Enhash在各种漂移类型中在时间和准确性方面都表现出高度竞争力。

Conclusion: FiRE/FiRE.1和Enhash分别是针对单细胞RNA测序数据异常检测和流数据概念漂移检测的有效解决方案，在性能和效率方面均优于现有方法。

Abstract: This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.

</details>


### [45] [Scaling Internal-State Policy-Gradient Methods for POMDPs](https://arxiv.org/abs/2512.03204)
*Douglas Aberdeen,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文提出改进的算法，用于在无限时域设置中学习带记忆的策略，包括已知环境模型和模拟两种情况，并在大型POMDP问题上进行测试。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在部分可观测环境中学习动作机制方面受到关注，但在需要记忆时效果不佳，因此需要开发改进的算法来处理带记忆的策略学习。

Method: 开发了多种改进算法，用于在无限时域设置中学习带记忆的策略：当环境模型已知时直接学习，否则通过模拟学习。

Result: 在大型POMDP问题上进行比较测试，包括噪声机器人导航和多智能体问题。

Conclusion: 提出了改进的带记忆策略学习算法，为部分可观测环境中的策略梯度方法提供了更有效的解决方案。

Abstract: Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.

</details>


### [46] [A Multi-Agent, Policy-Gradient approach to Network Routing](https://arxiv.org/abs/2512.03211)
*Nigel Tao,Jonathan Baxter,Lex Weaver*

Main category: cs.LG

TL;DR: OLPOMDP算法成功应用于模拟网络路由，多个分布式路由器智能体无需显式通信即可学习协作行为，避免个体最优但群体有害的行为，通过奖励塑形显著提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 网络路由是一个分布式决策问题，具有自然的数值性能度量（如数据包从源到目的地的平均时间）。需要探索强化学习算法在分布式网络路由中的应用潜力。

Method: 采用OLPOMDP（一种策略梯度强化学习算法），应用于模拟网络路由场景。通过多个分布式智能体（路由器）学习协作行为，无需显式通信。使用奖励塑形技术，明确惩罚某些次优行为模式。

Result: OLPOMDP在多种网络模型下成功应用于模拟网络路由。分布式路由器智能体学会了协作行为，避免了仅对个体有利但对群体整体性能有害的行为。奖励塑形显著提高了算法的收敛速度。

Conclusion: 策略梯度强化学习算法OLPOMDP可以有效解决分布式网络路由问题，智能体能够学习协作行为而无需显式通信，奖励塑形是提升收敛性能的有效技术。

Abstract: Network routing is a distributed decision problem which naturally admits numerical performance measures, such as the average time for a packet to travel from source to destination. OLPOMDP, a policy-gradient reinforcement learning algorithm, was successfully applied to simulated network routing under a number of network models. Multiple distributed agents (routers) learned co-operative behavior without explicit inter-agent communication, and they avoided behavior which was individually desirable, but detrimental to the group's overall performance. Furthermore, shaping the reward signal by explicitly penalizing certain patterns of sub-optimal behavior was found to dramatically improve the convergence rate.

</details>


### [47] [Perch 2.0 transfers 'whale' to underwater tasks](https://arxiv.org/abs/2512.03219)
*Andrea Burns,Lauren Harrell,Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0生物声学基础模型在海洋哺乳动物音频任务上表现出色，通过少量样本迁移学习，其嵌入特征在多数任务上优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 评估Perch 2.0在海洋哺乳动物和水下音频任务上的性能，尽管其训练数据中几乎没有包含海洋哺乳动物音频或类别。

Method: 使用Perch 2.0生成的嵌入特征进行线性探测，通过少量样本迁移学习，并与多个预训练生物声学模型进行比较。

Result: Perch 2.0的嵌入特征在少量样本迁移学习中表现一致优异，在大多数任务上优于其他嵌入模型。

Conclusion: 当开发新的海洋哺乳动物分类线性分类器且只有少量标注样本时，推荐使用Perch 2.0模型。

Abstract: Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.

</details>


### [48] [SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning](https://arxiv.org/abs/2512.03244)
*Salman Rahman,Sruthi Gorantla,Arpit Gupta,Swastik Roy,Nanyun Peng,Yang Liu*

Main category: cs.LG

TL;DR: SPARK框架通过三阶段方法解决过程奖励模型训练数据稀缺问题：首先生成多样解并由验证器评估，然后使用验证输出作为合成数据训练生成式过程奖励模型，最后将该模型作为RL奖励信号，在数学推理任务上超越基于真实结果的监督方法。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型（PRMs）虽然能提供密集的步骤级反馈，但其应用受限于昂贵的步骤级标注或真实参考数据的需求。需要开发一种无需真实参考或昂贵标注的方法来训练高质量的过程奖励模型。

Method: SPARK三阶段框架：1) 生成器模型产生多样解，验证器模型通过并行扩展（自一致性）和顺序扩展（元批判）进行评估；2) 使用验证输出作为合成训练数据微调生成式过程奖励模型；3) 将训练好的PRM与思维链验证（PRM-CoT）结合作为RL奖励模型，并引入格式约束防止奖励攻击。

Result: 在ProcessBench上达到67.5 F1分数，优于参考引导训练的66.4和GPT-4o的61.9。使用Qwen2.5-Math-7B在六个数学推理基准测试中平均准确率达到47.4%，超越基于真实结果的RLVR（43.9%）。

Conclusion: SPARK框架实现了无需真实参考的强化学习训练，其性能超越基于真实结果的方法，为缺乏可验证答案或难以获取真实数据的领域开辟了新可能性。

Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.

</details>


### [49] [Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval](https://arxiv.org/abs/2512.03276)
*Constantin Venhoff,Ashkan Khakzar,Sonia Joseph,Philip Torr,Neel Nanda*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型在事实回忆任务上表现下降，主要原因是实体表示形成过晚，无法有效利用LLM原有的知识回忆机制。


<details>
  <summary>Details</summary>
Motivation: 许多视觉语言模型在事实回忆性能上比其LLM主干模型表现更差，这引发了对多模态微调在扩展LLM机制到视觉输入方面有效性的质疑。

Method: 对14种不同架构、规模和训练设置的VLM进行基准测试，使用归因修补、激活修补和探测技术分析性能差异，并测试两种性能恢复方法。

Result: 14个模型中有11个出现事实回忆性能下降；性能差的VLM实体表示形成过晚，无法有效利用LLM原有的事实回忆电路；通过修补实体表示或使用思维链提示可以恢复性能。

Conclusion: 早期实体表示形成的速度是决定VLM能否有效利用预存LLM机制的关键因素，机制分析可以解释和揭示多模态对齐中的系统性失败。

Abstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.

</details>


### [50] [BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark](https://arxiv.org/abs/2512.03280)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Matthew C. Jones,Faez Ahmed*

Main category: cs.LG

TL;DR: BlendedNet++是一个大规模气动数据集和基准测试，专注于翼身融合飞机，包含12,000多个独特几何形状的气动模拟数据，用于标准化前向代理预测和逆向设计任务。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习气动代理模型面临大型场解析数据集稀缺的问题，限制了精确点预测和可重复逆向设计的进展，需要标准化的数据集和基准来推动该领域研究。

Method: 构建包含12,000多个独特翼身融合飞机几何形状的数据集，每个几何在单一飞行条件下进行稳态RANS CFD模拟，提供集成的力/力矩系数和密集表面场数据。建立前向代理基准测试六种模型架构，并设计基于条件扩散模型的逆向设计任务。

Result: 创建了包含12,490个气动结果的BlendedNet++数据集，标准化了前向代理预测基准，实现了逆向设计任务，并提供了扩散模型与梯度优化混合方法的性能比较基准。

Conclusion: BlendedNet++提供了一个统一的前向和逆向协议与多模型基线，能够实现跨架构和优化范式的公平、可重复比较，有望推动场级气动学和逆向设计的可重复研究。

Abstract: Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.

</details>


### [51] [Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors](https://arxiv.org/abs/2512.03287)
*Dario Fenoglio,Mohan Li,Davide Casnici,Matias Laporte,Shkurta Gashi,Silvia Santini,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: 本文提出多频率联邦学习方法用于头戴式设备的人体活动识别，解决了隐私保护和设备采样频率差异问题。


<details>
  <summary>Details</summary>
Motivation: 传统人体活动识别方法依赖集中式用户数据，存在隐私风险；同时不同设备的采样频率差异给联合学习带来挑战。

Method: 提出多频率联邦学习框架，支持隐私保护的机器学习，并能在不同采样频率的设备间进行联合模型学习。

Result: 在两个数据集上相比频率特定方法取得了改进，表明多频率联邦学习在人体活动识别任务中具有良好前景。

Conclusion: 多频率联邦学习为头戴式设备的人体活动识别提供了隐私保护解决方案，代码已公开供进一步研究。

Abstract: Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.

</details>


### [52] [ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics](https://arxiv.org/abs/2512.03290)
*Julian Evan Chrisnanto,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: ASPEN网络通过自适应谱层解决PINNs在非线性多尺度系统上的频谱偏差问题，成功求解标准PINNs无法处理的复杂金兹堡-朗道方程。


<details>
  <summary>Details</summary>
Motivation: 标准PINNs在处理刚性、多尺度和非线性系统时存在严重局限性，主要原因是多层感知机的频谱偏差问题，无法充分表示高频分量，导致在复杂物理系统求解中失败。

Method: 提出自适应谱物理使能网络(ASPEN)，在网络的输入阶段集成具有可学习傅里叶特征的自适应谱层，使模型能够在训练过程中动态调整自身的谱基，高效学习和表示解所需的精确频率内容。

Result: 在复杂的金兹堡-朗道方程(CGLE)基准测试中，标准PINN架构完全失败并产生非物理振荡，而ASPEN成功求解该方程，预测解与高分辨率真实解视觉上无法区分，物理残差中位数低至5.10×10^-3，正确捕捉了自由能快速弛豫和畴壁前沿长期稳定性等物理特性。

Conclusion: 通过引入自适应谱基，ASPEN为复杂动力系统提供了稳健且物理一致的求解器，解决了标准PINNs的失败问题，为机器学习在挑战性物理领域的应用开辟了新途径。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.

</details>


### [53] [Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction](https://arxiv.org/abs/2512.03298)
*Echo Diyun LU,Charles Findling,Marianne Clausel,Alessandro Leite,Wei Gong,Pierric Kersaudy*

Main category: cs.LG

TL;DR: 该研究将自适应共形推理（ACI）与深度切换状态空间模型结合，为存在体制转换的非平稳时间序列提供分布无关的不确定性量化，确保有限样本下的边际覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 体制转换会破坏时间序列的平稳性，使得校准的不确定性变得与点预测准确性同等重要。需要为存在体制切换的预测问题提供分布无关的不确定性量化方法。

Method: 将深度切换状态空间模型与自适应共形推理（ACI）及其聚合变体（AgACI）耦合。提出统一的共形包装器，可应用于多种序列基线模型（包括S4、MC-Dropout GRU、稀疏高斯过程和变点局部模型），在非平稳性和模型误设下提供在线预测区间。

Result: 在合成和真实数据集上，共形化预测器实现了接近名义水平的覆盖度，具有竞争力的准确性，并且通常提高了区间效率。

Conclusion: 通过结合深度切换状态空间模型与自适应共形推理，可以为存在体制转换的非平稳时间序列提供可靠的分布无关不确定性量化，在保证覆盖度的同时保持预测准确性。

Abstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.

</details>


### [54] [HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction](https://arxiv.org/abs/2512.03300)
*Pengfei Hu,Fan Ming,Xiaoxue Han,Chang Lu,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: HydroDCM是一个用于跨水库入库流量预测的可扩展域泛化框架，通过空间元数据构建伪域标签指导对抗学习，并在推理时通过轻量级条件层进行特定位置适应。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在水库入库流量预测中表现良好，但在不同水库间应用时性能下降，存在域偏移问题。传统域泛化技术在水文多域系统中适用性有限，因为每个水库具有独特的入库模式，而空间信息等元数据具有间接但重要的影响。

Method: 提出HydroDCM框架：1) 利用水库空间元数据构建伪域标签，指导对抗学习提取不变时间特征；2) 推理时通过轻量级条件层，根据目标水库元数据适应这些特征，实现域泛化的不变性与位置特定适应的平衡。

Result: 在科罗拉多河上游流域30个真实水库上的实验结果表明，该方法在多域条件下显著优于最先进的域泛化基线方法，且保持计算高效性。

Conclusion: HydroDCM通过结合对抗学习和元数据驱动的条件适应，有效解决了水文多域系统中的域泛化问题，为跨水库入库流量预测提供了可扩展的解决方案。

Abstract: Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.

</details>


### [55] [Robust Tabular Foundation Models](https://arxiv.org/abs/2512.03307)
*Matthew Peroni,Franck Le,Vadim Sheinin*

Main category: cs.LG

TL;DR: RTFM框架通过对抗性训练提升表格基础模型的鲁棒性，使用最优性差距度量，在合成数据上实现性能提升


<details>
  <summary>Details</summary>
Motivation: 表格基础模型(TFMs)在结构化数据上展现出超越传统ML方法的潜力，但现有研究主要关注通过高质量先验提升预训练性能。本文提出从对抗鲁棒性角度出发，通过调整数据生成器来强调对模型具有挑战性的数据集

Method: 提出RTFM框架：1) 引入最优性差距度量，计算TFM性能与XGBoost、CatBoost、Random Forests等强基线最佳性能的差异；2) 参数化生成器分布，实现对抗性训练；3) 在TabPFN V2分类器上应用该框架

Result: RTFM在基准测试中性能提升显著：相比原始TabPFN和其他基线算法，平均归一化AUC提升高达6%，且仅需额外不到10万个合成数据集

Conclusion: RTFM为表格基础模型的针对性对抗训练和微调提供了有前景的新方向，证明了仅使用合成数据就能有效提升模型鲁棒性和性能

Abstract: The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.

</details>


### [56] [Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates](https://arxiv.org/abs/2512.03309)
*Aniruddha Bora,Shixuan Zhang,Khemraj Shukla,Bryce Harrop,George Em. Karniadakis,L. Ruby Leung*

Main category: cs.LG

TL;DR: 提出基于算子学习的在线偏差校正框架，通过U-Net架构实时修正地球系统模型的偏差，提高预测精度并保持长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统地球系统模型存在分辨率低、参数化不完善、初始状态和强迫场不确定等问题，传统的数据同化偏差校正方法在模型自由运行时效果有限，需要一种能够在模型集成过程中实时校正偏差的方法。

Method: 开发了基于U-Net的算子学习框架，包括Inception U-Net (IUNet)和多尺度网络(M&M)，能够将瞬时模型状态映射到偏差校正趋势，并在模型集成过程中在线应用这些校正。使用两年E3SM模拟数据（向ERA5再分析数据逼近）进行训练。

Result: 两种架构在离线测试中都优于标准U-Net基线，表明功能丰富性而非参数数量驱动性能。在线混合E3SM运行中，M&M在变量和垂直层次上提供最一致的偏差减少。ML增强配置在多年模拟中保持稳定且计算可行。

Conclusion: 该框架强调长期稳定性、可移植性和更新频率限制，展示了表达性ML算子在学习结构化跨尺度关系和改造传统ESMs方面的实用性，为可扩展混合建模提供了实用途径。

Abstract: Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\&M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\&M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.

</details>


### [57] [Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs](https://arxiv.org/abs/2512.03324)
*Ngoc Bui,Shubham Sharma,Simran Lamba,Saumitra Mishra,Rex Ying*

Main category: cs.LG

TL;DR: TRIM-KV：一种通过学习令牌内在重要性来优化KV缓存管理的新方法，使用轻量级保留门预测令牌保留分数，在内存受限时淘汰低分令牌，在多种长上下文任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长序列LLM推理中，自注意力的二次计算成本和不断增长的KV缓存是核心瓶颈。现有方法如量化、卸载或启发式KV淘汰要么协调成本高，要么依赖不可靠的重要性代理。

Method: 提出TRIM-KV方法，通过轻量级保留门学习每个令牌在创建时的内在重要性。每个门预测一个随时间衰减的标量保留分数，反映令牌对特定层和头的长期效用。当内存预算超限时淘汰低分令牌，确保缓存始终包含最关键令牌。通过蒸馏和容量损失进行高效训练，仅需微调门且推理开销可忽略。

Result: 在数学推理（GSM8K、MATH-500、AIME24）、过程生成（LongProc）、对话长记忆基准（LongMemEval）和长上下文理解（LongBench和SCBench）等任务中，TRIM-KV始终优于强淘汰和可学习检索基线，尤其在低内存情况下表现突出。在某些设置中甚至超越全缓存模型，表明选择性保留可作为正则化形式，抑制无信息令牌的噪声。

Conclusion: TRIM-KV不仅提高了内存受限LLM推理的效率，其学习到的保留分数还与人类直觉一致，自然恢复sink令牌、滑动窗口和要点压缩等启发式方法。保留分数还提供了层和头特定角色的洞察，为LLM可解释性开辟了新路径。

Abstract: Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.

</details>


### [58] [Single-Round Scalable Analytic Federated Learning](https://arxiv.org/abs/2512.03336)
*Alan T. L. Bacellar,Mustafa Munir,Felipe M. G. França,Priscila M. V. Lima,Radu Marculescu,Lizy K. John*

Main category: cs.LG

TL;DR: SAFLe框架通过引入结构化头部和稀疏分组嵌入，实现了非线性表达能力，同时保持了单轮通信优势，在联邦学习中取得了新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临两个关键挑战：高通信开销和在异构数据上的性能下降。现有方法如AFL仅限于线性模型，而DeepAFL虽然恢复了准确性但牺牲了单轮通信优势。需要打破这种权衡。

Method: 提出SAFLe框架，通过引入结构化头部（分桶特征）和稀疏分组嵌入来实现可扩展的非线性表达能力。证明这种非线性架构在数学上等价于高维线性回归，从而可以利用AFL的单轮、不变聚合定律。

Result: SAFLe在联邦视觉任务中建立了新的最先进水平，在所有基准测试中显著优于线性AFL和多轮DeepAFL，实现了高效率和可扩展的解决方案。

Conclusion: SAFLe成功打破了联邦学习中准确性和通信效率之间的权衡，通过数学等价性将非线性表达能力与单轮通信优势相结合，为联邦视觉提供了高效可扩展的解决方案。

Abstract: Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.

</details>


### [59] [UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs](https://arxiv.org/abs/2512.03383)
*Hung-Yueh Chiang,Chi-Chih Chang,Yu-Chen Lu,Chien-Yu Lin,Kai-Chiang Wu,Mohamed S. Abdelfattah,Diana Marculescu*

Main category: cs.LG

TL;DR: UniQL是一个统一的边缘LLM后训练量化和低秩压缩框架，支持在设备上配置剪枝率，实现4-5.7倍内存减少和2.7-3.4倍吞吐量提升，精度损失控制在5%以内。


<details>
  <summary>Details</summary>
Motivation: 在移动平台上部署大型语言模型面临内存有限和计算资源共享的挑战，设备工作负载直接影响资源可用性，增加了模型部署的不确定性。

Method: 提出UniQL统一框架，集成量化和低秩压缩，支持Transformer、状态空间模型和混合模型；引入高效结构化权重排序方法（加速20倍）、量化感知奇异值分解、SSM状态感知权重排序和融合RoPE内核；支持云端单次工作流和最高35%的设备端可配置剪枝率。

Result: 量化剪枝模型实现4-5.7倍内存减少和2.7-3.4倍token吞吐量提升，在15%剪枝率下精度损失控制在原始模型5%以内，测试涵盖Llama3、Qwen2.5、Mamba2、Nemotron-H和Bamba-v2等多种模型架构。

Conclusion: UniQL为边缘LLM部署提供了有效的统一压缩框架，显著减少内存占用并提升推理速度，同时保持模型精度，支持多种模型架构和灵活的剪枝配置。

Abstract: Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.

</details>


### [60] [Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions](https://arxiv.org/abs/2512.03354)
*Hongseon Yeom,Jaeyoul Shin,Soojin Min,Jeongmin Yoon,Seunghak Yu,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出首个用于确定性广告拍卖的离策略评估框架，通过重新利用出价景观模型近似倾向得分，实现稳定评估，显著减少在线A/B测试的成本和风险。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试消耗大量工程资源且存在收入损失风险，而传统离策略评估方法无法应用于确定性拍卖环境（最高出价者获胜，非获胜广告曝光概率为零）。

Method: 重新利用出价景观模型近似倾向得分，推导稳健的近似倾向得分，使自归一化逆倾向评分等稳定估计器能够在确定性拍卖环境中进行反事实评估。

Result: 在AuctionNet模拟基准和大规模工业平台2周在线A/B测试中验证，方法显示与在线结果显著一致，点击率预测的平均方向准确率达到92%，显著优于参数基线。

Conclusion: 贡献了首个实用且经过验证的确定性拍卖环境可靠离策略评估框架，为成本高昂且风险大的在线实验提供了高效替代方案。

Abstract: Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.

</details>


### [61] [VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing](https://arxiv.org/abs/2512.03394)
*Hamed Poursiami,Shay Snyder,Guojing Cong,Thomas Potok,Maryam Parsa*

Main category: cs.LG

TL;DR: VS-Graph是一个基于超维计算的图学习框架，通过尖峰扩散机制和关联消息传递方案，在保持高维向量空间操作的同时实现了与图神经网络相竞争的性能，训练速度提升高达450倍。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在图分类任务中表现出色但计算成本高，限制了其在资源受限设备上的部署。超维计算提供了轻量级的替代方案，但现有HDC方法在预测性能上难以匹敌GNNs。本研究旨在缩小HDC效率与消息传递表达能力之间的差距。

Method: 提出了VS-Graph框架，包含两个核心组件：1）尖峰扩散机制用于拓扑驱动的节点识别；2）关联消息传递方案用于在高维向量空间内实现多跳邻域聚合。该方法无需基于梯度的优化或反向传播。

Result: 在MUTAG和DD等标准基准测试中，VS-Graph比先前的HDC基线提高了4-5%的准确率，与多个现代GNNs达到竞争性性能。训练速度提升高达450倍，即使在超向量维度降至D=128时仍保持高准确率。

Conclusion: VS-Graph成功缩小了HDC效率与消息传递表达能力之间的差距，展示了在边缘和神经形态硬件上实现超高效执行的潜力，为资源受限环境中的图学习提供了有前景的解决方案。

Abstract: Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.

</details>


### [62] [A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning](https://arxiv.org/abs/2512.03363)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 本文提出A2G（自适应双增益聚合）框架，用于解决量子增强异构经典网络中联邦学习面临的性能下降问题，通过几何增益和QoS增益联合调节聚合过程。


<details>
  <summary>Details</summary>
Motivation: 量子增强异构经典网络中的联邦学习面临客户端质量不均、量子隐形传态保真度随机、设备不稳定以及局部与全局模型几何不匹配等问题，传统基于欧几里得拓扑和均匀通信可靠性的聚合规则不再适用。

Method: 提出A2G（自适应双增益聚合）框架，包含两个增益：几何增益调节几何混合，QoS增益基于隐形传态保真度、延迟和不稳定性调节客户端重要性。开发了A2G更新规则，并在平滑性和有界方差假设下建立了收敛保证。

Result: A2G在量子经典混合测试平台上表现出改进的稳定性和更高的准确性，特别是在异构和噪声条件下。该框架能够恢复FedAvg、QoS感知平均和基于流形的聚合作为特例。

Conclusion: A2G为量子联邦系统提供了一种有效的自适应聚合框架，能够处理异构网络条件和量子通信的不确定性，为量子增强联邦学习的实际部署提供了解决方案。

Abstract: Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.

</details>


### [63] [Better World Models Can Lead to Better Post-Training Performance](https://arxiv.org/abs/2512.03400)
*Prakhar Gupta,Henry Conklin,Sarah-Jane Leslie,Andrew Lee*

Main category: cs.LG

TL;DR: 研究显式世界建模目标如何影响Transformer在不同训练阶段的内部表示和下游能力，使用2x2x2魔方作为测试环境，发现显式世界建模能产生更线性可解码和因果可控的状态表示，从而提升强化学习后训练效果


<details>
  <summary>Details</summary>
Motivation: 探索显式世界建模目标对Transformer内部表示和下游能力的影响，特别是在不同训练阶段如何影响模型的状态表示质量和强化学习性能

Method: 使用2x2x2魔方作为测试环境，比较标准的下一个token预测与两种显式世界建模策略：(i)状态预测预训练和(ii)状态预测+下一个token联合目标，通过GRPO进行后训练，使用线性探针和因果干预评估表示质量

Result: 显式世界建模产生更线性可解码和因果可控的状态表示，改进的状态表示能显著提升GRPO性能，特别是在更难的魔方状态下效果更明显

Conclusion: 通过锐化状态表示可以改善序列规划任务的后训练效果，显式世界建模有助于提升Transformer在复杂任务中的表现

Abstract: In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.

</details>


### [64] [MAGE-ID: A Multimodal Generative Framework for Intrusion Detection Systems](https://arxiv.org/abs/2512.03375)
*Mahdi Arab Loodaricheh,Mohammad Hossein Manshaei,Anita Raja*

Main category: cs.LG

TL;DR: MAGE-ID是一个基于扩散的多模态攻击生成框架，用于入侵检测系统的数据增强，通过结合表格流量特征和图像转换来解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代入侵检测系统面临异构网络流量、不断演变的网络威胁以及良性流量与攻击流量之间严重数据不平衡的挑战。现有的生成模型方法局限于单一模态，无法捕捉跨域依赖关系。

Method: MAGE-ID是一个基于扩散的多模态生成框架，通过统一的潜在先验将表格流量特征与其转换后的图像耦合。联合训练基于Transformer和CNN的变分编码器与EDM风格的去噪器，实现平衡且连贯的多模态合成。

Result: 在CIC-IDS-2017和NSL-KDD数据集上的评估表明，MAGE-ID在保真度、多样性和下游检测性能方面相比TabSyn和TabDDPM有显著提升。

Conclusion: MAGE-ID作为多模态入侵检测系统增强框架具有有效性，能够生成平衡且连贯的多模态数据，提升入侵检测性能。

Abstract: Modern Intrusion Detection Systems (IDS) face severe challenges due to heterogeneous network traffic, evolving cyber threats, and pronounced data imbalance between benign and attack flows. While generative models have shown promise in data augmentation, existing approaches are limited to single modalities and fail to capture cross-domain dependencies. This paper introduces MAGE-ID (Multimodal Attack Generator for Intrusion Detection), a diffusion-based generative framework that couples tabular flow features with their transformed images through a unified latent prior. By jointly training Transformer and CNN-based variational encoders with an EDM style denoiser, MAGE-ID achieves balanced and coherent multimodal synthesis. Evaluations on CIC-IDS-2017 and NSL-KDD demonstrate significant improvements in fidelity, diversity, and downstream detection performance over TabSyn and TabDDPM, highlighting the effectiveness of MAGE-ID for multimodal IDS augmentation.

</details>


### [65] [ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms](https://arxiv.org/abs/2512.03476)
*Juan Diego Toscano,Daniel T. Chen,George Em Karniadakis*

Main category: cs.LG

TL;DR: ATHENA是一个自主实验室框架，通过知识驱动的诊断过程（HENA循环）管理端到端计算研究生命周期，在科学计算和科学机器学习中实现超人类性能，验证误差达到10^-14。


<details>
  <summary>Details</summary>
Motivation: 解决科学计算和科学机器学习中理论概念化与计算实现之间的鸿沟，这是一个主要的瓶颈问题。当前标准自动化方法无法有效处理复杂的科学问题，需要更智能的系统来加速科学发现。

Method: 提出ATHENA框架，核心是HENA循环（知识驱动的诊断过程），将其构建为上下文多臂赌博机问题。系统作为在线学习器分析先前试验，从组合空间中选择结构"动作"，这些动作基于专家蓝图（如通用逼近、物理信息约束），然后转换为可执行代码生成科学奖励。系统结合混合符号-数值工作流，并支持"人在回路"协作干预。

Result: ATHENA实现超人类性能，验证误差达到10^-14。在科学计算中，能自主识别数学对称性获得精确解析解，或推导稳定数值求解器；在科学机器学习中，能深度诊断处理不适定问题，结合混合方法解决多物理问题。通过人类干预，结果可提高一个数量级。

Conclusion: ATHENA代表了范式转变，将重点从实现机制转向方法论创新，加速科学发现。该框架超越了标准自动化，通过知识驱动的自主学习和人类协作干预，有效弥合了理论概念与计算实现之间的差距。

Abstract: Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.

</details>


### [66] [Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization](https://arxiv.org/abs/2512.03393)
*Lakshmi Jayalal,Sheetal Kalyani*

Main category: cs.LG

TL;DR: 提出一种基于隐式正则化的免调参框架，用于多测量向量（MMV）中的联合稀疏信号恢复，无需先验知识或参数调整。


<details>
  <summary>Details</summary>
Motivation: 传统MMV方法如M-OMP和M-FOCUSS需要仔细的参数调整或对信号稀疏度和噪声方差的先验知识，这在实际应用中存在局限性。

Method: 通过过参数化引入隐式正则化，将估计矩阵重新参数化为因子，分离共享行支持与个体向量条目。对标准最小二乘目标应用梯度下降，优化动态自然促进期望的行稀疏结构。

Result: 理论证明：在足够小且平衡的初始化下，优化动态表现出"类动量"效应，使真实支持中的行范数比其他行增长快得多，保证解轨迹收敛于理想化的行稀疏解。实证结果：性能与现有方法相当，无需任何先验信息或调参。

Conclusion: 提出的免调参隐式正则化框架有效解决了MMV中的联合稀疏信号恢复问题，克服了传统方法对参数调整和先验知识的依赖。

Abstract: Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a "momentum-like" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.

</details>


### [67] [Physics-Driven Learning Framework for Tomographic Tactile Sensing](https://arxiv.org/abs/2512.03512)
*Xuanxuan Yang,Xiuyang Zhang,Haofeng Chen,Gang Ma,Xiaojie Wang*

Main category: cs.LG

TL;DR: PhyDNN：一种物理驱动的深度学习重建框架，通过将EIT前向模型嵌入学习目标，减少深度网络的黑盒特性，提高电阻抗断层扫描触觉传感的重建质量。


<details>
  <summary>Details</summary>
Motivation: 电阻抗断层扫描（EIT）因其布线简单和形状灵活而成为大面积触觉传感的有吸引力的解决方案，但其非线性逆问题常导致严重伪影和不准确的接触重建。

Method: 提出PhyDNN框架，将EIT前向模型直接嵌入学习目标，通过联合最小化预测与真实电导率图之间的差异并强制与正向偏微分方程的一致性来改善重建。设计了可微分的前向算子网络，准确近似非线性EIT响应，实现快速物理引导训练。

Result: 在16电极软传感器上的大量仿真和真实触觉实验表明，PhyDNN在重建接触形状、位置和压力分布方面始终优于NOSER、TV和标准DNN方法，产生更少伪影、更清晰边界和更高度量分数。

Conclusion: PhyDNN通过将物理模型嵌入深度学习框架，有效提高了电阻抗断层扫描触觉传感的重建质量，展示了其在高质量断层触觉传感中的有效性。

Abstract: Electrical impedance tomography (EIT) provides an attractive solution for large-area tactile sensing due to its minimal wiring and shape flexibility, but its nonlinear inverse problem often leads to severe artifacts and inaccurate contact reconstruction. This work presents PhyDNN, a physics-driven deep reconstruction framework that embeds the EIT forward model directly into the learning objective. By jointly minimizing the discrepancy between predicted and ground-truth conductivity maps and enforcing consistency with the forward PDE, PhyDNN reduces the black-box nature of deep networks and improves both physical plausibility and generalization. To enable efficient backpropagation, we design a differentiable forward-operator network that accurately approximates the nonlinear EIT response, allowing fast physics-guided training. Extensive simulations and real tactile experiments on a 16-electrode soft sensor show that PhyDNN consistently outperforms NOSER, TV, and standard DNNs in reconstructing contact shape, location, and pressure distribution. PhyDNN yields fewer artifacts, sharper boundaries, and higher metric scores, demonstrating its effectiveness for high-quality tomographic tactile sensing.

</details>


### [68] [Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value](https://arxiv.org/abs/2512.03399)
*Joe Edelman,Tan Zhi-Xuan,Ryan Lowe,Oliver Klingefjord,Vincent Wang-Mascianica,Matija Franklin,Ryan Othniel Kearns,Ellie Hain,Atrisha Sarkar,Michiel Bakker,Fazl Barez,David Duvenaud,Jakob Foerster,Iason Gabriel,Joseph Gubbels,Bryce Goodman,Andreas Haupt,Jobst Heitzig,Julian Jara-Ettinger,Atoosa Kasirzadeh,James Ravi Kirkpatrick,Andrew Koh,W. Bradley Knox,Philipp Koralus,Joel Lehman,Sydney Levine,Samuele Marro,Manon Revel,Toby Shorin,Morgan Sutherland,Michael Henry Tessler,Ivan Vendrov,James Wilken-Smith*

Main category: cs.LG

TL;DR: 该论文提出"全栈对齐"概念，认为仅对齐单个AI系统与其操作者意图不足以保证良好社会结果，需要同时对齐AI系统和塑造它们的机构与人类价值观，并建议使用"厚价值模型"来有效表示价值观。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法存在局限性：即使AI系统完美对齐其操作组织的意图，如果该组织的目标与其他机构和个人目标不一致，仍可能导致不良社会结果。现有价值观表示方法（如效用函数、偏好排序或无结构文本）难以有效区分价值观与其他信号、支持规范性推理和建模集体利益。

Method: 提出"厚价值模型"方法，结构化表示价值观和规范，使系统能够区分持久价值观与短暂偏好，建模个体选择的社会嵌入性，并进行规范性推理，将价值观应用于新领域。在五个领域进行演示：AI价值管理、规范性能力代理、双赢谈判系统、意义保持经济机制和民主监管机构。

Result: 通过厚价值模型方法，能够更有效地解决AI对齐中的关键问题：区分价值观与其他信号，支持规范性推理，建模集体利益，并实现AI系统与机构的同时对齐。

Conclusion: 需要全栈对齐——同时对齐AI系统和塑造它们的机构与人类价值观，而厚价值模型是实现这一目标的有效方法，能够在多个应用领域支持更负责任的AI发展。

Abstract: Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.

</details>


### [69] [When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate](https://arxiv.org/abs/2512.03578)
*Florent Forest,Amaury Wei,Olga Fink*

Main category: cs.LG

TL;DR: MAGNETS是一个用于时间序列外生回归的固有可解释神经网络架构，它通过学习一组人类可理解的概念来提供透明解释，无需概念标注。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列外生回归模型虽然预测性能强，但通常是黑盒模型，难以理解驱动决策的时间模式。现有的后验可解释性技术产生的解释粗糙、嘈杂或不稳定，而固有可解释方法需要概念标注、无法捕捉特征交互、对复杂时间模式表达能力有限，且难以扩展到高维多元数据。

Method: 提出MAGNETS（Mask-and-AGgregate NEtwork for Time Series）架构，它通过学习一组紧凑的人类可理解概念，每个概念对应基于掩码的选定输入特征聚合，明确揭示哪些特征驱动预测以及何时在序列中重要。预测通过这些学习概念的透明加性结构组合形成。

Result: 该方法能够在不需任何标注的情况下学习人类可理解的概念，提供清晰透明的决策过程洞察，解决了现有方法在可解释性方面的局限性。

Conclusion: MAGNETS为时间序列外生回归提供了一种固有可解释的解决方案，通过基于掩码的聚合和透明加性结构，实现了准确预测与可信推理的结合。

Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.

</details>


### [70] [The promising potential of vision language models for the generation of textual weather forecasts](https://arxiv.org/abs/2512.03623)
*Edward C. C. Steele,Dinesh Mane,Emilio Monti,Luis Orus,Rebecca Chantrill-Cheyette,Matthew Couch,Kirstine I. Dale,Simon Eaton,Govindarajan Rangarajan,Amir Majlesi,Steven Ramsdale,Michael Sharpe,Craig Smith,Jonathan Smith,Rebecca Yates,Holly Ellis,Charles Ewen*

Main category: cs.LG

TL;DR: 使用视觉语言模型从视频编码的网格天气数据直接生成航运预报文本，探索多模态基础模型在气象产品服务中的应用


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型具有潜力，但在气象产品和服务生成方面的应用仍处于起步阶段。为了加速其应用和采纳，探索将视觉语言模型用于直接从视频编码的网格天气数据生成航运预报文本

Method: 采用视觉语言模型，将网格化的天气数据编码为视频格式，然后直接从这些视频编码数据生成航运预报文本

Result: 早期结果显示，这种方法为提升气象企业及其他领域的生产效率和服务创新提供了有前景的可扩展技术机会

Conclusion: 该研究展示了多模态基础模型在气象服务中的创新应用潜力，为提升气象产品生产效率和服务创新提供了新的技术途径

Abstract: Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.

</details>


### [71] [GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test](https://arxiv.org/abs/2512.03428)
*Ziyi Ding,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出GaussDetect-LiNGAM方法，通过利用前向模型噪声高斯性与反向模型残差独立性之间的等价关系，无需显式高斯性检验即可进行双变量因果发现


<details>
  <summary>Details</summary>
Motivation: 传统LiNGAM方法依赖脆弱且对样本敏感的高斯性检验，这限制了其在实际应用中的可靠性和可访问性。需要一种更稳健的方法来改进因果推断的效率和实用性

Method: 基于理论证明：在LiNGAM的线性、无环和外生性假设下，前向模型噪声的高斯性等价于反向模型中回归变量与残差的独立性。利用这一等价关系，用稳健的基于核的独立性检验替代脆弱的高斯性检验

Result: 实验验证了所提出的等价关系，GaussDetect-LiNGAM在各种噪声类型和样本量下保持高一致性，同时减少了每个决策的测试次数(TPD)

Conclusion: 该方法提高了因果推断的效率和实际应用性，使LiNGAM在现实场景中更加可访问和可靠，无需显式高斯性检验即可进行双变量因果发现

Abstract: We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.

</details>


### [72] [Dynamically Scaled Activation Steering](https://arxiv.org/abs/2512.03661)
*Alex Ferrando,Xavier Suau,Jordi Gonzàlez,Pau Rodriguez*

Main category: cs.LG

TL;DR: DSAS是一种动态缩放激活引导框架，通过自适应调节现有引导方法的强度，只在检测到不良行为时进行干预，实现毒性缓解与模型性能的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法通常对所有输入统一应用干预，这会降低模型在不需要引导时的性能，需要一种更智能的引导方法。

Method: DSAS将"何时引导"与"如何引导"解耦，通过计算上下文相关的缩放因子来选择性调节任何引导方法的强度，并可端到端联合优化。

Result: DSAS与现有引导方法结合时，能持续改进帕累托前沿，在毒性缓解和效用保持之间实现更好的权衡，还可应用于文本到图像扩散模型。

Conclusion: DSAS提供了一种方法无关的引导框架，通过自适应调节干预强度，在最小计算开销下改善性能平衡和可解释性。

Abstract: Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.

</details>


### [73] [Grokked Models are Better Unlearners](https://arxiv.org/abs/2512.03437)
*Yuanbang Liang,Yang Li*

Main category: cs.LG

TL;DR: 论文研究了Grokking现象（延迟泛化）对机器学习遗忘的影响，发现在Grokking后的模型检查点上应用标准遗忘方法，相比早期停止的模型，能实现更高效的遗忘、更少的副作用和更稳定的更新。


<details>
  <summary>Details</summary>
Motivation: 研究Grokking现象（模型在完全拟合训练数据后才出现泛化能力）是否有助于机器学习遗忘任务，即在不完全重新训练的情况下移除特定数据的影响。

Method: 在视觉（CNN/ResNet在CIFAR、SVHN、ImageNet）和语言（Transformer在TOFU风格设置）任务上，比较在Grokking过渡前后应用标准遗忘方法的效果。分析特征和曲率以理解机制。

Result: 从Grokked检查点开始遗忘能实现：(1) 更高效的遗忘（达到目标遗忘水平需要更少的更新），(2) 更少的附带损害（在保留数据和测试数据上的性能下降更小），(3) 跨种子的更新更稳定。分析表明Grokking后的模型学习到更模块化的表示，减少了遗忘和保留子集之间的梯度对齐。

Conclusion: 模型何时被训练（Grokking前vs后）是影响遗忘效果的独立因素，为改进现有遗忘方法提供了实用方案，无需改变算法本身。Grokking后的模型具有更模块化的表示，有利于选择性遗忘。

Abstract: Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.

</details>


### [74] [Quantum Topological Graph Neural Networks for Detecting Complex Fraud Patterns](https://arxiv.org/abs/2512.03696)
*Mohammad Doost,Mohammad Manthouri*

Main category: cs.LG

TL;DR: 提出QTGNN框架，结合量子嵌入、变分图卷积和拓扑数据分析，用于大规模金融网络中的欺诈交易检测，在NISQ设备上实现稳定训练并达到良好性能。


<details>
  <summary>Details</summary>
Motivation: 传统欺诈检测方法难以捕捉金融交易网络中的复杂动态和结构异常，需要结合量子机器学习、图理论和拓扑分析的新方法来解决大规模金融欺诈检测问题。

Method: 1. 量子数据嵌入与纠缠增强；2. 变分量子图卷积与非线形动态；3. 高阶拓扑不变量提取；4. 混合量子-经典异常学习与自适应优化；5. 基于拓扑归因的可解释决策。

Result: 在PaySim和Elliptic等金融数据集上的模拟实验显示，QTGNN在ROC-AUC、精确率和误报率等指标上优于经典和量子基线方法，消融研究验证了各组件贡献。

Conclusion: QTGNN为金融欺诈检测提供了理论严谨、可解释且实用的解决方案，成功桥接了量子机器学习、图理论和拓扑分析，适用于NISQ硬件并具有良好的扩展性。

Abstract: We propose a novel QTGNN framework for detecting fraudulent transactions in large-scale financial networks. By integrating quantum embedding, variational graph convolutions, and topological data analysis, QTGNN captures complex transaction dynamics and structural anomalies indicative of fraud. The methodology includes quantum data embedding with entanglement enhancement, variational quantum graph convolutions with non-linear dynamics, extraction of higher-order topological invariants, hybrid quantum-classical anomaly learning with adaptive optimization, and interpretable decision-making via topological attribution. Rigorous convergence guarantees ensure stable training on noisy intermediate-scale quantum (NISQ) devices, while stability of topological signatures provides robust fraud detection. Optimized for NISQ hardware with circuit simplifications and graph sampling, the framework scales to large transaction networks. Simulations on financial datasets, such as PaySim and Elliptic, benchmark QTGNN against classical and quantum baselines, using metrics like ROC-AUC, precision, and false positive rate. An ablation study evaluates the contributions of quantum embeddings, topological features, non-linear channels, and hybrid learning. QTGNN offers a theoretically sound, interpretable, and practical solution for financial fraud detection, bridging quantum machine learning, graph theory, and topological analysis.

</details>


### [75] [Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention](https://arxiv.org/abs/2512.03464)
*Yujing Liu,Chen Yang*

Main category: cs.LG

TL;DR: 本文提出了一种用于金融情感分析的端到端深度学习框架，通过跨模态注意力机制整合时效性意见和流行性意见两种模态，显著提升了情感分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有金融情感分析方法难以有效整合多样化的意见模态并捕捉模态间的细粒度交互，而时效性意见（市场更新）和流行性意见（集体情感）代表了不同的信息渠道，需要专门的方法来融合这些信息以支持更准确的市场预测和风险评估。

Method: 提出端到端深度学习框架，使用BERT（中文wwm-ext）进行特征嵌入，设计金融多头跨注意力机制（FMHCA）促进时效性意见和流行性意见两种模态间的信息交换，通过Transformer层优化特征，采用多模态因子双线性池化进行特征融合，最终分类为负面、中性和正面情感。

Result: 在涵盖837家公司的综合数据集上进行广泛实验，该方法达到83.5%的准确率，显著优于包括BERT+Transformer在内的基线方法，提升幅度达21%。

Conclusion: 该框架通过有效整合金融意见的不同模态，能够支持更准确的金融决策和风险管理，展示了跨模态注意力机制在金融情感分析中的潜力。

Abstract: In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.

</details>


### [76] [DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training](https://arxiv.org/abs/2512.03847)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Yuhui Wang,Sixian Li,Junjie Ye,Honglin Guo,Shichun Liu,Chenhao Huang,Yajie Yang,Junlin Shang,Senjie Jin,Ming Zhang,Jiazheng Zhang,Caishuang Huang,Yunke Zhang,Demei Yan,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: DVPO是一个新的强化学习框架，结合条件风险理论和分布价值建模，通过令牌级价值分布提供细粒度监督，使用非对称风险正则化平衡鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现实世界部署中常涉及噪声或不完整的监督信号，复杂不可靠的监督会破坏训练稳定性并损害泛化能力。现有方法如最坏情况优化和均值方法往往忽视泛化性，可能产生过于保守的策略，在不同真实场景中表现不均。

Method: DVPO结合条件风险理论与分布价值建模，学习令牌级价值分布以提供细粒度监督，并应用非对称风险正则化来塑造分布尾部：收缩下尾部以抑制噪声负偏差，同时扩展上尾部以保持探索多样性。

Result: 在多轮对话、数学推理和科学问答的广泛实验和分析中，DVPO在噪声监督下始终优于PPO、GRPO和基于鲁棒贝尔曼的PPO。

Conclusion: DVPO展示了在现实世界LLM后训练中的潜力，通过平衡鲁棒性和泛化性，在噪声监督环境下表现优异。

Abstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.

</details>


### [77] [Bayesian Event-Based Model for Disease Subtype and Stage Inference](https://arxiv.org/abs/2512.03467)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: 本文提出了一种新的贝叶斯事件模型BEBMS，用于疾病亚型分析，相比现有方法SuStaIn在合成数据和真实阿尔茨海默病数据上表现更优。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病在不同患者中的进展方式存在差异，通常存在少数几种进展亚型。现有的SuStaIn方法被广泛用于识别疾病亚型，但其鲁棒性尚未得到充分验证。

Method: 开发了基于贝叶斯的事件模型变体BEBMS，通过合成数据实验比较BEBMS与SuStaIn在不同程度的模型误设下的性能，并在真实阿尔茨海默病数据集上应用两种方法。

Result: BEBMS在排序、分期和亚型分配任务上显著优于SuStaIn。在阿尔茨海默病数据应用中，BEBMS的结果与科学共识更加一致。

Conclusion: BEBMS作为一种新的贝叶斯亚型分析方法，相比SuStaIn具有更好的鲁棒性和准确性，能够更可靠地揭示疾病进展的亚型结构。

Abstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.

</details>


### [78] [Scalable Decision Focused Learning via Online Trainable Surrogates](https://arxiv.org/abs/2512.03861)
*Gaetano Signorelli,Michele Lombardi*

Main category: cs.LG

TL;DR: 提出一种基于无偏估计替代模型的决策聚焦学习加速方法，减少训练时昂贵的内层求解器调用，同时保持解质量。


<details>
  <summary>Details</summary>
Motivation: 传统训练的估计器在决策支持系统中可能导致次优解，而决策聚焦学习方法虽然能解决此问题，但训练时计算代价过高，可扩展性差。

Method: 提出基于无偏估计替代模型的加速方法，用高效的替代函数替代昂贵的损失函数评估。该方法适用于黑盒设置，能补偿优化模型的简化并考虑补救行动。

Result: 该方法显著减少了昂贵的内层求解器调用次数，同时解质量与其他最先进技术相当。

Conclusion: 提出的无偏估计替代模型方法有效解决了决策聚焦学习的可扩展性问题，在保持解质量的同时大幅降低了计算成本。

Abstract: Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.

</details>


### [79] [SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening](https://arxiv.org/abs/2512.03471)
*Ian Henriques,Lynda Elhassar,Sarvesh Relekar,Denis Walrave,Shayan Hassantabar,Vishu Ghanakota,Adel Laoui,Mahmoud Aich,Rafia Tir,Mohamed Zerguine,Samir Louafi,Moncef Kimouche,Emmanuel Cosson,Niraj K Jha*

Main category: cs.LG

TL;DR: SweetDeep是一个轻量级神经网络，利用三星Galaxy Watch 7收集的生理和人口统计数据，在自由生活条件下实现了82.5%的2型糖尿病检测准确率。


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病全球发病率上升，需要可扩展且经济有效的筛查方法。现有诊断方法依赖侵入性、昂贵的生化检测，而可穿戴设备为机器学习疾病检测提供了新可能，但先前研究局限于受控环境。

Method: 开发了SweetDeep紧凑神经网络，使用285名（糖尿病和非糖尿病）参与者的生理和人口统计数据训练，数据通过三星Galaxy Watch 7在自由生活条件下收集，持续6天。每个参与者每天提供多个2分钟传感器记录，总计约20个记录/人。模型参数少于3,000个。

Result: 在三折交叉验证下，SweetDeep达到82.5%的患者级准确率（82.1% macro-F1，79.7%灵敏度，84.6%特异度），预期校准误差5.5%。允许模型对低于10%的低置信度预测弃权时，剩余患者准确率达84.5%。

Conclusion: 工程化特征与轻量级架构结合，可在真实世界可穿戴设备环境中实现准确、快速且可泛化的2型糖尿病检测。

Abstract: The global rise in type 2 diabetes underscores the need for scalable and cost-effective screening methods. Current diagnosis requires biochemical assays, which are invasive and costly. Advances in consumer wearables have enabled early explorations of machine learning-based disease detection, but prior studies were limited to controlled settings. We present SweetDeep, a compact neural network trained on physiological and demographic data from 285 (diabetic and non-diabetic) participants in the EU and MENA regions, collected using Samsung Galaxy Watch 7 devices in free-living conditions over six days. Each participant contributed multiple 2-minute sensor recordings per day, totaling approximately 20 recordings per individual. Despite comprising fewer than 3,000 parameters, SweetDeep achieves 82.5% patient-level accuracy (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) under three-fold cross-validation, with an expected calibration error of 5.5%. Allowing the model to abstain on less than 10% of low-confidence patient predictions yields an accuracy of 84.5% on the remaining patients. These findings demonstrate that combining engineered features with lightweight architectures can support accurate, rapid, and generalizable detection of type 2 diabetes in real-world wearable settings.

</details>


### [80] [Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment](https://arxiv.org/abs/2512.03864)
*Danny Hoang,Anandkumar Patel,Ruimen Chen,Rajiv Malhotra,Farhad Imani*

Main category: cs.LG

TL;DR: 该研究比较了智能加工中AI模型的能耗、精度和速度，引入超维计算作为替代方案，在保持精度的同时大幅降低能耗和计算时间。


<details>
  <summary>Details</summary>
Motivation: 智能制造虽能提高效率和降低能耗，但AI模型的高能耗可能抵消这些优势。需要寻找既能保持预测精度又能显著降低能耗的AI解决方案。

Method: 使用原位传感预测智能加工的几何质量，比较常见AI模型的能耗、精度和速度。引入超维计算作为替代方法，与传统模型进行对比分析。

Result: 超维计算达到与传统模型相当的精度，同时大幅降低能耗：训练能耗降低200倍，推理能耗降低175-1000倍。训练时间减少200倍，推理时间减少300-600倍。

Conclusion: 超维计算在智能加工几何质量预测中展现出巨大潜力，能够在保持精度的同时显著降低能耗和计算时间，为实现节能智能制造提供了有前景的解决方案。

Abstract: Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\times$ for training and 175 to 1000$\times$ for inference. Furthermore, HDC reduces training times by 200$\times$ and inference times by 300 to 600$\times$, showcasing its potential for energy-efficient smart manufacturing.

</details>


### [81] [Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression](https://arxiv.org/abs/2512.03475)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: JPM是一个概率框架，用于从横断面数据推断混合神经退行性疾病的联合进展，通过将单病轨迹视为部分排序来构建联合进展先验，相比传统单病模型提高了排序准确性。


<details>
  <summary>Details</summary>
Motivation: 传统事件模型假设个体只有单一疾病，但神经退行性疾病中混合病理很常见，需要能够处理多种疾病同时进展的模型。

Method: 提出联合进展模型(JPM)，将单病轨迹视为部分排序，构建联合进展先验，研究了四种变体：Pairwise、Bradley-Terry、Plackett-Luce和Mallows，并分析校准性、分离性和锐度三个属性。

Result: 所有JPM变体都具有校准性，分离性接近完美；锐度因变体而异，可通过输入部分排序的简单特征预测；在合成实验中，JPM比强基线SA-EBM提高约21%的排序准确性；在NACC数据中，Mallows变体和基线模型的结果与AD和VaD混合病理进展的先前文献更一致。

Conclusion: JPM为混合神经退行性疾病的进展建模提供了有效框架，能够处理多种病理同时进展的复杂情况，相比传统单病模型有显著改进。

Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.

</details>


### [82] [Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning](https://arxiv.org/abs/2512.03973)
*Franki Nguimatsia Tiofack,Théotime Le Hellard,Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: GFP通过结合多步流匹配策略和蒸馏的单步演员，在离线强化学习中实现了对高价值动作的选择性模仿，而不是盲目模仿所有数据集动作，从而在多个基准测试中取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习方法使用行为正则化来保持策略接近数据集分布，但这种方法无法区分高价值和低价值动作，导致性能受限。

Method: 提出引导流策略(GFP)，将多步流匹配策略与蒸馏的单步演员耦合。演员通过加权行为克隆指导流策略专注于克隆数据集中的高价值动作，而不是盲目模仿所有状态-动作对。流策略反过来约束演员与数据集中的最佳转移对齐，同时最大化批评者价值。

Result: GFP在OGBench、Minari和D4RL基准测试的144个状态和像素任务中实现了最先进的性能，在次优数据集和挑战性任务上取得了显著提升。

Conclusion: GFP通过演员和流策略的相互指导机制，实现了对高价值动作的选择性模仿，有效解决了传统行为正则化方法的局限性，在离线强化学习中取得了突破性进展。

Abstract: Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/

</details>


### [83] [MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking](https://arxiv.org/abs/2512.04044)
*Yizhou Zhao,Zhiwei Steven Wu,Adam Block*

Main category: cs.LG

TL;DR: MarkTune是一种用于开放权重语言模型的理论化、基于策略的微调框架，通过将水印信号作为奖励进行优化，在保持文本质量的同时显著提升水印检测能力，接近推理时水印的性能。


<details>
  <summary>Details</summary>
Motivation: 开放权重语言模型对水印技术提出了严峻挑战，因为一旦模型权重公开，推理时的干预措施就无法强制执行。现有的开放权重模型水印技术（如GaussMark）通常需要对模型权重进行微小修改，这虽然能产生可检测的信号，但要达到与推理时水印相当的检测能力，通常需要显著降低生成质量的权重扰动。

Method: MarkTune是一个理论化、基于策略的微调框架，将GaussMark水印信号作为奖励函数，同时通过正则化防止文本质量下降。该方法在模型的表示空间中进行更精细的、水印感知的权重更新，在保持生成质量的同时优化水印检测能力。

Result: MarkTune在质量-可检测性权衡方面持续优于GaussMark，将GaussMark的性能边界推近到接近推理时水印的水平。该方法对改写和微调攻击具有鲁棒性，并表现出很强的泛化能力：在一个数据集上微调的模型在未见数据集上仍保持显著的水印检测能力。

Conclusion: MarkTune为开放权重语言模型嵌入鲁棒、高质量水印提供了一种通用策略，解决了开放权重模型水印技术中的关键挑战，在保持文本质量的同时实现了接近推理时水印的检测性能。

Abstract: Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.

</details>


### [84] [Modal Logical Neural Networks](https://arxiv.org/abs/2512.03491)
*Antonin Sulc*

Main category: cs.LG

TL;DR: MLNNs是一个将深度学习与模态逻辑形式语义相结合的神经符号框架，能够进行必然性和可能性推理，通过可微分的"逻辑护栏"机制增强模型的逻辑一致性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型缺乏形式逻辑推理能力，特别是在处理必然性和可能性等模态概念时。需要一种能够将深度学习与模态逻辑语义相结合的方法，既能进行符号推理，又能从数据中学习逻辑关系结构。

Method: 基于Kripke语义，设计了专门的模态算子神经元（□和◇），在可能世界集合上操作。框架具有高度灵活性：世界间的可达关系可以由用户固定以强制执行已知规则，或者作为归纳特征由神经网络参数化学习。整个框架端到端可微分，通过最小化逻辑矛盾损失进行学习。

Result: 在四个案例研究中展示了MLNNs的应用：语法护栏、未知的公理检测、多智能体认知信任、自然语言谈判中的建设性欺骗检测。实验表明，强制执行或学习可达关系可以提高逻辑一致性和可解释性，而不改变底层任务架构。

Conclusion: MLNNs提供了一个灵活的神经符号框架，能够将模态逻辑推理与深度学习相结合，既能强制执行已知逻辑规则，又能从数据中学习逻辑关系结构，提高了模型的逻辑一致性和可解释性。

Abstract: We propose Modal Logical Neural Networks (MLNNs), a neurosymbolic framework that integrates deep learning with the formal semantics of modal logic, enabling reasoning about necessity and possibility. Drawing on Kripke semantics, we introduce specialized neurons for the modal operators $\Box$ and $\Diamond$ that operate over a set of possible worlds, enabling the framework to act as a differentiable ``logical guardrail.'' The architecture is highly flexible: the accessibility relation between worlds can either be fixed by the user to enforce known rules or, as an inductive feature, be parameterized by a neural network. This allows the model to optionally learn the relational structure of a logical system from data while simultaneously performing deductive reasoning within that structure.
  This versatile construction is designed for flexibility. The entire framework is differentiable from end to end, with learning driven by minimizing a logical contradiction loss. This not only makes the system resilient to inconsistent knowledge but also enables it to learn nonlinear relationships that can help define the logic of a problem space. We illustrate MLNNs on four case studies: grammatical guardrailing, axiomatic detection of the unknown, multi-agent epistemic trust, and detecting constructive deception in natural language negotiation. These experiments demonstrate how enforcing or learning accessibility can increase logical consistency and interpretability without changing the underlying task architecture.

</details>


### [85] [Fare Comparison App of Uber, Ola and Rapido](https://arxiv.org/abs/2512.04065)
*Ashlesha Gopinath Sawant,Sahil S. Jadhav,Vidhan R. Jain,Shriraj S. Jagtap,Prachi Jadhav,Soham Jadhav,Ichha Raina*

Main category: cs.LG

TL;DR: 开发了一个网约车比价Web应用，通过API获取Ola、Uber、Rapido的实时价格，为用户提供最优出行选择。


<details>
  <summary>Details</summary>
Motivation: 用户在日常出行中选择网约车服务时面临困难，难以在价格和时间效率之间做出最优选择，需要透明化的比价工具来提升出行体验。

Method: 开发Web应用程序，使用Python后端通过API获取Ola、Uber、Rapido的实时价格数据，结合Android Studio模拟器、Appium和位置比较技术处理数据获取的挑战。

Result: 创建了一个功能完整的比价系统，能够为用户提供不同网约车服务的实时价格比较，并推荐最优出行选项。

Conclusion: 该项目通过技术手段解决了网约车比价问题，提高了出行服务的透明度、效率和用户体验。

Abstract: In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.

</details>


### [86] [Adaptive sampling using variational autoencoder and reinforcement learning](https://arxiv.org/abs/2512.03525)
*Adil Rasheed,Mikael Aleksander Jansen Shahly,Muhammad Faisal Aftab*

Main category: cs.LG

TL;DR: 提出自适应稀疏感知框架，结合变分自编码器先验和强化学习进行顺序测量选择，优于传统压缩感知、最优传感器放置和生成模型方法


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知使用通用基和随机测量，效率和质量有限；最优传感器放置基于历史数据但使用固定线性基，无法适应非线性或样本特异性变化；生成模型压缩感知使用深度生成先验但采样仍非最优

Method: 提出自适应稀疏感知框架，将变分自编码器先验与强化学习耦合，通过强化学习顺序选择测量点，变分自编码器提供生成先验

Result: 实验表明该方法在稀疏测量重建方面优于压缩感知、最优传感器放置和生成模型重建方法

Conclusion: 结合变分自编码器先验和强化学习的自适应稀疏感知框架能够有效提升稀疏采样重建质量，优于现有方法

Abstract: Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.

</details>


### [87] [Parameter-Efficient Augment Plugin for Class-Incremental Learning](https://arxiv.org/abs/2512.03537)
*Zhiming Xu,Baile Xu,Jian Zhao,Furao Shen,Suorong Yang*

Main category: cs.LG

TL;DR: DLC是一种基于LoRA组件的插件式扩展范式，用于非预训练的类增量学习场景，通过向基础模型注入任务特定的残差并聚合表示来提升性能，同时保持参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有的类增量学习方法存在遗忘问题、稳定性-可塑性困境，或者需要大量参数增加。需要一种既高效又能保持高性能的增量学习解决方案。

Method: 将基于回放或蒸馏训练的特征提取器作为基础模型，为每个任务使用低秩适应(LoRA)向基础模型的深层注入任务特定残差。推理时聚合带任务特定残差的表示，并引入轻量级加权单元来减轻非目标LoRA插件的干扰。

Result: 在ImageNet-100上，仅使用标准ResNet-18 4%的参数，DLC模型实现了8%的准确率提升，并在固定内存预算下超越了最先进的方法。

Conclusion: DLC作为一种即插即用的增强方法，能够高效扩展基础方法，在类增量学习中实现了参数效率和性能的平衡，展示了卓越的效率。

Abstract: Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.

</details>


### [88] [Towards Irreversible Machine Unlearning for Diffusion Models](https://arxiv.org/abs/2512.03564)
*Xun Yuan,Zilong Zhao,Jiayu Li,Aryan Pasikhani,Prosanta Gope,Biplab Sikdar*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散模型遗忘学习的攻击方法DiMRA，能够逆转基于微调的遗忘学习技术，并提出了更鲁棒的遗忘学习方法DiMUM来应对这种攻击。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成合成图像方面表现出色，但存在安全、隐私和版权问题，需要机器遗忘技术来让模型忘记特定训练数据。现有基于微调的遗忘学习方法虽然高效，但存在被逆转攻击的脆弱性。

Method: 1. 提出DiMRA攻击：在无先验知识的情况下，通过在辅助数据集上优化已遗忘的扩散模型来逆转遗忘过程；2. 提出DiMUM防御方法：通过记忆替代数据或特征来替换目标遗忘数据，防止生成不需要的内容。

Result: 实验证明DiMRA能够有效逆转当前最先进的基于微调的扩散模型遗忘学习方法，揭示了现有技术的脆弱性。DiMUM在保持扩散模型生成性能的同时，显著增强了对DiMRA攻击的鲁棒性。

Conclusion: 基于微调的扩散模型遗忘学习方法存在被逆转攻击的严重漏洞，需要更鲁棒的解决方案。DiMUM通过记忆替代策略提供了一种更安全的遗忘学习方案，能够有效防御DiMRA攻击。

Abstract: Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.

</details>


### [89] [Optimal Transportation and Alignment Between Gaussian Measures](https://arxiv.org/abs/2512.03579)
*Sanjit Dandapanthula,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan,Aaditya Ramdas,Ziv Goldfeld*

Main category: cs.LG

TL;DR: 该论文系统研究了高斯分布下的最优传输和Gromov-Wasserstein对齐问题，针对二次成本函数提供了闭式解，解决了未中心化高斯分布的对齐问题，并提出了高效的多边际优化算法。


<details>
  <summary>Details</summary>
Motivation: 最优传输和Gromov-Wasserstein对齐是数据科学中比较、转换和聚合异构数据集的重要几何框架，但由于计算成本高昂，大规模应用通常依赖于高斯分布下的闭式解。现有研究在处理未中心化高斯分布和多边际优化方面存在空白。

Method: 针对可分离希尔伯特空间上的未中心化高斯分布，给出了内积GW对齐的闭式表达式（需要酉算子优化），并推导了紧致的上下界。对于至少一个高斯分布中心化的情况，提供了完全闭式解。同时将高斯多边际最优传输简化为可处理的优化问题，并设计了基于秩缺陷约束的高效算法。

Result: 解决了未中心化高斯分布IGW对齐的开放问题，提供了闭式表达式和解析上下界；对于中心化高斯分布，获得了完全闭式解并扩展到IGW重心计算；提出了高效的多边际OT优化算法。在知识蒸馏和异构聚类任务中验证了方法的有效性。

Conclusion: 该研究填补了高斯分布下最优传输和Gromov-Wasserstein对齐理论的多个空白，为大规模应用提供了理论基础和实用算法，在知识蒸馏和异构聚类等任务中展示了实际应用价值。

Abstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.

</details>


### [90] [Federated Learning and Trajectory Compression for Enhanced AIS Coverage](https://arxiv.org/abs/2512.03584)
*Thomas Gräupl,Andreas Reisenbauer,Marcel Hecko,Anil Rasouli,Anita Graser,Melitta Dragaschnig,Axel Weissenfeld,Gilles Dejaegere,Mahmoud Sakr*

Main category: cs.LG

TL;DR: VesselEdge系统利用联邦学习和带宽受限轨迹压缩技术，通过扩展AIS覆盖范围来增强海上态势感知能力，将船舶转变为移动传感器，实现实时异常检测和低带宽下的高效数据传输。


<details>
  <summary>Details</summary>
Motivation: 当前海上态势感知面临AIS覆盖范围有限、数据传输带宽受限的挑战，需要一种能够在低带宽条件下扩展AIS覆盖并实现实时异常检测的系统。

Method: 系统集成了M3fed联邦学习模型用于分布式学习，以及BWC-DR-A算法用于轨迹压缩，特别优先处理异常数据，将船舶转变为移动传感器节点。

Result: 初步结果显示VesselEdge系统在提高AIS覆盖范围和增强海上态势感知方面具有有效性，这些结果基于历史数据验证。

Conclusion: VesselEdge系统通过结合联邦学习和轨迹压缩技术，成功解决了海上态势感知中的AIS覆盖扩展和低带宽数据传输问题，为实时异常检测提供了可行方案。

Abstract: This paper presents the VesselEdge system, which leverages federated learning and bandwidth-constrained trajectory compression to enhance maritime situational awareness by extending AIS coverage. VesselEdge transforms vessels into mobile sensors, enabling real-time anomaly detection and efficient data transmission over low-bandwidth connections. The system integrates the M3fed model for federated learning and the BWC-DR-A algorithm for trajectory compression, prioritizing anomalous data. Preliminary results demonstrate the effectiveness of VesselEdge in improving AIS coverage and situational awareness using historical data.

</details>


### [91] [Observation-driven correction of numerical weather prediction for marine winds](https://arxiv.org/abs/2512.03606)
*Matteo Peduto,Qidong Yang,Jonathan Giezendanner,Devis Tuia,Sherrie Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Transformer的深度学习架构，通过同化实时观测数据来校正全球天气预报模型(GFS)的风速预测，显著提高了海洋风速预报的准确性。


<details>
  <summary>Details</summary>
Motivation: 海洋风速预报对于航行安全、船舶航线规划和能源运营至关重要，但由于海洋观测数据稀疏、异构且时间变化大，准确预报一直面临挑战。现有数值天气预报模型存在系统性误差，需要实时观测数据进行校正。

Method: 将风速预报重新定义为观测信息驱动的数值天气预报模型校正问题。提出基于Transformer的深度学习架构：1) 通过掩码和基于集合的注意力机制处理不规则和时间变化的观测数据集；2) 通过交叉注意力将预测条件化于最近的观测-预报对；3) 使用循环时间嵌入和坐标感知的位置表示，实现任意空间坐标的单次推理。模型能够处理多种观测平台数据。

Result: 在大西洋区域使用ICOADS观测数据进行评估，模型在所有48小时预报时效内都降低了GFS 10米风速的RMSE：1小时预报时效改善45%，48小时预报时效改善13%。空间分析显示在海岸线和航线等观测密集区域改进最为显著。架构能够自然处理异构观测平台数据，并同时生成站点特定预测和流域尺度网格产品。

Conclusion: 该方法展示了一种实用、低延迟的后处理策略，通过校正系统性预报误差来补充数值天气预报，显著提高了海洋风速预报的准确性，特别适用于观测密集区域。

Abstract: Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.

</details>


### [92] [CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion](https://arxiv.org/abs/2512.03610)
*Julius Lenz*

Main category: cs.LG

TL;DR: CoGraM是一种用于联邦学习和分布式学习中合并神经网络的多阶段、上下文敏感的优化方法，通过层级、神经元和权重级别的迭代优化来提升合并效果，避免传统方法（如权重平均、Fisher合并）的精度损失和不稳定性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习和分布式学习中需要在不重新训练的情况下合并神经网络，但现有方法如权重平均或Fisher合并经常导致精度损失且在不同随机种子下不稳定，因此需要更有效的合并方法。

Method: CoGraM是一种多阶段、上下文敏感的、基于损失的迭代优化方法，在层级、神经元和权重级别进行操作，通过决策对齐损失差异和阈值，并通过回滚机制防止有害更新。

Result: CoGraM能够显著改善合并后的网络性能，解决了Fisher等方法的弱点，在合并神经网络时表现出更好的效果。

Conclusion: CoGraM为联邦学习和分布式学习中的神经网络合并问题提供了一个有效的解决方案，通过其多阶段、上下文敏感的优化方法，能够显著提升合并网络的性能并保持稳定性。

Abstract: Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.

</details>


### [93] [Conditional updates of neural network weights for increased out of training performance](https://arxiv.org/abs/2512.03653)
*Jan Saynisch-Wagner,Saran Rajendran Sari*

Main category: cs.LG

TL;DR: 提出一种增强神经网络在训练数据与应用数据不相似（如分布外问题、模式和机制变化）时性能的方法，通过重训练获取权重异常，建立预测因子回归，外推权重以适应应用数据。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在训练数据与应用数据不相似时的性能下降问题，特别是在分布外问题、模式变化和机制变化等场景中，这在气候科学等实际应用中很常见。

Method: 方法包含三个主要步骤：1）对训练数据集的合理子集进行神经网络重训练，记录权重异常；2）选择合理的预测因子，建立预测因子与权重异常之间的回归关系；3）外推权重，从而将神经网络外推到应用数据。

Result: 在气候科学的三个用例中展示了该方法，成功实现了神经网络的时间、空间和跨域外推，证明了方法的有效性。

Conclusion: 该方法能够有效增强神经网络在训练数据与应用数据不相似时的性能，为处理分布外问题、模式变化和机制变化提供了实用解决方案，在气候科学等领域具有应用价值。

Abstract: This study proposes a method to enhance neural network performance when training data and application data are not very similar, e.g., out of distribution problems, as well as pattern and regime shifts. The method consists of three main steps: 1) Retrain the neural network towards reasonable subsets of the training data set and note down the resulting weight anomalies. 2) Choose reasonable predictors and derive a regression between the predictors and the weight anomalies. 3) Extrapolate the weights, and thereby the neural network, to the application data. We show and discuss this method in three use cases from the climate sciences, which include successful temporal, spatial and cross-domain extrapolations of neural networks.

</details>


### [94] [Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting](https://arxiv.org/abs/2512.03656)
*Salim Khazem,Houssam Kanso*

Main category: cs.LG

TL;DR: 本文提出了一种结合循环时间编码与混合LSTM-CNN架构的统一深度学习框架，用于提升多步能源预测精度，通过实验验证了该框架在电力消费预测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 准确的电力消费预测对于需求管理和智能电网运营至关重要。现有方法在同时利用循环时间表示、日历特征和混合集成架构方面存在不足，需要统一的预测框架来提升多步预测性能。

Method: 1. 使用正弦余弦编码系统转换基于日历的属性，保留周期结构；2. 通过相关性分析评估预测相关性；3. 构建集成模型，包含LSTM、CNN和针对每个预测时域的MLP回归元学习器，以同时利用长期季节效应和短期局部模式；4. 使用一年全国消费数据集进行实验，包括消融分析和与现有基线的比较。

Result: 在所有七个预测时域上均取得一致改进，混合模型相比单个架构和先前方法实现了更低的RMSE和MAE。实验证实了循环时间表示与互补深度学习结构结合的优势。

Conclusion: 结合循环时间编码与混合LSTM-CNN架构的统一框架能有效提升短期能源预测精度。这是首个在统一短期能源预测框架中同时评估时间编码、日历特征和混合集成架构的工作。

Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.

</details>


### [95] [Feature-aware Modulation for Learning from Temporal Tabular Data](https://arxiv.org/abs/2512.03678)
*Hao-Run Cai,Han-Jia Ye*

Main category: cs.LG

TL;DR: 该论文提出了一种特征感知的时间调制机制，通过调节特征的统计属性来应对表格数据中的时间分布偏移问题，在保持轻量级的同时有效平衡了泛化性和适应性。


<details>
  <summary>Details</summary>
Motivation: 表格机器学习在现实部署中面临时间分布偏移的挑战，特征与标签之间的关系持续演变。静态模型假设固定映射以确保泛化，而自适应模型可能过度拟合瞬态模式，形成了鲁棒性与适应性之间的困境。

Method: 提出特征感知的时间调制机制，将特征表示条件化于时间上下文，调节特征的统计属性（如尺度和偏度）。通过跨时间对齐特征语义，实现轻量级但强大的适应能力。

Result: 基准评估验证了该方法在处理表格数据时间偏移方面的有效性，能够有效平衡泛化性和适应性。

Conclusion: 特征转换策略能够缓解跨时间阶段特征表示的差异，特征感知的时间调制机制通过调节统计属性来对齐特征语义，实现了对表格数据时间分布偏移的有效处理。

Abstract: While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.

</details>


### [96] [Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm](https://arxiv.org/abs/2512.03744)
*Xuhui Lin,Qiuchen Lu*

Main category: cs.LG

TL;DR: 提出了一种基于物理约束的哈密顿学习算法，通过"结构不可逆性检测"和"能量景观重建"来识别城市交通系统的隐藏结构损伤，解决传统表面指标无法检测"虚假恢复"的问题。


<details>
  <summary>Details</summary>
Motivation: 当前城市交通系统的韧性评估方法依赖表面恢复指标，无法检测隐藏的结构损伤，导致无法区分真正的恢复和"虚假恢复"（交通指标正常化但系统动力学永久退化）。

Method: 开发了一种物理约束的哈密顿学习算法，结合结构不可逆性检测和能量景观重建，通过提取低维状态表示、物理约束优化识别准哈密顿结构，并通过能量景观比较量化结构变化。

Result: 对伦敦2021年极端降雨的分析表明，虽然表面指标完全恢复，但该算法检测到64.8%的结构损伤被传统监测方法遗漏。

Conclusion: 该框架为主动结构风险评估提供了工具，使基础设施投资能够基于真实的系统健康状况而非误导性的表面指标。

Abstract: Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and "false recovery," where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining "structural irreversibility detection" and "energy landscape reconstruction" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.

</details>


### [97] [Universally Converging Representations of Matter Across Scientific Foundation Models](https://arxiv.org/abs/2512.03750)
*Sathya Edamadaka,Soojung Yang,Ju Li,Rafael Gómez-Bombarelli*

Main category: cs.LG

TL;DR: 研究发现近60个科学模型在化学系统上具有高度对齐的内部表示，表明基础模型学习到了物理现实的共同底层表示，但当前模型仍受训练数据和归纳偏置限制，未能编码真正通用的结构。


<details>
  <summary>Details</summary>
Motivation: 理解不同模态和架构的机器学习模型是否学习到相似的内部表示对于构建可靠泛化的科学基础模型至关重要。虽然语言和视觉领域已观察到表示收敛现象，但在科学领域尚未系统探索。

Method: 分析了近60个科学模型（涵盖字符串、图、3D原子和蛋白质等模态）的表示对齐情况，研究了在不同化学系统上的表示相似性，并考察了模型在不同输入条件下的表示行为。

Result: 1) 在不同数据集上训练的模型对小分子具有高度相似的表示；2) 机器学习原子间势能模型在性能提升时表示空间收敛；3) 发现两种不同的表示行为模式：在训练相似输入上，高性能模型紧密对齐，弱模型在表示空间中发散到局部最优；在训练差异大的结构上，几乎所有模型都坍缩到低信息表示。

Conclusion: 表示对齐可作为科学基础模型通用性的量化基准。当前模型仍受训练数据和归纳偏置限制，未能编码真正通用的结构。该工作可用于追踪物质通用表示的出现，并选择跨模态、物质领域和科学任务转移性最好的模型。

Abstract: Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.

</details>


### [98] [Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition](https://arxiv.org/abs/2512.03755)
*Stephen Law,Tao Yang,Nanjiang Chen,Xuhui Lin*

Main category: cs.LG

TL;DR: 提出条件轨迹编码器，联合学习空间和运动表示，解决轨迹分析中空间与时间分离、方向不对称性忽略、过度依赖辅助数据的问题，通过几何特征量化城市形态造成的认知不平等。


<details>
  <summary>Details</summary>
Motivation: 当前城市轨迹分析方法存在三个主要问题：1) 空间表示与时间动态分离训练；2) 忽略导航中的方向不对称性（A→B ≠ B→A）；3) 过度依赖POI、图像等辅助数据而非城市空间的基本几何特性。这些限制阻碍了对城市认知模式的全面理解。

Method: 提出条件轨迹编码器，使用双向LSTM处理可见性比率和曲率等几何特征，条件于可学习的起点嵌入。通过对比学习将表示分解为共享的城市模式和起点特定的签名，保留起点依赖的不对称性。

Result: 在六个合成城市和北京西城区的真实数据验证表明，城市形态会产生系统性的认知不平等。该方法能够量化不同起点位置的认知不对称性，为城市规划和导航系统提供可操作的洞察。

Conclusion: 该框架为城市规划者提供了评估体验公平性的定量工具，为建筑师提供了布局决策认知影响的洞察，并为导航系统实现了起点感知的分析能力，推动了城市分析从描述性向解释性的转变。

Abstract: Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \to B \ne B \to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.

</details>


### [99] [Deep Unfolding: Recent Developments, Theory, and Design Guidelines](https://arxiv.org/abs/2512.03768)
*Nir Shlezinger,Santiago Segarra,Yi Zhang,Dvir Avrahami,Zohar Davidov,Tirza Routtenberg,Yonina C. Eldar*

Main category: cs.LG

TL;DR: 深度展开是一种将迭代优化算法转化为结构化可训练机器学习架构的框架，旨在融合经典优化方法的理论保证与机器学习的数据驱动能力。


<details>
  <summary>Details</summary>
Motivation: 经典迭代优化算法虽然具有可解释性和理论保证，但通常依赖代理目标、需要仔细的超参数调优且计算延迟较大；而机器学习虽然具有强大的数据驱动建模能力，但缺乏优化驱动推理所需的结构、透明性和效率。深度展开旨在弥合这两种范式之间的差距。

Method: 深度展开通过系统地将迭代优化算法转化为结构化、可训练的机器学习架构来实现。文章介绍了四种代表性的设计范式，并讨论了由其迭代性质产生的独特训练方案。

Result: 文章综述了深度展开的理论进展，包括收敛性和泛化保证的建立，并提供了比较性的定性和实证研究，说明了深度展开在复杂性、可解释性和鲁棒性方面的相对权衡。

Conclusion: 深度展开为信号处理中的优化方法提供了一个有前景的框架，它结合了经典优化算法的理论优势和机器学习的数据驱动能力，在复杂性、可解释性和鲁棒性之间实现了平衡。

Abstract: Optimization methods play a central role in signal processing, serving as the mathematical foundation for inference, estimation, and control. While classical iterative optimization algorithms provide interpretability and theoretical guarantees, they often rely on surrogate objectives, require careful hyperparameter tuning, and exhibit substantial computational latency. Conversely, machine learning (ML ) offers powerful data-driven modeling capabilities but lacks the structure, transparency, and efficiency needed for optimization-driven inference. Deep unfolding has recently emerged as a compelling framework that bridges these two paradigms by systematically transforming iterative optimization algorithms into structured, trainable ML architectures. This article provides a tutorial-style overview of deep unfolding, presenting a unified perspective of methodologies for converting optimization solvers into ML models and highlighting their conceptual, theoretical, and practical implications. We review the foundations of optimization for inference and for learning, introduce four representative design paradigms for deep unfolding, and discuss the distinctive training schemes that arise from their iterative nature. Furthermore, we survey recent theoretical advances that establish convergence and generalization guarantees for unfolded optimizers, and provide comparative qualitative and empirical studies illustrating their relative trade-offs in complexity, interpretability, and robustness.

</details>


### [100] [Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach](https://arxiv.org/abs/2512.03786)
*Conor McCarthy,Jan Peter van Zandwijk,Marcel Worring,Zeno Geradts*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的方法，将智能手机和智能手表传感器数据转化为不同身体活动类型的似然比，用于法医调查中的活动识别。


<details>
  <summary>Details</summary>
Motivation: 智能手机和智能手表在日常生活中的普及提供了丰富的用户行为信息，特别是手机内置运动传感器产生的数字痕迹为法医调查人员了解个人身体活动提供了机会。

Method: 开发了基于机器学习的方法，将数字痕迹转化为不同身体活动类型的似然比；使用包含四种不同iPhone型号、标注了19种活动的新数据集NFI_FARED进行评估；扩展方法以同时分析多个活动（或活动组）并创建活动时间线。

Result: 在171个可能的活动配对中，该方法能够为167个配对产生有用的似然比系统；数据集和所有代码已公开以促进进一步研究。

Conclusion: 该方法能够有效利用智能手机传感器数据进行法医调查中的活动识别，为调查的早期和后期阶段提供支持，公开数据集和代码有助于推动该领域研究。

Abstract: Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.

</details>


### [101] [Adaptive Identification and Modeling of Clinical Pathways with Process Mining](https://arxiv.org/abs/2512.03787)
*Francesco Vitale,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 本文提出了一种基于过程挖掘的两阶段临床路径建模方法，通过一致性检查诊断扩展临床路径知识库，能够针对疾病变体或组合创建更具体的模型。


<details>
  <summary>Details</summary>
Motivation: 临床路径是基于标准的患者治疗计划，但手动建模困难且难以反映不同疾病变体或组合的最佳实践。需要自动化方法来扩展临床路径知识库。

Method: 采用两阶段过程挖掘方法：第一阶段收集特定疾病的历史数据构建过程模型；第二阶段将新数据与参考模型进行一致性检查，根据检查结果扩展知识库，为新的疾病变体或组合创建更具体的模型。

Result: 使用Synthea基准数据集（模拟SARS-CoV-2感染及COVID-19并发症）验证方法，结果显示能够以足够精度扩展临床路径知识库，AUC峰值达到95.62%，同时保持67.11%的弧度简单性。

Conclusion: 该方法能够有效扩展临床路径知识库，为不同疾病变体和组合提供更精准的治疗模型，有助于改善医疗质量、减少资源使用并加速患者康复。

Abstract: Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.

</details>


### [102] [EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification](https://arxiv.org/abs/2512.03804)
*Hanhui Deng,Xinglin Li,Jie Luo,Zhanpeng Jin,Di Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于EfficientNet的轻量级ECG分类模型EfficientECG，以及一个跨注意力特征融合模型，用于处理多导联ECG数据并融合多种特征（如性别、年龄），实现了高精度、轻量化的心电图分析。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）是一种快速、无创且信息丰富的诊断信号，但现有ECG模型误诊率高。研究旨在开发深度学习技术，有效管理和分析ECG数据，构建准确快速的诊断模型，减轻医疗工作者负担。

Method: 1. 提出EfficientECG模型：基于EfficientNet的准确轻量级ECG分类模型，能有效处理高频长序列ECG数据及多种导联类型。2. 提出跨注意力特征融合模型：在EfficientECG基础上，用于分析包含多种特征（性别、年龄等）的多导联ECG数据。

Result: 在代表性ECG数据集上的评估验证了该模型在以下方面的优越性：高精度、多特征融合能力和轻量化特性，优于现有最先进方法。

Conclusion: 该研究提出的深度学习方法能够自动提取ECG数据的特征，通过端到端训练构建准确快速的诊断模型，有效降低了ECG分析的误诊率，为医疗工作者提供了实用的辅助工具。

Abstract: Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.

</details>


### [103] [Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA](https://arxiv.org/abs/2512.03805)
*Tai Nguyen,Phong Le,André Biedenkapp,Carola Doerr,Nguyen Dang*

Main category: cs.LG

TL;DR: 本文系统研究了深度强化学习在动态算法配置中的应用，针对(1+(λ,λ))-GA算法在OneMax问题上的种群规模参数控制，揭示了DDQN和PPO存在的可扩展性下降和学习不稳定性问题，并提出了自适应奖励偏移机制等解决方案。


<details>
  <summary>Details</summary>
Motivation: 动态算法配置(DAC)旨在为参数化优化算法寻找高效的控制策略。虽然强化学习(RL)已被用于解决算法配置的优化挑战，但将RL应用于DAC仍面临困难且需要大量领域专业知识。本文旨在通过系统分析深度RL算法在DAC中的表现，识别并解决其核心挑战。

Method: 研究采用系统分析方法，以控制(1+(λ,λ))-GA在OneMax实例上的种群规模参数为案例，深入分析DDQN和PPO算法。针对发现的探索不足和规划视野覆盖问题，提出了自适应奖励偏移机制（利用奖励分布统计增强DDQN探索）和无折扣学习解决方案。同时分析了PPO的超参数依赖性。

Result: 研究发现DDQN和PPO在DAC中存在两个根本挑战：可扩展性下降和学习不稳定性。这些问题主要源于探索不足和规划视野覆盖不足。自适应奖励偏移机制有效解决了探索不足问题，无需实例特定的超参数调优。无折扣学习解决了DDQN的规划视野覆盖问题，而PPO存在基本方差问题。配备自适应奖励偏移策略的DDQN实现了与理论推导策略相当的性能，样本效率大幅提升，比先前DAC方法高出几个数量级。

Conclusion: 深度强化学习在动态算法配置中面临可扩展性和稳定性挑战，但通过针对性的解决方案可以克服。自适应奖励偏移机制和无折扣学习是有效的改进策略。DDQN配合自适应奖励偏移策略在DAC中表现优异，为算法参数控制提供了高效解决方案。

Abstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

</details>


### [104] [Log Probability Tracking of LLM APIs](https://arxiv.org/abs/2512.03816)
*Timothée Chauvin,Erwan Le Merrer,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 提出了一种基于logprobs的廉价LLM API监控方法，仅需单个token输出即可检测微小模型变化，比现有方法敏感1000倍且成本更低


<details>
  <summary>Details</summary>
Motivation: LLM API用户期望模型保持一致性以确保下游应用可靠性和研究可复现性，但现有审计方法成本过高无法定期监控广泛可用的LLM API，导致模型更新在实践中基本未被监控

Method: 利用LLM对数概率(logprobs)的非确定性特性，基于每个token对数概率的平均值应用简单统计测试，仅需请求单个token输出

Result: 该方法能够检测到小至一次微调步骤的模型变化，比现有方法更敏感，同时成本降低1000倍；引入了TinyChange基准来衡量审计方法对小型现实模型变化的敏感性

Conclusion: 基于logprobs的监控方法为LLM API提供了经济高效的连续监控方案，解决了现有审计方法成本过高的问题，确保模型一致性

Abstract: When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.

</details>


### [105] [Transmit Weights, Not Features: Orthogonal-Basis Aided Wireless Point-Cloud Transmission](https://arxiv.org/abs/2512.03819)
*Junlin Chang,Yubo Han,Hnag Yue,John S Thompson,Rongke Liu*

Main category: cs.LG

TL;DR: 提出基于深度联合信源信道编码的语义无线传输框架，通过预测接收端语义正交特征池的组合权重实现紧凑表示和鲁棒重建，在带宽受限场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度传感器的普及，点云获取门槛降低，但高效传输点云数据面临挑战。传统方法在带宽受限时性能下降，需要更紧凑、鲁棒的语义传输方案。

Method: 基于DeepJSCC的语义无线传输框架：1）发送端预测接收端语义正交特征池的组合权重而非原始特征；2）折叠式解码器将2D网格变形为3D点云，保持流形连续性和几何保真度；3）使用Chamfer距离和正交正则化器进行训练。

Result: 在ModelNet40数据集上测试不同信噪比和带宽：1）高带宽时性能与SEPT相当；2）带宽受限时明显优于SEPT；3）PSNR和Chamfer距离均有持续改进；4）消融实验验证正交化和折叠先验的有效性。

Conclusion: 提出的语义无线传输框架通过正交特征池和折叠解码器，在保持几何保真度的同时实现紧凑表示，特别在带宽受限场景下优于现有方法，为点云高效传输提供了有效解决方案。

Abstract: The widespread adoption of depth sensors has substantially lowered the barrier to point-cloud acquisition. This letter proposes a semantic wireless transmission framework for three dimension (3D) point clouds built on Deep Joint Source - Channel Coding (DeepJSCC). Instead of sending raw features, the transmitter predicts combination weights over a receiver-side semantic orthogonal feature pool, enabling compact representations and robust reconstruction. A folding-based decoder deforms a 2D grid into 3D, enforcing manifold continuity while preserving geometric fidelity. Trained with Chamfer Distance (CD) and an orthogonality regularizer, the system is evaluated on ModelNet40 across varying Signal-to-Noise Ratios (SNRs) and bandwidths. Results show performance on par with SEmantic Point cloud Transmission (SEPT) at high bandwidth and clear gains in bandwidth-constrained regimes, with consistent improvements in both Peak Signal-to-Noise Ratio (PSNR) and CD. Ablation experiments confirm the benefits of orthogonalization and the folding prior.

</details>


### [106] [Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models](https://arxiv.org/abs/2512.03882)
*Haidong Kang,Wei Wu,Hanling Wang*

Main category: cs.LG

TL;DR: 本文提出ACraft方法，利用大语言模型自动生成针对少样本类增量学习（FSCIL）的攻击方法，无需人工专家参与，显著超越传统攻击方法效果


<details>
  <summary>Details</summary>
Motivation: FSCIL是持续学习中更具现实性和挑战性的范式，但以往研究主要关注提升FSCIL性能，较少关注其安全问题。现有攻击方法（如PGD、FGSM）要么无法有效攻击基类，要么依赖大量专家知识导致成本高昂，需要专门针对FSCIL的攻击方法

Method: 提出ACraft方法：1）利用大语言模型自动发现针对FSCIL的最优攻击方法，无需人工专家；2）引入基于近端策略优化的强化学习来优化大语言模型与FSCIL之间的推理，通过建立正反馈让大语言模型在下一代生成更好的攻击方法

Result: 在主流基准测试中，ACraft方法显著降低了最先进FSCIL方法的性能，大幅超越人工专家设计的攻击方法，同时保持最低的攻击成本

Conclusion: 本文首次对FSCIL的安全问题进行系统性研究，提出的ACraft方法能够自动生成针对FSCIL的有效攻击，为FSCIL的安全性评估提供了新工具，同时揭示了FSCIL系统在实际应用中的安全脆弱性

Abstract: Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.

</details>


### [107] [Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2512.03899)
*Janis Keck,Lukas Silvester Barth,Fatemeh,Fahimi,Parvaneh Joharinad,Jürgen Jost*

Main category: cs.LG

TL;DR: 该论文为模糊单纯集提供了一个概率框架，将其解释为单纯集上概率测度的边际，从而为UMAP等降维方法提供了理论基础，并能够推导新的嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 模糊单纯集在降维和流形学习中（特别是UMAP）很重要，但其代数拓扑定义缺乏明确的概率解释，与这些领域常用的理论框架脱节。需要建立一个概率框架来统一理解模糊单纯集。

Method: 引入一个框架，将模糊单纯集解释为单纯集上概率测度的边际。具体展示了UMAP的模糊权重源于一个生成模型，该模型在随机尺度上采样Vietoris-Rips滤过，产生成对距离的累积分布函数。框架还连接了模糊单纯集与面偏序集上的概率模型。

Result: 该框架为模糊单纯集提供了统一的概率理论基础，阐明了UMAP在该框架中的角色，澄清了Kullback-Leibler散度与模糊交叉熵的关系，并通过底层单纯集的布尔运算恢复了标准的t-范数和t-余范数。还展示了如何从该框架推导新的嵌入方法，例如使用Čech滤过和三重采样推广UMAP。

Conclusion: 概率视角为模糊单纯集提供了统一的理论基础，阐明了UMAP在该框架中的作用，并使得能够系统性地推导新的降维方法。

Abstract: Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.

</details>


### [108] [Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations](https://arxiv.org/abs/2512.03923)
*Xiang Rao,Yina Liu,Yuxuan Shen*

Main category: cs.LG

TL;DR: 提出了一种离散变量量子-经典物理信息神经网络（DV-QCPINN），用于解决三种典型油藏渗流模型，相比传统PINNs具有更高的参数效率和预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法存在网格依赖误差和计算成本高的问题，而经典物理信息神经网络在参数效率、高维表达和强非线性拟合方面存在瓶颈，需要新的解决方案。

Method: 提出DV-QCPINN，将经典预处理/后处理网络与离散变量量子核心集成，利用量子叠加和纠缠增强高维特征映射，同时嵌入物理约束确保解的一致性。测试了三种量子电路拓扑结构。

Result: QCPINNs比经典PINNs使用更少的参数实现了更高的预测精度。Alternate拓扑在非均质单相流和两相BL方程模拟中表现最佳，Cascade拓扑在对流-扩散-吸附耦合的组分流中表现最优。

Conclusion: 验证了QCPINN在油藏工程应用中的可行性，弥合了量子计算研究与油气工程工业实践之间的差距。

Abstract: Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.

</details>


### [109] [Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization](https://arxiv.org/abs/2512.03928)
*Michele Alessi,Alessio Ansuini,Alex Rodriguez*

Main category: cs.LG

TL;DR: DiVAE是一种轻量级数据驱动正则化方法，通过将VAE对数先验概率与数据估计的对数密度对齐，改进潜在空间与数据空间密度结构的匹配。


<details>
  <summary>Details</summary>
Motivation: 标准VAE将潜在变量匹配到简单先验分布，忽略了数据空间中的密度结构。这导致潜在空间无法充分反映数据的真实分布特征。

Method: DiVAE在ELBO中添加了一个鲁棒的、精度加权的惩罚项，鼓励编码器根据数据空间密度按比例分配后验质量，并在先验可学习时推动先验向高密度区域靠拢。

Result: 在合成数据集上：1) 改进了潜在对数密度与真实分布的匹配；2) 提高了先验覆盖范围；3) 获得了更好的OOD不确定性校准。在MNIST上：改进了先验与外部密度估计的对齐，提供更好的可解释性，并提高了可学习先验的OOD检测性能。

Conclusion: DiVAE是一种计算开销可忽略的轻量级正则化方法，能够有效对齐VAE先验与数据空间密度结构，改善分布对齐、先验覆盖和OOD不确定性校准。

Abstract: We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.

</details>


### [110] [Technical Report on Text Dataset Distillation](https://arxiv.org/abs/2512.03967)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Rosimeire Pereira Costa,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 本文综述了文本数据集蒸馏领域的发展历程，从最初借鉴视觉领域方法到形成独立研究方向，涵盖了基于Transformer的方法、离散文本生成、大规模解码器模型等里程碑，并指出了该领域在基准标准化、文本离散性处理、复杂任务应对和实际应用展示等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 文本数据集蒸馏领域虽然从视觉领域借鉴了初始方法，但由于文本模态的特殊性（如离散性），需要专门的研究方法。目前该领域处于发展阶段，缺乏统一的基准标准，在处理复杂任务和实际应用方面仍有改进空间，因此需要系统性的综述来梳理发展脉络和未来方向。

Method: 本文采用文献综述方法，系统回顾了文本数据集蒸馏领域的历史和最新进展。重点分析了不同的蒸馏策略，包括：1）从视觉领域方法的适应性迁移；2）基于Transformer模型的蒸馏方法；3）离散合成文本生成技术；4）大规模解码器模型（超过10亿参数）的扩展应用。

Result: 通过综述分析，识别了文本数据集蒸馏领域的几个关键里程碑：基于Transformer的方法引入、离散合成文本生成技术发展、以及大规模解码器模型的扩展。同时指出了该领域当前面临的挑战：基准标准化不足、文本离散性处理困难、复杂任务应对能力有限、以及缺乏明确的实际应用案例展示。

Conclusion: 文本数据集蒸馏作为一个新兴研究领域，已经从最初的视觉方法适应发展成为独立的研究分支。虽然取得了显著进展，但仍处于成熟阶段，需要在基准标准化、离散文本处理、复杂任务应对和实际应用展示等方面进一步改进。未来的研究应关注这些挑战，推动该领域向更成熟的方向发展。

Abstract: In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.

</details>


### [111] [Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs](https://arxiv.org/abs/2512.03994)
*Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein*

Main category: cs.LG

TL;DR: 提出一种无需训练的高效方法，将策略违规检测视为分布外检测问题，通过白化技术处理隐藏激活，使用欧几里得范数作为合规分数，在策略基准上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感领域（法律、金融、医疗）的部署，组织需要可靠机制检测内部策略违规，现有安全过滤器和微调方法存在延迟高、可解释性差、无法捕捉细微策略等问题。

Method: 将策略违规检测视为分布外检测问题，采用白化技术对模型隐藏激活进行线性变换（去相关、标准化为零均值和单位方差），在变换空间中使用欧几里得范数作为合规分数，仅需策略文本和少量示例样本。

Result: 在具有挑战性的策略基准测试中，该方法取得了最先进的结果，超越了现有护栏和微调推理模型，为组织提供了实用且统计基础的政策感知监督框架。

Conclusion: 该方法为组织提供了轻量级、易于部署的策略违规检测解决方案，推进了可部署AI治理的广泛目标，代码已开源。

Abstract: Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection

</details>


### [112] [Physics-Embedded Gaussian Process for Traffic State Estimation](https://arxiv.org/abs/2512.04004)
*Yanlin Chen,Kehua Chen,Yinhai Wang*

Main category: cs.LG

TL;DR: 提出了一种新颖的物理嵌入高斯过程（PEGP）框架，用于结合交通流物理模型和数据驱动方法进行交通状态估计，解决了传统方法在稀疏观测下的泛化问题和物理模型的不确定性整合难题。


<details>
  <summary>Details</summary>
Motivation: 交通状态估计在探测车辆渗透率低、观测空间稀疏时面临挑战。纯数据驱动方法缺乏物理解释且稀疏数据下泛化能力差，而物理模型难以整合不确定性和捕捉真实交通复杂性。现有结合方法依赖惩罚调优且缺乏原则性的不确定性校准，对模型误设敏感。

Method: 提出物理嵌入高斯过程（PEGP），通过线性化微分算子的显式应用，设计了基于经典交通流模型（LWR和ARZ）的多输出核函数，将领域知识嵌入到高斯过程框架中。

Result: 在HighD和NGSIM数据集上的实验显示，PEGP相比非物理基线方法有持续改进。PEGP-ARZ在稀疏观测下更可靠，PEGP-LWR在密集观测下误差更低。消融研究表明PEGP-ARZ残差更符合物理规律且产生可校准、可解释的不确定性，而PEGP-LWR残差更正交且产生几乎恒定的方差场。

Conclusion: PEGP框架成功结合了物理先验和不确定性量化，为交通状态估计提供了可靠支持，解决了传统方法在稀疏观测和模型不确定性方面的局限性。

Abstract: Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.

</details>


### [113] [Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics](https://arxiv.org/abs/2512.04006)
*Connall Garrod,Jonathan P. Keating,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 该论文分析了交叉熵损失在多类分类中的优化动力学，首次证明了梯度流在交叉熵损失下会收敛到神经坍缩几何结构，通过构造Lyapunov函数建立了全局收敛性，尽管存在虚假临界点。


<details>
  <summary>Details</summary>
Motivation: 现有理论通常用平方损失替代交叉熵损失或限制在凸模型，无法捕捉交叉熵训练的本质行为。交叉熵和平方损失产生根本不同的动力学，凸线性模型无法捕捉非凸优化的复杂性。

Method: 分析一个具有标准基向量作为输入的两层线性神经网络，这是最简单的非凸扩展模型。使用Hadamard初始化对角化softmax算子，冻结权重矩阵的奇异向量，将动力学完全简化为奇异值。构造显式Lyapunov函数证明全局收敛。

Result: 首次证明了梯度流在交叉熵损失下收敛到神经坍缩几何结构。发现Hadamard初始化对角化softmax算子，冻结奇异向量，将动力学简化为奇异值。尽管存在虚假临界点，但建立了全局收敛性。

Conclusion: 该工作为分析交叉熵训练动力学开辟了新途径，超越了现有简化理论。Hadamard初始化技术为分析更广泛设置中的交叉熵训练动力学提供了方法路径。

Abstract: Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.

</details>


### [114] [Efficient Public Verification of Private ML via Regularization](https://arxiv.org/abs/2512.04008)
*Zoë Ruha Bell,Anvith Thudi,Olive Franzese-McLaughlin,Nicolas Papernot,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 本文提出首个差分隐私算法，其隐私-效用权衡接近最优，且验证成本远低于训练成本，显著降低了大规模数据集上的验证开销。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私算法的验证成本与训练成本相当，数据提供者和公众缺乏有效方法来验证模型是否真正满足差分隐私保证。这限制了差分隐私在实际应用中的可验证性和可信度。

Method: 针对差分隐私随机凸优化问题，通过私有化最小化一系列正则化目标，仅使用标准差分隐私组合边界，实现了接近最优的隐私-效用权衡。

Result: 获得了接近最优的隐私-效用权衡，且验证成本远低于训练成本，这是首个验证成本优于训练成本的差分隐私随机凸优化算法。

Conclusion: 该方法显著降低了差分隐私验证的计算开销，为大规模数据集上的差分隐私验证提供了实用解决方案，增强了差分隐私在实际应用中的可验证性。

Abstract: Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.

</details>


### [115] [Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions](https://arxiv.org/abs/2512.04034)
*Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 该论文从信息论角度解释了为什么在单域数据集上训练的模型在OOD检测中会出现灾难性失败，提出了"域特征坍缩"理论，并通过域过滤方法验证了这一理论。


<details>
  <summary>Details</summary>
Motivation: 解释为什么在单域数据集上训练的最先进OOD检测方法会出现灾难性失败现象，从理论层面理解这一经验观察到的难题。

Method: 1. 从信息论角度提出"域特征坍缩"理论，证明在单域数据上的监督学习会导致I(x_d; z)=0；2. 使用Fano不等式量化实际场景中的部分坍缩；3. 引入Domain Bench基准数据集；4. 通过域过滤方法（使用预训练表示）验证理论。

Result: 1. 理论证明单域训练会导致域特征完全丢弃；2. 实验显示在MNIST上仅达到53% FPR@95的灾难性失败；3. 域过滤方法能有效解决失败模式，保持I(x_d; z)>0；4. 为信息论框架提供了强有力的经验证据。

Conclusion: 该工作解释了OOD检测中的经验难题，揭示了监督学习在窄域中的根本局限性，对迁移学习以及何时微调与冻结预训练模型具有更广泛的意义。

Abstract: Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.

</details>


### [116] [Convergence for Discrete Parameter Updates](https://arxiv.org/abs/2512.04051)
*Paul Wilson,Fabio Zanasi,George Constantinides*

Main category: cs.LG

TL;DR: 提出一种离散更新规则的量化训练方法，避免连续更新的离散化，为具有固有离散结构的模型开辟高效训练新途径


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型需要巨大的计算资源，量化训练通过低比特整数表示训练组件来解决这个问题，但通常依赖于离散化实值更新

Method: 引入离散更新规则方法，避免通过设计来量化连续更新；建立此类离散方案的收敛保证，并提出多项式更新规则作为具体示例

Result: 通过经验评估支持了该方法，为具有固有离散结构的模型开辟了高效训练的新途径

Conclusion: 离散更新规则方法提供了一种替代传统量化训练的新视角，避免了连续更新的离散化，特别适用于具有固有离散结构的模型

Abstract: Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.

</details>


### [117] [Eval Factsheets: A Structured Framework for Documenting AI Evaluations](https://arxiv.org/abs/2512.04062)
*Florian Bordes,Candace Ross,Justine T Kao,Evangelia Spiliopoulou,Adina Williams*

Main category: cs.LG

TL;DR: 提出了Eval Factsheets框架，为AI系统评估提供结构化文档标准，解决评估方法缺乏系统化文档的问题


<details>
  <summary>Details</summary>
Motivation: 当前AI领域基准测试激增，但评估方法缺乏像Datasheets和Model Cards那样的结构化文档框架，导致可复现性、透明度和决策制定方面存在挑战

Method: 开发了Eval Factsheets框架，通过五维分类法（Context、Scope、Structure、Method、Alignment）和基于问卷的方法来系统化记录评估特征

Result: 通过多个基准测试的案例研究证明，Eval Factsheets能够有效捕捉从传统基准到LLM-as-judge方法的各种评估范式，同时保持一致性和可比性

Conclusion: Eval Factsheets应被纳入现有和新发布的评估框架中，以促进更高的透明度和可复现性

Abstract: The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.

</details>


### [118] [Learning Steerable Clarification Policies with Collaborative Self-play](https://arxiv.org/abs/2512.04068)
*Jonathan Berant,Maximillian Chen,Adam Fisch,Reza Aghajani,Fantine Huot,Mirella Lapata,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 训练可调控的AI助手策略，通过自博弈学习在不确定情况下决定何时猜测用户意图、枚举可能意图或提问澄清，根据成本优化奖励


<details>
  <summary>Details</summary>
Motivation: AI助手需要处理不明确或模糊的查询，但现有的不确定性管理策略缺乏对不同上下文（如用户偏好、设备模态）的适应性。例如，在小屏幕或语音环境中枚举多个可能意图会显得繁琐。

Method: 使用自博弈方法训练可调控的策略：两个代理分别模拟用户和AI助手，生成对话场景。模型输入每个澄清问题和生成单词的数值成本，目标是最大化最终奖励（成本惩罚后的准确性）。采用强化自训练（ReST）方法进行训练。

Result: 训练出的策略能够根据提供的成本预测性地改变行为，获得更高的奖励和准确性。该方法还能泛化到训练时未观察到的数值成本。

Conclusion: 通过自博弈和强化自训练，可以开发出可调控的不确定性管理策略，使AI助手能够根据上下文成本因素灵活调整响应方式，提高系统性能。

Abstract: To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [119] [BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents](https://arxiv.org/abs/2512.03413)
*Shu Wang,Yingli Zhou,Yixiang Fang*

Main category: cs.IR

TL;DR: BookRAG：针对具有层次结构的文档（如书籍、手册等）提出的新型检索增强生成方法，通过构建层次化索引和实体关系图来提升问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要针对一般文档，忽略了现实世界中许多文档（如书籍、手册等）具有层次结构的特点，导致在问答任务上性能不佳。需要专门针对层次结构文档的RAG方法。

Method: 1. 构建BookIndex索引结构：从文档中提取层次树（类似目录），用图捕捉实体间复杂关系，并将实体映射到树节点。2. 基于信息觅食理论的代理查询方法：动态分类查询并采用定制化检索工作流。

Result: 在三个广泛采用的基准测试上进行大量实验，BookRAG在检索召回率和问答准确性方面均达到最先进性能，显著优于基线方法，同时保持竞争力效率。

Conclusion: BookRAG是针对层次结构文档的有效RAG方法，通过利用逻辑层次结构和追踪实体关系来查询高度相关信息，显著提升了问答任务的性能。

Abstract: As an effective method to boost the performance of Large Language Models (LLMs) on the question answering (QA) task, Retrieval-Augmented Generation (RAG), which queries highly relevant information from external complex documents, has attracted tremendous attention from both industry and academia. Existing RAG approaches often focus on general documents, and they overlook the fact that many real-world documents (such as books, booklets, handbooks, etc.) have a hierarchical structure, which organizes their content from different granularity levels, leading to poor performance for the QA task. To address these limitations, we introduce BookRAG, a novel RAG approach targeted for documents with a hierarchical structure, which exploits logical hierarchies and traces entity relations to query the highly relevant information. Specifically, we build a novel index structure, called BookIndex, by extracting a hierarchical tree from the document, which serves as the role of its table of contents, using a graph to capture the intricate relationships between entities, and mapping entities to tree nodes. Leveraging the BookIndex, we then propose an agent-based query method inspired by the Information Foraging Theory, which dynamically classifies queries and employs a tailored retrieval workflow. Extensive experiments on three widely adopted benchmarks demonstrate that BookRAG achieves state-of-the-art performance, significantly outperforming baselines in both retrieval recall and QA accuracy while maintaining competitive efficiency.

</details>


### [120] [LLM as Explainable Re-Ranker for Recommendation System](https://arxiv.org/abs/2512.03439)
*Yaqi Wang,Haojia Sun,Shuting Zhang*

Main category: cs.IR

TL;DR: 本文提出使用大语言模型作为可解释的重新排序器，将传统推荐模型与LLMs结合，以提高推荐准确性和可解释性，解决了传统推荐系统缺乏解释性和流行度偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统缺乏可解释性且存在流行度偏差等问题，而先前研究表明LLMs作为独立预测器无法达到传统模型的准确性。因此需要一种结合两者优势的混合方法。

Method: 使用LLM作为可解释的重新排序器，构建数据集训练重新排序器LLM，采用两阶段训练过程，评估生成数据集与人类期望的对齐度。

Result: 模型显著提高了NDCG排名指标，重新排序器在排名准确性和可解释性方面均优于零样本基线。

Conclusion: 将传统推荐模型与LLMs结合能够解决现有系统的局限性，为更可解释和公平的推荐框架铺平道路。

Abstract: The application of large language models (LLMs) in recommendation systems has recently gained traction. Traditional recommendation systems often lack explainability and suffer from issues such as popularity bias. Previous research has also indicated that LLMs, when used as standalone predictors, fail to achieve accuracy comparable to traditional models. To address these challenges, we propose to use LLM as an explainable re-ranker, a hybrid approach that combines traditional recommendation models with LLMs to enhance both accuracy and interpretability. We constructed a dataset to train the re-ranker LLM and evaluated the alignment between the generated dataset and human expectations. Leveraging a two-stage training process, our model significantly improved NDCG, a key ranking metric. Moreover, the re-ranker outperformed a zero-shot baseline in ranking accuracy and interpretability. These results highlight the potential of integrating traditional recommendation models with LLMs to address limitations in existing systems and pave the way for more explainable and fair recommendation frameworks.

</details>


### [121] [M3DR: Towards Universal Multilingual Multimodal Document Retrieval](https://arxiv.org/abs/2512.03514)
*Adithya S Kolavi,Vyoman Jain*

Main category: cs.IR

TL;DR: M3DR是一个多语言多模态文档检索框架，通过合成多语言文档数据和对比训练，在22种不同语言上实现了跨语言和跨模态对齐，在跨语言检索任务上取得了约150%的相对性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态文档检索系统主要针对英语设计，在多语言环境中的效果有限。需要开发一个能够跨越不同语言和文化背景的框架，以支持多样化的语言和文字变体。

Method: M3DR框架利用合成的多语言文档数据，采用对比训练方法学习文本和文档图像的统一表示。该框架支持不同的视觉-语言架构和模型规模，并适用于单密集向量和ColBERT风格的标记级多向量检索范式。

Result: 在22种类型多样的语言上验证了模型的有效性，展示了跨语言和文字变体的稳定性能和适应性。NetraEmbed和ColNetraEmbed模型在跨语言检索任务上实现了约150%的相对性能提升，达到了最先进的性能水平。

Conclusion: M3DR框架成功解决了多语言多模态文档检索的挑战，通过统一的表示学习实现了有效的跨语言和跨模态对齐，为真实世界的多语言场景提供了强大的检索能力。

Abstract: Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.

</details>


### [122] [Algorithms for Boolean Matrix Factorization using Integer Programming and Heuristics](https://arxiv.org/abs/2512.03807)
*Christos Kolomvakis,Thomas Bobille,Arnaud Vandaele,Nicolas Gillis*

Main category: cs.IR

TL;DR: 本文提出布尔矩阵分解(BMF)的新算法，包括交替优化整数规划方法、多运行结果优化选择策略、可扩展的启发式算法，以及高效的C++数据结构，在多个真实数据集上展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 布尔矩阵分解相比基于标准算术的分解具有更好的可解释性和更低的近似误差，但现有方法在可扩展性方面存在限制，需要更高效的算法来处理大规模数据集。

Method: 1) 提出基于交替优化(AO)和整数规划(IP)的BMF算法；2) 设计从多次运行中选择最优秩一因子子集的方法；3) 引入贪心和局部搜索启发式算法解决IP方法的可扩展性问题；4) 构建高效的C++布尔向量和矩阵数据结构。

Result: 新方法在多个真实数据集上（包括有缺失数据和无缺失数据的情况）表现出色，在主题建模和图像处理应用中超越了现有技术水平，新数据结构显著提升了计算效率。

Conclusion: 本文提出的BMF算法框架结合了精确优化和启发式方法，通过高效的数据结构实现大规模数据处理，为布尔矩阵分解提供了全面且可扩展的解决方案。

Abstract: Boolean matrix factorization (BMF) approximates a given binary input matrix as the product of two smaller binary factors. Unlike binary matrix factorization based on standard arithmetic, BMF employs the Boolean OR and AND operations for the matrix product, which improves interpretability and reduces the approximation error. It is also used in role mining and computer vision. In this paper, we first propose algorithms for BMF that perform alternating optimization (AO) of the factor matrices, where each subproblem is solved via integer programming (IP). We then design different approaches to further enhance AO-based algorithms by selecting an optimal subset of rank-one factors from multiple runs. To address the scalability limits of IP-based methods, we introduce new greedy and local-search heuristics. We also construct a new C++ data structure for Boolean vectors and matrices that is significantly faster than existing ones and is of independent interest, allowing our heuristics to scale to large datasets. We illustrate the performance of all our proposed methods and compare them with the state of the art on various real datasets, both with and without missing data, including applications in topic modeling and imaging.

</details>


### [123] [Learning to Comparison-Shop](https://arxiv.org/abs/2512.04009)
*Jie Tang,Daochen Zha,Xin Liu,Huiji Gao,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的Learning-to-Comparison-Shop (LTCS) 排序系统，专门针对Airbnb等在线市场的用户比较购物行为进行建模，相比传统孤立评估商品的排序方法，显著提升了业务指标和用户体验。


<details>
  <summary>Details</summary>
Motivation: 在线市场（如Airbnb）用户经常进行对比购物，但主流电商搜索引擎与用户比较需求之间存在显著脱节。传统排序模型通常孤立评估商品，忽略了用户在搜索结果页面对多个商品进行比较的上下文环境。

Method: 提出了Learning-to-Comparison-Shop (LTCS) 排序架构，该架构明确建模并学习用户的比较购物行为，通过考虑列表上下文来优化排序结果。

Result: 通过大量离线和在线实验，LTCS在关键业务指标上取得了统计显著的提升：NDCG提高了1.7%，预订转化率在A/B测试中提升了0.6%，同时改善了用户体验。与最先进方法相比，LTCS显著优于其他模型。

Conclusion: LTCS系统成功解决了传统排序模型忽视用户比较购物行为的问题，通过建模列表上下文显著提升了搜索效果和业务指标，为在线市场的搜索排序提供了更有效的解决方案。

Abstract: In online marketplaces like Airbnb, users frequently engage in comparison shopping before making purchase decisions. Despite the prevalence of this behavior, a significant disconnect persists between mainstream e-commerce search engines and users' comparison needs. Traditional ranking models often evaluate items in isolation, disregarding the context in which users compare multiple items on a search results page. While recent advances in deep learning have sought to improve ranking accuracy, diversity, and fairness by encoding listwise context, the challenge of aligning search rankings with user comparison shopping behavior remains inadequately addressed. In this paper, we propose a novel ranking architecture - Learning-to-Comparison-Shop (LTCS) System - that explicitly models and learns users' comparison shopping behaviors. Through extensive offline and online experiments, we demonstrate that our approach yields statistically significant gains in key business metrics - improving NDCG by 1.7% and boosting booking conversion rate by 0.6% in A/B testing - while also enhancing user experience. We also compare our model against state-of-the-art approaches and demonstrate that LTCS significantly outperforms them.

</details>
