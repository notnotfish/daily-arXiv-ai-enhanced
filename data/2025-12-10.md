<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 88]
- [cs.AI](#cs.AI) [Total: 37]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction](https://arxiv.org/abs/2512.07846)
*Guoyao Li,Ran He,Shusen Jing,Kayhan Behdin,Yubo Wang,Sundara Raman Ramachandran,Chanh Nguyen,Jian Sheng,Xiaojing Ma,Chuanrui Zhu,Sriram Vasudevan,Muchen Wu,Sayan Ghosh,Lin Su,Qingquan Song,Xiaoqing Wang,Zhipeng Wang,Qing Lan,Yanning Chen,Jingwei Wu,Luke Simon,Wenjing Zhang,Qi Guo,Fedor Borisyuk*

Main category: cs.IR

TL;DR: MixLM是一种新型LLM排序框架，通过将物品描述编码为少量嵌入令牌来减少输入上下文长度，在保持相关性的同时显著提升系统吞吐量


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语义理解方面表现出色，但在工业级延迟和吞吐量要求下计算开销过高。交叉编码器排序系统通常需要处理长上下文预填充工作负载，因为模型需要同时处理用户、查询和物品信息

Method: 提出MixLM框架，使用混合交互（文本和嵌入令牌的混合）表示输入。将所有物品编码为少量嵌入令牌并存储在近线缓存中，在线推理时使用编码后的物品描述，将物品长度从数千文本令牌减少到几个嵌入令牌

Result: 在相同延迟预算下，MixLM将吞吐量提高了10.0倍，同时保持相关性指标。在LinkedIn搜索应用中的在线A/B测试显示，实现了0.47%的日活跃用户显著增长

Conclusion: MixLM通过减少输入上下文长度显著提升了LLM排序系统的效率，使得基于LLM的搜索能够全流量部署，在实际应用中取得了显著的用户参与度提升

Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.

</details>


### [2] [Detecting Privileged Documents by Ranking Connected Network Entities](https://arxiv.org/abs/2512.08073)
*Jianping Zhang,Han Qin,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 提出基于邮件头元数据构建人际关系网络的方法，通过分析实体与律师的互动频率来识别特权文档


<details>
  <summary>Details</summary>
Motivation: 传统特权文档识别方法效率有限，需要更智能的方法来从大量邮件通信中准确识别涉及法律专业人员的特权通信

Method: 基于邮件头元数据构建人际关系网络，将实体分类为法律专业人士和非法律专业人士，通过分析实体与律师的互动频率计算特权可能性得分

Result: 实验结果表明该方法能有效对法律实体进行排序，提高特权文档检测的准确性

Conclusion: 基于链接分析的方法通过量化实体与律师的互动关系，为特权文档识别提供了有效的技术方案

Abstract: This paper presents a link analysis approach for identifying privileged documents by constructing a network of human entities derived from email header metadata. Entities are classified as either counsel or non-counsel based on a predefined list of known legal professionals. The core assumption is that individuals with frequent interactions with lawyers are more likely to participate in privileged communications. To quantify this likelihood, an algorithm assigns a score to each entity within the network. By utilizing both entity scores and the strength of their connections, the method enhances the identification of privileged documents. Experimental results demonstrate the algorithm's effectiveness in ranking legal entities for privileged document detection.

</details>


### [3] [A Comparative Study of Retrieval Methods in Azure AI Search](https://arxiv.org/abs/2512.08078)
*Qiang Mao,Han Qin,Robert Neary,Charles Wang,Fusheng Wei,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本文评估了微软Azure RAG框架中的不同检索策略在电子取证早期案件评估中的应用效果，比较了关键词、语义、向量、混合和混合语义检索方法的性能。


<details>
  <summary>Details</summary>
Motivation: 律师们希望超越传统的关键词和语义搜索，利用大语言模型在文档审查中通过自然语言提问获得准确简洁的答案，以提高电子取证早期案件评估的效率。

Method: 研究在微软Azure的检索增强生成（RAG）框架内，比较了Azure AI Search的五种检索方法：关键词检索、语义检索、向量检索、混合检索和混合语义检索，评估它们在早期案件评估中的表现。

Result: 研究展示了每种方法生成AI响应的准确性、相关性和一致性，为法律从业者提供了不同RAG配置的性能数据。

Conclusion: 法律从业者可以利用本研究的结果来改进未来RAG配置的选择，从而在电子取证的早期案件评估中更有效地利用大语言模型技术。

Abstract: Increasingly, attorneys are interested in moving beyond keyword and semantic search to improve the efficiency of how they find key information during a document review task. Large language models (LLMs) are now seen as tools that attorneys can use to ask natural language questions of their data during document review to receive accurate and concise answers. This study evaluates retrieval strategies within Microsoft Azure's Retrieval-Augmented Generation (RAG) framework to identify effective approaches for Early Case Assessment (ECA) in eDiscovery. During ECA, legal teams analyze data at the outset of a matter to gain a general understanding of the data and attempt to determine key facts and risks before beginning full-scale review. In this paper, we compare the performance of Azure AI Search's keyword, semantic, vector, hybrid, and hybrid-semantic retrieval methods. We then present the accuracy, relevance, and consistency of each method's AI-generated responses. Legal practitioners can use the results of this study to enhance how they select RAG configurations in the future.

</details>


### [4] [Leveraging Machine Learning and Large Language Models for Automated Image Clustering and Description in Legal Discovery](https://arxiv.org/abs/2512.08079)
*Qiang Mao,Fusheng Wei,Robert Neary,Charles Wang,Han Qin,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 本文系统研究了自动化图像聚类描述生成方法，通过集成图像聚类、图像描述和大型语言模型，评估了不同采样策略、提示技术和描述生成方法的效果，为法律发现等领域的图像管理提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 数字图像创建和保留的快速增长给法律发现、数字存档和内容管理带来了巨大挑战。企业和法律团队需要在严格的时间压力下组织、分析和从大量图像集合中提取有意义的见解，手动审查既不切实际又成本高昂，因此迫切需要能够高效组织和描述大规模图像数据集的自动化方法。

Method: 采用K-means聚类将图像分为20个视觉连贯的聚类，使用Azure AI Vision API生成基础描述。评估了三个关键维度：(1)图像采样策略：比较随机、基于质心、分层、混合和基于密度的采样与使用所有聚类图像；(2)提示技术：对比标准提示与思维链提示；(3)描述生成方法：比较基于LLM的生成与传统TF-IDF和基于模板的方法。使用语义相似性和覆盖度指标评估描述质量。

Result: 结果显示，每个聚类使用20张图像的战略采样与穷尽包含方法表现相当，同时显著降低了计算成本，只有分层采样显示出适度的性能下降。基于LLM的方法始终优于TF-IDF基线，并且对于此任务，标准提示优于思维链提示。

Conclusion: 这些发现为部署可扩展、准确的聚类描述系统提供了实用指导，支持法律发现和其他需要自动化组织大型图像集合的领域的高容量工作流程。战略采样与LLM结合的方法能够在保持质量的同时显著提高效率。

Abstract: The rapid increase in digital image creation and retention presents substantial challenges during legal discovery, digital archive, and content management. Corporations and legal teams must organize, analyze, and extract meaningful insights from large image collections under strict time pressures, making manual review impractical and costly. These demands have intensified interest in automated methods that can efficiently organize and describe large-scale image datasets. This paper presents a systematic investigation of automated cluster description generation through the integration of image clustering, image captioning, and large language models (LLMs). We apply K-means clustering to group images into 20 visually coherent clusters and generate base captions using the Azure AI Vision API. We then evaluate three critical dimensions of the cluster description process: (1) image sampling strategies, comparing random, centroid-based, stratified, hybrid, and density-based sampling against using all cluster images; (2) prompting techniques, contrasting standard prompting with chain-of-thought prompting; and (3) description generation methods, comparing LLM-based generation with traditional TF-IDF and template-based approaches. We assess description quality using semantic similarity and coverage metrics. Results show that strategic sampling with 20 images per cluster performs comparably to exhaustive inclusion while significantly reducing computational cost, with only stratified sampling showing modest degradation. LLM-based methods consistently outperform TF-IDF baselines, and standard prompts outperform chain-of-thought prompts for this task. These findings provide practical guidance for deploying scalable, accurate cluster description systems that support high-volume workflows in legal discovery and other domains requiring automated organization of large image collections.

</details>


### [5] [Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters](https://arxiv.org/abs/2512.08083)
*Keith Huffman,Jianping Zhang,Nathaniel Huber-Fliflet,Fusheng Wei,Peter Gronvall*

Main category: cs.IR

TL;DR: 本文通过实证研究探讨了随机性在基于LLM的法律特权文件检测分类中的作用，发现LLM能有效识别特权文件，随机性控制参数对分类性能影响有限，但利用随机性的方法可显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 在法律事务中，文本分类模型常用于筛选大型数据集以查找符合特定标准（如法律特权通信和律师指导文件）的文档。虽然大语言模型在此领域表现出色，但随机性对其分类输出的影响尚未得到充分研究，特别是在法律特权文件检测这一关键应用场景中。

Method: 本研究采用实证方法，从四个维度分析随机性在LLM分类中的作用：(1) LLM识别法律特权文件的有效性；(2) 随机性控制参数对分类输出的影响；(3) 随机性对整体分类性能的影响；(4) 利用随机性提升准确性的方法论。研究聚焦于律师-客户特权文件检测这一具体应用。

Result: 实验结果显示：1) LLM能有效识别特权文件；2) 随机性控制参数对分类性能影响有限；3) 开发的利用随机性的方法能显著提升准确性。该方法还能增强企业对LLM输出的信心，特别是在制裁合规流程中。

Conclusion: LLM在法律特权文件检测中表现优异，随机性控制参数对分类性能影响不大，但通过特定方法利用随机性可显著提升准确性。这一发现对组织在合规工作流程中采用LLM具有重要意义，有助于减少输出变异性，建立内部和监管机构对LLM衍生制裁筛查决策的信心。

Abstract: In legal matters, text classification models are most often used to filter through large datasets in search of documents that meet certain pre-selected criteria like relevance to a certain subject matter, such as legally privileged communications and attorney-directed documents. In this context, large language models have demonstrated strong performance. This paper presents an empirical study investigating the role of randomness in LLM-based classification for attorney-client privileged document detection, focusing on four key dimensions: (1) the effectiveness of LLMs in identifying legally privileged documents, (2) the influence of randomness control parameters on classification outputs, (3) their impact on overall classification performance, and (4) a methodology for leveraging randomness to enhance accuracy. Experimental results showed that LLMs can identify privileged documents effectively, randomness control parameters have minimal impact on classification performance, and importantly, our developed methodology for leveraging randomness can have a significant impact on improving accuracy. Notably, this methodology that leverages randomness could also enhance a corporation's confidence in an LLM's output when incorporated into its sanctions-compliance processes. As organizations increasingly rely on LLMs to augment compliance workflows, reducing output variability helps build internal and regulatory confidence in LLM-derived sanctions-screening decisions.

</details>


### [6] [Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring](https://arxiv.org/abs/2512.08398)
*Jiin Park,Hyuna Jeon,Yoonseo Lee,Jisu Hong,Misuk Kim*

Main category: cs.IR

TL;DR: 本文提出了一种基于本体的知识图谱构建方法，专门针对工业标准文档中复杂的表格、条件约束和数值计算，通过分层语义结构和LLM三元组抽取，实现了优于现有KG-RAG方法的性能。


<details>
  <summary>Details</summary>
Motivation: 工业标准文档包含大量技术信息和复杂规则，采用高度结构化的格式（表格、适用范围、约束、例外、数值计算），使得知识图谱构建特别具有挑战性。传统方法难以有效捕捉这些文档的层次结构和逻辑关系。

Method: 1) 将文档组织成分层语义结构；2) 将句子和表格分解为基于条件和数值规则的原子命题；3) 通过LLM三元组抽取将其整合到本体知识图谱中；4) 构建规则、表格、多跳QA和有毒条款检测数据集；5) 实现本体感知的KG-RAG框架进行对比评估。

Result: 实验结果表明，该方法在所有QA类型上都比现有KG-RAG方法取得了显著性能提升。证明了即使在条件、约束和适用范围交织的工业文档中，也能实现可靠且可扩展的知识表示。

Conclusion: 该方法能够有效捕捉文档的层次和逻辑结构，准确表示传统方法无法反映的领域特定语义，为未来领域特定RAG开发和智能文档管理做出了贡献。

Abstract: Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.

</details>


### [7] [VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation](https://arxiv.org/abs/2512.08702)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Zitong Wan,Hewei Wang,Weijie Liu,Yijie Li,Edith C. H. Ngai*

Main category: cs.IR

TL;DR: VI-MMRec是一个模型无关、无需训练成本的框架，通过基于模态特征相似性的虚拟用户-物品交互来增强稀疏的多模态推荐数据，提高现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐模型受到数据稀疏问题的限制，因为用户通常只与少量物品交互，导致模型将未观察到的物品任意视为负样本，影响了推荐效果。

Method: 提出两种策略：1) Overlay策略独立聚合模态特定相似性以保留模态特定的用户偏好；2) Synergistic策略整体融合跨模态相似性以捕捉互补的用户偏好。设计了基于统计信息的权重分配机制，根据数据集特定的模态相关性自适应地为虚拟交互分配权重。

Result: 在六个真实世界数据集上使用七个最先进的多模态推荐模型进行的综合实验验证了VI-MMRec的有效性。该框架能够无缝集成到现有模型中，无需修改核心架构，且训练时无额外开销。

Conclusion: VI-MMRec是一个即插即用的框架，通过相似性感知的虚拟用户-物品交互来丰富稀疏的交互数据，显著提升了多模态推荐模型的性能，且具有实际部署的显著优势。

Abstract: Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver是一个自适应并行推理框架，在保持与顺序推理模型相当精度的同时，显著降低推理延迟，通过并行推理线程提高LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂任务上的顺序解码导致显著延迟，现有并行推理方法要么局限于监督行为克隆，要么相比顺序CoT基线精度显著下降，且需要定制化推理引擎，部署复杂。

Method: 1) 两阶段并行轨迹生成器产生大规模高质量带并行标注的CoT数据用于监督微调；2) 基于trie的训练-推理协同设计，可在任何现成自回归推理引擎上实现并行推理，无需修改位置嵌入或KV缓存；3) 并行感知强化学习框架，教导模型在精度和有效并行化之间取得平衡。

Result: 在六个具有挑战性的数学推理基准测试中，基于Qwen3-8B训练的ThreadWeaver实现了与前沿顺序推理模型相当的精度（平均71.9%，AIME24上79.9%），同时提供高达1.53倍的平均token延迟加速，建立了精度与效率之间的新帕累托前沿。

Conclusion: ThreadWeaver框架成功实现了自适应并行推理，在保持高精度的同时显著提升推理效率，且无需修改现有推理引擎，具有较好的部署便利性。

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [9] [Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning](https://arxiv.org/abs/2512.07844)
*Jinping Wang,Zhiqiang Gao,Zhiwu Xie*

Main category: cs.LG

TL;DR: 该论文针对长尾分布下神经坍缩现象缺失的问题，提出了特征与分类器权重空间对齐策略，显著提升了现有长尾方法的性能。


<details>
  <summary>Details</summary>
Motivation: 在长尾分布场景中，样本不平衡阻碍了神经坍缩现象的出现，导致泛化性能下降。现有方法主要关注恢复ETF几何结构，但忽视了特征空间与分类器权重空间之间的严重不对齐问题。

Method: 通过最优误差指数分析从理论上量化不对齐的危害，并提出了三种显式对齐策略，这些策略可以即插即用地集成到现有长尾方法中，无需改变网络架构。

Result: 在CIFAR-10-LT、CIFAR-100-LT和ImageNet-LT数据集上的大量实验表明，所提方法能够持续提升现有基线方法的性能，并达到了最先进的性能水平。

Conclusion: 特征与分类器权重空间的对齐对于长尾分布下的模型性能至关重要，提出的对齐策略为解决长尾学习中的神经坍缩缺失问题提供了有效解决方案。

Abstract: Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.

</details>


### [10] [CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics](https://arxiv.org/abs/2512.07847)
*Mohamed Elrefaie,Dule Shu,Matt Klenk,Faez Ahmed*

Main category: cs.LG

TL;DR: CarBench是首个专注于大规模3D汽车空气动力学仿真的基准测试，在最大的公共汽车空气动力学数据集DrivAerNet++（包含8000多个高保真仿真）上评估了11种先进模型，涵盖神经算子、几何深度学习、基于Transformer的求解器和隐式场网络等架构。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模计算流体动力学（CFD）数据集为机器学习在空气动力学和工程设计中的应用提供了新机会，但工程设计中大规模数值仿真领域缺乏标准化基准。现有计算机视觉和自然语言处理领域的基准测试推动了算法创新，但工程仿真领域尚未建立类似的标准评估体系。

Method: 在DrivAerNet++数据集（超过8000个高保真汽车仿真）上评估11种架构，包括：神经算子方法（如傅里叶神经算子）、几何深度学习（PointNet、RegDGCNN、PointMAE、PointTransformer）、基于Transformer的神经求解器（Transolver、Transolver++、AB-UPT）和隐式场网络（TripNet）。除了标准插值任务外，还进行了跨类别实验，评估在单一汽车原型上训练的Transformer求解器在未见类别上的表现。

Result: 分析涵盖了预测准确性、物理一致性、计算效率和统计不确定性。研究开源了基准框架，包括训练流程、基于自助重采样的不确定性估计例程和预训练模型权重，为从高保真CFD仿真中进行大规模学习建立了首个可复现的基础。

Conclusion: CarBench填补了工程设计中大规模数值仿真标准化基准的空白，为数据驱动工程领域的进展提供了首个全面的评估框架，通过开源基准框架加速了该领域的研究进展。

Abstract: Benchmarking has been the cornerstone of progress in computer vision, natural language processing, and the broader deep learning domain, driving algorithmic innovation through standardized datasets and reproducible evaluation protocols. The growing availability of large-scale Computational Fluid Dynamics (CFD) datasets has opened new opportunities for applying machine learning to aerodynamic and engineering design. Yet, despite this progress, there exists no standardized benchmark for large-scale numerical simulations in engineering design. In this work, we introduce CarBench, the first comprehensive benchmark dedicated to large-scale 3D car aerodynamics, performing a large-scale evaluation of state-of-the-art models on DrivAerNet++, the largest public dataset for automotive aerodynamics, containing over 8,000 high-fidelity car simulations. We assess eleven architectures spanning neural operator methods (e.g., Fourier Neural Operator), geometric deep learning (PointNet, RegDGCNN, PointMAE, PointTransformer), transformer-based neural solvers (Transolver, Transolver++, AB-UPT), and implicit field networks (TripNet). Beyond standard interpolation tasks, we perform cross-category experiments in which transformer-based solvers trained on a single car archetype are evaluated on unseen categories. Our analysis covers predictive accuracy, physical consistency, computational efficiency, and statistical uncertainty. To accelerate progress in data-driven engineering, we open-source the benchmark framework, including training pipelines, uncertainty estimation routines based on bootstrap resampling, and pretrained model weights, establishing the first reproducible foundation for large-scale learning from high-fidelity CFD simulations, available at https://github.com/Mohamedelrefaie/CarBench.

</details>


### [11] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash是一个用于纽约市机动车碰撞伤害严重程度预测的资源高效、可解释的小型模型管道，使用树集成模型在结构化数据上优于小型语言模型。


<details>
  <summary>Details</summary>
Motivation: 纽约市每年发生超过十万起机动车碰撞事故，造成严重的伤害和公共卫生负担，需要高效可解释的预测模型来支持城市规模的事故分析。

Method: 整合三个关联表（数千万条记录），构建统一特征模式的分区存储，训练紧凑的树集成模型（随机森林和XGBoost）并与小型语言模型（SLMs）在文本摘要上进行比较。

Result: XGBoost和随机森林在时间保留测试集上的准确率分别为0.7828和0.7794，明显优于SLMs（0.594和0.496）；类别不平衡分析显示简单类别加权可提高致命事故召回率，SHAP归因分析突出了人类脆弱性因素、时间和位置是严重程度的主要驱动因素。

Conclusion: 可解释的小型模型集成仍然是城市规模伤害分析的强基线，而将表格预测器与SLM生成的叙述相结合的混合管道可以在不牺牲可扩展性的情况下改善沟通效果。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [12] [SABER: Small Actions, Big Errors - Safeguarding Mutating Steps in LLM Agents](https://arxiv.org/abs/2512.07850)
*Alejandro Cuadron,Pengfei Yu,Yang Liu,Arpit Gupta*

Main category: cs.LG

TL;DR: 该研究发现LLM智能体在长时程工具使用任务中，突变性操作（改变环境状态）的偏差对成功率影响最大，并提出了一种模型无关的测试时保护机制CM，在多个基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM智能体快速发展，但在长时程工具使用任务上的性能仍然脆弱。为了理解这种脆弱性，研究探索了不同操作类型对任务失败的影响差异，特别是突变性操作与非突变性操作的区别。

Method: 1. 在τ-Bench（航空/零售）和SWE-Bench Verified上分析执行轨迹，将操作分解为突变性（改变环境状态）和非突变性步骤；2. 形式化"决定性偏差"概念（最早导致成功转为失败的操作级分歧）；3. 使用逻辑回归分析偏差影响；4. 提出CM保护机制：突变门控验证、针对性反思、基于块的情境清理。

Result: 1. 每个突变性操作偏差使成功率降低：航空任务92%，零售任务96%（SoTA模型）；2. 非突变性操作偏差影响很小；3. 错误随上下文长度增加而增长；4. CM机制显著提升性能：Qwen3-Thinking相对提升28%（航空）、11%（零售）、7%（SWE-Bench）；Claude提升9%/7%；5. 发现τ-Bench存在天花板效应，发布τ-Bench Verified修复基准问题。

Conclusion: 研究强调操作级分析、针对性保护机制和可靠评估是构建鲁棒多轮智能体的先决条件。突变性操作偏差是任务失败的主要因素，而提出的CM机制能有效提升智能体在长时程工具使用任务上的鲁棒性。

Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $τ$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $τ$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $τ$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.

</details>


### [13] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 提出一个预测多模态模型GPU内存使用量的框架，通过分解模型架构和分析训练行为来准确预测峰值内存使用，平均MAPE达到8.7%


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统中深度学习模型的规模和复杂性增加，GPU内存需求经常超过可用容量，导致内存溢出错误。现有研究仅关注单模态架构，无法推广到多模态模型，而多模态模型在智能体AI系统中很常见。

Method: 提出一个框架，通过分析多模态模型的架构和训练行为来预测峰值GPU内存使用。具体方法是将多模态模型分解为组成层，并应用因子化方法来估计每层的内存使用。

Result: 评估显示该框架实现了约8.7%的平均MAPE（平均绝对百分比误差），具有较高的预测准确性。

Conclusion: 该框架能够有效预测多模态模型的GPU内存使用，解决了现有方法无法处理多模态架构的局限性，有助于防止内存溢出错误和节省计算资源。

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [14] [HSTMixer: A Hierarchical MLP-Mixer for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2512.07854)
*Yongyao Wang,Jingyuan Wang,Xie Yu,Jiahao Ji,Chao Li*

Main category: cs.LG

TL;DR: 提出HSTMixer框架，使用全MLP架构进行高效的大规模交通预测，通过分层时空混合块和多分辨率特征提取，以及自适应区域混合器动态捕捉不同区域的时空模式。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型在大规模场景下存在二次计算复杂度问题，难以应用于真实世界的大规模交通网络，需要更高效且有效的解决方案。

Method: 提出分层时空混合器(HSTMixer)框架，采用全MLP架构，包含分层时空混合块进行多分辨率特征提取（自底向上聚合和自顶向下传播），以及自适应区域混合器基于区域语义生成变换矩阵，动态捕捉不同区域的时空演化模式。

Result: 在四个大规模真实世界数据集上的实验表明，该方法不仅达到了最先进的性能，而且具有竞争力的计算效率。

Conclusion: HSTMixer框架通过全MLP架构和分层时空混合机制，为大规模交通预测提供了高效且有效的解决方案，能够更好地处理真实世界交通网络的复杂性。

Abstract: Traffic forecasting task is significant to modern urban management. Recently, there is growing attention on large-scale forecasting, as it better reflects the complexity of real-world traffic networks. However, existing models often exhibit quadratic computational complexity, making them impractical for large-scale real-world scenarios. In this paper, we propose a novel framework, Hierarchical Spatio-Temporal Mixer (HSTMixer), which leverages an all-MLP architecture for efficient and effective large-scale traffic forecasting. HSTMixer employs a hierarchical spatiotemporal mixing block to extract multi-resolution features through bottom-up aggregation and top-down propagation. Furthermore, an adaptive region mixer generates transformation matrices based on regional semantics, enabling our model to dynamically capture evolving spatiotemporal patterns for different regions. Extensive experiments conducted on four large-scale real-world datasets demonstrate that the proposed method not only achieves state-of-the-art performance but also exhibits competitive computational efficiency.

</details>


### [15] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: LAPA是一种针对Transformer模型的跨阶段稀疏加速算法-架构协同设计，通过log域注意力预测、非对称前导一计算、混合精度多轮移位累加等创新技术，显著提升能效比。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在不同阶段的计算瓶颈会随输入序列长度变化而动态变化，需要跨阶段稀疏加速策略。现有稀疏Transformer方法多为单阶段设计，其稀疏预测机制在多阶段应用时会产生显著的功耗开销。

Method: 提出LAPA算法-架构协同设计：1）非对称前导一计算（ALOC）方案消除昂贵乘法运算；2）混合精度多轮移位累加（MRSA）机制降低累加开销；3）数据特征依赖滤波器（DDF）策略与MRSA协同工作；4）设计专用加速器将理论改进转化为实际硬件提升。

Result: 实验结果显示，LAPA相比当前最先进的工作Spatten、Sanger和FACT，能效比分别提升3.52倍、3.24倍和2.79倍。

Conclusion: LAPA通过创新的算法-架构协同设计，有效解决了Transformer模型跨阶段稀疏加速问题，显著提升了能效比，为Transformer硬件加速提供了有效解决方案。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [16] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: 提出CLDD模型，通过图神经网络利用疾病关联和患者相似性进行大规模疾病检测，减少医疗检测依赖


<details>
  <summary>Details</summary>
Motivation: 疾病检测需要大量医疗测试且成本高昂，难以对每个患者进行所有可能的测试来诊断数千种疾病

Method: 提出CLDD图深度学习模型，将疾病检测构建为协作学习任务，自适应利用疾病间关联和患者间相似性，整合患者-疾病交互和人口统计学特征

Result: 在MIMIC-IV数据集（61,191患者，2,000疾病）上，CLDD在多个指标上优于基线方法，召回率提升6.33%，精确率提升7.63%，并能成功恢复掩蔽疾病

Conclusion: CLDD通过降低诊断成本和提高可及性，有望用于大规模疾病筛查和社会健康保障

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [17] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: SA^2GFM是一个鲁棒的图基础模型框架，通过结构感知语义增强提升领域自适应表示能力，在节点和图分类任务中表现出优越的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前图基础模型在各种任务中取得了显著进展，但其对领域噪声、结构扰动和对抗攻击的鲁棒性仍未充分探索。关键限制在于对层次结构语义的建模不足，而这对于泛化能力至关重要。

Method: 1. 通过将基于熵的编码树转化为结构感知文本提示来编码层次结构先验，进行特征增强；2. 使用自监督信息瓶颈机制通过结构引导压缩蒸馏鲁棒、可迁移的表示；3. 引入专家自适应路由机制，结合混合专家架构和空专家设计解决跨领域适应的负迁移问题；4. 提出微调模块，通过联合社区内和社区间结构学习优化层次结构。

Result: 大量实验表明，SA^2GFM在节点和图分类任务中，在对抗随机噪声和对抗扰动的有效性和鲁棒性方面优于9个最先进的基线方法。

Conclusion: SA^2GFM通过结构感知语义增强和自适应机制，显著提升了图基础模型在噪声和对抗环境下的鲁棒性和领域自适应能力。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [18] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: FAIM：轻量级频率感知交互式Mamba模型，通过自适应滤波块和交互式Mamba块实现高效时间序列分类，在准确性和效率之间取得优越平衡


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在环境监测、医疗诊断等应用中至关重要。现有深度学习模型虽然能捕捉时间依赖性，但存在计算成本高、对噪声敏感、在小数据集上容易过拟合等问题，需要更轻量且鲁棒的解决方案。

Method: 提出FAIM模型，包含两个核心组件：1）自适应滤波块（AFB），利用傅里叶变换提取频域特征，通过可学习自适应阈值动态抑制噪声，结合全局和局部语义自适应滤波；2）交互式Mamba块（IMB），促进高效多粒度信息交互，平衡细粒度判别特征和全局上下文信息提取。此外还加入自监督预训练机制增强模型鲁棒性。

Result: 在多个基准测试上的广泛实验表明，FAIM持续优于现有最先进方法，在准确性和效率之间取得了优越的平衡，并在各种领域和高噪声场景中表现出色。

Conclusion: FAIM通过频域特征提取和高效信息交互机制，为时间序列分类任务提供了轻量级且强大的解决方案，有效解决了传统深度学习模型的局限性。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [19] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: SetAD将半监督异常检测重构为集合级任务，通过注意力集合编码器和分级学习目标，直接建模定义异常的复杂群体交互，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法主要关注单个点或简单对的评分，忽略了异常的上下文性质（异常是相对于群体的偏离），也无法利用集合组合生成的丰富监督信号，难以利用数据中的高阶交互来学习判别性表示。

Method: 提出SetAD框架，将半监督异常检测重构为集合级异常检测任务。采用基于注意力的集合编码器，通过分级学习目标训练模型量化整个集合的异常程度。同时提出上下文校准的异常评分机制，通过聚合点在多个不同上下文集合中相对于同伴行为的归一化偏差来评估点的异常分数。

Result: 在10个真实世界数据集上的广泛实验表明，SetAD显著优于最先进的模型。特别值得注意的是，模型性能随着集合大小的增加而持续提升，为基于集合的异常检测公式提供了强有力的实证支持。

Conclusion: SetAD通过集合级视角重新构建异常检测问题，能够更好地建模定义异常的复杂群体交互，提供更准确和鲁棒的异常检测性能，特别是在利用有限标记数据的半监督设置中表现出色。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [20] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 本文提出了一种使用无监督机器学习框架来监测环境条约（如蒙特利尔议定书）执行情况的方法，通过分析海关大数据自动检测可疑贸易模式，为监管机构提供优先审查的线索。


<details>
  <summary>Details</summary>
Motivation: 需要新的方法来监测环境条约的执行情况，特别是通过审查庞大复杂的海关数据集来发现违规贸易活动。传统人工审查方式难以处理海量数据，需要自动化工具来系统性地识别可疑贸易模式。

Method: 采用无监督机器学习框架，结合多种技术：1) 无监督聚类（K-Means）基于货物价值和重量发现自然贸易原型；2) 异常检测（隔离森林和IQR）识别罕见的"大宗贸易"和商业上不合理的单价；3) 启发式标记发现模糊描述等规避策略。这些层被整合成一个优先级评分系统。

Result: 应用于10万条贸易记录，成功识别出1,351个价格异常值和1,288个高优先级货物供海关审查。关键发现：高优先级商品的价值重量比与普通商品不同且更高。通过可解释AI（SHAP）验证，确认模糊描述和高价值是最显著的风险预测因子。模型成功检测到2021年初"大宗贸易"的激增，这与美国AIM法案的实际监管影响直接相关。

Conclusion: 该研究提出了一个可重复的无监督学习流程，能够将原始贸易数据转化为监管机构可用的优先情报，为环境条约监测提供了有效的自动化工具。

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [21] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 将瑞典大规模登记数据转化为文本化生命轨迹，解决分类变量高基数性和编码不一致问题，预测居住流动性，比较多种NLP模型效果


<details>
  <summary>Details</summary>
Motivation: 解决数据分析中的两个长期挑战：分类变量的高基数性和编码方案随时间的不一致性，同时利用瑞典独特全面的人口登记数据进行纵向分析

Method: 将690万个体（2001-2013年）的登记数据转化为语义丰富的文本生命轨迹，结合人口统计信息和年度居住、工作、教育、收入、家庭状况变化，使用多种NLP架构（LSTM、DistilBERT、BERT、Qwen）预测2013-2017年的居住流动性

Result: 序列化和基于Transformer的模型比基线模型更有效地捕捉时间和语义结构，文本化的登记数据保留了关于个体路径的有意义信息，支持复杂、可扩展的建模

Conclusion: 将语义丰富的登记数据与现代语言模型结合可以显著推进社会科学中的纵向分析，该数据集为开发和评估新的序列建模方法提供了严格的测试平台

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [22] [Command & Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing](https://arxiv.org/abs/2512.07866)
*Maria Milena Araujo Felix*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习和自然语言处理技术的DGA域名检测方法，使用LSTM神经网络模型，在混合数据集上达到了97.2%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现代恶意软件通过使用域名生成算法（DGA）与C2服务器通信，每天生成数千个动态地址，使得传统的基于静态黑名单的防御机制失效，需要更先进的检测方法。

Method: 收集包含5万个合法域名和5万个恶意域名的混合数据库，提取词汇特征，然后训练一个基于LSTM的循环神经网络模型。

Result: 实验结果表明，虽然统计熵分析对简单DGA有效，但神经网络方法在检测复杂模式方面表现更优，达到了97.2%的准确率，并在模糊的合法流量场景中降低了误报率。

Conclusion: 深度学习与NLP技术相结合的方法能够有效检测DGA生成的恶意域名，特别是在处理复杂模式和降低误报率方面优于传统统计方法。

Abstract: The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.

</details>


### [23] [Bayesian Optimization for Function-Valued Responses under Min-Max Criteria](https://arxiv.org/abs/2512.07868)
*Pouya Ahadi,Reza Marzban,Ali Adibi,Kamran Paynabar*

Main category: cs.LG

TL;DR: 提出MM-FBO框架，针对函数响应进行贝叶斯优化，直接最小化函数域上的最大误差，而非传统集成误差方法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化主要针对标量响应，但在科学和工程应用中，响应通常是函数形式的（如随时间或波长变化）。现有方法通常最小化集成误差，这忽略了最坏情况偏差。

Method: 使用函数主成分分析表示函数响应，为PC分数构建高斯过程代理模型。提出集成不确定性采集函数，平衡最坏情况期望误差的利用和函数域上的探索。

Result: 提供两个理论保证：最坏情况目标的离散化边界，以及代理模型准确时采集函数收敛到真实最小最大目标的收敛性。在合成基准和物理案例研究中，MM-FBO始终优于现有基线方法。

Conclusion: MM-FBO框架有效解决了函数响应贝叶斯优化问题，强调了在贝叶斯优化中显式建模函数不确定性的重要性，在电磁散射和蒸汽相渗透等物理应用中表现出色。

Abstract: Bayesian optimization is widely used for optimizing expensive black box functions, but most existing approaches focus on scalar responses. In many scientific and engineering settings the response is functional, varying smoothly over an index such as time or wavelength, which makes classical formulations inadequate. Existing methods often minimize integrated error, which captures average performance but neglects worst case deviations. To address this limitation we propose min-max Functional Bayesian Optimization (MM-FBO), a framework that directly minimizes the maximum error across the functional domain. Functional responses are represented using functional principal component analysis, and Gaussian process surrogates are constructed for the principal component scores. Building on this representation, MM-FBO introduces an integrated uncertainty acquisition function that balances exploitation of worst case expected error with exploration across the functional domain. We provide two theoretical guarantees: a discretization bound for the worst case objective, and a consistency result showing that as the surrogate becomes accurate and uncertainty vanishes, the acquisition converges to the true min-max objective. We validate the method through experiments on synthetic benchmarks and physics inspired case studies involving electromagnetic scattering by metaphotonic devices and vapor phase infiltration. Results show that MM-FBO consistently outperforms existing baselines and highlights the importance of explicitly modeling functional uncertainty in Bayesian optimization.

</details>


### [24] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出基于专家混合（MoE）的噪声估计器，用于医学时间序列信号的扩散模型重建，通过RFAMoE模块自适应选择感受野，并通过Fusion MoE模块在单次推理中融合多个噪声信号，显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列信号具有多变量、高时变、噪声大、易受伪影影响等独特特性，使得基于深度学习的插补等任务仍然具有挑战性。现有扩散模型方法在医学时间序列领域尚未充分探索，且多推理平均方法虽然能减少重建误差但计算成本高。

Method: 1. 在基于分数的扩散框架中引入专家混合（MoE）噪声估计器；2. 设计RFAMoE模块，使每个通道在扩散过程中自适应选择所需感受野；3. 设计Fusion MoE模块，利用MoE特性并行生成K个噪声信号，通过路由机制融合，实现单次推理完成信号重建。

Result: 提出的框架在不同任务和数据集上持续优于基于扩散模型的SOTA工作，不仅提高了性能，还消除了多推理过程带来的显著计算成本和延迟。

Conclusion: 通过创新的MoE-based噪声估计器和融合机制，成功解决了医学时间序列信号重建中的挑战，在保持高性能的同时大幅降低了计算复杂度，为医学时间序列分析提供了有效的扩散模型解决方案。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [25] [Controllable risk scenario generation from human crash data for autonomous vehicle testing](https://arxiv.org/abs/2512.07874)
*Qiujing Lu,Xuanhan Wang,Runze Yuan,Wei Lu,Xinyi Gong,Shuo Feng*

Main category: cs.LG

TL;DR: CRAG框架统一建模自动驾驶车辆测试中的正常驾驶行为和罕见安全关键行为，通过解耦潜在空间实现有限事故数据的高效利用，支持可控风险场景生成以评估AV鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要在实际驾驶和罕见安全关键条件下进行严格测试，但现有方法难以同时模拟背景车辆和弱势道路使用者的正常交通行为与真实事故中的风险行为。

Method: CRAG构建结构化潜在空间解耦正常行为和风险相关行为，结合风险感知潜在表示和基于优化的模式转换机制，使智能体能够在长时间范围内平滑、合理地从安全状态过渡到风险状态。

Result: 实验表明CRAG相比现有基线提高了多样性，同时能够可控生成风险场景，实现针对性和高效的自动驾驶车辆鲁棒性评估。

Conclusion: CRAG框架通过统一建模正常和安全关键行为，有效解决了自动驾驶测试中环境智能体模拟的挑战，为AV安全评估提供了更全面和可控的测试方法。

Abstract: Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.

</details>


### [26] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: S2KAN在KAN基础上引入符号化原语，通过可学习门控实现稀疏表示，结合最小描述长度目标，在保持精度的同时提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统Kolmogorov-Arnold Networks（KANs）虽然提供了可解释机器学习的路径，但实际训练中激活函数往往缺乏符号保真度，学习到的分解没有有意义的对应关系，无法达到真正的可解释性。

Method: 提出Softly Symbolified Kolmogorov-Arnold Networks（S2KAN），将符号原语直接集成到训练中。每个激活函数从符号项和密集项的字典中提取，通过可学习门控实现稀疏表示。这种稀疏化是可微分的，支持端到端优化，并由最小描述长度目标指导。

Result: 在符号基准测试、动态系统预测和实际预测任务中，S2KAN实现了竞争性或更优的准确性，同时模型规模显著减小。即使没有正则化压力，也观察到涌现的自稀疏化证据。

Conclusion: S2KAN在保持KAN准确性的同时，通过集成符号原语和稀疏化机制，显著提升了模型的可解释性，为可解释机器学习提供了新的有效途径。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [27] [Fourier-Enhanced Recurrent Neural Networks for Electrical Load Time Series Downscaling](https://arxiv.org/abs/2512.07876)
*Qi Chen,Mihai Anitescu*

Main category: cs.LG

TL;DR: 提出一种傅里叶增强的循环神经网络，用于电力负荷降尺度预测，结合循环主干、傅里叶季节性嵌入和自注意力机制，在PJM地区优于传统基准方法。


<details>
  <summary>Details</summary>
Motivation: 电力负荷预测需要从低分辨率输入生成高分辨率预测，传统方法在捕捉季节性模式和跨时间依赖关系方面存在局限，需要更精确的降尺度模型。

Method: 提出傅里叶增强循环神经网络，包含三个核心组件：1）由低分辨率输入驱动的循环主干网络；2）在潜在空间融合的显式傅里叶季节性嵌入；3）捕捉周期内高分辨率分量间依赖关系的自注意力层。

Result: 在四个PJM地区测试中，该方法相比传统Prophet基准（无论是否包含季节性/LAA）以及没有注意力或傅里叶特征的RNN消融模型，均获得更低的RMSE和更平坦的预测误差曲线。

Conclusion: 傅里叶季节性嵌入与自注意力机制的结合能有效提升电力负荷降尺度预测的精度，为时间序列预测提供了新的架构设计思路。

Abstract: We present a Fourier-enhanced recurrent neural network (RNN) for downscaling electrical loads. The model combines (i) a recurrent backbone driven by low-resolution inputs, (ii) explicit Fourier seasonal embeddings fused in latent space, and (iii) a self-attention layer that captures dependencies among high-resolution components within each period. Across four PJM territories, the approach yields RMSE lower and flatter horizon-wise than classical Prophet baselines (with and without seasonality/LAA) and than RNN ablations without attention or Fourier features.

</details>


### [28] [Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design](https://arxiv.org/abs/2512.07877)
*Amogh Anshu N,Harish BP*

Main category: cs.LG

TL;DR: 提出基于机器学习的NoC设计空间探索框架，使用BookSim仿真和反向神经网络模型，比较MLP、条件扩散模型和CVAE三种架构，条件扩散模型在未见数据上获得最低MSE（0.463），大幅缩短设计探索时间。


<details>
  <summary>Details</summary>
Motivation: NoC设计需要探索高维配置空间以满足严格的吞吐量和延迟约束，传统设计空间探索方法速度慢且难以处理复杂的非线性参数交互。

Method: 提出机器学习驱动的框架，自动化NoC设计空间探索，使用BookSim仿真生成超过150,000个数据点，比较三种神经网络架构：多层感知器（MLP）、条件扩散模型和条件变分自编码器（CVAE），用于根据目标性能指标预测最优NoC参数。

Result: 条件扩散模型获得最高的预测准确率，在未见数据上达到0.463的均方误差（MSE），提出的框架将设计探索时间减少数个数量级。

Conclusion: 该框架为快速、可扩展的NoC协同设计提供了实用解决方案，机器学习方法显著提升了NoC设计空间探索的效率和准确性。

Abstract: Network-on-Chip (NoC) design requires exploring a high-dimensional configuration space to satisfy stringent throughput requirements and latency constraints.Traditional design space exploration techniques are often slow and struggle to handle complex, non-linear parameter interactions.This work presents a machine learning-driven framework that automates NoC design space exploration using BookSim simulations and reverse neural network models.Specifically, we compare three architectures - a Multi-Layer Perceptron (MLP),a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE) to predict optimal NoC parameters given target performance metrics.Our pipeline generates over 150,000 simulation data points across varied mesh topologies.The Conditional Diffusion Model achieved the highest predictive accuracy, attaining a mean squared error (MSE) of 0.463 on unseen data.Furthermore, the proposed framework reduces design exploration time by several orders of magnitude, making it a practical solution for rapid and scalable NoC co-design.

</details>


### [29] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: 提出SpecMatch-CL损失函数，通过最小化视图特定图-图的归一化拉普拉斯矩阵差异来对齐图嵌入，在无监督、半监督和迁移学习任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如InfoNCE）仅优化跨视图的图嵌入对齐，但缺乏对视图特定图-图全局结构的控制机制。

Method: 提出SpecMatch-CL损失函数，通过最小化视图特定图-图的归一化拉普拉斯矩阵差异来对齐图嵌入，理论上证明该差异为理想对齐损失提供了上界。

Result: 在8个TU基准测试的无监督和低标签率半监督学习中达到SOTA，在PPI-306K和ZINC 2M数据集的迁移学习中获得一致性能提升。

Conclusion: SpecMatch-CL通过控制图-图全局结构对齐，显著提升了图对比学习在各种任务中的性能。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [30] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 本文提出了一种基于几何视角的锥体坍缩算法（Cone Collapse），用于恢复数据的最小生成锥，并在此基础上开发了锥体感知的正交非负矩阵分解模型（CC-NMF），在聚类任务中表现优于传统NMF方法。


<details>
  <summary>Details</summary>
Motivation: 现有NMF算法主要从优化角度出发，未能充分利用NMF诱导的锥体几何结构。数据点位于凸锥中，其极端射线编码了基本方向或"主题"。本文旨在从几何视角重新审视NMF，通过恢复数据锥来改进聚类性能。

Method: 提出了锥体坍缩算法（Cone Collapse），从完整的非负象限开始，迭代收缩到数据生成的最小锥体。在温和的数据假设下，该算法在有限步内终止并恢复X^⊤的最小生成锥。基于此，通过将单正交NMF应用于恢复的极端射线，推导出锥体感知的正交NMF模型（CC-NMF）。

Result: 在16个基准基因表达、文本和图像数据集上的实验表明，CC-NMF在聚类纯度方面始终匹配或优于强NMF基线方法，包括乘法更新、ANLS、投影NMF、ONMF和稀疏NMF。

Conclusion: 明确恢复数据锥可以产生既有理论依据又在实证上强大的基于NMF的聚类方法。几何视角为NMF提供了新的理解框架，锥体坍缩算法和CC-NMF模型在聚类任务中表现出优越性能。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [31] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: CLOP是一种新的半监督对比学习损失函数，通过促进类别嵌入形成正交线性子空间来防止维度坍缩，在图像分类和目标检测任务中表现出更好的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 对比学习在深度学习中表现出色，但维度坍缩问题在半监督和自监督设置中尤为突出，当嵌入收敛到低维空间时会严重影响表示学习效果。

Method: 首先识别了标准对比损失收敛到坍缩解的学习率阈值，然后提出了CLOP损失函数，该函数通过促进类别嵌入形成正交线性子空间来防止维度坍缩。

Result: 在真实和合成数据集上的广泛实验表明，CLOP在图像分类和目标检测任务中提高了性能，同时在不同学习率和批量大小下表现出更好的稳定性。

Conclusion: CLOP通过防止维度坍缩有效解决了对比学习中的关键问题，为半监督对比学习提供了更稳定和有效的解决方案。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [32] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2通过算法-系统联合重新设计，解决了GSPN在GPU实现中的性能瓶颈，包括内核启动开销、内存传输和冗余计算，在保持精度的同时显著提升了视觉Transformer的效率。


<details>
  <summary>Details</summary>
Motivation: 现有GSPN实现存在三个主要问题：1) GPU内核重复启动带来的沉重开销；2) 全局GPU内存的过度数据传输；3) 为每个通道维护单独传播权重导致的冗余计算。这些限制了GSPN在实际高分辨率图像和长视频应用中的效率。

Method: GSPN-2采用算法-系统联合重新设计：系统层面将数千个微内核启动合并为单个2D内核，为每个通道片固定一个warp，并将前一列的激活存储在共享内存中；算法层面引入紧凑通道传播策略，用共享权重替代每通道矩阵，减少参数并与Transformer注意力中的亲和力图自然对齐。

Result: 实验表明GSPN-2在图像分类和文本到图像合成任务中有效，能够匹配Transformer级别的准确性，同时显著降低计算成本。GSPN-2通过结构化矩阵变换和GPU优化实现的独特组合，为视觉应用中的全局空间上下文建模建立了新的效率前沿。

Conclusion: GSPN-2通过算法和系统的协同优化，解决了GSPN实现中的性能瓶颈，为高分辨率图像和长视频相关应用提供了高效的视觉Transformer解决方案，在保持模型精度的同时大幅提升了计算效率。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [33] [ByteStorm: a multi-step data-driven approach for Tropical Cyclones detection and tracking](https://arxiv.org/abs/2512.07885)
*Davide Donno,Donatello Elia,Gabriele Accarino,Marco De Carlo,Enrico Scoccimarro,Silvio Gualdi*

Main category: cs.LG

TL;DR: ByteStorm是一个基于深度学习和计算机视觉的热带气旋追踪框架，无需阈值调优，在东西北太平洋盆地表现出优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统热带气旋追踪方法主要依赖主观阈值，在不同地理区域应用中可能引入偏差，需要一种更准确、无需阈值调优的数据驱动方法。

Method: 使用深度学习网络通过分类和定位检测热带气旋中心（仅使用850mb相对涡度和平均海平面气压），然后通过BYTE算法将检测到的中心连接成热带气旋轨迹。

Result: 在东、西北太平洋盆地评估中，ByteStorm在检测概率（ENP 85.05%，WNP 79.48%）、误报率（ENP 23.26%，WNP 16.14%）和年际变率相关性（ENP 0.75，WNP 0.69）方面均优于现有确定性追踪器。

Conclusion: 深度学习与计算机视觉的结合为快速准确的热带气旋追踪提供了强大替代方案，展示了数据驱动方法在天气和气候科学中的潜力。

Abstract: Accurate tropical cyclones (TCs) tracking represents a critical challenge in the context of weather and climate science. Traditional tracking schemes mainly rely on subjective thresholds, which may introduce biases in their skills on the geographical region of application. We present ByteStorm, an efficient data-driven framework for reconstructing TC tracks without threshold tuning. It leverages deep learning networks to detect TC centers (via classification and localization), using only relative vorticity (850 mb) and mean sea-level pressure. Then, detected centers are linked into TC tracks through the BYTE algorithm. ByteStorm is evaluated against state-of-the-art deterministic trackers in the East- and West-North Pacific basins (ENP and WNP). The proposed framework achieves superior performance in terms of Probability of Detection ($85.05\%$ ENP, $79.48\%$ WNP), False Alarm Rate ($23.26\%$ ENP, $16.14\%$ WNP), and high Inter-Annual Variability correlations ($0.75$ ENP and $0.69$ WNP). These results highlight the potential of integrating deep learning and computer vision for fast and accurate TC tracking, offering a robust alternative to traditional approaches.

</details>


### [34] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: Brush是一种符号回归算法，将决策树分割算法与非线性常数优化相结合，能够将基于规则的逻辑无缝集成到符号回归和分类模型中，在临床风险评分开发中表现出色。


<details>
  <summary>Details</summary>
Motivation: 医疗决策经常使用结合风险方程和规则的算法，提供清晰标准化的治疗路径。传统符号回归将搜索空间限制在连续函数形式及其参数，难以建模这种决策过程。但由于符号回归能够推导数据驱动的可解释模型，在开发数据驱动的临床风险评分方面具有潜力。

Method: Brush算法结合了决策树式的分割算法和非线性常数优化，允许将基于规则的逻辑无缝集成到符号回归和分类模型中。该方法在SRBench基准测试中实现了帕累托最优性能。

Result: Brush成功复现了两个广泛使用的临床评分系统，实现了高准确性和可解释模型。与决策树、随机森林和其他符号回归方法相比，Brush实现了相当或更优的预测性能，同时产生更简单的模型。

Conclusion: Brush算法通过结合规则逻辑和符号回归，为开发数据驱动的临床风险评分提供了有前景的方法，在保持可解释性的同时实现了良好的预测性能。

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [35] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: CIP-Net是一种无需示例的自解释原型模型，用于持续学习，避免灾难性遗忘，同时保持架构简单和内存效率。


<details>
  <summary>Details</summary>
Motivation: 持续学习面临灾难性遗忘的挑战，现有可解释AI方法大多使用事后解释或需要额外内存存储任务示例，限制了可扩展性。

Method: 提出CIP-Net（无示例自解释原型网络），这是一种无需存储过去示例的自解释原型模型，保持简单架构的同时提供有用解释。

Result: CIP-Net在任务增量学习和类别增量学习设置中，相比之前的无示例和自解释方法，取得了最先进的性能，同时显著降低了内存开销。

Conclusion: CIP-Net为持续学习提供了一个实用且可解释的解决方案，平衡了性能、内存效率和可解释性。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [36] [HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability](https://arxiv.org/abs/2512.07988)
*Sudhanva Manjunath Athreya,Paul Rosen*

Main category: cs.LG

TL;DR: HOLE方法通过持久同调分析深度神经网络，利用拓扑特征和可视化工具来增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然取得了显著成功，但其学习到的表示和决策过程仍然不透明且难以解释，需要新的方法来理解和分析神经网络内部表示。

Method: 提出HOLE方法，从神经激活中提取拓扑特征，并使用桑基图、热图、树状图和blob图等多种可视化技术来呈现这些特征，从而分析各层的表示结构和质量。

Result: 在标准数据集上使用多种判别模型进行评估，结果显示拓扑分析能够揭示与类别分离、特征解耦和模型鲁棒性相关的模式。

Conclusion: 拓扑分析为理解和改进深度学习系统提供了互补的视角，有助于揭示神经网络内部表示的结构和质量。

Abstract: Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.

</details>


### [37] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 开发了一个面向医疗领域的时间序列预测网络平台，降低技术门槛，让研究人员和临床医生能够轻松进行数据分析、模型训练和结果解释


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在医疗等领域有广泛应用，但技术门槛高，需要专业知识进行数据分析、模型构建和结果解释，这限制了非技术背景的研究人员和临床医生使用这些技术

Method: 开发了一个网络平台，支持用户上传数据、生成可视化图表、训练多种可定制的预测模型，并集成了大语言模型来提供参数选择建议和结果解释

Result: 创建了一个易用的平台，使非技术用户能够进行复杂的时间序列分析，平台支持多种预测模型和训练技术，并提供智能推荐和解释功能

Conclusion: 该平台降低了时间序列预测的技术门槛，计划将其整合到学习型医疗系统中，实现从临床流程中持续收集数据和进行推断

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [38] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: 本文在ICU临床决策场景中，比较了离线多目标强化学习算法与单目标基线方法，发现PEDA DT算法在灵活性和性能上表现最优，为个性化医疗决策提供了有效框架。


<details>
  <summary>Details</summary>
Motivation: ICU临床决策面临生存率最大化与资源利用最小化的多目标权衡问题。传统单目标强化学习方法使用固定标量化奖励函数，导致策略僵化，无法适应变化的临床优先级。需要一种能在离线环境下学习多目标策略的方法。

Method: 在MIMIC-IV数据集上，对三种离线多目标强化学习算法（CPQL、Adaptive CPQL、PEDA DT）与三种单目标基线方法（BC、CQL、DDQN）进行基准测试。使用离线策略评估（OPE）指标进行性能比较。

Result: PEDA DT算法在灵活性方面优于静态标量化基线方法。研究结果扩展了单目标决策变换器在医疗领域的先前发现，证实序列建模架构在多目标条件生成中仍保持鲁棒性和有效性。

Conclusion: 离线多目标强化学习是一个有前景的框架，能够在无需重新训练的情况下实现重症监护中的个性化、可调整决策制定，为临床决策支持系统提供了新的可能性。

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [39] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY是一个医学世界模型，通过结构化潜在空间预测疾病演化，整合时间间隔和患者特异性数据，生成个体化治疗计划，并在胶质瘤数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前肿瘤学临床决策需要预测动态疾病演化，但现有静态AI预测器无法完成此任务。现有医学世界模型通常忽略患者特异性时间和临床上下文，缺乏将预测与治疗决策连接的反馈机制。

Method: CLARITY是一个医学世界模型，在结构化潜在空间中直接预测疾病演化。它明确整合时间间隔（时间上下文）和患者特异性数据（临床上下文），将治疗条件进展建模为平滑、可解释的轨迹，从而生成生理学上可信的个体化治疗计划。还引入了新颖的预测到决策框架，将潜在推演转化为透明、可操作的推荐。

Result: CLARITY在治疗规划方面展示了最先进的性能。在MU-Glioma-Post数据集上，该方法比最近的MeWM提高了12%，并显著超越所有其他医学专用大型语言模型。

Conclusion: CLARITY通过整合时间上下文和临床上下文，在结构化潜在空间中预测疾病演化，为肿瘤学临床决策提供了生理学上可信的个体化治疗计划，并通过预测到决策框架将推演转化为可操作的推荐。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [40] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: LUNA是一种核化线性注意力机制，通过可学习的核特征映射在保持线性计算成本的同时，达到甚至超越二次注意力机制的精度。


<details>
  <summary>Details</summary>
Motivation: 传统softmax注意力机制存在O(n²)的二次计算成本，限制了其在长序列领域的应用。现有的线性注意力机制虽然将成本降至O(n)，但依赖固定的随机特征映射，导致模型精度与计算效率之间存在根本性权衡。

Method: 提出LUNA机制，核心思想是核特征映射本身应该是可学习的而非固定的。通过参数化核函数，学习针对特定数据和任务的特征基，克服固定特征方法的表达能力限制。实现了一个可学习的特征映射，该映射诱导出正定核并支持流式形式，从而在序列长度上实现线性时间和内存扩展。

Result: 在Long Range Arena（LRA）基准测试中，LUNA在计算对等条件下（相同参数量、训练步数和近似FLOPs）达到了高效Transformer中的最先进平均精度。在事后转换任务中，用LUNA替换微调后的BERT和ViT-B/16检查点中的softmax并进行简短微调，能够恢复大部分原始性能，显著优于固定线性化方法。

Conclusion: LUNA通过可学习的核特征映射消除了线性注意力机制中精度与效率的权衡，在保持线性计算复杂度的同时，达到了与二次注意力机制相当甚至更优的性能，为长序列处理提供了有效的解决方案。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [41] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 提出可解释的深度竞争风险模型DKAJ，通过自动学习核函数将数据表示为聚类加权组合，在保持Aalen-Johansen估计器优点的同时提供可视化解释能力。


<details>
  <summary>Details</summary>
Motivation: 传统Aalen-Johansen非参数估计器在竞争风险分析中广泛应用，但缺乏可解释性和处理复杂数据的能力。需要开发既能保持经典方法统计特性，又能提供模型解释性的深度学习方法。

Method: 提出深度核Aalen-Johansen（DKAJ）估计器，将每个数据点表示为聚类的加权组合。通过自动学习的核函数度量数据点间的相似性，生成权重。当数据点仅对一个聚类有非零权重时，其预测的累积发生率函数对应于该聚类的经典Aalen-Johansen估计。

Result: 在四个标准竞争风险数据集上的实验表明，DKAJ与最先进的基线方法性能相当，同时能够提供可视化辅助模型解释，实现了预测性能与可解释性的平衡。

Conclusion: DKAJ成功地将经典Aalen-Johansen估计器推广到深度学习框架，在保持竞争风险分析统计特性的同时，通过聚类权重和可视化提供了模型可解释性，为临床决策支持提供了有价值的工具。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [42] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 提出因果引导的多模态域泛化框架，通过对抗解耦和统一表示学习提升社交媒体危机分类在未见灾难类型上的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体危机分类方法在未见灾难类型上泛化能力差，原因包括：1) 未能解耦虚假特征和因果特征，导致域偏移时性能下降；2) 未能对齐异构模态表示，阻碍单模态域泛化技术向多模态场景的迁移

Method: 提出因果引导的多模态域泛化框架，结合对抗解耦与统一表示学习。对抗目标促使模型解耦并关注域不变因果特征；统一表示将不同模态特征对齐到共享潜在空间，使单模态域泛化策略能无缝扩展到多模态学习

Result: 在不同数据集上的实验表明，该方法在未见灾难场景中取得了最佳性能

Conclusion: 通过因果引导的多模态域泛化框架，有效解决了社交媒体危机分类在未见灾难类型上的泛化挑战，提升了紧急响应中的情境感知能力

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [43] [Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders](https://arxiv.org/abs/2512.08077)
*Jaron Cohen,Alexander G. Hasson,Sara Tanovic*

Main category: cs.LG

TL;DR: 该研究将稀疏自编码器技术应用于化学语言模型，以揭示其内部可解释特征，发现模型编码了丰富的化学概念知识。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在药物和材料发现等高风险应用中的使用，机器学习可解释性问题日益紧迫。虽然化学语言模型在分子性质预测和生成方面表现出色，但其内部如何表示化学知识仍不清楚。

Method: 将稀疏自编码器技术扩展到化学语言模型，应用于FM4M SMI-TED化学基础模型，提取语义上有意义的潜在特征，并分析其在各种分子数据集上的激活模式。

Result: 研究发现这些模型编码了丰富的化学概念景观，识别出特定潜在特征与不同化学知识领域（包括结构基序、物理化学性质和药理学药物类别）之间的相关性。

Conclusion: 该方法为揭示化学AI系统中潜在知识提供了通用框架，对基础理解和实际部署都有重要意义，有望加速计算化学研究。

Abstract: Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.

</details>


### [44] [Complexity of One-Dimensional ReLU DNNs](https://arxiv.org/abs/2512.08091)
*Jonathan Kogan,Hayden Jananthan,Jeremy Kepner*

Main category: cs.LG

TL;DR: 研究一维ReLU深度神经网络在无限宽度极限下的表达能力，证明线性区域数量随神经元总数线性增长，并提出函数自适应稀疏性概念


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络表达能力与线性区域数量的关系，理解ReLU网络如何通过分段线性函数逼近复杂函数

Method: 分析随机初始化的一维全连接ReLU网络（He缩放且带非零偏置），在无限宽度极限下推导线性区域数量的期望值，并引入函数自适应稀疏性度量

Result: 证明线性区域数量期望为∑n_i + o(∑n_i) + 1，其中n_i为第i隐藏层神经元数，表明区域数量随神经元总数线性增长

Conclusion: 一维ReLU网络的表达能力可通过线性区域数量量化，函数自适应稀疏性概念有助于评估网络逼近效率

Abstract: We study the expressivity of one-dimensional (1D) ReLU deep neural networks through the lens of their linear regions. For randomly initialized, fully connected 1D ReLU networks (He scaling with nonzero bias) in the infinite-width limit, we prove that the expected number of linear regions grows as $\sum_{i = 1}^L n_i + \mathop{o}\left(\sum_{i = 1}^L{n_i}\right) + 1$, where $n_\ell$ denotes the number of neurons in the $\ell$-th hidden layer. We also propose a function-adaptive notion of sparsity that compares the expected regions used by the network to the minimal number needed to approximate a target within a fixed tolerance.

</details>


### [45] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 提出了一种通过"忏悔"机制让大语言模型诚实报告自身缺陷的方法，在训练中单独奖励忏悔的诚实性，不干扰主回答的奖励，从而激励模型在忏悔中诚实披露不当行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在报告自身行为和信念时可能存在不诚实行为，如夸大事实信心或掩盖不当行为。这种不诚实可能源于强化学习中的奖励塑造问题，训练过程可能无意中激励模型说谎或歪曲事实。

Method: 提出忏悔机制：在主回答后，模型被要求提供忏悔输出，完整说明其遵守政策和指令的情况。忏悔的奖励仅基于其诚实性，不影响主回答的奖励。只要最大化忏悔奖励的"最小阻力路径"是揭露不当行为而非掩盖，就能激励模型在忏悔中保持诚实。

Result: 训练GPT-5-Thinking产生忏悔，在分布外场景评估其诚实性（包括幻觉、指令遵循、阴谋和奖励攻击）。发现当模型在主回答中说谎或隐瞒缺陷时，通常能在忏悔中诚实承认这些行为，且忏悔诚实性随训练适度提高。

Conclusion: 忏悔机制可行，能够激励模型诚实报告自身缺陷。忏悔可实现多种推理时干预，包括监控、拒绝采样和向用户提示问题，为提升大语言模型透明度提供了有效方法。

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [46] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: MAC提出了一种基于模型的离线强化学习方法，通过动作块模型减少长期预测误差，结合拒绝采样防止模型利用，在复杂长时域任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 研究基于模型的强化学习（特别是基于模型的价值扩展）是否能为离线RL中的复杂长时域任务提供可扩展的解决方案。传统方法中，更大的n值减少价值引导偏差，但会放大长期累积的模型误差

Method: 提出MAC方法：1）使用动作块模型预测未来状态（基于动作序列而非单个动作），减少复合误差；2）采用拒绝采样从表达性行为动作块策略中采样，防止模型利用和分布外动作问题

Result: 在包含高达1亿个转移的大规模数据集上的高度挑战性任务实验中，MAC在离线基于模型的RL算法中表现最佳，特别是在挑战性长时域任务上

Conclusion: MAC通过动作块模型和拒绝采样的组合，有效解决了基于模型价值扩展中的偏差-误差权衡问题，为离线RL中的复杂长时域任务提供了有效的解决方案

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [47] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 论文提出在评估大语言模型时，使用Youden's J统计量和平衡准确率来选择最佳分类器（评估者），而不是传统的准确率、精确率或F1分数，因为这些传统指标对类别不平衡敏感且可能扭曲流行率估计。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的严格评估依赖于比较模型行为的流行率（如任务通过率或策略违规率）。这些流行率估计由分类器（LLM作为评估者或人工标注者）产生，因此分类器的选择对可信评估至关重要。传统指标如准确率、精确率和F1分数对类别不平衡敏感，且受正类任意选择的影响，可能导致选择扭曲流行率估计的评估者。

Method: 论文提出使用Youden's J统计量作为选择最佳评估者的理论依据，并证明平衡准确率是J的等价线性变换。通过理论分析和实证示例及模拟，展示了使用平衡准确率选择分类器的方法。

Result: 研究表明，Youden's J统计量在理论上与选择最佳模型比较评估者相一致。使用平衡准确率选择分类器能够产生更好、更稳健的评估者选择，避免传统指标在类别不平衡情况下的偏差。

Conclusion: 在大语言模型评估中，应使用Youden's J统计量或其等价指标平衡准确率来选择分类器，而不是传统的准确率、精确率或F1分数，以确保流行率估计的准确性和模型比较的可信度。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [48] [Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach](https://arxiv.org/abs/2512.08124)
*Zijiang Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的加密货币投资组合管理新方法，通过分析加密货币之间的相对关系而非独立预测，利用神经网络预测未来收益排名进行权重分配，在3.5年的市场周期中实现了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注特定加密货币（如比特币）的价格走势预测并进行交易，将加密货币视为独立个体。本文认为通过分析加密货币之间的相对关系，利用跨截面信息进行投资组合管理可能获得更好的收益。

Method: 提出了一种基于神经网络的加密货币投资组合管理方法：在每个时间步，利用神经网络预测所管理加密货币的未来收益排名，并根据排名结果分配投资权重。该方法强调分析加密货币之间的相对关系而非独立预测。

Result: 在2020年5月至2023年11月的真实加密货币日数据上进行回测实验，期间市场经历了完整的牛市、熊市和横盘周期。该方法在复杂市场条件下表现优异，夏普比率为1.01，年化收益率为64.26%，且对交易费用增加具有鲁棒性。

Conclusion: 通过分析加密货币之间的相对关系而非独立预测，基于神经网络预测未来收益排名的投资组合管理方法在加密货币市场中表现优异，能够在不同市场条件下获得稳定收益，且对交易成本具有鲁棒性。

Abstract: This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.

</details>


### [49] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 提出CSO方法，通过抑制内在特征来增强后门检测的敏感性，解决传统方法在易区分类别和隐蔽后门攻击中的失效问题


<details>
  <summary>Details</summary>
Motivation: 传统后门检测方法依赖目标类表现出极端异常检测统计量，但在两类情况下会失效：1）某些非目标类本身就容易与其他类区分，自然获得极端统计量；2）后门特征相对内在类别区分特征较弱时。需要更敏感的检测方法。

Method: 提出类子空间正交化（CSO）方法：利用给定类别的小规模干净样本，通过约束优化问题，在优化检测统计量的同时与类别的内在特征正交化，从而抑制内在特征的影响。

Result: CSO方法能够有效对抗具有挑战性的混合标签攻击和自适应攻击，相比传统方法具有更高的检测敏感性。

Conclusion: 通过抑制内在特征并保留后门触发器的贡献，CSO方法能够更敏感地检测后门攻击，特别是在传统方法容易失效的场景中表现优异。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [50] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 该论文提出了首个生物威胁基准生成框架，专门用于评估AI模型在生物安全风险方面的潜在危害，重点关注细菌生物威胁。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型特别是大语言模型的快速发展，需要可靠的方法来量化这些模型可能促进生物恐怖主义或获取生物武器的风险。当前缺乏能够全面评估生物安全风险提升的基准测试方法。

Method: 开发了生物威胁基准生成框架，采用分层结构的生物威胁类别、要素和任务，构建了细菌生物威胁模式作为任务-查询架构的基础。该框架考虑了不同行为者能力水平和操作风险因素。

Result: 建立了细菌生物威胁模式，为后续将查询转化为模型提示和模型评估奠定了基础。该框架提供了可重复使用的结构，能够跨多个聚合层面评估LLM带来的细菌生物风险。

Conclusion: BBG框架包括细菌生物威胁模式，旨在为评估LLM在细菌生物风险方面提供稳健、可重复使用的评估结构，全面涵盖技术和操作要求，并考虑广泛的生物对手能力范围。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [51] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 该论文通过开放性和多智能体学习方法，训练能够泛化到新环境、分布外输入和与其他智能体交互的鲁棒AI智能体，涉及MiniHack框架、Maestro对抗课程生成、质量多样性方法分析RL策略漏洞，以及LLM对抗提示鲁棒性研究。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各种应用中的普及，需要能够成功导航和适应不断变化的开放世界的智能体。关键挑战是确保这些AI智能体具有鲁棒性，不仅在训练期间熟悉的设置中表现出色，还能有效地泛化到先前未见过的多样化场景。

Method: 1. 引入MiniHack框架，通过程序内容生成创建多样化环境；2. 提出Maestro方法，生成对抗性课程以增强RL智能体在双人零和游戏中的鲁棒性；3. 使用质量多样性方法系统识别预训练RL策略在复杂足球游戏中的漏洞；4. 通过进化搜索生成多样化有效输入，诊断和增强LLM对抗对抗提示的鲁棒性。

Result: 该研究为AI鲁棒性的未来发展铺平了道路，使智能体不仅能够适应不断发展的世界，还能在面对不可预见的挑战和交互时蓬勃发展。通过多种方法在不同领域验证了鲁棒性增强的有效性。

Conclusion: 该论文通过整合开放性和多智能体学习方法，在多个领域（RL环境、多智能体游戏、LLM）系统地解决了AI鲁棒性问题，为构建能够适应开放世界并有效泛化的鲁棒AI智能体提供了综合框架和方法论。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [52] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: PolyLingua是一个轻量级Transformer模型，用于语言识别和细粒度语言分类，通过两级对比学习框架在计算和延迟受限环境中实现高精度


<details>
  <summary>Details</summary>
Motivation: 语言识别是多语言系统的关键第一步，现有工具在特定场景（如音乐请求）中表现不佳，开源工具速度快但精度低，大语言模型有效但成本高，不适合低延迟或低资源环境

Method: 采用轻量级Transformer架构，设计两级对比学习框架，结合实例级分离和类级对齐，使用自适应边距，为相近语言生成紧凑且分离良好的嵌入表示

Result: 在Amazon Massive数据集上达到99.25% F1分数，在Song数据集上达到98.15% F1分数，超越Sonnet 3.5，同时参数减少10倍

Conclusion: PolyLingua在计算和延迟受限环境中实现了高精度语言识别，特别适合处理音乐请求等具有代码切换的挑战性场景，为多语言系统提供了高效解决方案

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [53] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO是一种新颖的强化学习框架，通过将去噪过程重构为搜索树，显著提高生成模型与人类偏好对齐的训练效率，实现2.4倍加速训练


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练对于将生成模型与人类偏好对齐至关重要，但其高昂的计算成本阻碍了广泛应用。现有方法在样本效率和信用分配方面存在局限性。

Method: 将去噪过程重构为搜索树，从共享的初始噪声样本出发，策略性地分支生成多个候选轨迹，同时高效重用它们的共同前缀。采用树状结构实现高样本效率、细粒度信用分配和摊销计算。

Result: 在扩散模型和流模型上的实验表明，TreeGRPO实现了2.4倍更快的训练速度，并在效率-奖励权衡空间中建立了更优的帕累托前沿。在多个基准测试和奖励模型上一致优于GRPO基线。

Conclusion: TreeGRPO为基于强化学习的视觉生成模型对齐提供了一条可扩展且有效的途径，通过树状结构显著提高了训练效率和性能。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [54] [LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks](https://arxiv.org/abs/2512.08160)
*Nanda K. Unnikrishnan,Keshab K. Parhi*

Main category: cs.LG

TL;DR: LayerPipe2通过形式化推导解决了LayerPipe中梯度延迟插入的理论基础问题，提出了基于网络结构的延迟分配原则，并开发了管道感知移动平均方法来减少内存开销。


<details>
  <summary>Details</summary>
Motivation: 虽然LayerPipe在卷积、全连接和脉冲神经网络训练加速方面取得了经验性成功，但缺乏对梯度延迟插入位置和数量的理论理解，需要建立原则性框架来指导管道化训练。

Method: 使用变量延迟梯度适应和重定时技术形式化推导LayerPipe；分析网络结构确定合法延迟插入位置；开发管道感知移动平均方法重构历史权重状态而非显式存储。

Result: 发现延迟需求与网络结构直接相关：内部层需要较少延迟，外部层需要较长延迟；当每层都管道化时，延迟仅取决于下游阶段数量；分组管道化时组内层共享相同延迟分配。

Conclusion: LayerPipe2提供了一个原则性框架，能够指导LayerPipe架构构建、预测延迟需求并减轻存储负担，实现了可扩展的管道化训练，同时控制通信计算权衡。

Abstract: In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.

</details>


### [55] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: MobileFineTuner是一个开源框架，支持在普通手机上直接进行端到端的大语言模型微调，解决了隐私保护和资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公开数据接近枯竭，设备端微调成为利用私有用户数据同时保护隐私的重要途径。然而现有方法主要基于模拟或依赖IoT设备和PC，普通手机领域尚未充分探索，缺乏实用的开源框架。

Method: 提出了MobileFineTuner统一开源框架，支持全参数微调和参数高效微调。针对手机内存和能耗限制，引入了参数分片、梯度累积和能量感知计算调度等系统级优化。

Result: 在真实手机上成功微调了GPT-2、Gemma 3和Qwen 2.5模型。大量实验和消融研究验证了所提优化的有效性，证明MobileFineTuner可作为设备端LLM训练研究的可行基础。

Conclusion: MobileFineTuner填补了手机端LLM微调框架的空白，为利用私有数据保护隐私提供了实用解决方案，并为未来设备端LLM训练研究奠定了基础。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [56] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 该论文挑战了AdamW中权重衰减与学习率γ成正比的传统设定，提出权重衰减应与γ²成正比，基于稳态时权重范数稳定的假设，并通过实验验证了这一设定能改善训练动态和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统AdamW优化器中解耦权重衰减被设定为与学习率γ成正比，但作者质疑这一设定的理论基础，认为需要重新审视权重衰减与学习率的关系，以更好地控制训练动态。

Method: 作者基于稳态时更新与权重无关的简单假设，推导出解耦权重衰减应与γ²成正比的理论依据。同时分析了Scion优化器中每个小批量的总更新贡献，提出动量相关的有效学习率概念，并通过实验验证理论推导。

Result: 研究发现权重衰减∝γ²能实现稳定的权重范数和梯度范数，更好地控制训练动态。实验验证了该设定能改善模型性能，且最优的有效学习率值在不同设置间具有可迁移性。

Conclusion: 解耦权重衰减应与学习率的平方γ²成正比，而非传统的线性关系。这一设定基于稳态时权重范数稳定的理论推导，能实现更稳定的训练动态并提升模型性能，为优化器设计提供了新的理论基础。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [57] [PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning](https://arxiv.org/abs/2512.08218)
*Ye Qin,Jingchao Wang,Yang Shi,Haiying Huang,Junxu Li,Weijian Liu,Tinghui Chen,Jinghui Qin*

Main category: cs.LG

TL;DR: PR-CapsNet将胶囊网络扩展到伪黎曼流形，通过自适应曲率路由解决传统胶囊网络在固定曲率空间中建模复杂图结构的局限性，显著提升了图表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统胶囊网络在固定曲率欧几里得空间中建模复杂图结构存在局限性，而伪黎曼流形能为图数据提供更好的归纳偏置，但如何利用伪黎曼几何改进胶囊网络尚未充分探索。

Method: 1. 伪黎曼切空间路由：将胶囊状态分解为球面-时间和欧几里得-空间子空间；2. 自适应曲率路由：通过可学习曲率张量和几何注意力自适应融合不同曲率空间特征；3. 伪黎曼胶囊分类器：将胶囊嵌入投影到切空间并使用曲率加权softmax进行分类。

Result: 在节点和图分类基准测试中，PR-CapsNet超越了最先进的模型，验证了其对复杂图结构的强大表示能力。

Conclusion: PR-CapsNet成功将胶囊网络扩展到伪黎曼流形，通过自适应曲率机制有效建模复杂图结构，为图表示学习提供了新的几何视角。

Abstract: Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\-world graphs poorly by fixed\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\-Euclidean pseudo\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\-Riemannian manifolds and derive a Pseudo\-Riemannian Capsule Network (PR\-CapsNet), which models data in pseudo\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\-CapsNet enhances the CapsNet with Adaptive Pseudo\-Riemannian Tangent Space Routing by utilizing pseudo\-Riemannian geometry. Unlike single\-curvature or subspace\-partitioning methods, PR\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\-Riemannian metric. It first deploys Pseudo\-Riemannian Tangent Space Routing to decompose capsule states into spherical\-temporal and Euclidean\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\-preserved Pseudo\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\-CapsNet outperforms SOTA models, validating PR\-CapsNet's strong representation power for complex graph structures.

</details>


### [58] [Persistent Topological Structures and Cohomological Flows as a Mathematical Framework for Brain-Inspired Representation Learning](https://arxiv.org/abs/2512.08241)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Shipra Prashant*

Main category: cs.LG

TL;DR: 提出基于持久拓扑结构与上同调流的数学严谨框架，将神经计算重构为动态单纯复形上的上链映射演化，实现跨时空和功能脑状态的表示学习。


<details>
  <summary>Details</summary>
Motivation: 建立大脑启发的表示学习的数学严谨基础，通过拓扑结构和上同调流的相互作用来捕捉神经表示中的不变性特征。

Method: 将神经计算重构为动态单纯复形上的上链映射演化，结合代数拓扑与微分几何构建上同调算子，使用持久同调、层上同调和谱拉普拉斯分析合成与真实神经数据。

Result: 模型在流形一致性和噪声鲁棒性方面优于图神经网络和基于流形的深度架构，为拓扑驱动的表示学习建立了连贯的数学基础。

Conclusion: 该框架为大脑启发的表示学习提供了数学严谨的基础，通过拓扑结构和上同调流的相互作用实现了对神经表示中不变性特征的捕捉，在多个指标上优于现有方法。

Abstract: This paper presents a mathematically rigorous framework for brain-inspired representation learning founded on the interplay between persistent topological structures and cohomological flows. Neural computation is reformulated as the evolution of cochain maps over dynamic simplicial complexes, enabling representations that capture invariants across temporal, spatial, and functional brain states. The proposed architecture integrates algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning within a homological landscape. Synthetic data with controlled topological signatures and real neural datasets are jointly analyzed using persistent homology, sheaf cohomology, and spectral Laplacians to quantify stability, continuity, and structural preservation. Empirical results demonstrate that the model achieves superior manifold consistency and noise resilience compared to graph neural and manifold-based deep architectures, establishing a coherent mathematical foundation for topology-driven representation learning.

</details>


### [59] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: SPROCKET是一种基于原型的新型时间序列特征工程方法，在UCR和UEA数据集上表现与现有卷积算法相当，其MR-HY-SP集成模型超越了之前最好的卷积集成HYDRA-MR。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类算法主要依赖特征工程策略，其中ROCKET通过随机核特征取得了优异性能。研究者希望开发一种基于原型的新特征工程策略来进一步提升时间序列分类的性能。

Method: SPROCKET（Selected Prototype Random Convolutional Kernel Transform）采用基于原型的特征工程策略，通过原型选择来构建卷积核特征。同时提出了MR-HY-SP集成模型，将MultiROCKET、HYDRA和SPROCKET三种方法结合。

Result: 在大多数UCR和UEA时间序列分类数据集上，SPROCKET取得了与现有卷积算法相当的性能。MR-HY-SP集成模型的平均准确率排名超过了之前最好的卷积集成HYDRA-MR。

Conclusion: 基于原型的特征变换方法能够提升时间序列分类的准确性和鲁棒性，为时间序列分类提供了新的有效特征工程策略。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [60] [Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations](https://arxiv.org/abs/2512.08256)
*Deepak Gupta,Himanshu Pandey,Ratikanta Behera*

Main category: cs.LG

TL;DR: 提出基于小波的物理信息量子神经网络框架，用于高效求解具有尖锐梯度、刚度、快速局部变化和高振荡行为的多尺度偏微分方程，相比传统方法减少计算复杂度并提高精度。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络(PINNs)及其量子版本(量子PINNs)在求解多尺度特征时面临挑战，且依赖自动微分构建损失函数导致计算开销大、训练时间长。

Method: 开发小波加速的物理信息量子神经网络，消除对自动微分的依赖，将小波的多分辨率特性融入量子神经网络架构，增强网络捕捉多尺度问题局部和全局特征的能力。

Result: 相比经典小波PINNs，所需可训练参数少于5%，收敛更快；相比现有量子PINNs，速度提升3-5倍，在求解多尺度和振荡问题时表现出优越精度。

Conclusion: 所提出的框架通过结合小波多分辨率特性和量子神经网络优势，为高效求解具有挑战性的多尺度和振荡问题提供了有前景的方法。

Abstract: This work proposes a wavelet-based physics-informed quantum neural network framework to efficiently address multiscale partial differential equations that involve sharp gradients, stiffness, rapid local variations, and highly oscillatory behavior. Traditional physics-informed neural networks (PINNs) have demonstrated substantial potential in solving differential equations, and their quantum counterparts, quantum-PINNs, exhibit enhanced representational capacity with fewer trainable parameters. However, both approaches face notable challenges in accurately solving multiscale features. Furthermore, their reliance on automatic differentiation for constructing loss functions introduces considerable computational overhead, resulting in longer training times. To overcome these challenges, we developed a wavelet-accelerated physics-informed quantum neural network that eliminates the need for automatic differentiation, significantly reducing computational complexity. The proposed framework incorporates the multiresolution property of wavelets within the quantum neural network architecture, thereby enhancing the network's ability to effectively capture both local and global features of multiscale problems. Numerical experiments demonstrate that our proposed method achieves superior accuracy while requiring less than five percent of the trainable parameters compared to classical wavelet-based PINNs, resulting in faster convergence. Moreover, it offers a speedup of three to five times compared to existing quantum PINNs, highlighting the potential of the proposed approach for efficiently solving challenging multiscale and oscillatory problems.

</details>


### [61] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 提出一个统一的几何-随机多模态深度学习框架，整合EEG、ECG、呼吸、SpO2、EMG和fMRI信号，用于建模癫痫猝死和脑卒中易感性。


<details>
  <summary>Details</summary>
Motivation: 癫痫猝死和急性缺血性脑卒中是涉及皮层、脑干和自主神经系统复杂相互作用的危及生命的疾病，需要综合多模态信号进行早期检测和风险分层。

Method: 结合黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流建模和跨模态注意力机制，使用分数流行病扩散在结构脑图上模拟卒中传播。

Result: 在MULTI-CLARID数据集上实验显示提高了预测准确性，并获得了可解释的生物标志物，包括流形曲率、分数记忆指数、注意力熵和扩散中心性。

Conclusion: 该框架为神经自主神经疾病的早期检测、风险分层和可解释的多模态建模提供了数学原理基础。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [62] [Mathematical Foundations of Neural Tangents and Infinite-Width Networks](https://arxiv.org/abs/2512.08264)
*Rachana Mysore,Preksha Girish,Kavitha Jayaram,Shrey Kumar,Preksha Girish,Shravan Sanjeev Bagal,Kavitha Jayaram,Shreya Aravind Shastry*

Main category: cs.LG

TL;DR: 该论文提出NTK-ECRN网络架构，通过傅里叶特征嵌入、残差连接和随机深度等技术，在无限宽度理论框架下分析神经正切核的演化动态，建立了谱特性与泛化性能的理论联系。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在无限宽度机制下的数学基础，特别是神经正切核的理论分析，旨在建立无限宽度理论与实际深度学习架构之间的桥梁。

Method: 提出NTK-ECRN架构，整合傅里叶特征嵌入、带层间缩放的残差连接和随机深度技术，通过理论推导分析NTK动态演化、特征值变化，并建立谱特性与泛化、优化稳定性的联系。

Result: 在合成和基准数据集上的实验验证了预测的核行为，展示了改进的训练稳定性和泛化性能，为无限宽度理论与实际架构提供了综合分析框架。

Conclusion: 该工作提供了一个连接无限宽度理论与实际深度学习架构的全面框架，通过NTK-ECRN架构实现了对核演化的严格分析，为理解神经网络训练动态提供了理论基础。

Abstract: We investigate the mathematical foundations of neural networks in the infinite-width regime through the Neural Tangent Kernel (NTK). We propose the NTK-Eigenvalue-Controlled Residual Network (NTK-ECRN), an architecture integrating Fourier feature embeddings, residual connections with layerwise scaling, and stochastic depth to enable rigorous analysis of kernel evolution during training. Our theoretical contributions include deriving bounds on NTK dynamics, characterizing eigenvalue evolution, and linking spectral properties to generalization and optimization stability. Empirical results on synthetic and benchmark datasets validate the predicted kernel behavior and demonstrate improved training stability and generalization. This work provides a comprehensive framework bridging infinite-width theory and practical deep-learning architectures.

</details>


### [63] [SOFA-FL: Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing](https://arxiv.org/abs/2512.08267)
*Yi Ni,Xinkun Wang,Han Zhang*

Main category: cs.LG

TL;DR: SOFA-FL是一个自组织分层联邦学习框架，通过动态聚类、拓扑重构和自适应数据共享来解决数据异构性和固定网络拓扑的挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在动态环境中面临数据异构性和固定网络拓扑的挑战，需要能够自适应演化的解决方案。

Method: 提出SOFA-FL框架，包含三个核心机制：1) DMAC动态多分支凝聚聚类构建初始分层结构；2) SHAPE自组织分层自适应传播与演化，通过嫁接、修剪、合并和净化操作动态重构拓扑；3) 自适应聚类数据共享，通过客户端与集群节点间的受控数据交换缓解数据异构性。

Result: 该框架能够有效捕捉客户端间的动态关系，增强个性化能力，且不依赖预定义的集群结构。

Conclusion: SOFA-FL为动态环境中的联邦学习提供了自组织、自适应的分层解决方案，能够应对数据分布变化和网络拓扑僵化问题。

Abstract: Federated Learning (FL) faces significant challenges in evolving environments, particularly regarding data heterogeneity and the rigidity of fixed network topologies. To address these issues, this paper proposes \textbf{SOFA-FL} (Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing), a novel framework that enables hierarchical federated systems to self-organize and adapt over time.
  The framework is built upon three core mechanisms: (1) \textbf{Dynamic Multi-branch Agglomerative Clustering (DMAC)}, which constructs an initial efficient hierarchical structure; (2) \textbf{Self-organizing Hierarchical Adaptive Propagation and Evolution (SHAPE)}, which allows the system to dynamically restructure its topology through atomic operations -- grafting, pruning, consolidation, and purification -- to adapt to changes in data distribution; and (3) \textbf{Adaptive Clustered Data Sharing}, which mitigates data heterogeneity by enabling controlled partial data exchange between clients and cluster nodes.
  By integrating these mechanisms, SOFA-FL effectively captures dynamic relationships among clients and enhances personalization capabilities without relying on predetermined cluster structures.

</details>


### [64] [gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs](https://arxiv.org/abs/2512.08274)
*Humera Sabir,Fatima Farooq,Ashraf Aboulnaga*

Main category: cs.LG

TL;DR: gHAWK是一个可扩展的图神经网络训练框架，通过预计算捕捉节点局部和全局结构的特征来加速大型知识图谱上的GNN训练。


<details>
  <summary>Details</summary>
Motivation: 现有的消息传递GNN在大型知识图谱上难以扩展，因为它们依赖迭代的消息传递过程来学习图结构，这在mini-batch训练中效率低下，节点只能看到其邻域的局部视图。

Method: gHAWK引入预处理步骤：1) 使用Bloom过滤器紧凑编码局部邻域结构；2) 使用TransE嵌入表示节点在图中全局位置。这些结构特征与领域特定特征融合，生成可融入任何GNN技术的节点特征向量。

Result: 在Open Graph Benchmark大型数据集上的广泛实验表明，gHAWK在节点属性预测和链接预测任务上实现了最先进的准确性和更低的训练时间，在三个图的OGB排行榜上名列前茅。

Conclusion: 通过用结构先验增强消息传递训练，gHAWK显著减少了内存使用、加速了收敛并提高了模型准确性，为大型知识图谱上的可扩展GNN训练提供了有效解决方案。

Abstract: Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.

</details>


### [65] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process](https://arxiv.org/abs/2512.08451)
*Gary Ackerman,Zachary Kallenborn,Anna Wetzel,Hayley Peterson,Jenna LaTourette,Olivia Shoemaker,Brandon Behlendorf,Sheriff Almakki,Doug Clifford,Noah Sheinbaum*

Main category: cs.LG

TL;DR: 本文介绍了细菌生物威胁基准数据集(B3)的构建过程，这是生物威胁基准生成框架的第二部分，通过三种方法生成了1010个最终基准，用于评估AI模型在生物安全方面的风险。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型（特别是大语言模型）可能被用于生物恐怖主义或获取生物武器，这引起了政策、学术和公众的广泛关注。模型开发者和政策制定者需要量化和减轻这种风险，因此需要开发能够评估特定模型生物安全风险的基准测试。

Method: 采用三种互补方法生成基准：1) 基于网络的提示生成，2) 红队测试，3) 挖掘现有基准语料库。生成了7000多个潜在基准，然后通过去重、提升诊断性评估和质量控制措施，最终筛选出1010个基准。

Result: 成功开发了包含1010个最终基准的细菌生物威胁基准数据集(B3)。这些基准具有提升诊断性、与生物安全威胁直接相关，并且与更大的生物安全架构保持一致，允许在不同分析层次进行细致分析。

Conclusion: B3数据集为评估AI模型在生物安全方面的风险提供了系统化的基准测试工具，有助于模型开发者和政策制定者更好地理解和减轻前沿AI技术可能带来的生物安全威胁。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.

</details>


### [66] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: JARF通过计算随机森林预测的梯度，构建全局线性预处理器来旋转特征空间，使轴对齐决策树能处理旋转或交互依赖的决策边界，在保持简单性的同时接近斜向森林的性能。


<details>
  <summary>Details</summary>
Motivation: 轴对齐决策树在处理旋转或特征交互依赖的决策边界时效果不佳，而斜向森林虽然能解决这些问题但计算成本高且实现复杂。需要一种既能处理复杂边界又能保持简单性的方法。

Method: 首先训练轴对齐随机森林来估计类别概率或回归输出，然后计算预测相对于每个特征的有限差分梯度，将这些梯度聚合成期望雅可比外积（EGOP的推广），作为全局线性预处理器。这个监督预处理器对特征空间进行单一全局旋转，然后将转换后的数据交给标准轴对齐森林处理。

Result: 在表格分类和回归基准测试中，这种预处理方法持续改进了轴对齐森林的性能，通常匹配或超过斜向基线方法，同时提高了训练时间。实验结果表明监督预处理可以恢复斜向森林的大部分准确性，同时保持轴对齐树的简单性和鲁棒性。

Conclusion: JARF通过监督预处理器提供了一种简单有效的方法，使轴对齐决策树能够处理复杂的决策边界问题，在保持计算效率和实现简单性的同时，获得了接近斜向森林的性能表现。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [67] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: 本文介绍了细菌生物威胁基准（B3）数据集的试点实施，这是生物威胁基准生成（BBG）框架的第三篇论文，展示了使用前沿AI模型评估生物安全风险的方法。


<details>
  <summary>Details</summary>
Motivation: 前沿人工智能模型（特别是大语言模型）可能被用于生物恐怖主义或获取生物武器，这引起了政策制定者、学术界和公众的广泛关注。需要开发能够评估特定模型生物安全风险的基准测试方法。

Method: 通过试点实施B3数据集，使用样本前沿AI模型运行基准测试，然后进行人工评估模型响应，并从多个维度对结果进行应用风险分析。

Result: 试点研究表明，B3数据集提供了一种可行且细致的方法，能够快速评估大语言模型的生物安全风险，识别风险的关键来源，并为优先缓解领域提供指导。

Conclusion: B3基准测试框架为评估AI模型的生物安全风险提供了有效的工具，有助于模型开发者和政策制定者量化并缓解相关风险，特别是在生物武器获取和生物恐怖主义方面的潜在威胁。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [68] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 提出MAN正则化技术，通过最小化客户端模型每层激活范数来约束FL优化问题的平坦性，提高联邦学习模型的泛化性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习训练可能导致全局模型收敛到"尖锐最小值"，从而影响模型的泛化能力。需要改进联邦学习框架下的模型泛化性能。

Method: 引入平坦性约束的FL优化问题，通过最小化客户端模型每层激活范数（MAN正则化）来降低Hessian矩阵的最大特征值，确保收敛到平坦最小值。

Result: 将提出的平坦性约束优化应用于现有FL技术，获得了显著改进，建立了新的最先进性能。

Conclusion: MAN正则化技术通过约束平坦性有效提高了联邦学习模型的泛化能力，理论证明和实验验证了其有效性。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [69] [Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction](https://arxiv.org/abs/2512.08499)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出两种基于距离感知的不确定性方法PG-SNGP和PG-SNER，用于旋转机械轴承退化预测，通过谱归一化和物理引导神经网络提高OOD泛化能力和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性方法在安全关键系统（如旋转机械轴承）中存在置信度校准不足、计算成本高、缺乏距离感知能力、在分布外数据下泛化能力差等问题，需要更可靠的不确定性估计方法。

Method: 1) 提出PG-SNGP：基于谱归一化高斯过程，在隐藏层应用谱归一化保持输入到潜在空间的距离，用高斯过程层替换最终密集层实现距离敏感的不确定性；2) 提出PG-SNER：基于深度证据回归，输出正态逆伽马参数以连贯概率形式建模不确定性；3) 设计基于皮尔逊相关系数的新距离感知度量；4) 在损失函数中设计动态加权方案平衡数据保真度和物理一致性。

Result: 在PRONOSTIA轴承退化数据集上测试，与蒙特卡洛和深度集成PGNNs相比，PG-SNGP和PG-SNER提高了预测精度，在OOD条件下可靠泛化，对对抗攻击和噪声保持鲁棒性。

Conclusion: PG-SNGP和PG-SNER方法能够提供准确且距离感知的不确定性估计，在安全关键系统的预测性维护中具有实际应用价值，特别是在分布外数据条件下表现出色。

Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.

</details>


### [70] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 提出了一种考虑标签依赖关系的多标签数据集采样算法，通过估计多元伯努利分布参数和计算标签组合权重，生成更平衡的子样本以增强少数类别的代表性。


<details>
  <summary>Details</summary>
Motivation: 多标签数据集中标签通常不互斥且频率差异很大，传统采样方法难以在保持标签依赖关系的同时获得足够少数标签的样本，也无法控制样本与总体频率的偏差。

Method: 使用多元伯努利分布作为多标签问题的底层分布，基于观测标签频率估计分布参数，计算每个标签组合的权重，设计考虑标签依赖关系的加权采样算法。

Result: 将方法应用于Web of Science的64个生物医学主题类别研究文章样本，成功保持了类别频率顺序，减少了最常⻅和最不常⻅类别间的频率差异，同时考虑了类别依赖关系。

Conclusion: 该方法能够生成更平衡的子样本，显著增强了少数类别的代表性，为处理多标签不平衡数据集提供了一种有效的采样解决方案。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [71] [A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks](https://arxiv.org/abs/2512.08567)
*Nader Sadek,Mirette Moawad,Christina Naguib,Mariam Elzahaby*

Main category: cs.LG

TL;DR: 该研究提出了一种多模态方法，结合公司新闻和历史股价数据来改进股票市场预测，使用图神经网络（GNN）模型优于传统的LSTM基线模型。


<details>
  <summary>Details</summary>
Motivation: 股票市场预测是金融领域的长期挑战，传统模型主要依赖历史价格数据，但近期研究表明金融新闻可以提供有用的外部信号，因此需要探索如何有效整合这些多源信息来提升预测性能。

Method: 采用多模态方法整合公司新闻和历史股票数据：1）使用LSTM编码每家公司的历史数据；2）使用语言模型嵌入新闻标题；3）将这些嵌入作为异构图中的节点；4）使用GraphSAGE捕捉文章、公司和行业之间的交互关系。比较了GNN模型与LSTM基线模型，评估了两种目标：二元方向变化标签和基于重要性的标签。

Result: 在美国股票和彭博数据集上的实验显示：1）GNN模型优于LSTM基线，在第一个目标上达到53%的准确率，在第二个目标上获得4%的精确度提升；2）拥有更多相关新闻的公司预测准确率更高；3）新闻标题比完整文章包含更强的预测信号，表明简洁的新闻摘要在短期市场反应中起重要作用。

Conclusion: 多模态方法通过整合新闻和历史数据有效提升了股票市场预测性能，图神经网络能够捕捉不同实体间的复杂关系，新闻标题作为简洁摘要对短期市场反应具有重要预测价值，为金融预测提供了新的技术路径。

Abstract: Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.

</details>


### [72] [Fully Decentralized Certified Unlearning](https://arxiv.org/abs/2512.08443)
*Hithem Lamri,Michail Maniatakos*

Main category: cs.LG

TL;DR: 提出RR-DU方法，在去中心化网络中实现认证的机器遗忘，通过随机游走结合梯度操作、子采样高斯噪声和信任区域投影，提供收敛保证、隐私证书和删除容量界限。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘需要从训练模型中移除指定数据以响应隐私请求或数据中毒。虽然认证遗忘已在集中式和联邦学习设置中分析，但去中心化设置（无协调器的对等通信）仍未被充分探索。

Method: 提出RR-DU方法：在遗忘客户端对遗忘集执行一步投影梯度上升，在其他地方对保留数据执行几何分布数量的投影下降步骤，结合子采样高斯噪声和投影到原始模型周围的信任区域。

Result: 提供：(1)凸情况下的收敛保证和非凸情况下的平稳性保证；(2)通过子采样高斯Rényi DP的(ε,δ)网络遗忘证书；(3)与遗忘-本地数据比率成比例的删除容量界限。在MNIST和CIFAR-10上，RR-DU在给定(ε,δ)下比去中心化DP基线获得更高测试精度，并将遗忘精度降至随机猜测水平(≈10%)。

Conclusion: RR-DU是首个在去中心化网络中提供认证遗忘的方法，有效平衡隐私与效用，在图像基准测试中表现优于现有去中心化DP基线。

Abstract: Machine unlearning (MU) seeks to remove the influence of specified data from a trained model in response to privacy requests or data poisoning. While certified unlearning has been analyzed in centralized and server-orchestrated federated settings (via guarantees analogous to differential privacy, DP), the decentralized setting -- where peers communicate without a coordinator remains underexplored. We study certified unlearning in decentralized networks with fixed topologies and propose RR-DU, a random-walk procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps on the retained data elsewhere, combined with subsampled Gaussian noise and projection onto a trust region around the original model. We provide (i) convergence guarantees in the convex case and stationarity guarantees in the nonconvex case, (ii) $(\varepsilon,δ)$ network-unlearning certificates on client views via subsampled Gaussian $Rényi$ DP (RDP) with segment-level subsampling, and (iii) deletion-capacity bounds that scale with the forget-to-local data ratio and quantify the effect of decentralization (network mixing and randomized subsampling) on the privacy--utility trade-off. Empirically, on image benchmarks (MNIST, CIFAR-10), RR-DU matches a given $(\varepsilon,δ)$ while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing ($\approx 10\%$).

</details>


### [73] [Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?](https://arxiv.org/abs/2512.08798)
*Jeongwhan Choi,Woosung Kang,Minseo Kim,Jongwoo Kim,Noseong Park*

Main category: cs.LG

TL;DR: TabPFN-GN将图节点分类问题转化为表格学习问题，通过特征工程提取节点属性、结构特征、位置编码等，使TabPFN模型能在无需图特定训练的情况下实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 基于TabPFN在表格数据上的成功及其在时间序列上的扩展，研究是否可以将图节点分类问题有效地重新表述为表格学习问题，探索表格学习与图学习之间的桥梁

Method: 提出TabPFN-GN方法，将图数据转换为表格特征：提取节点属性、结构属性、位置编码，以及可选的平滑邻域特征，使TabPFN能够直接进行节点分类，无需图特定训练或语言模型依赖

Result: 在12个基准数据集上的实验表明，TabPFN-GN在同质图上与GNNs性能相当，在异质图上始终优于GNNs，证明了特征工程可以弥合表格和图领域之间的差距

Conclusion: 通过原则性特征工程，TabPFN-GN为图节点分类提供了实用的替代方案，避免了任务特定的GNN训练和LLM依赖的图基础模型，展示了表格学习在图领域的潜力

Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

</details>


### [74] [Differentially Private Synthetic Data Generation Using Context-Aware GANs](https://arxiv.org/abs/2512.08869)
*Anantaa Kotal,Anupam Joshi*

Main category: cs.LG

TL;DR: ContextGAN：一种上下文感知的差分隐私生成对抗网络，通过约束矩阵整合领域特定规则，生成既保护隐私又符合领域约束的高质量合成数据。


<details>
  <summary>Details</summary>
Motivation: 大数据应用引发隐私担忧，GDPR和HIPAA等法规对数据处理有严格限制。传统合成数据方法难以捕捉复杂隐式规则（如医疗领域的处方指南），导致生成的数据可能不符合领域约束，缺乏实用性和真实性。

Method: 提出ContextGAN（上下文感知差分隐私生成对抗网络），通过约束矩阵编码显式和隐式领域知识，约束感知判别器评估合成数据是否符合领域规则，同时使用差分隐私保护原始数据的敏感信息。

Result: 在医疗、安全和金融领域验证ContextGAN，结果显示其能生成高质量合成数据，既尊重领域规则又保护隐私，相比传统方法显著提升了数据的真实性和实用性。

Conclusion: ContextGAN通过整合领域约束和差分隐私保护，解决了合成数据在复杂领域应用中缺乏真实性和合规性的问题，适用于需要同时满足显式模式和隐式规则的应用场景。

Abstract: The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.

</details>


### [75] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: Fed-SE框架：针对LLM智能体在隐私约束下的联邦自进化学习，通过局部进化-全局聚合范式解决异构任务和稀疏奖励带来的梯度冲突问题


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂交互任务中广泛应用，但隐私约束限制了集中式优化和跨动态环境的协同进化。虽然联邦学习在静态数据集上有效，但在开放式的智能体自进化场景中仍待探索。直接应用标准联邦学习面临挑战：异构任务和稀疏的轨迹级奖励会导致严重的梯度冲突，破坏全局优化过程。

Method: 提出Fed-SE联邦自进化框架，采用局部进化-全局聚合范式。局部层面：智能体在过滤后的高回报轨迹上进行参数高效微调，实现稳定的梯度更新。全局层面：在低秩子空间内聚合更新，解耦环境特定的动态特性，有效减少客户端间的负迁移。

Result: 在五个异构环境中的实验表明，Fed-SE相比联邦基线方法将平均任务成功率提升了约18%，验证了其在隐私约束部署中实现鲁棒的跨环境知识转移的有效性。

Conclusion: Fed-SE框架成功解决了LLM智能体在隐私约束下的联邦自进化问题，通过局部稳定优化和全局解耦聚合，实现了跨异构环境的有效知识转移，为隐私保护的智能体协同进化提供了可行方案。

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [76] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 提出一种结合fMRI数据和DICOM元数据的transformer多模态框架，用于解码脑状态，提升准确性、可解释性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习方法在处理fMRI数据时，未能充分利用DICOM元数据提供的丰富上下文信息，限制了脑状态解码的准确性和应用潜力

Method: 采用基于transformer的架构，整合多模态输入（包括fMRI数据和DICOM元数据），利用注意力机制捕捉复杂的时空模式和上下文关系

Result: 提出的方法增强了模型的准确性、可解释性和鲁棒性，在临床诊断、认知神经科学和个性化医疗等领域具有应用潜力

Conclusion: 该框架有效利用了DICOM元数据的上下文信息，提升了fMRI脑状态解码性能，同时讨论了元数据可变性和计算需求等局限性，并提出了未来优化可扩展性和泛化性的方向

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [77] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 研究发现LLM生成表格数据时存在隐私泄露风险，数字序列容易被记忆和复现，提出LevAtt攻击方法和两种防御策略


<details>
  <summary>Details</summary>
Motivation: LLM在生成高质量表格合成数据方面表现出色，但现有方法（微调小模型或提示大模型）存在隐私风险，可能泄露训练数据中的数字模式

Method: 提出LevAtt黑盒成员推理攻击，仅访问生成的合成数据，针对数字序列进行攻击；同时提出两种防御方法，包括在生成过程中策略性扰动数字的采样策略

Result: 攻击方法在多种模型和数据集上暴露了显著的隐私泄露，在某些情况下甚至成为完美的成员分类器；提出的防御方法能以最小的保真度和效用损失抵御这些攻击

Conclusion: LLM基于合成数据生成存在独特的隐私漏洞，需要有效的防御措施；提出的数字扰动策略能够有效保护隐私同时保持数据质量

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [78] [Solving Over-Smoothing in GNNs via Nonlocal Message Passing: Algebraic Smoothing and Depth Scalability](https://arxiv.org/abs/2512.08475)
*Weiqi Guan,Junlin He*

Main category: cs.LG

TL;DR: 论文提出了一种解决图神经网络中Layer Normalization位置困境的新方法：基于Post-LN架构诱导代数平滑，避免过平滑和深度诅咒，支持更深网络（达256层）且无需额外参数。


<details>
  <summary>Details</summary>
Motivation: Layer Normalization在图神经网络中的位置选择存在关键困境：Pre-LN架构能避免过平滑但受深度诅咒影响，Post-LN架构能绕过深度诅咒但会出现过平滑现象。两者关系尚未充分探索，需要找到能同时解决这两个问题的方法。

Method: 提出基于Post-LN架构的新方法，通过诱导代数平滑来防止过平滑，同时避免深度诅咒。该方法参数高效，无需额外参数，支持构建更深层的网络结构。

Result: 在五个基准测试上的实证结果表明，该方法能支持更深网络（最多256层），并提升性能表现。相比传统方法，在保持参数效率的同时解决了过平滑和深度诅咒问题。

Conclusion: 成功解决了Layer Normalization位置选择的困境，提出的方法既能避免过平滑又能绕过深度诅咒，为构建更深层图神经网络提供了理论支持和实用解决方案。

Abstract: The relationship between Layer Normalization (LN) placement and the over-smoothing phenomenon remains underexplored. We identify a critical dilemma: Pre-LN architectures avoid over-smoothing but suffer from the curse of depth, while Post-LN architectures bypass the curse of depth but experience over-smoothing.
  To resolve this, we propose a new method based on Post-LN that induces algebraic smoothing, preventing over-smoothing without the curse of depth. Empirical results across five benchmarks demonstrate that our approach supports deeper networks (up to 256 layers) and improves performance, requiring no additional parameters.
  Key contributions:
  Theoretical Characterization: Analysis of LN dynamics and their impact on over-smoothing and the curse of depth.
  A Principled Solution: A parameter-efficient method that induces algebraic smoothing and avoids over-smoothing and the curse of depth.
  Empirical Validation: Extensive experiments showing the effectiveness of the method in deeper GNNs.

</details>


### [79] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: DAO-GP是一种新型的漂移感知在线高斯过程模型，能够动态适应数据分布变化，无需手动调整超参数，具有内存高效和稀疏特性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现随时间变化的数据分布（概念漂移），忽视这一现象会显著降低模型预测精度。传统在线高斯过程方法存在多个关键限制：缺乏漂移感知能力、依赖固定超参数、易受数据窥探影响、缺乏理论衰减机制和内存效率低下。

Method: 提出DAO-GP（漂移感知在线高斯过程），这是一种完全自适应、无超参数、衰减和稀疏的非线性回归模型。该模型内置漂移检测和适应机制，能够根据漂移严重程度动态调整模型行为。

Result: 广泛的实证评估证实DAO-GP在平稳条件、多种漂移类型（突变、增量、渐进）和不同数据特征下都具有鲁棒性。分析展示了其动态适应能力、高效的内存和衰减管理以及演化诱导点。与最先进的参数和非参数模型相比，DAO-GP始终实现优越或竞争性性能。

Conclusion: DAO-GP被确立为在线非线性回归的漂移弹性解决方案，能够有效应对概念漂移问题，提供动态适应和高效计算。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [80] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 提出一种针对离线强化学习的全局预算分配攻击策略，通过TD误差敏感度分配扰动预算，相比均匀扰动更高效且隐蔽


<details>
  <summary>Details</summary>
Motivation: 现有离线RL数据投毒攻击采用局部均匀扰动，对所有样本不加区分，导致效率低下（浪费扰动预算在低影响样本上）且缺乏隐蔽性（产生显著统计偏差）

Method: 提出全局预算分配攻击策略，基于理论洞察：样本对价值函数收敛的影响与其TD误差成正比。将攻击建模为全局资源分配问题，在全局L2约束下推导出闭式解，扰动幅度按TD误差敏感度比例分配

Result: 在D4RL基准测试中，该方法显著优于基线策略，能以最小扰动实现高达80%的性能下降，并能逃避最先进的统计和频谱防御检测

Conclusion: 全局预算分配攻击策略通过智能分配扰动预算，实现了对离线RL系统的高效、隐蔽攻击，揭示了现有防御机制的脆弱性

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [81] [Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training](https://arxiv.org/abs/2512.08894)
*Jakub Krajewski,Amitis Shidani,Dan Busbridge,Sam Wiseman,Jason Ramapuram*

Main category: cs.LG

TL;DR: 本文提出了一种直接建模基准性能随训练预算扩展的框架，挑战了传统上认为下游任务性能难以预测的观点。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型扩展定律主要关注预训练损失等代理指标，而预测下游任务性能被认为不可靠。本文旨在挑战这一观点，建立直接预测下游任务性能的扩展模型。

Method: 提出直接框架，在固定token-参数比条件下，使用简单幂律描述多个下游任务的对数准确率扩展行为。引入能跨token-参数比预测准确率并考虑重复采样推理计算的功能形式。

Result: 直接方法比之前提出的两阶段程序外推效果更好，后者容易产生复合误差。在参数达170亿、训练token达3500亿的模型上验证了发现，并发布了完整的预训练损失和下游评估结果。

Conclusion: 下游任务性能可以通过直接建模训练预算扩展来可靠预测，这挑战了传统观点。直接方法优于两阶段程序，新功能形式能跨不同token-参数比预测性能。

Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.

</details>


### [82] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 该研究构建了一个覆盖2004-05至2024-25赛季的纵向NBA数据集，并开发了基于LSTM的深度学习框架，通过长达9840场比赛（相当于8个完整赛季）的序列长度来捕捉球队长期表现趋势，在NBA比赛结果预测中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 预测NBA比赛结果对于教练策略、球迷参与和体育博彩越来越重要，但现有模型存在概念漂移、时间上下文有限和跨赛季不稳定性等问题，需要更强大的预测方法。

Method: 构建了覆盖20个赛季的纵向NBA数据集，设计了基于LSTM的深度学习框架，使用长达9840场比赛（8个赛季）的序列长度来建模长期表现趋势，并与逻辑回归、随机森林、MLP和CNN等传统ML和DL基线模型进行比较。

Result: LSTM模型在所有指标上表现最佳：准确率72.35%，精确率73.15%，AUC-ROC 76.13%，显著优于其他基线模型，证明了长序列时间建模在篮球结果预测中的重要性。

Conclusion: 研究表明，利用长序列时间建模能够有效捕捉球队动态演变和赛季间依赖关系，新构建的多赛季数据集对于开发稳健、可泛化的NBA预测系统具有重要价值。

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [83] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 本文改进了FedProxGrad算法在非凸复合优化问题中的收敛性分析，提出了DS FedProxGrad框架，证明了在Robbins-Monro步长调度下能够达到渐近平稳点，消除了方差诱导噪声底限的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有FedProxGrad算法在解决群公平联邦学习的非凸复合优化问题时，只能收敛到噪声主导的平稳邻域，且收敛性明确依赖于方差诱导的噪声底限。本文旨在改进这一收敛性分析，消除噪声底限的依赖。

Method: 提出了DS FedProxGrad（衰减步长FedProxGrad）分析框架，该框架包含不精确的局部近端解和显式公平正则化。采用Robbins-Monro步长调度，并在局部不精确性满足温和衰减条件下进行分析。

Result: 证明了liminf_{r→∞} E[‖∇F(x^r)‖^2] = 0，即算法达到渐近平稳点，收敛速率不再依赖于方差诱导的噪声底限。

Conclusion: DS FedProxGrad框架在适当条件下能够实现渐近收敛到平稳点，改进了原有FedProxGrad算法的收敛性分析，为群公平联邦学习提供了更优的理论保证。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [84] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 本文提出了一种基于动态时间规整和迁移学习的框架，用于增材制造零件认证，通过将低成本聚合物的应力-应变行为知识迁移到金属材料，提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 增材制造零件认证对确保关键应用中的零件质量和可靠性至关重要。由于增材制造零件的应力-应变行为复杂且预测困难，需要开发有效的方法来准确预测这些行为，特别是在金属材料成本高昂的情况下。

Method: 开发了DTW-TL（动态时间规整-迁移学习）框架：1）使用DTW算法选择与目标金属数据集最相关的聚合物数据集作为源域；2）采用LSTM模型进行知识迁移；3）使用四种聚合物（尼龙、PLA、CF-ABS、树脂）和三种金属（AlSi10Mg、Ti6Al4V、碳钢）进行验证。

Result: 实验结果表明：1）DTW-TL框架能准确识别聚合物与金属之间的最佳匹配，选择单一聚合物数据集作为源域；2）当三种金属作为目标域时，DTW-TL模型达到最低平均绝对百分比误差12.41%和最高决定系数0.96；3）性能优于无迁移学习的普通LSTM模型以及使用四种聚合物预训练的迁移学习模型。

Conclusion: 提出的DTW-TL框架能有效将聚合物材料的应力-应变行为知识迁移到金属材料，显著提高了增材制造零件应力-应变行为的预测精度，为增材制造零件认证提供了一种高效且成本较低的方法。

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [85] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: 本文提出了一种名为Bias-Guided Prompt Search (BGPS)的框架，用于自动生成能最大化文本到图像扩散模型中偏见的提示词，从而发现模型中的微妙偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型虽然视觉质量很高，但经常表现出性别、种族、年龄等敏感属性的社会偏见。现有的去偏见方法依赖于手动或LLM生成的提示词数据集，这种方法成本高且可能忽略那些能触发偏见的微妙提示词，即使模型已经过过去偏见处理。

Method: BGPS框架包含两个组件：(1) 一个被指令生成属性中性提示词的大型语言模型；(2) 作用于TTI内部表示的属性分类器，引导LLM的解码过程朝向能放大感兴趣图像属性的提示空间区域。该方法自动生成能最大化图像中偏见存在的提示词。

Result: 在Stable Diffusion 1.5和最新的去偏见模型上进行广泛实验，发现了一系列微妙且先前未记录的偏见，这些偏见严重降低了公平性指标。发现的提示词具有可解释性，可以像普通用户一样输入，在困惑度指标上相比硬提示优化方法有定量改进。

Conclusion: BGPS揭示了文本到图像模型的脆弱性，同时扩展了偏见搜索空间，可以作为偏见缓解的新评估工具。该方法能发现传统方法可能忽略的微妙偏见，为模型公平性评估提供了更全面的方法。

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [86] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 该研究引入神经常微分方程（NODEs）作为学习蛋白质组和代谢组复杂相互作用的动态框架，应用于工程大肠杆菌的时间序列数据，相比传统机器学习方法在预测精度和推理速度上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 人类健康寿命和生物工程的发展严重依赖于预测复杂生物系统的行为。虽然高通量多组学数据日益丰富，但将这些数据转化为可操作的预测模型仍然是一个瓶颈。需要高容量、数据驱动的模拟系统来直接从观测数据中推断潜在相互作用，实现时间轨迹模拟和预测干预效果。

Method: 引入神经常微分方程（NODEs）作为动态框架，用于学习蛋白质组和代谢组之间的复杂相互作用。将该框架应用于工程大肠杆菌菌株的时间序列数据，对代谢途径的连续动态进行建模。

Result: NODE架构在捕捉系统动态方面表现出优越性能：在柠檬烯途径数据集上RMSE改善高达94.38%，在异戊烯醇途径数据集上改善高达97.65%，均超过90%的改进。此外，NODE模型的推理时间加速了1000倍。

Conclusion: NODE模型被确立为可扩展、高保真的工具，适用于下一代代谢工程和生物发现，为个性化医疗和合成生物学中的下游干预效果预测提供了有效解决方案。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [87] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: LEAP提出了一种新的通用图提示调优方法，通过在所有节点添加提示来保持理论基础，同时使用强化学习选择节点并编辑提示以获得更理想的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的选择性节点图提示调优方法虽然追求更理想的提示，但破坏了通用图提示调优的理论基础。需要一种既能保持理论基础又能获得更好性能的方法。

Method: 提出LEAP模型：1）在所有节点构建基本通用图提示以保持理论基础；2）使用actor-critic强化学习选择节点并编辑提示以获得更理想的性能。

Result: 在图级和节点级任务的各种预训练策略下，无论是全样本还是少样本场景，LEAP都持续优于微调和其他基于提示的方法。

Conclusion: LEAP通过在所有节点添加提示并采用强化学习进行节点选择和提示编辑，既保持了通用图提示调优的理论基础，又实现了更好的性能表现。

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [88] [De novo generation of functional terpene synthases using TpsGPT](https://arxiv.org/abs/2512.08772)
*Hamsini Ramanathan,Roman Bushuiev,Matouš Soldát,Jirí Kohout,Téo Hebra,Joshua David Smith,Josef Sivic,Tomáš Pluskal*

Main category: cs.LG

TL;DR: TpsGPT是基于ProtGPT2微调生成的萜烯合酶设计模型，通过多指标验证筛选出7个候选酶，其中至少2个具有实验验证的酶活性。


<details>
  <summary>Details</summary>
Motivation: 萜烯合酶是合成抗癌药物等天然产物的关键酶，但传统的定向进化方法成本高、速度慢，需要开发更高效的酶设计方法。

Method: 在UniProt中挖掘79k个TPS序列，微调蛋白质语言模型ProtGPT2构建TpsGPT模型，生成新酶序列并通过酶分类、结构置信度、序列多样性、结构比对等多重验证指标筛选。

Result: 从28k个生成序列中筛选出7个满足所有验证标准的候选TPS酶，实验验证显示其中至少2个序列具有TPS酶活性。

Conclusion: 在精心策划的酶类特定数据集上微调蛋白质语言模型，结合严格筛选，能够从头生成功能性的、进化距离较远的酶。

Abstract: Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.

</details>


### [89] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 论文提出了一种利用提升模型进行反事实估计的方法，通过拟合双变量beta分布到预测的提升分数，获得反事实结果的后验分布。


<details>
  <summary>Details</summary>
Motivation: 提升建模只能估计干预的因果效应（处理组与对照组的潜在结果差异），而反事实识别旨在恢复这些潜在结果的联合分布（如"如果给客户提供营销优惠，他们是否仍会流失？"）。反事实分布比提升提供更丰富的信息但更难估计。然而这两种方法是协同的：可以利用提升模型进行反事实估计。

Method: 提出一种反事实估计器，将双变量beta分布拟合到预测的提升分数上，从而产生反事实结果的后验分布。该方法除了提升建模所需的因果假设外，不需要额外的因果假设。

Result: 模拟实验显示了该方法的有效性，可以应用于电信客户流失等问题，揭示了标准机器学习或单独提升模型无法获得的洞察。

Conclusion: 该方法成功地将提升建模与反事实估计相结合，通过利用提升分数来估计更丰富的反事实联合分布，为因果推断提供了更全面的分析工具。

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [90] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: WAAPO框架生成针对AI天气预报模型的对抗性扰动，通过通道稀疏性、空间定位和平滑性约束确保扰动有效且隐蔽，揭示了AI天气预报系统在对抗攻击下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在天气预报中的广泛应用，需要评估其对对抗性扰动的脆弱性，以识别潜在的安全风险并推动鲁棒性保障措施。

Method: 提出Weather Adaptive Adversarial Perturbation Optimization (WAAPO)框架，通过通道稀疏性、空间定位和平滑性约束生成对抗性扰动，使用ERA5数据集和FourCastNet模型进行实验验证。

Result: WAAPO能够生成与预定目标紧密对齐的对抗性轨迹，即使在约束条件下也能有效操纵天气预报结果，表明AI天气预报模型对初始条件的小扰动非常敏感。

Conclusion: AI驱动的天气预报模型存在严重脆弱性，微小的对抗性扰动可导致预测结果显著偏离，强调了在业务预报系统中实施鲁棒性保障措施的必要性。

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [91] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: 该论文提出STD(λ)算法，改进传统TD(λ)在函数逼近中的问题，通过关注状态值的相对排序而非绝对值来提升策略性能


<details>
  <summary>Details</summary>
Motivation: 传统TD(λ)算法在函数逼近中最小化状态值的平方误差，但对于策略学习而言，状态值的相对排序比绝对值更重要。作者发现TD(λ)即使从最优策略开始，也可能收敛到次优策略

Method: 提出STD(λ)算法，在二元决策问题中基于状态值的相对关系训练函数逼近器。该方法包含理论分析，包括在两状态系统中证明策略单调改进，并与Bertsekas的差分训练方法进行比较

Result: 在两状态系统和acrobot问题的变体上成功演示了STD(λ)算法的有效性。理论分析证明STD(λ)在两状态系统中能实现策略的单调改进

Conclusion: STD(λ)通过关注状态值的相对排序而非绝对值，解决了TD(λ)在函数逼近中的策略退化问题，为强化学习中的函数逼近提供了更有效的训练方法

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [92] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 提出一种文本到IMU运动合成框架，通过使用加速度二阶损失微调预训练扩散模型，提高合成IMU数据的真实性和加速度信号保真度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动模型生成的IMU数据在加速度信号保真度方面存在不足，需要专门针对IMU传感器特性进行优化，以提高合成数据的真实性和下游任务性能。

Method: 提出加速度二阶损失函数(L_acc)，强制生成运动在离散二阶时间差分上的一致性，将L_acc集成到现有扩散模型的训练目标中，通过微调获得IMU特定的运动先验，结合表面建模和虚拟传感器模拟框架生成IMU数据。

Result: L_acc损失相对原始模型降低12.7%，高动态活动改善更显著；合成IMU数据在低维嵌入中更接近真实IMU分布；仅使用精炼合成IMU数据训练的HAR分类性能比原始扩散模型提高8.7%，比最佳对比模型提高7.6%。

Conclusion: 加速度感知的扩散精炼为对齐运动生成和IMU合成提供了有效方法，展示了深度学习管道在将通用文本到运动先验专门化到传感器特定任务方面的灵活性。

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [93] [Explainable Anomaly Detection for Industrial IoT Data Streams](https://arxiv.org/abs/2512.08885)
*Ana Rita Paupério,Diogo Risca,Afonso Lourenço,Goreti Marreiros,Ricardo Martins*

Main category: cs.LG

TL;DR: 本文提出了一种协作式数据流挖掘框架，结合无监督异常检测和人机交互学习，用于工业维护中的实时故障检测。


<details>
  <summary>Details</summary>
Motivation: 工业维护在物联网和边缘计算推动下产生连续数据流，需要实时自适应决策，但实际应用中地面真实标签往往延迟或缺失，现有数据流挖掘方法大多假设完全监督设置，无法满足实际需求。

Method: 采用协作式数据流挖掘框架，集成无监督异常检测与交互式人机循环学习；使用在线隔离森林算法，通过增量部分依赖图和基于个体条件期望曲线与衰减平均偏差的特征重要性评分增强可解释性，允许用户动态重新评估特征相关性和调整异常阈值。

Result: 实现了实时系统，并在提花织机单元故障检测中提供了初步结果；正在进行的工作目标是持续监控以预测和解释即将发生的轴承故障。

Conclusion: 该框架通过结合无监督异常检测与人机交互学习，解决了工业维护中标签延迟或缺失的问题，为实时自适应决策提供了可行方案，并增强了模型的可解释性。

Abstract: Industrial maintenance is being transformed by the Internet of Things and edge computing, generating continuous data streams that demand real-time, adaptive decision-making under limited computational resources. While data stream mining (DSM) addresses this challenge, most methods assume fully supervised settings, yet in practice, ground-truth labels are often delayed or unavailable. This paper presents a collaborative DSM framework that integrates unsupervised anomaly detection with interactive, human-in-the-loop learning to support maintenance decisions. We employ an online Isolation Forest and enhance interpretability using incremental Partial Dependence Plots and a feature importance score, derived from deviations of Individual Conditional Expectation curves from a fading average, enabling users to dynamically reassess feature relevance and adjust anomaly thresholds. We describe the real-time implementation and provide initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.

</details>


### [94] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 提出一种基于拓扑数据分析的损失函数，用于无监督自动选择核密度估计的最优带宽


<details>
  <summary>Details</summary>
Motivation: 核密度估计中的带宽选择是一个关键但需要人工调优的超参数，它直接影响偏差-方差权衡和拓扑特征的平滑程度。传统方法需要人工干预，而拓扑数据分析可以在高维空间中量化拓扑特征，为自动化选择提供可能。

Method: 使用基于拓扑的损失函数构建无监督学习框架，通过量化密度估计的拓扑特征（如连通分量、环、空洞等）来自动选择最优带宽，无需人工干预。

Result: 该方法在不同维度下进行了基准测试，展示了其相对于经典技术的潜力，能够有效自动选择带宽并保持拓扑特征。

Conclusion: 基于拓扑的损失函数为核密度估计的带宽选择提供了一种有效的无监督自动化方法，特别适用于高维数据可视化困难的情况。

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [95] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: Open Polymer Challenge发布了首个聚合物信息学社区基准数据集，包含1万个聚合物和5种性质，通过多任务预测竞赛推动可持续聚合物材料的机器学习发现。


<details>
  <summary>Details</summary>
Motivation: 机器学习在发现可持续聚合物材料方面具有巨大潜力，但缺乏大规模、高质量、开放可访问的聚合物数据集限制了进展。需要建立社区基准来推动聚合物信息学发展。

Method: 发布包含10K聚合物和5种性质（热导率、回转半径、密度、自由体积分数、玻璃化转变温度）的数据集。组织多任务聚合物性质预测竞赛，参与者在真实约束下开发模型，包括小数据、标签不平衡、异质模拟源等挑战，采用特征增强、迁移学习、自监督预训练、目标集成策略等技术。

Result: 竞赛揭示了关于数据准备、分布偏移和跨组模拟一致性的重要经验教训，为未来大规模聚合物数据集的最佳实践提供了指导。产生的模型、分析和发布数据为聚合物科学中的分子AI建立了新基础。

Conclusion: Open Polymer Challenge通过发布首个社区基准数据集和竞赛，加速了可持续和节能材料的开发，为聚合物信息学建立了新的基础，并提供了数据生成管道供社区使用。

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [96] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: 该研究比较了数据导向设计(DOD)与传统面向对象设计(OOD)在多线程环境下的性能表现，通过A*搜索算法的四种实现版本进行测试，发现DOD在多线程场景中展现出更优的执行时间和缓存效率。


<details>
  <summary>Details</summary>
Motivation: 随着多核CPU与主内存性能差距的扩大，需要硬件感知的软件设计范式。本研究旨在全面分析数据导向设计(DOD)与传统面向对象设计(OOD)在缓存利用和多线程环境效率方面的性能差异。

Method: 开发并比较了A*搜索算法的四个版本：单线程OOD(ST-OOD)、单线程DOD(ST-DOD)、多线程OOD(MT-OOD)和多线程DOD(MT-DOD)。评估指标包括执行时间、内存使用和CPU缓存缺失。

Result: 在多线程测试中，DOD实现表现出显著的性能优势，执行时间更快，原始系统调用和缓存缺失更少。对于A*算法这类细粒度任务，线程管理开销导致单线程版本在两个范式中都明显优于多线程版本。

Conclusion: 即使在简单算法中性能差异看似细微，但DOD在关键指标上的持续优势凸显了其基础架构的优越性，表明它是最大化复杂、大规模AI和并行计算任务硬件效率的更有效方法。

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [97] [Can AI autonomously build, operate, and use the entire data stack?](https://arxiv.org/abs/2512.07926)
*Arvind Agarwal,Lisa Amini,Sameep Mehta,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.AI

TL;DR: 论文主张从AI辅助数据管理转向完全自主的数据资产，通过智能代理实现整个数据生命周期的自动化管理。


<details>
  <summary>Details</summary>
Motivation: 当前AI助手仅能辅助特定角色（如数据工程师、数据管理员）进行数据堆栈的配置和管理，但无法实现完全自动化。随着AI能力提升，处理传统上因复杂性而难以自动化的任务成为可能，这为构建完全自主的数据资产提供了契机。

Method: 提出范式转变：从AI在独立数据组件中的使用转向更整体、自主地处理整个数据生命周期。探索现代数据堆栈的每个阶段如何通过智能代理进行自主管理，构建不仅服务于人类终端用户，也能服务于AI自身的自足系统。

Result: 论文分析了推动这一范式转变的力量和机遇，探讨了代理如何简化数据生命周期，并指出了需要进一步研究的开放问题和领域。

Conclusion: 希望这项工作能激发活跃讨论，促进进一步研究，推动协作方法，为数据系统实现更自主的未来铺平道路。

Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.

</details>


### [98] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: SkipKV是一种无需训练的KV缓存压缩方法，通过句子级评分识别并移除高度相似的句子来减少推理过程中的KV缓存开销，同时保持语义连贯性，提高推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中KV缓存随思维链线性增长，导致内存和吞吐量瓶颈。现有KV缓存驱逐方法在多批次设置下因不稳定的令牌级评分和填充令牌导致的KV预算减少而无法保持准确性，且往往生成更长的序列。

Method: 提出SkipKV方法：1）引入句子评分指标识别并移除高度相似的句子，进行粗粒度的句子级序列移除；2）动态调整转向向量更新隐藏激活状态，抑制冗余生成，强制模型生成简洁响应。

Result: 在多个推理基准测试中，SkipKV在相似压缩预算下相比替代方法保持高达26.7%的准确性提升，同时生成长度减少1.6倍，吞吐量提高1.7倍。

Conclusion: SkipKV通过句子级KV压缩有效解决了大型推理模型在思维链推理中的KV缓存开销问题，在保持准确性的同时显著提高了推理效率。

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [99] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 开发了一个基于大语言模型的AI增强患者-临床试验匹配系统，旨在自动化筛选过程，减轻协调员负担，提高匹配效率


<details>
  <summary>Details</summary>
Motivation: 临床试验患者筛选目前仍依赖人工操作，过程耗时且资源密集，需要更高效、智能的解决方案来改善这一瓶颈

Method: 利用开源、具备推理能力的大语言模型，构建安全可扩展的系统，整合异构电子健康记录数据，生成结构化资格评估和可解释的推理链，支持人机协同审查

Result: 系统能够超越简单的二元分类，将资格表示为动态状态而非固定判定，识别当前匹配并提供未来可能使患者符合资格的行动建议

Conclusion: 该决策支持工具旨在减轻协调员负担，智能扩展每位患者考虑的试验范围，并保证所有AI生成输出的全面可审计性，有望改善临床试验招募流程

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [100] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 本研究对ChatGPT和DeepSeek两大LLM在教育研究领域进行综合评估，包括技术分析、实证实验和用户调查，比较它们在文本生成、编程和专业问题解决等方面的表现。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型在教育研究领域展现出巨大潜力，ChatGPT和DeepSeek作为当前领先模型，在数学、科学、医学、文学和编程方面表现出色，需要系统评估它们在实际应用中的性能差异和适用场景。

Method: 采用背景技术分析、实证实验和真实用户调查的综合评估方法。通过基准测试评估模型在文本生成、编程和专业问题解决方面的表现，并调查学生、教育工作者和研究人员的实际使用体验。

Result: ChatGPT在通用语言理解和文本生成方面表现优异，DeepSeek凭借效率优先的设计在编程任务中表现更佳。两个模型都能提供医学准确的诊断输出并有效解决复杂数学问题。用户调查揭示了这些模型在教育研究中的实际益处和局限性。

Conclusion: 研究为ChatGPT和DeepSeek在教育研究领域的应用提供了全面评估，揭示了各自的优势和适用场景，为推进教育研究领域的大语言模型应用提供了实证依据和实用指导。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [101] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 开发了一个用于糖尿病预测移动应用的可扩展后端系统，采用水平扩展、数据库分片和异步通信架构，成功处理1万并发用户，83%功能达到性能目标


<details>
  <summary>Details</summary>
Motivation: 全球糖尿病患病率上升需要早期检测，AI预测应用需要响应迅速且可扩展的后端架构来服务大规模用户群体

Method: 采用水平扩展、数据库分片和通过消息队列（RabbitMQ）的异步通信架构，设计可扩展的后端系统

Result: 83%的系统功能（24项中的20项）达到性能目标（故障率低于5%，平均延迟低于1000ms），能处理1万并发用户，异步通信显著降低了计算密集型预测请求的错误率

Conclusion: 开发的可扩展后端系统有效支持糖尿病预测移动应用，满足大规模用户需求，异步通信机制在重负载下确保系统可靠性和数据完整性

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [102] [Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions](https://arxiv.org/abs/2512.08230)
*Eunice Yiu,Kelsey Allen,Shiry Ginosar,Alison Gopnik*

Main category: cs.AI

TL;DR: 论文探讨了因果学习在人类认知中的重要性，提出"赋能"（empowerment）作为连接贝叶斯因果学习与强化学习的桥梁，并通过实证研究检验儿童和成人如何利用赋能线索推断因果关系和设计有效干预。


<details>
  <summary>Details</summary>
Motivation: 理解世界的因果结构是人类认知的基本问题。虽然大型预训练模型在因果学习方面存在困难，但认知科学家已成功应用因果贝叶斯网络形式化来理解人类因果学习。本文旨在探索"赋能"这一概念如何连接经典贝叶斯因果学习与强化学习，并解释人类（特别是儿童）的因果学习机制。

Method: 采用理论分析和实证研究相结合的方法。理论上，探讨赋能（行动与结果间互信息最大化）作为因果学习的内在奖励信号。实证上，系统测试儿童和成人如何利用赋能线索推断因果关系，并设计有效的因果干预措施。

Result: 研究发现赋能可以作为连接贝叶斯因果学习与强化学习的重要桥梁。当智能体学习准确的因果世界模型时，其赋能会相应增加；反之，增加赋能也会导致更准确的因果世界模型。赋能还能解释儿童因果学习的独特特征，并提供更易处理的因果学习计算解释。

Conclusion: 赋能概念为理解人类因果学习提供了新的理论框架，特别是解释了儿童学习特征，并为机器实现因果学习提供了更可行的计算路径。该研究为因果学习领域提供了重要的理论贡献和实证支持。

Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.

</details>


### [103] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: 该论文提出了KPI框架，通过知识图谱增强、原型感知和可解释性技术，从患者基本信息预测疾病，解决现有方法在数据不平衡和可解释性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 仅基于患者基本信息（如人口统计学和自述症状）预测疾病具有重要价值，可提高患者意识、促进早期医疗参与和提升医疗系统效率。但现有方法面临疾病分布不平衡和缺乏可解释性等关键挑战，导致预测结果存在偏差或不可靠。

Method: 提出KPI框架：1）将结构化可信医疗知识整合为统一疾病知识图谱；2）构建具有临床意义的疾病原型；3）使用对比学习提高预测准确性，特别是对长尾疾病；4）利用大语言模型生成患者特异性、医学相关的解释，提高可解释性和可靠性。

Result: 在真实世界数据集上的广泛实验表明，KPI在预测准确性方面优于最先进方法，并能提供与患者叙述高度一致的临床有效解释，突显其在以患者为中心的医疗保健服务中的实用价值。

Conclusion: KPI框架通过整合医疗知识图谱、构建疾病原型、采用对比学习和利用大语言模型生成解释，有效解决了疾病预测中的不平衡数据和可解释性问题，为患者为中心的医疗保健提供了实用解决方案。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [104] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 最新推理模型在CFA考试中表现出色，多个模型通过所有三个级别，其中Gemini 3.0 Pro在Level I取得97.6%的历史最高分


<details>
  <summary>Details</summary>
Motivation: 先前研究表明大语言模型在CFA考试中表现不佳，但近期推理模型在各种专业考试中取得优异成绩，需要评估这些模型在CFA考试中的实际表现

Method: 使用980道模拟CFA考题（包括Level I三个考试、Level II两个考试、Level III三个考试），评估最先进的推理模型，采用与先前研究相同的通过/不通过标准

Result: 大多数模型通过了所有三个级别，按总体表现排序：Gemini 3.0 Pro、Gemini 2.5 Pro、GPT-5、Grok 4、Claude Opus 4.1、DeepSeek-V3.1。Gemini 3.0 Pro在Level I获得97.6%的历史最高分，GPT-5在Level II以94.3%领先，Gemini 2.5 Pro在Level III选择题中获得86.4%最高分，Gemini 3.0 Pro在构建回答题中获得92.0%

Conclusion: 最新推理模型在CFA考试中表现出色，显著超越了先前大语言模型的性能，表明这些模型在专业金融知识评估方面具有强大能力

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [105] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: 该研究提出使用生成式智能体来评估AI生成内容的质量，以解决传统人工评估成本高、效率低的问题，帮助企业实现高效、自动化的内容生成与评估。


<details>
  <summary>Details</summary>
Motivation: 现代企业在内容生成和评估方面面临时间和成本挑战：人工写作受时间限制，外部评估成本高昂。虽然大型语言模型在内容创作方面有潜力，但AI生成内容的质量问题仍然存在，而传统的人工评估方法进一步增加了运营成本，因此需要高效、自动化的解决方案。

Method: 研究引入生成式智能体作为解决方案。这些智能体能够快速、经济地评估AI生成内容，通过模拟人类判断来评估内容的连贯性、趣味性、清晰度、公平性和相关性等方面。

Result: 通过整合生成式智能体，企业可以简化内容生成流程，确保一致的高质量输出，同时减少对昂贵人工评估的依赖。

Conclusion: 该研究为增强大型语言模型生成符合业务需求的高质量内容提供了重要见解，在自动化内容生成和评估方面取得了显著进展。

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [106] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: 该研究为AI智能体系统建立了定量扩展原则，通过180个配置实验发现工具协调权衡、能力饱和和拓扑依赖错误放大三个主导效应，能够预测87%配置的最优协调策略。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的智能体系统在现实AI应用中越来越普及，但决定其性能的原则仍未被充分探索，导致从业者依赖启发式方法而非基于原则的设计选择。

Method: 在四个不同基准测试上评估五种典型架构，使用三个LLM家族实例化，进行180个配置的受控评估，使用标准化工具和token预算，通过经验协调指标建立预测模型。

Result: 预测模型达到交叉验证R^2=0.513，识别出三个主导效应：工具协调权衡、能力饱和和拓扑依赖错误放大。集中协调在并行任务上提升80.9%，分散协调在动态网络导航上表现更好，但所有多智能体变体在顺序推理任务上性能下降39-70%。

Conclusion: 该框架能够预测87%保留配置的最优协调策略，为智能体扩展提供了基于可测量任务特性的预测原则，使智能体系统设计从启发式转向基于原则的方法。

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [107] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: 提出rSIM机制，通过小型规划器引导LLM的思维链，使用多智能体强化学习训练规划器与LLM，使普通LLM升级为推理语言模型


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过强化学习后训练可以进化为推理语言模型，表现出"顿悟"时刻和策略性推理。为了将这种能力赋予任何LLM，需要一种机制来引导思维链中的推理策略

Method: 提出强化策略注入机制(rSIM)，使用小型规划器引导LLM的思维链，通过自适应注入推理策略。采用领导者-跟随者框架，使用多智能体强化学习联合训练规划器(领导者)和LLM(跟随者)，基于简单的基于规则的奖励

Result: rSIM使Qwen2.5-0.5B模型进化为推理语言模型，性能显著超过Qwen2.5-14B。规划器具有泛化性：只需训练一次即可作为插件大幅提升现有LLM的推理能力。规划器支持跨任务持续学习，规划能力逐步提升并泛化到更广泛问题

Conclusion: rSIM机制通过小型规划器引导LLM思维链，使用多智能体强化学习训练，能够有效将普通LLM升级为推理语言模型，显著提升推理性能，且规划器具有良好的泛化性和持续学习能力

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [108] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 该研究开发了一个机器学习框架，使用土耳其382个土壤样本的物理化学特性来预测加州承载比(CBR)，随机森林回归器表现最佳，R²分数达到0.95(训练)、0.76(验证)和0.83(测试)。


<details>
  <summary>Details</summary>
Motivation: 传统CBR实验室测试耗时、昂贵且不适用于大规模或多样化土壤剖面，需要更快速、精确的预测方法。人工智能和机器学习的发展为建模复杂土壤行为提供了数据驱动的方法。

Method: 收集土耳其不同地理气候区域的382个土壤样本，包含与承载能力相关的物理化学特性。测试了12种机器学习算法，包括决策树、随机森林、极端随机树、梯度提升、XGBoost、K近邻、支持向量回归、多层感知器、AdaBoost、Bagging、投票和堆叠回归器。对每个模型进行训练、验证和评估。

Result: 随机森林回归器表现最佳，获得R²分数：训练集0.95、验证集0.76、测试集0.83。该模型展示了强大的非线性映射能力，是预测性岩土工程任务的有前景工具。

Conclusion: 该研究支持在岩土工程中集成智能、数据中心的模型，为传统方法提供有效替代方案，促进基础设施分析和设计的数字化转型。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [109] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 本研究提出了一种基于自动机器学习（AutoML）的方法来预测土壤压实参数（最优含水量OMC和最大干密度MDD），相比传统实验方法和现有ML模型，AutoML方法在跨不同土壤类型时表现出更好的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统确定土壤压实参数的方法需要大量实验室实验，而现有的经验回归模型和机器学习模型在不同土壤类型上的适用性和准确性有限，特别是在处理异质数据集时预测精度和泛化能力不足。

Method: 采用自动机器学习（AutoML）方法，自动进行算法选择和超参数优化，以预测最优含水量（OMC）和最大干密度（MDD）。通过大量实验比较不同算法性能。

Result: 极端梯度提升（XGBoost）算法表现最佳，在独立数据集上对MDD的R平方值达到80.4%，对OMC的R平方值达到89.1%。AutoML方法在预测不同土壤类型的压实参数方面效果显著。

Conclusion: AutoML方法能够有效预测土壤压实参数，提高预测准确性和模型泛化能力，异质数据集对提升ML模型性能至关重要。该研究有助于提高建筑工程实践的效率和可靠性。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [110] [Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions](https://arxiv.org/abs/2512.08344)
*Tien Cuong Bui*

Main category: cs.AI

TL;DR: 该论文提出了一种针对图神经网络的新型可解释AI框架，旨在解决现有方法在解释图结构决策过程中的不足，提供更高效、更可靠的可解释性方案。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在众多应用中表现出强大能力，但其复杂性阻碍了对其决策过程的理解。现有的可解释AI方法难以解析图中复杂的交互关系，后处理方法需要额外计算资源且可靠性有限，而自解释模型的可泛化性存在担忧。

Method: 开发一个专门针对图机器学习的新型XAI框架，该框架旨在提供适应性强、计算效率高的解释，超越传统的个体特征分析，捕捉图结构如何影响预测结果。

Result: 论文摘要未提供具体实验结果，但提出了一个旨在解决现有方法不足的新型框架设计方向。

Conclusion: 需要开发专门针对图神经网络的可解释性框架，以平衡解释质量、计算效率和模型泛化能力，从而更好地理解图结构对预测的影响。

Abstract: Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.

</details>


### [111] [The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations](https://arxiv.org/abs/2512.08345)
*Benedikt Mangold*

Main category: cs.AI

TL;DR: 使用LLM多智能体系统模拟职场毒性对讨论效率的影响，发现毒性参与者使对话时间显著增加约25%


<details>
  <summary>Details</summary>
Motivation: 职场毒性对组织文化有害，但量化其对运营效率的直接影响存在方法学挑战，因为难以在人类受试者中重现冲突

Method: 利用基于大语言模型的多智能体系统模拟1对1对抗性辩论，创建受控的"社会学沙盒"；采用蒙特卡洛方法模拟数百次讨论，测量基线对照组与包含"毒性"系统提示的智能体治疗组之间的收敛时间

Result: 涉及毒性参与者的对话持续时间显著增加约25%；"毒性延迟"可作为企业和学术环境中财务损失的代理指标

Conclusion: 基于智能体的建模为测量社会摩擦机制提供了可重复、符合伦理的人类受试者研究替代方案

Abstract: Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled "sociological sandbox". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with "toxic" system prompts. Our results demonstrate a statistically significant increase of approximately 25\% in the duration of conversations involving toxic participants. We propose that this "latency of toxicity" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.

</details>


### [112] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: DuSAR是一种无需演示的LLM智能体框架，通过双策略协调（高层整体规划和上下文驱动的局部策略）和轻量级反思机制，实现自适应推理，在ALFWorld和Mind2Web上达到SOTA性能，同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体依赖外部演示或检索增强规划，导致脆弱性、泛化能力差和计算开销高。受人类问题解决启发，需要一种无需演示的框架，让单个冻结LLM能够通过互补策略进行自适应推理。

Method: 提出DuSAR框架：1）高层整体规划策略；2）上下文驱动的局部策略；3）轻量级反思机制，通过策略适应度分数评估进展，动态调整全局计划或细化策略；4）可选集成专家演示增强性能。

Result: 在ALFWorld上达到37.1%成功率（Llama3.1-70B），比之前最佳结果（13.0%）提高一倍以上；在Mind2Web上达到4.02%，同样比最强基线提高一倍以上；每步token消耗减少3-9倍，同时保持强性能。

Conclusion: DuSAR通过双策略协调和反思机制，实现了无需演示的高效LLM智能体推理，显著提升性能并降低计算成本，同时保持与外部知识集成的灵活性。

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [113] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: DeepFeature是一个基于大语言模型的上下文感知特征生成框架，用于可穿戴生物信号分析，通过多源特征生成、迭代特征精炼和鲁棒代码转换，显著提升医疗任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前可穿戴设备生物信号的特征提取方法存在三个主要问题：缺乏任务特定的上下文知识、难以在高维特征空间中找到最优特征提取设置、容易产生代码生成和自动化错误。

Method: DeepFeature采用多源特征生成机制整合专家知识和任务设置，使用基于特征评估反馈的迭代特征精炼过程，并通过多层过滤和验证方法实现鲁棒的特征到代码转换。

Result: 在八个不同任务上，DeepFeature相比基线方法平均AUROC提升4.21-9.67%，在五个任务上优于最先进方法，其余任务上保持可比性能。

Conclusion: DeepFeature是首个基于大语言模型的上下文感知特征生成框架，能够有效解决传统特征提取方法的局限性，显著提升可穿戴生物信号分析任务的性能。

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [114] [Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems](https://arxiv.org/abs/2512.08411)
*Mingwei Li,Xiaoyuan Zhang,Chengwei Yang,Zilong Zheng,Yaodong Yang*

Main category: cs.AI

TL;DR: PRISM-WM是一种结构化世界模型，通过混合专家框架分解混合动力学，减少长时域规划中的复合误差


<details>
  <summary>Details</summary>
Motivation: 传统世界模型使用全局连续的神经网络，会过度平滑混合动力学中的离散模式转换（如接触、碰撞），导致长时域规划时在物理边界处产生灾难性的复合误差

Method: 提出棱镜世界模型（PRISM-WM），采用上下文感知的混合专家框架：门控机制隐式识别当前物理模式，专门化的专家预测相应过渡动力学；引入潜在正交化目标确保专家多样性，防止模式崩溃

Result: PRISM-WM显著减少了rollout漂移，在具有挑战性的连续控制基准测试（包括高维人形机器人和多样化多任务设置）中表现优异，为轨迹优化算法提供了高质量基础

Conclusion: PRISM-WM通过准确建模系统动力学中的急剧模式转换，为下一代基于模型的智能体提供了强大的基础模型，显著提升了长时域规划的可靠性

Abstract: Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.

</details>


### [115] [From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change](https://arxiv.org/abs/2512.08449)
*Yong-Woon Kim*

Main category: cs.AI

TL;DR: IDAIF是一个将变革理论与AI架构设计结合的新框架，通过五层架构映射确保AI系统符合人类价值观和社会影响


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在医疗、金融等高风险领域应用日益广泛，确保AI行为与人类价值观和意图对齐变得至关重要。当前方法主要优化技术性能指标，忽视了AI部署的社会技术维度，需要新的框架来填补这一空白。

Method: IDAIF将变革理论的五阶段模型（输入-活动-输出-成果-影响）映射到AI架构的五层（数据层-管道层-推理层-代理层-规范层）。每层采用严格的理论基础：多目标帕累托优化用于价值对齐、分层多代理编排用于成果实现、因果有向无环图用于缓解幻觉、对抗性去偏结合RLHF用于公平保证，并引入保证层管理假设失败。

Result: 提供了每个组件的正式数学公式，并通过医疗、网络安全和软件工程三个案例研究展示了IDAIF的应用。框架实现了从模型中心到影响中心的AI开发范式转变。

Conclusion: IDAIF代表了从模型中心到影响中心的AI开发范式转变，为工程师提供了构建符合伦理、可信赖且对社会有益的AI系统的具体架构模式。

Abstract: This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.

</details>


### [116] [Using reinforcement learning to probe the role of feedback in skill acquisition](https://arxiv.org/abs/2512.08463)
*Antonio Terpin,Raffaello D'Andrea*

Main category: cs.AI

TL;DR: 研究通过强化学习智能体控制旋转圆柱体在水中通道的阻力，发现学习高性能技能需要比执行技能更丰富的信息反馈，特别是流动反馈对阻力最大化学习至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究人类在无外部反馈情况下执行高性能技能（如花样滑冰、投球等）的学习过程，但绕过人类受试者，通过完全受控的物理系统来研究技能获取过程。

Method: 使用通用强化学习智能体直接控制桌面循环水通道中的旋转圆柱体，以最大化或最小化阻力。系统具有物理世界的丰富交互和复杂动力学特性，且目标明确易于奖励设计。

Result: 高维流动反馈使智能体仅需几分钟真实世界交互就能发现高性能阻力控制策略；学习后无需反馈即可执行相同性能；无流动反馈时智能体无法发现阻力最大化的良好策略，但能较慢且不可靠地实现阻力最小化。

Conclusion: 学习高性能技能可能需要比执行技能更丰富的信息，学习条件的好坏可能仅取决于目标而非动力学或策略复杂性，流动反馈对某些目标的学习至关重要。

Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.

</details>


### [117] [Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance](https://arxiv.org/abs/2512.08492)
*Aliaksei Kaliutau*

Main category: cs.AI

TL;DR: 该论文提出了一种从控制流图转向数据转换图的新范式，通过多智能体框架解决仓库级程序修复中的语义陷阱问题，实现了87.1%的修复成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在函数级代码生成方面取得进展，但仓库级自动程序修复仍面临挑战。现有方法采用控制中心范式，迫使智能体处理复杂的目录结构和无关的控制逻辑，存在"语义陷阱"问题。

Method: 提出从标准代码属性图转向数据转换图的范式转变，将数据状态建模为节点、函数建模为边，使智能体能够通过数据谱系而非控制流追踪逻辑缺陷。引入多智能体框架，协调数据完整性导航与控制流逻辑，并实现为自主问题解决器系统。

Result: 在多个软件工程基准测试中取得良好结果，在SWE-Verified基准测试上达到87.1%的解决率，直接解决了当前AI代码助手工具的核心限制。

Conclusion: 该方法通过数据转换图和多智能体框架，解决了仓库级程序修复中的语义陷阱问题，为零接触代码维护提供了可扩展的逻辑修复方案，为日益依赖软件的世界建立了更稳健的基础。

Abstract: Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the "Semantic Trap" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.

</details>


### [118] [A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles](https://arxiv.org/abs/2512.08512)
*Jiang Liu,Yan Qin,Wei Dai,Chau Yuen*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级迁移学习方法CITL，用于便携移动设备中锂离子电池的健康状态监测，通过增量网络节点构建和半监督机制，在减少计算资源消耗的同时提高监测精度。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习方法在便携移动设备上不可行，因为迁移学习阶段消耗大量计算资源，意外降低了设备的工作续航时间。需要一种轻量级的迁移学习方法来解决这一挑战。

Method: 提出构建性增量迁移学习方法CITL：1）利用目标域未标记数据，提出半监督迁移学习机制，通过迭代添加网络节点最小化监测残差；2）通过结构风险最小化、迁移失配最小化和流形一致性最大化保证节点参数的跨域学习能力；3）提供CITL收敛性分析。

Result: 在真实无人机电池数据集上的实验验证表明，CITL在SOH估计方面优于SS-TCA、MMD-LSTM-DA、DDAN、BO-CNN-TL和AS$^3$LSTM，均方根误差分别提高了83.73%、61.15%、28.24%、87.70%和57.34%。

Conclusion: CITL方法有效解决了便携移动设备中迁移学习计算资源消耗大的问题，实现了轻量级、高精度的电池健康状态监测，为实际应用提供了可行的解决方案。

Abstract: Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.

</details>


### [119] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: Principles2Plan：人类与LLM协作生成情境化伦理规则以指导自动化规划的系统原型


<details>
  <summary>Details</summary>
Motivation: 机器人在人类环境中操作需要伦理意识，但现有自动化规划工具缺乏伦理支持。手动指定伦理规则劳动密集且高度依赖具体情境。

Method: 开发交互式研究原型Principles2Plan，通过人类与大型语言模型协作：领域专家提供规划领域、问题细节和相关高级原则（如仁慈、隐私），系统生成可操作的伦理规则，用户审查、优先排序后提供给规划器生成伦理知情计划。

Result: 创建了首个支持用户在经典规划情境中生成基于原则的伦理规则的系统，展示了人类-LLM协作在使伦理自动化规划更实用可行方面的潜力。

Conclusion: Principles2Plan展示了人类与LLM协作在生成情境敏感伦理规则和指导自动化规划方面的潜力，为伦理自动化规划提供了更实用的解决方案。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [120] [The SMART+ Framework for AI Systems](https://arxiv.org/abs/2512.08592)
*Laxmiraju Kandikatla,Branislav Radeljic*

Main category: cs.AI

TL;DR: 论文提出了SMART+框架，这是一个基于安全、监控、问责、可靠性、透明度等支柱的结构化模型，用于评估和治理跨行业AI系统，特别关注临床研究中的负责任AI采用。


<details>
  <summary>Details</summary>
Motivation: AI系统已广泛应用于医疗、金融、制造等多个行业，虽然提高了运营效率，但也带来了安全、问责和监管合规等新挑战，需要建立一个全面的治理框架来解决这些问题。

Method: 提出了SMART+框架，该框架基于安全、监控、问责、可靠性、透明度五大支柱，并进一步增强了隐私与安全、数据治理、公平性与偏见、防护措施等维度，提供了一个实用的综合性AI系统评估和治理方法。

Result: SMART+框架能够与不断发展的机制和监管指南保持一致，整合运营保障措施、监督程序以及强化的隐私和治理控制，展示了风险缓解、信任建立和合规准备的能力。

Conclusion: 通过支持负责任的AI采用并确保可审计性，SMART+为临床研究中的有效AI治理提供了坚实基础，为跨行业AI系统的负责任部署提供了实用框架。

Abstract: Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.

</details>


### [121] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: CogMCTS框架通过将LLM认知引导与MCTS紧密结合，利用多轮认知反馈、双轨节点扩展和策略突变，在自动启发式设计中实现了更好的探索-利用平衡和优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的进化方法容易陷入局部最优，而LLM与MCTS结合的方法在多轮认知整合和搜索多样性方面仍有局限，需要更有效的自动启发式设计框架。

Method: 提出认知引导的MCTS框架(CogMCTS)：1) 多轮认知反馈整合历史经验、节点信息和负面结果；2) 双轨节点扩展结合精英启发式管理平衡探索与利用；3) 策略突变修改启发式形式和参数增强多样性。

Result: 实验结果表明，CogMCTS在稳定性、效率和解决方案质量方面优于现有的基于LLM的自动启发式设计方法。

Conclusion: CogMCTS通过紧密集成LLM认知引导与MCTS，实现了更有效的自动启发式优化，在探索-利用平衡和搜索多样性方面表现出色。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [122] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer的蛋白质二级结构预测模型，使用注意力机制处理蛋白质序列数据，通过滑动窗口数据增强技术扩展训练样本，在CB513数据集上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 从氨基酸序列预测蛋白质二级结构（如α螺旋、β折叠和卷曲）对于理解蛋白质功能至关重要。传统方法可能难以有效捕捉长程残基相互作用，因此需要更先进的序列建模方法。

Method: 采用基于Transformer的模型，应用注意力机制处理蛋白质序列数据。使用滑动窗口数据增强技术在CB513数据集上扩展训练样本。该模型能够处理可变长度序列，有效捕捉局部和长程残基相互作用。

Result: Transformer模型展现出强大的泛化能力，能够处理不同长度的蛋白质序列，同时有效捕捉残基间的局部和长程相互作用，在蛋白质二级结构预测任务中表现良好。

Conclusion: 基于Transformer的模型为蛋白质二级结构预测提供了一种有效方法，其注意力机制能够更好地建模蛋白质序列中的复杂模式，为理解蛋白质功能提供了有力工具。

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [123] [See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm](https://arxiv.org/abs/2512.08629)
*Haoyu Zhao,Weizhong Ding,Yuhao Yang,Zheng Tian,Linyi Yang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: See-Control是一个通过低自由度机械臂直接物理交互操作智能手机的框架，解决了现有方法依赖Android调试桥的局限性，提供了平台无关的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型的智能手机操作方法依赖Android调试桥进行数据传输和动作执行，这限制了其仅适用于Android设备，缺乏平台无关的解决方案。

Method: 提出See-Control框架，包含三个核心组件：1) 155个任务的ESO基准测试和评估指标；2) 基于MLLM的具身智能体，无需ADB或系统后端访问即可生成机器人控制命令；3) 丰富标注的操作数据集。

Result: 开发了一个平台无关的智能手机操作框架，通过低自由度机械臂实现直接物理交互，为家庭机器人在真实环境中执行依赖智能手机的任务提供了具体步骤。

Conclusion: See-Control通过弥合数字智能体与物理世界之间的差距，为家庭机器人执行依赖智能手机的任务提供了平台无关的解决方案，推动了具身智能在现实环境中的应用。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.

</details>


### [124] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: 提出分层多智能体框架模拟多学科团队协作，解决多模态临床推理中的上下文稀释和幻觉问题，在胃肠道肿瘤学中显著提升推理逻辑和医学准确性。


<details>
  <summary>Details</summary>
Motivation: 胃肠道肿瘤学的多模态临床推理需要整合内镜影像、放射学数据和生化标志物。尽管多模态大语言模型（MLLMs）展现出潜力，但在处理复杂、异质的医疗记录时经常遇到上下文稀释和幻觉等挑战。

Method: 提出分层多智能体框架，模拟人类多学科团队（MDT）的协作工作流程。该框架通过多个专门化的智能体协同工作，解决单一模型在处理复杂医疗信息时的局限性。

Result: 系统获得了4.60/5.00的复合专家评估分数，相比单一基线模型有显著提升。智能体架构在推理逻辑和医学准确性方面带来了最显著的改进。

Conclusion: 模拟性、基于智能体的合作为肿瘤学中的自动化决策支持提供了可扩展、可解释且临床稳健的范式，能够有效处理复杂的多模态医疗信息。

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [125] [Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance](https://arxiv.org/abs/2512.08740)
*Yiming Lu*

Main category: cs.AI

TL;DR: 提出"人机协同认知增强"新范式，通过结构化"元交互"将人类专家的"认知黑箱"和AI的"计算黑箱"转化为可组合、可审计、可扩展的"功能白盒"系统。


<details>
  <summary>Details</summary>
Motivation: 解决人类专家直觉认知与AI不可信决策之间的根本性隔阂，实现从"AI作为工具"到"AI作为思维伙伴"的范式转变。

Method: 提出"即插即用认知框架"——可从专家对话中提取的可计算知识包，加载到递归对抗元思维网络(RAMTN)中，通过结构化"元交互"实现专家思维的转化和复用。

Result: 实现了专家思维(如医疗诊断逻辑和教学直觉)向可重用、可扩展公共资产的转化，为"认知公平"提供了首个工程证明，并为AI治理开辟了新路径。

Conclusion: 该框架通过"交互协议透明化"而非窥探模型内部机制，构建了可验证、可干预的治理范式，促进技术向善和认知包容，已开源以推动发展。

Abstract: Currently, there exists a fundamental divide between the "cognitive black box" (implicit intuition) of human experts and the "computational black box" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of "human-AI collaborative cognitive enhancement," aiming to transform the dual black boxes into a composable, auditable, and extensible "functional white-box" system through structured "meta-interaction." The core breakthrough lies in the "plug-and-play cognitive framework"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from "AI as a tool" to "AI as a thinking partner." This work not only provides the first engineering proof for "cognitive equity" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through "transparency of interaction protocols" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.

</details>


### [126] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 当前基础模型主要具备单智能体能力，但缺乏原生多智能体智能，需要专门研究来弥补这一差距


<details>
  <summary>Details</summary>
Motivation: 基础模型正成为AI智能体的"大脑"，但现有研究主要关注单智能体能力（如GUI交互、工具使用），而多智能体智能是下一个前沿领域。研究发现强大的单智能体性能并不能自动转化为稳健的多智能体智能

Method: 识别了基础模型在多智能体环境中的四个核心能力：理解、规划、高效通信和适应。通过对41个大语言模型进行广泛实证研究，验证了单智能体性能与多智能体智能之间的差距

Result: 实证证据表明，强大的单智能体性能并不能自动产生稳健的多智能体智能。需要专门的研究方向来构建具有原生多智能体智能的基础模型

Conclusion: 需要从数据集构建、评估、训练范式和安全性考虑等多个关键研究方向入手，专门开发具有原生多智能体智能的基础模型，以弥补当前单智能体能力与多智能体智能之间的差距

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [127] [Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments](https://arxiv.org/abs/2512.08755)
*Dongdong Yang,Bin Li,Jiguang He*

Main category: cs.AI

TL;DR: 本文对无人机搭载的可重构智能表面(RIS)和同时透射反射智能表面(STAR-RIS)在三维无线环境中的性能进行了全面比较，发现STAR-RIS在低空场景表现更好，而RIS在基站附近的高空场景更优。


<details>
  <summary>Details</summary>
Motivation: RIS和STAR-RIS作为下一代网络的关键使能技术，在无人机平台上具有灵活部署和改善视距条件的优势。然而，空中RIS和STAR-RIS架构之间的全面性能比较尚未得到深入研究。

Method: 建立了包含定向辐射模式的精确信道模型，并深入研究了部署高度和方向的影响。为优化系统和速率，为两种架构制定了联合优化问题，并提出了基于加权最小均方误差和块坐标下降算法的高效解决方案。

Result: 仿真结果表明：STAR-RIS在低空场景中由于具备全空间覆盖能力而优于RIS，而RIS在基站附近的高空场景中表现更好。部署高度和方向对系统性能有显著影响。

Conclusion: 研究结果为未来6G通信系统中空中智能表面的部署提供了实用见解，表明应根据具体场景需求选择合适的智能表面架构。

Abstract: Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.

</details>


### [128] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提供了构建生产级智能体AI工作流的端到端实用指南，涵盖设计、开发和部署全流程，提出了9个核心最佳实践，并通过多模态新闻分析案例进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI在行业和研究中的加速采用，组织面临一个核心挑战：如何设计、工程化和运营生产级的智能体AI工作流，使其可靠、可观察、可维护，并符合安全和治理要求。

Method: 引入结构化工程生命周期，包括工作流分解、多智能体设计模式、模型上下文协议(MCP)和工具集成、确定性编排、负责任AI考虑因素以及环境感知部署策略。提出了9个核心最佳实践，并通过多模态新闻分析和媒体生成工作流案例进行验证。

Result: 提供了构建稳健、可扩展和生产就绪的智能体AI工作流的基础参考，结合架构指导、操作模式和实际实施见解，为工程团队提供了实用的方法论。

Conclusion: 本文通过系统化的工程方法和具体的最佳实践，为设计和部署生产级智能体AI系统提供了全面的指导框架，有助于解决智能体AI在真实生产环境中的可靠性和可维护性挑战。

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [129] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: CARLoS是一个大规模LoRA分析框架，通过CLIP嵌入和与基础模型的差异，定义方向、强度和一致性三个维度来表征LoRA，实现高效的语义检索，并支持版权法律分析。


<details>
  <summary>Details</summary>
Motivation: 生成式组件（如LoRA）的快速扩散形成了一个庞大但非结构化的生态系统。现有的发现方法依赖于不可靠的用户描述或有偏见的流行度指标，影响了可用性。

Method: 分析650多个LoRA，通过多种提示和种子进行图像生成来评估其行为。使用CLIP嵌入及其与基础模型生成的差异，定义三部分表示：方向（定义语义偏移）、强度（量化效果显著性）和一致性（量化效果稳定性）。基于此表示开发高效的检索框架。

Result: CARLoS在自动和人工评估中优于文本基线，能够语义匹配文本查询到相关LoRA，同时过滤过强或不稳定的LoRA。该表示还支持将强度和一致性与版权法律中的实质性（substantiality）和意志（volition）概念联系起来进行分析。

Conclusion: CARLoS是一个实用的系统，不仅专注于检索，其表示方法对LoRA分析具有更广泛的相关性，特别是与版权法律概念的联系。

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [130] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 本文综述了描述逻辑和逻辑编程中Craig插值和均匀插值的理论结果与计算方法，探讨了它们在知识表示中的应用与挑战


<details>
  <summary>Details</summary>
Motivation: Craig插值和均匀插值在知识表示中有重要应用，包括可解释性、遗忘、模块化和重用等，但许多相关形式化方法通常缺乏这些插值特性，实际计算插值也面临挑战

Method: 深入分析两种主要知识表示形式化方法：描述逻辑和逻辑编程，讨论计算插值的理论结果和实用方法

Result: 系统梳理了描述逻辑和逻辑编程中插值问题的理论性质，并提供了实际计算插值的方法和技术

Conclusion: Craig插值和均匀插值在知识表示中具有重要价值，虽然许多形式化方法缺乏这些特性，但通过深入分析描述逻辑和逻辑编程，可以为实际应用提供理论支持和计算方法

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [131] [EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce](https://arxiv.org/abs/2512.08868)
*Rui Min,Zile Qiao,Ze Xu,Jiawen Zhai,Wenyu Gao,Xuanzhong Chen,Haozhen Sun,Zhen Zhang,Xinyu Wang,Hong Zhou,Wenbiao Yin,Xuan Zhou,Yong Jiang,Haicheng Liu,Liang Ding,Ling Zou,Yi R.,Fung,Yalong Li,Pengjun Xie*

Main category: cs.AI

TL;DR: EcomBench是一个基于真实电商环境的基准测试，用于评估智能代理在复杂电商场景中的实际能力，涵盖多任务类别和三个难度级别。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多关注学术或人工设计场景，忽略了真实应用中的挑战。电商领域具有大规模用户交互、动态市场条件和真实决策过程的特点，需要更贴近实际的评估方法。

Method: 从全球领先电商生态系统的真实用户需求中构建基准，通过人类专家精心策划和标注，确保清晰性、准确性和领域相关性。涵盖电商场景中的多个任务类别，定义三个难度级别。

Result: EcomBench提供了一个严格且动态的测试平台，用于评估智能代理在深度信息检索、多步推理和跨源知识整合等关键能力上的表现。

Conclusion: 通过将评估建立在真实电商环境中，EcomBench为衡量智能代理在现代电商中的实际能力提供了有效的基准测试工具。

Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.

</details>


### [132] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 论文提出了REST和REST+两个新基准测试，用于系统评估多模态大语言模型中的跨模态不一致性问题，发现即使语义信息相同，模型在不同模态（图像、文本、混合）上的推理表现也不一致。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型被训练在同一个嵌入空间中表示视觉和语言信息，但它们无法在两种模态上执行相同的任务。现有评估方法缺乏对跨模态一致性的系统性测试，因此需要新的基准来评估模型在不同模态间的一致性表现。

Method: 创建了REST和REST+两个基准测试，包含在三种模态（图像、文本、混合）中具有相同语义信息的样本。评估了15个最先进的多模态大语言模型，分析了OCR准确性、视觉特征（文本颜色、分辨率、字体）和视觉token数量对模型性能的影响。

Result: 研究发现：1）最先进的MLLMs无法在不同模态间保持一致的推理；2）跨模态不一致程度在不同模型间差异显著；3）将文本渲染为图像或将图像渲染为文本都无法解决不一致问题；4）视觉特征（文本颜色和分辨率）和视觉token数量影响模型性能；5）一致性得分与文本和图像之间的模态差距相关。

Conclusion: 多模态大语言模型存在显著的跨模态不一致问题，这种不一致与模态间的表示差距有机制性关联。需要开发新的方法来提高模型在不同模态间的一致性表现。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>
