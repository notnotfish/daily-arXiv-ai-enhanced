{"id": "2512.10895", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10895", "abs": "https://arxiv.org/abs/2512.10895", "authors": ["Lijie Ding", "Janell Thomson", "Jon Taylor", "Changwoo Do"], "title": "LLMs Can Assist with Proposal Selection at Large User Facilities", "comment": "9 pages, 8figures", "summary": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $\u03c1\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.", "AI": {"tldr": "LLMs\u53ef\u7528\u4e8e\u5927\u578b\u7528\u6237\u8bbe\u65bd\u7684\u63d0\u6848\u8bc4\u5ba1\uff0c\u901a\u8fc7\u6210\u5bf9\u504f\u597d\u65b9\u6cd5\u63d0\u4f9b\u6bd4\u4f20\u7edf\u4eba\u5de5\u8bc4\u5206\u66f4\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u4e14\u6210\u672c\u66f4\u4f4e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u8bc6\u522b\u9ad8\u53d1\u8868\u6f5c\u529b\u63d0\u6848\u65b9\u9762\u8868\u73b0\u4e0d\u4e9a\u4e8e\u4eba\u7c7b\u8bc4\u5ba1\uff0c\u6210\u672c\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u63d0\u6848\u8bc4\u5ba1\u5b58\u5728\u63d0\u6848\u95f4\u76f8\u5173\u6027\u5f31\u3001\u8bc4\u5ba1\u8005\u504f\u89c1\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002\u6210\u5bf9\u504f\u597d\u65b9\u6cd5\u5728\u903b\u8f91\u4e0a\u66f4\u4f18\u8d8a\uff0c\u4f46\u4e8c\u6b21\u65b9\u5de5\u4f5c\u91cf\u4f7f\u5176\u5bf9\u4eba\u5de5\u8bc4\u5ba1\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u5229\u7528LLMs\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u5229\u7528LLMs\u8fdb\u884c\u6210\u5bf9\u504f\u597d\u8bc4\u4f30\uff0c\u57fa\u4e8e\u7f8e\u56fd\u6a61\u6811\u5cad\u56fd\u5bb6\u5b9e\u9a8c\u5ba4\u6563\u88c2\u4e2d\u5b50\u6e90\u4e09\u4e2a\u675f\u7ebf\u7684\u7cbe\u5fc3\u7b56\u5212\u63d0\u6848\u548c\u53d1\u8868\u8bb0\u5f55\uff0c\u6bd4\u8f83LLM\u6392\u540d\u4e0e\u4eba\u5de5\u6392\u540d\u7684\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u6a21\u578b\u5b9a\u91cf\u8bc4\u4f30\u63d0\u6848\u76f8\u4f3c\u6027\u3002", "result": "LLM\u6392\u540d\u4e0e\u4eba\u5de5\u6392\u540d\u5f3a\u76f8\u5173\uff08Spearman \u03c1\u22480.2-0.8\uff0c\u53bb\u966410%\u5f02\u5e38\u503c\u540e\u22650.5\uff09\u3002LLM\u5728\u8bc6\u522b\u9ad8\u53d1\u8868\u6f5c\u529b\u63d0\u6848\u65b9\u9762\u8868\u73b0\u4e0d\u4e9a\u4e8e\u4eba\u7c7b\u8bc4\u5ba1\uff0c\u6210\u672c\u964d\u4f4e\u4e24\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff0c\u5e76\u80fd\u8fdb\u884c\u4eba\u7c7b\u96be\u4ee5\u5b8c\u6210\u7684\u63d0\u6848\u76f8\u4f3c\u6027\u5206\u6790\u3002", "conclusion": "LLMs\u4e3a\u5927\u578b\u7528\u6237\u8bbe\u65bd\u7684\u63d0\u6848\u9009\u62e9\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4e00\u81f4\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e0d\u4ec5\u80fd\u6709\u6548\u6392\u540d\u63d0\u6848\uff0c\u8fd8\u80fd\u63d0\u4f9b\u5bf9\u8bc4\u5ba1\u59d4\u5458\u4f1a\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u7ea7\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2512.10903", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10903", "abs": "https://arxiv.org/abs/2512.10903", "authors": ["Muhammad Umair Haider", "Hammad Rizwan", "Hassan Sajjad", "A. B. Siddique"], "title": "Multi-Granular Node Pruning for Circuit Discovery", "comment": null, "summary": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8282\u70b9\u7ea7\u526a\u679d\u6846\u67b6\u7528\u4e8e\u7535\u8def\u53d1\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u7c92\u5ea6\u4e0a\u7684\u9650\u5236\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u53ef\u5b66\u4e60\u63a9\u7801\u548c\u7c92\u5ea6\u7279\u5b9a\u7a00\u758f\u60e9\u7f5a\uff0c\u5728\u5355\u6b21\u5fae\u8c03\u4e2d\u5b9e\u73b0\u5168\u9762\u538b\u7f29\u3002", "motivation": "\u73b0\u6709\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8fed\u4ee3\u8fb9\u526a\u679d\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4ec5\u9650\u4e8e\u7c97\u7c92\u5ea6\u5355\u5143\uff08\u5982\u6ce8\u610f\u529b\u5934\u6216MLP\u5757\uff09\uff0c\u5ffd\u7565\u4e86\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u7ed3\u6784\u3002\u9700\u8981\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u548c\u7c92\u5ea6\u9650\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8282\u70b9\u7ea7\u526a\u679d\u6846\u67b6\uff0c\u5f15\u5165\u8de8\u591a\u4e2a\u7c92\u5ea6\u7ea7\u522b\u7684\u53ef\u5b66\u4e60\u63a9\u7801\uff08\u4ece\u6574\u4e2a\u5757\u5230\u5355\u4e2a\u795e\u7ecf\u5143\uff09\uff0c\u5728\u7edf\u4e00\u4f18\u5316\u76ee\u6807\u4e2d\u4f7f\u7528\u7c92\u5ea6\u7279\u5b9a\u7a00\u758f\u60e9\u7f5a\u6307\u5bfc\u526a\u679d\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u5355\u6b21\u5fae\u8c03\u4e2d\u7684\u5168\u9762\u538b\u7f29\u3002", "result": "\u8be5\u65b9\u6cd5\u53d1\u73b0\u7684\u7535\u8def\u8282\u70b9\u6570\u5c11\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u8bc1\u660e\u8bb8\u591a\u7c97\u7c92\u5ea6\u65b9\u6cd5\u8ba4\u4e3a\u91cd\u8981\u7684\u795e\u7ecf\u5143\u5b9e\u9645\u4e0a\u65e0\u5173\u7d27\u8981\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002\u5185\u5b58\u5360\u7528\u663e\u8457\u964d\u4f4e5-10\u500d\uff0c\u56e0\u4e3a\u4e0d\u9700\u8981\u5728\u5185\u5b58\u4e2d\u4fdd\u7559\u4e2d\u95f4\u6fc0\u6d3b\u3002", "conclusion": "\u63d0\u51fa\u7684\u8282\u70b9\u7ea7\u526a\u679d\u6846\u67b6\u5728\u7535\u8def\u53d1\u73b0\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u7c92\u5ea6\u63a7\u5236\uff0c\u80fd\u591f\u8bc6\u522b\u66f4\u7cbe\u7ec6\u7684\u7535\u8def\u7ed3\u6784\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2512.10937", "categories": ["cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.10937", "abs": "https://arxiv.org/abs/2512.10937", "authors": ["Matt Wilson"], "title": "On Decision-Making Agents and Higher-Order Causal Processes", "comment": null, "summary": "We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u4e2d\u7684\u667a\u80fd\u4f53\u4e0e\u5355\u8f93\u5165\u8fc7\u7a0b\u51fd\u6570\uff08\u9ad8\u9636\u91cf\u5b50\u64cd\u4f5c\u7684\u7ecf\u5178\u6781\u9650\uff09\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u51b3\u7b56\u7406\u8bba\u4e0e\u7269\u7406\u5b66\u4e2d\u91cf\u5b50\u64cd\u4f5c\u7406\u8bba\u4e4b\u95f4\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u4e3a\u7406\u89e3\u667a\u80fd\u4f53\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u65b0\u7684\u6570\u5b66\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5c06\u667a\u80fd\u4f53\u7684\u7b56\u7565\u548c\u8bb0\u5fc6\u66f4\u65b0\u7ec4\u5408\u6210\u8fc7\u7a0b\u51fd\u6570w\uff0c\u4f7f\u7528\u94fe\u63a5\u79ef\u4e0ePOMDP\u73af\u5883\u4ea4\u4e92\uff0c\u5efa\u7acb\u667a\u80fd\u4f53\u4e0e\u8fc7\u7a0b\u51fd\u6570\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86POMDP\u667a\u80fd\u4f53\u4e0e\u5355\u8f93\u5165\u8fc7\u7a0b\u51fd\u6570\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u5e94\uff0c\u5e76\u53d1\u73b0\u8fd9\u79cd\u5bf9\u5e94\u5177\u6709\u53cc\u91cd\u89e3\u91ca\uff1a\u4ece\u7269\u7406\u89c6\u89d2\u770b\uff0c\u8fc7\u7a0b\u51fd\u6570\u4f5c\u4e3a\u73af\u5883\uff1b\u4eceAI\u89c6\u89d2\u770b\uff0c\u8fc7\u7a0b\u51fd\u6570\u7f16\u7801\u667a\u80fd\u4f53\u3002", "conclusion": "\u8be5\u5bf9\u5e94\u5173\u7cfb\u4e3a\u7406\u89e3\u667a\u80fd\u4f53\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5c06\u89c2\u6d4b\u65e0\u5173\u7684\u5206\u6563POMDP\u8bc6\u522b\u4e3a\u591a\u8f93\u5165\u8fc7\u7a0b\u51fd\u6570\u7684\u81ea\u7136\u9886\u57df\u3002"}}
{"id": "2512.10866", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10866", "abs": "https://arxiv.org/abs/2512.10866", "authors": ["Ruslan Gokhman"], "title": "UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting", "comment": "NeurIPS 2025 Workshop UrbanAI", "summary": "We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.", "AI": {"tldr": "\u7ebf\u6027\u6a21\u578b\uff08\u7279\u522b\u662fDLinear\uff09\u5728\u4ec5\u4f7f\u7528\u5386\u53f2\u5ba4\u5185\u6e29\u5ea6\u6570\u636e\u7684\u957f\u671f\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6bd4\u590d\u6742\u7684Transformer\u7cfb\u5217\u6a21\u578b\u8868\u73b0\u66f4\u597d", "motivation": "\u7814\u7a76\u5728\u4ec5\u4f7f\u7528\u5386\u53f2\u6e29\u5ea6\u6570\u636e\uff08\u5916\u751f\u53d8\u91cf\uff09\u7684\u6311\u6218\u6027\u5355\u53d8\u91cf\u8bbe\u7f6e\u4e0b\uff0c\u7ebf\u6027\u6a21\u578b\u548cTransformer\u7cfb\u5217\u6a21\u578b\u5728\u957f\u671f\u6e29\u5ea6\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\u5bf9\u6bd4", "method": "\u4f7f\u7528Linear\u3001NLinear\u3001DLinear\u3001Transformer\u3001Informer\u548cAutoformer\u6a21\u578b\uff0c\u5728\u6807\u51c6\u5316\u7684\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u5206\u5272\u4e0b\u8fdb\u884c\u957f\u671f\u5916\u751f\u6e29\u5ea6\u9884\u6d4b\u8bc4\u4f30", "result": "\u7ebf\u6027\u57fa\u7ebf\u6a21\u578b\uff08Linear\u3001NLinear\u3001DLinear\uff09\u5728\u6240\u6709\u5206\u5272\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u66f4\u590d\u6742\u7684Transformer\u7cfb\u5217\u67b6\u6784\uff0c\u5176\u4e2dDLinear\u5728\u6240\u6709\u5206\u5272\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u6574\u4f53\u51c6\u786e\u6027", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7ebf\u6027\u6a21\u578b\u5728\u4ec5\u4f7f\u7528\u5916\u751f\u53d8\u91cf\u7684\u6311\u6218\u6027\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u573a\u666f\u4e2d\u4ecd\u7136\u662f\u5f3a\u5927\u7684\u57fa\u7ebf\u65b9\u6cd5"}}
{"id": "2512.10877", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10877", "abs": "https://arxiv.org/abs/2512.10877", "authors": ["Julian Kleutgens", "Claudio Battiloro", "Lingkai Kong", "Benjamin Grewe", "Francesca Dominici", "Mauricio Tec"], "title": "Guided Transfer Learning for Discrete Diffusion Models", "comment": "7 pages (main text) + appendix", "summary": "Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.", "AI": {"tldr": "\u63d0\u51faGTL\u65b9\u6cd5\uff0c\u5728\u4e0d\u4fee\u6539\u9884\u8bad\u7ec3\u53bb\u566a\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5f15\u5bfc\u5b9e\u73b0\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5e76\u5f00\u53d1\u9ad8\u6548\u91c7\u6837\u5668\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u8bed\u8a00\u7b49\u79bb\u6563\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u65b0\u9886\u57df\u9002\u5e94\u6210\u672c\u9ad8\u3002\u73b0\u6709\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5fae\u8c03\u5927\u578b\u6269\u6563\u6a21\u578b\uff0c\u8ba1\u7b97\u6602\u8d35\u4e14\u4e0d\u5b9e\u7528", "method": "\u57fa\u4e8e\u8fde\u7eed\u6269\u6563\u7684\u6bd4\u7387\u8fc1\u79fb\u5b66\u4e60\uff0c\u63d0\u51faGTL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u4ece\u76ee\u6807\u5206\u5e03\u91c7\u6837\u800c\u4e0d\u4fee\u6539\u9884\u8bad\u7ec3\u53bb\u566a\u5668\u3002\u65b9\u6cd5\u9002\u7528\u4e8e\u79bb\u6563\u65f6\u95f4\u6269\u6563\u548c\u8fde\u7eed\u65f6\u95f4\u57fa\u4e8e\u5206\u6570\u7684\u79bb\u6563\u6269\u6563\u3002\u8fdb\u4e00\u6b65\u5f00\u53d1\u9ad8\u6548\u5f15\u5bfc\u91c7\u6837\u5668\uff0c\u96c6\u4e2d\u8bc4\u4f30\u89c4\u5212\u9009\u62e9\u7684\u4f4d\u7f6e\u548c\u5019\u9009\u6807\u8bb0", "result": "GTL\u5728\u5e8f\u5217\u6570\u636e\uff08\u5305\u62ec\u5408\u6210\u9a6c\u5c14\u53ef\u592b\u94fe\u548c\u8bed\u8a00\u5efa\u6a21\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u5176\u884c\u4e3a\u7684\u5b9e\u8bc1\u5206\u6790\u3002\u9ad8\u6548\u91c7\u6837\u5668\u4f7f\u5927\u89c4\u6a21\u8bed\u8a00\u5efa\u6a21\u5728\u5927\u578b\u8bcd\u6c47\u8868\u548c\u957f\u5e8f\u5217\u4e2d\u53d8\u5f97\u5b9e\u7528", "conclusion": "GTL\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u91c7\u6837\u5668\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5e94\u7528\u7684\u8ba1\u7b97\u6311\u6218"}}
{"id": "2512.10878", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10878", "abs": "https://arxiv.org/abs/2512.10878", "authors": ["Xuan Zhao", "Zhuo Cao", "Arya Bangun", "Hanno Scharr", "Ira Assent"], "title": "Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes", "comment": "Accepted by Actionable Interpretability Workshop at ICML 2025", "summary": "Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6539\u8fdb\u6a21\u578b\u91cd\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u539f\u59cb\u6570\u636e\u4e0e\u53cd\u4e8b\u5b9e\u6837\u672c\u7ed3\u5408\uff0c\u4f7f\u7528Wasserstein\u91cd\u5fc3\u8fd1\u4f3c\u7c7b\u522b\u539f\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u4ee3\u7406\u6a21\u578b\u7684\u8d28\u91cf\u5e76\u51cf\u5c11\u51b3\u7b56\u8fb9\u754c\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u53cd\u4e8b\u5b9e\u89e3\u91ca\u901a\u5e38\u4f4d\u4e8e\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4fe1\u606f\u4e30\u5bcc\u7684\u6837\u672c\u7528\u4e8e\u6a21\u578b\u91cd\u6784\uff0c\u7279\u522b\u662f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5c06\u53cd\u4e8b\u5b9e\u4f5c\u4e3a\u666e\u901a\u8bad\u7ec3\u5b9e\u4f8b\u4f7f\u7528\u4f1a\u5bfc\u81f4\u51b3\u7b56\u8fb9\u754c\u504f\u79fb\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5229\u7528\u8fd9\u4e9b\u6837\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u539f\u59cb\u6570\u636e\u6837\u672c\u4e0e\u53cd\u4e8b\u5b9e\u6837\u672c\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528Wasserstein\u91cd\u5fc3\u6765\u8fd1\u4f3c\u6bcf\u4e2a\u7c7b\u522b\u7684\u539f\u578b\uff0c\u4ece\u800c\u4fdd\u6301\u7c7b\u522b\u5e95\u5c42\u5206\u5e03\u7ed3\u6784\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u53cd\u4e8b\u5b9e\u6837\u672c\u7684\u4fe1\u606f\u4ef7\u503c\uff0c\u540c\u65f6\u907f\u514d\u51b3\u7b56\u8fb9\u754c\u504f\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u6a21\u578b\u4e0e\u76ee\u6807\u6a21\u578b\u4e4b\u95f4\u7684\u4fdd\u771f\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u76f8\u6bd4\u76f4\u63a5\u5c06\u53cd\u4e8b\u5b9e\u4f5c\u4e3a\u666e\u901a\u8bad\u7ec3\u5b9e\u4f8b\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u578b\u91cd\u6784\u8d28\u91cf\u4e0a\u6709\u660e\u663e\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53cd\u4e8b\u5b9e\u6837\u672c\u4e0e\u539f\u59cb\u6570\u636e\u7ed3\u5408\uff0c\u5e76\u4f7f\u7528Wasserstein\u91cd\u5fc3\u8fd1\u4f3c\u7c7b\u522b\u539f\u578b\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u6a21\u578b\u91cd\u6784\u7684\u8d28\u91cf\u3002\u8fd9\u79cd\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u573a\u666f\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u53cd\u4e8b\u5b9e\u6837\u672c\u7684\u4fe1\u606f\u4ef7\u503c\uff0c\u540c\u65f6\u907f\u514d\u51b3\u7b56\u8fb9\u754c\u504f\u79fb\u95ee\u9898\u3002"}}
{"id": "2512.10886", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2512.10886", "abs": "https://arxiv.org/abs/2512.10886", "authors": ["Stefan Matthes", "Markus Schramm"], "title": "Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields", "comment": null, "summary": "Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.\n  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.\n  The model accurately reconstructs loop temperatures (RMSE $<2^\\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4ece\u5e38\u89c4\u8fd0\u884c\u6570\u636e\u4e2d\u63a8\u65ad\u69fd\u5f0f\u5149\u70ed\u7535\u7ad9\u7684\u56de\u8def\u8d28\u91cf\u6d41\u91cf\u6bd4\u548c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u63a5\u6536\u5668\u4f20\u70ed\u7cfb\u6570\uff0c\u89e3\u51b3\u6db2\u538b\u4e0d\u5e73\u8861\u548c\u63a5\u6536\u5668\u9000\u5316\u8bca\u65ad\u95ee\u9898\u3002", "motivation": "\u69fd\u5f0f\u5149\u70ed\u7535\u7ad9\u7684\u5927\u578b\u6db2\u538b\u6536\u96c6\u5668\u56de\u8def\u7f51\u7edc\u9700\u8981\u63d0\u4f9b\u5747\u5300\u7684\u51fa\u53e3\u6e29\u5ea6\uff0c\u4f46\u7531\u4e8e\u7a7a\u95f4\u5f02\u8d28\u6027\u7684\u5149\u5b66\u6027\u80fd\u3001\u70ed\u635f\u5931\u548c\u538b\u964d\uff0c\u96be\u4ee5\u5b9e\u73b0\u3002\u56de\u8def\u6e29\u5ea6\u53ef\u6d4b\u91cf\uff0c\u4f46\u56de\u8def\u7ea7\u8d28\u91cf\u6d41\u91cf\u548c\u63a5\u6536\u5668\u70ed\u635f\u5931\u53c2\u6570\u65e0\u6cd5\u89c2\u6d4b\uff0c\u5bfc\u81f4\u65e0\u6cd5\u4f7f\u7528\u6807\u51c6\u76d1\u6d4b\u5de5\u5177\u8bca\u65ad\u6db2\u538b\u4e0d\u5e73\u8861\u6216\u63a5\u6536\u5668\u9000\u5316\u3002", "method": "\u63d0\u51fa\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u591c\u95f4\u5747\u5300\u5316\u65f6\u6bb5\uff08\u70ed\u6cb9\u5728\u975e\u8f90\u7167\u573a\u4e2d\u5faa\u73af\uff09\u6765\u5206\u79bb\u6db2\u538b\u548c\u70ed\u635f\u5931\u6548\u5e94\u3002\u5c06\u53ef\u5fae\u5171\u8f6d\u4f20\u70ed\u6a21\u578b\u79bb\u6563\u5316\u5e76\u5d4c\u5165\u7aef\u5230\u7aef\u5b66\u4e60\u7ba1\u9053\uff0c\u4f7f\u752850MW Andasol 3\u592a\u9633\u80fd\u573a\u7684\u5386\u53f2\u6570\u636e\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u6a21\u578b\u51c6\u786e\u91cd\u5efa\u56de\u8def\u6e29\u5ea6\uff08RMSE <2\u00b0C\uff09\uff0c\u4ea7\u751f\u7269\u7406\u4e0a\u6709\u610f\u4e49\u7684\u56de\u8def\u4e0d\u5e73\u8861\u548c\u63a5\u6536\u5668\u70ed\u635f\u5931\u4f30\u8ba1\u3002\u4e0e\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u7ea2\u5916\u70ed\u6210\u50cf\uff08QScan\uff09\u6bd4\u8f83\u663e\u793a\u5f3a\u76f8\u5173\u6027\uff0c\u6b63\u786e\u8bc6\u522b\u6240\u6709\u9ad8\u635f\u5931\u63a5\u6536\u5668\u533a\u57df\u3002", "conclusion": "\u5608\u6742\u7684\u771f\u5b9e\u4e16\u754c\u5149\u70ed\u7535\u7ad9\u8fd0\u884c\u6570\u636e\u5305\u542b\u8db3\u591f\u4fe1\u606f\u6765\u6062\u590d\u6f5c\u5728\u7269\u7406\u53c2\u6570\uff0c\u5f53\u4e0e\u9002\u5f53\u7684\u5efa\u6a21\u548c\u53ef\u5fae\u4f18\u5316\u7ed3\u5408\u65f6\u3002\u8be5\u65b9\u6cd5\u4e3a\u8bca\u65ad\u6db2\u538b\u4e0d\u5e73\u8861\u548c\u63a5\u6536\u5668\u9000\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2512.10922", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10922", "abs": "https://arxiv.org/abs/2512.10922", "authors": ["Max Zimmer", "Christophe Roux", "Moritz Wagner", "Deborah Hendrych", "Sebastian Pokutta"], "title": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale", "comment": "15 pages, 2 figures, 4 tables", "summary": "The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u884c\u7ea7\u7b49\u7a00\u758f\u7ea6\u675f\u548c1-swap\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u526a\u679d\u8bef\u5dee\uff0c\u65e0\u9700\u5b8c\u6574\u91cd\u8bad\u7ec3\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u526a\u679d\u9762\u4e34\u6311\u6218\uff1a\u5b8c\u6574\u91cd\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\uff0c\u4f20\u7edf\u5168\u5c40\u5e45\u5ea6\u526a\u679d\u5bf9Transformer\u67b6\u6784\u4e0d\u4f18\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8fd1\u4f3c\u6216\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u6574\u6570\u89c4\u5212\u6c42\u89e3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002", "method": "1. \u901a\u8fc7\u5f3a\u5236\u6bcf\u884c\u76f8\u7b49\u7a00\u758f\u5ea6\u89e3\u8026\u884c\u95f4\u4f9d\u8d56\uff1b2. \u63a8\u5bfc\u53ef\u9ad8\u6548\u8ba1\u7b97\u7684Gram\u77e9\u9635\u6700\u4f181-swap\uff08\u4ea4\u6362\u4e00\u4e2a\u4fdd\u7559\u6743\u91cd\u548c\u4e00\u4e2a\u526a\u679d\u6743\u91cd\uff09\uff1b3. \u63d0\u51fa\u4ece\u4efb\u610f\u526a\u679d\u63a9\u7801\u5f00\u59cb\u3001\u5728GPU\u4e0a\u9ad8\u6548\u8fd0\u884c\u3001\u57fa\u672c\u65e0\u8d85\u53c2\u6570\u76841-swap\u7b97\u6cd5\u3002", "result": "\u76f8\u6bd4Wanda\u65b9\u6cd5\u51cf\u5c11\u6bcf\u5c42\u526a\u679d\u8bef\u5dee\u8fbe60%\uff0c\u5728\u5148\u8fdbGPT\u67b6\u6784\u4e0a\u6301\u7eed\u6539\u5584\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u884c\u7ea7\u7b49\u7a00\u758f\u7ea6\u675f\u548c\u9ad8\u65481-swap\u7b97\u6cd5\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u7684\u63a9\u7801\u9009\u62e9\u95ee\u9898\u53d8\u5f97\u53ef\u884c\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u3001\u65e0\u8d85\u53c2\u6570\u7684\u526a\u679d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10925", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.10925", "abs": "https://arxiv.org/abs/2512.10925", "authors": ["Zamirddine Mari", "Mohamad Motasem Nawaf", "Pierre Drap"], "title": "Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation", "comment": null, "summary": "Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8ePPO\u7b97\u6cd5\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8eBlueROV2\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u65e0GPS\u3001\u80fd\u89c1\u5ea6\u5dee\u4e14\u6709\u969c\u788d\u7269\u7684\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u76f8\u6bd4\u4f20\u7edfDWA\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1aGPS\u4fe1\u53f7\u7f3a\u5931\u3001\u80fd\u89c1\u5ea6\u964d\u4f4e\u4ee5\u53ca\u6c34\u4e0b\u969c\u788d\u7269\u5b58\u5728\u3002\u8fd9\u4e9b\u56e0\u7d20\u4f7f\u5f97\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u6548\u679c\u53d7\u9650\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8ePPO\u7b97\u6cd5\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u89c2\u6d4b\u7a7a\u95f4\u7ed3\u5408\u4e86\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u4fe1\u606f\u3001\u865a\u62df\u5360\u7528\u7f51\u683c\u548c\u64cd\u4f5c\u533a\u57df\u8fb9\u754c\u7684\u5c04\u7ebf\u6295\u5c04\u3002\u901a\u8fc7\u4e0eDWA\u786e\u5b9a\u6027\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u5e76\u5728\u7269\u7406BlueROV2\u4e0a\u9a8c\u8bc1\uff0c\u4f7f\u7528\u6d4b\u8bd5\u573a\u5730\u76843D\u6570\u5b57\u5b6a\u751f\u8fdb\u884c\u76d1\u7763\u3002", "result": "PPO\u7b56\u7565\u5728\u9ad8\u5ea6\u6742\u4e71\u73af\u5883\u4e2d\u6301\u7eed\u4f18\u4e8eDWA\u65b9\u6cd5\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u66f4\u597d\u7684\u5c40\u90e8\u9002\u5e94\u6027\u548c\u66f4\u5c11\u7684\u78b0\u649e\u6b21\u6570\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u4ece\u4eff\u771f\u5230\u771f\u5b9e\u4e16\u754c\u7684\u5b66\u4e60\u884c\u4e3a\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5bf9\u4e8e\u6c34\u4e0b\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0cPPO\u7b97\u6cd5\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e14\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6548\u679c\u826f\u597d\u3002"}}
{"id": "2512.10926", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.10926", "abs": "https://arxiv.org/abs/2512.10926", "authors": ["Qiyang Li", "Seohong Park", "Sergey Levine"], "title": "Decoupled Q-Chunking", "comment": "76 pages, 14 figures", "summary": "Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences (\"chunks\") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u89e3\u8026\u6279\u8bc4\u5bb6\u4e0e\u7b56\u7565\u5757\u957f\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u84b8\u998f\u90e8\u5206\u52a8\u4f5c\u5757\u7684\u4e50\u89c2\u4ef7\u503c\u4f30\u8ba1\uff0c\u89e3\u51b3\u957f\u52a8\u4f5c\u5757\u7b56\u7565\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5728\u957f\u65f6\u57df\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65f6\u95f4\u5dee\u5206\u65b9\u6cd5\u5b58\u5728\u81ea\u4e3e\u504f\u5dee\u95ee\u9898\uff0c\u5757\u72b6\u6279\u8bc4\u5bb6\u867d\u7136\u52a0\u901f\u4e86\u4ef7\u503c\u5907\u4efd\uff0c\u4f46\u8981\u6c42\u7b56\u7565\u4ee5\u5f00\u73af\u65b9\u5f0f\u8f93\u51fa\u6574\u4e2a\u52a8\u4f5c\u5757\uff0c\u8fd9\u5728\u9700\u8981\u7b56\u7565\u53cd\u5e94\u6027\u7684\u73af\u5883\u4e2d\u53ef\u80fd\u6b21\u4f18\uff0c\u4e14\u957f\u52a8\u4f5c\u5757\u5efa\u6a21\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u6279\u8bc4\u5bb6\u4e0e\u7b56\u7565\u5757\u957f\u5ea6\u7684\u7b97\u6cd5\uff1a\u901a\u8fc7\u4ece\u539f\u59cb\u5757\u72b6\u6279\u8bc4\u5bb6\u4e50\u89c2\u56de\u63a8\uff0c\u84b8\u998f\u51fa\u90e8\u5206\u52a8\u4f5c\u5757\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u8fd1\u4f3c\u5f53\u90e8\u5206\u52a8\u4f5c\u5757\u6269\u5c55\u4e3a\u5b8c\u6574\u5757\u65f6\u53ef\u8fbe\u5230\u7684\u6700\u5927\u4ef7\u503c\uff0c\u4ece\u800c\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u957f\u65f6\u57df\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u53ef\u9760\u5730\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86\u591a\u6b65\u4ef7\u503c\u4f20\u64ad\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5f00\u73af\u6b21\u4f18\u6027\u548c\u5b66\u4e60\u957f\u52a8\u4f5c\u5757\u7b56\u7565\u7684\u56f0\u96be\uff0c\u4e3a\u5757\u72b6\u6279\u8bc4\u5bb6\u4e0e\u7b56\u7565\u7684\u534f\u8c03\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10931", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10931", "abs": "https://arxiv.org/abs/2512.10931", "authors": ["George Yakushev", "Nataliia Babina", "Masoud Vahid Dastgerdi", "Vyacheslav Zhdanovskiy", "Alina Shutova", "Denis Kuznedelev"], "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs", "comment": "Preprint, work in progress", "summary": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u540c\u65f6\u601d\u8003\u3001\u542c\u53d6\u548c\u751f\u6210\u8f93\u51fa\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u63a8\u7406\u6a21\u578b\u9700\u8981\u505c\u6b62\u601d\u8003\u624d\u80fd\u54cd\u5e94\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4ea4\u4e92\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u9700\u8981\u5148\u5b8c\u6210\u601d\u8003\u624d\u80fd\u7ed9\u51fa\u7b54\u6848\uff0c\u8fd9\u79cd\u987a\u5e8f\u4ea4\u4e92\u65b9\u5f0f\u4e0d\u9002\u5408\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u7684\u5e94\u7528\u573a\u666f\uff08\u5982\u8bed\u97f3\u52a9\u624b\uff09\u3002\u4eba\u7c7b\u53ef\u4ee5\u5f02\u6b65\u5730\u542c\u3001\u60f3\u3001\u8bf4\uff0c\u800c\u73b0\u6709LLM\u65e0\u6cd5\u505a\u5230\u8fd9\u4e00\u70b9\u3002", "method": "\u5229\u7528\u65cb\u8f6c\u5d4c\u5165\u7684\u7279\u6027\uff0c\u4f7f\u539f\u672c\u8bbe\u8ba1\u7528\u4e8e\u987a\u5e8f\u4ea4\u4e92\u7684LLM\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u601d\u8003\u3001\u542c\u53d6\u548c\u751f\u6210\u8f93\u51fa\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u5b89\u5168\u63a8\u7406\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u751f\u6210\u51c6\u786e\u7684\u601d\u8003\u589e\u5f3a\u7b54\u6848\uff0c\u5c06\u7b2c\u4e00\u4e2a\u975e\u601d\u8003token\u7684\u751f\u6210\u65f6\u95f4\u4ece\u51e0\u5206\u949f\u51cf\u5c11\u5230\u22645\u79d2\uff0c\u6574\u4f53\u5b9e\u65f6\u5ef6\u8fdf\u964d\u4f4e\u4e866-11\u500d\u3002", "conclusion": "\u901a\u8fc7\u65cb\u8f6c\u5d4c\u5165\u6280\u672f\uff0c\u53ef\u4ee5\u4f7f\u63a8\u7406\u578bLLM\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5f02\u6b65\u542c\u3001\u60f3\u3001\u8bf4\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u5b9e\u65f6\u4ea4\u4e92\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.10936", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10936", "abs": "https://arxiv.org/abs/2512.10936", "authors": ["Kristina Korotkova", "Aleksandr Katrutsa"], "title": "Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks", "comment": null, "summary": "The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6539\u8fdb\u7684Frank-Wolfe\u65b9\u6cd5\uff08\u65e0\u6295\u5f71\u4f18\u5316\u7b97\u6cd5\uff09\u6784\u5efa\u9ad8\u6548\u7684\u767d\u76d2\u5bf9\u6297\u653b\u51fb\uff0c\u4ece\u6570\u503c\u4f18\u5316\u89d2\u5ea6\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u7684\u6784\u9020\u6548\u7387", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5728\u5404\u7c7b\u670d\u52a1\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u5bf9\u6297\u653b\u51fb\u7684\u6311\u6218\uff0c\u9700\u8981\u5feb\u901f\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u6784\u5efa\u65b9\u6cd5\u6765\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002\u7531\u4e8e\u5bf9\u6297\u653b\u51fb\u6784\u5efa\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u4ece\u6570\u503c\u4f18\u5316\u89d2\u5ea6\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684Frank-Wolfe\u65b9\u6cd5\uff08\u65e0\u6295\u5f71\u4f18\u5316\u7b97\u6cd5\uff09\u6784\u5efa\u767d\u76d2\u5bf9\u6297\u653b\u51fb\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u6295\u5f71\u64cd\u4f5c\u3002\u5728MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u591a\u7c7b\u903b\u8f91\u56de\u5f52\u6a21\u578b\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff08ViT\uff09\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u6539\u8fdb\u7684Frank-Wolfe\u65b9\u6cd5\uff0c\u5e76\u4e0e\u57fa\u4e8e\u6295\u5f71\u64cd\u4f5c\u6216\u51e0\u4f55\u76f4\u89c9\u7684\u6807\u51c6\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5bf9\u6297\u653b\u51fb\u6784\u5efa\u65b9\u9762\u5177\u6709\u6548\u7387\u548c\u6548\u679c\u4f18\u52bf\u3002", "conclusion": "\u4ece\u6570\u503c\u4f18\u5316\u89d2\u5ea6\u51fa\u53d1\uff0c\u6539\u8fdb\u7684Frank-Wolfe\u65b9\u6cd5\u4e3a\u6784\u5efa\u9ad8\u6548\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u66f4\u597d\u5730\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.10938", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.10938", "abs": "https://arxiv.org/abs/2512.10938", "authors": ["Mingzhi Chen", "Taiming Lu", "Jiachen Zhu", "Mingjie Sun", "Zhuang Liu"], "title": "Stronger Normalization-Free Transformers", "comment": null, "summary": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\\mathrm{Derf}(x) = \\mathrm{erf}(\u03b1x + s)$, where $\\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u70b9\u72b6\u51fd\u6570Derf\uff0c\u4f5c\u4e3a\u5f52\u4e00\u5316\u5c42\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u4e86LayerNorm\u3001RMSNorm\u548cDyT\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u5f52\u4e00\u5316\u5c42\u957f\u671f\u4ee5\u6765\u88ab\u89c6\u4e3a\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\u4e0d\u53ef\u6216\u7f3a\u7684\u7ec4\u4ef6\uff0c\u4f46\u6700\u8fd1\u5f15\u5165\u7684Dynamic Tanh (DyT)\u8868\u660e\u5b58\u5728\u66ff\u4ee3\u65b9\u6848\u3002DyT\u901a\u8fc7\u7ea6\u675f\u6781\u503c\u5b9e\u73b0\u7a33\u5b9a\u6536\u655b\u5e76\u8fbe\u5230\u5f52\u4e00\u5316\u7ea7\u522b\u7684\u6027\u80fd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u80fd\u591f\u8d85\u8d8aDyT\u7684\u51fd\u6570\u8bbe\u8ba1\u3002", "method": "\u9996\u5148\u7814\u7a76\u70b9\u72b6\u51fd\u6570\u7684\u5185\u5728\u7279\u6027\u5982\u4f55\u5f71\u54cd\u8bad\u7ec3\u548c\u6027\u80fd\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\u8fdb\u884c\u5927\u89c4\u6a21\u641c\u7d22\u4ee5\u5bfb\u627e\u66f4\u6709\u6548\u7684\u51fd\u6570\u8bbe\u8ba1\u3002\u901a\u8fc7\u63a2\u7d22\uff0c\u5f15\u5165\u4e86Derf(x) = erf(\u03b1x + s)\uff0c\u5176\u4e2derf(x)\u662f\u91cd\u65b0\u7f29\u653e\u7684\u9ad8\u65af\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u786e\u5b9a\u4e3a\u6027\u80fd\u6700\u4f73\u7684\u8bbe\u8ba1\u3002", "result": "Derf\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u4e86LayerNorm\u3001RMSNorm\u548cDyT\uff0c\u5305\u62ec\u89c6\u89c9\uff08\u56fe\u50cf\u8bc6\u522b\u548c\u751f\u6210\uff09\u3001\u8bed\u97f3\u8868\u793a\u548cDNA\u5e8f\u5217\u5efa\u6a21\u3002\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6e90\u4e8e\u5176\u6539\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\u800c\u975e\u66f4\u5f3a\u7684\u62df\u5408\u80fd\u529b\u3002", "conclusion": "Derf\u7684\u7b80\u5355\u6027\u548c\u66f4\u5f3a\u7684\u6027\u80fd\u4f7f\u5176\u6210\u4e3a\u65e0\u5f52\u4e00\u5316Transformer\u67b6\u6784\u7684\u5b9e\u7528\u9009\u62e9\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2512.10952", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10952", "abs": "https://arxiv.org/abs/2512.10952", "authors": ["Xiaona Zhou", "Yingyan Zeng", "Ran Jin", "Ismini Lourentzou"], "title": "Hierarchical Dataset Selection for High-Quality Data Sharing", "comment": null, "summary": "The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.", "AI": {"tldr": "DaSH\u65b9\u6cd5\u901a\u8fc7\u5c42\u6b21\u5316\u5efa\u6a21\u6570\u636e\u96c6\u548c\u7ec4\u7ea7\u6548\u7528\uff0c\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u9ad8\u6548\u9009\u62e9\u6570\u636e\u96c6\u4ee5\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u6570\u636e\u901a\u5e38\u4ee5\u79bb\u6563\u6570\u636e\u96c6\u5f62\u5f0f\u7ec4\u7ec7\uff0c\u4e0d\u540c\u6570\u636e\u96c6\u5728\u76f8\u5173\u6027\u3001\u8d28\u91cf\u548c\u6548\u7528\u4e0a\u5b58\u5728\u5dee\u5f02\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9009\u62e9\u5355\u4e2a\u6837\u672c\u4e14\u5c06\u6240\u6709\u6570\u636e\u89c6\u4e3a\u540c\u7b49\u76f8\u5173\uff0c\u5ffd\u7565\u4e86\u6570\u636e\u96c6\u53ca\u5176\u6765\u6e90\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9009\u62e9\u6574\u4e2a\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u6765\u6539\u5584\u4e0b\u6e38\u6027\u80fd\u3002", "method": "\u63d0\u51faDataset Selection via Hierarchies (DaSH)\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u96c6\u548c\u7ec4\uff08\u5982\u96c6\u5408\u3001\u673a\u6784\uff09\u4e24\u4e2a\u5c42\u6b21\u4e0a\u5efa\u6a21\u6548\u7528\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u4ece\u6709\u9650\u89c2\u5bdf\u4e2d\u9ad8\u6548\u6cdb\u5316\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u8d44\u6e90\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u6570\u636e\u96c6\u9009\u62e9\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\uff08Digit-Five\u548cDomainNet\uff09\u4e0a\uff0cDaSH\u6bd4\u6700\u5148\u8fdb\u7684\u6570\u636e\u9009\u62e9\u57fa\u7ebf\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u5347\u4e86\u9ad8\u8fbe26.2%\uff0c\u540c\u65f6\u9700\u8981\u663e\u8457\u66f4\u5c11\u7684\u63a2\u7d22\u6b65\u9aa4\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660eDaSH\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u548c\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "DaSH\u65b9\u6cd5\u9002\u7528\u4e8e\u5b9e\u9645\u591a\u6e90\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u7684\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u6570\u636e\u96c6\u9009\u62e9\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4ece\u5f02\u6784\u6570\u636e\u6c60\u4e2d\u9009\u62e9\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002"}}
{"id": "2512.10953", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.10953", "abs": "https://arxiv.org/abs/2512.10953", "authors": ["Yiyang Lu", "Qiao Sun", "Xianbang Wang", "Zhicheng Jiang", "Hanhong Zhao", "Kaiming He"], "title": "Bidirectional Normalizing Flow: From Data to Noise and Back", "comment": "Tech report", "summary": "Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation (\"1-NFE\") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.", "AI": {"tldr": "BiFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u5f52\u4e00\u5316\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u8fd1\u4f3c\u9006\u6620\u5c04\u800c\u975e\u7cbe\u786e\u89e3\u6790\u9006\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6d41\u4e2d\u56e0\u679c\u89e3\u7801\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5728ImageNet\u4e0a\u5b9e\u73b0\u4e86\u751f\u6210\u8d28\u91cf\u63d0\u5347\u548c\u91c7\u6837\u901f\u5ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u5f52\u4e00\u5316\u6d41\u9700\u8981\u7cbe\u786e\u89e3\u6790\u9006\u53d8\u6362\uff0c\u9650\u5236\u4e86\u67b6\u6784\u7075\u6d3b\u6027\u3002TARFlow\u7b49\u57fa\u4e8eTransformer\u7684\u81ea\u56de\u5f52\u6d41\u867d\u7136\u590d\u5174\u4e86NF\u65b9\u6cd5\uff0c\u4f46\u66b4\u9732\u4e86\u56e0\u679c\u89e3\u7801\u4f5c\u4e3a\u4e3b\u8981\u74f6\u9888\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301NF\u539f\u5219\u6027\u6846\u67b6\uff0c\u53c8\u80fd\u7a81\u7834\u56e0\u679c\u89e3\u7801\u9650\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u5f52\u4e00\u5316\u6d41(BiFlow)\u6846\u67b6\uff0c\u653e\u5f03\u5bf9\u7cbe\u786e\u89e3\u6790\u9006\u7684\u8981\u6c42\uff0c\u5b66\u4e60\u4e00\u4e2a\u8fd1\u4f3c\u566a\u58f0\u5230\u6570\u636e\u7684\u9006\u6620\u5c04\u6a21\u578b\u3002\u8fd9\u4f7f\u5f97\u53ef\u4ee5\u4f7f\u7528\u66f4\u7075\u6d3b\u7684\u635f\u5931\u51fd\u6570\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u4e0d\u518d\u53d7\u56e0\u679c\u89e3\u7801\u7ea6\u675f\u3002", "result": "\u5728ImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u56e0\u679c\u89e3\u7801\u5bf9\u5e94\u65b9\u6cd5\uff0cBiFlow\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u5c06\u91c7\u6837\u901f\u5ea6\u52a0\u901f\u4e86\u9ad8\u8fbe\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002\u5728\u57fa\u4e8eNF\u7684\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u5355\u6b21\u8bc4\u4f30(\"1-NFE\")\u65b9\u6cd5\u4e2d\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "BiFlow\u901a\u8fc7\u8fd1\u4f3c\u9006\u6620\u5c04\u800c\u975e\u7cbe\u786e\u89e3\u6790\u9006\uff0c\u89e3\u51b3\u4e86\u5f52\u4e00\u5316\u6d41\u4e2d\u7684\u56e0\u679c\u89e3\u7801\u74f6\u9888\uff0c\u4e3aNF\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u6709\u671b\u8fdb\u4e00\u6b65\u63a8\u52a8\u8fd9\u4e00\u7ecf\u5178\u8303\u5f0f\u7684\u53d1\u5c55\u3002"}}
