<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 69]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Documenting SME Processes with Conversational AI: From Tacit Knowledge to BPMN](https://arxiv.org/abs/2512.05122)
*Unnikrishnan Radhakrishnan*

Main category: cs.AI

TL;DR: 本文介绍了一个基于大语言模型的对话助手，帮助中小企业捕获隐性知识并自动转换为BPMN 2.0流程图，降低流程文档化的技能和成本门槛。


<details>
  <summary>Details</summary>
Motivation: 中小企业严重依赖隐性、基于经验的专有知识，这些知识很少被正式记录，导致机构知识流失和运营透明度不足。需要一种低成本、低技能门槛的方法来捕获这些知识并转换为标准化的流程文档。

Method: 开发了一个基于Gemini 2.5 Pro大语言模型的对话助手，通过轻量级Gradio前端和客户端bpmn-js可视化界面，以访谈式对话方式引导用户描述流程细节，支持澄清对话和按需分析，实时生成并允许用户精修BPMN 2.0图表。

Result: 在设备维护场景的概念验证评估中，聊天机器人在约12分钟内生成了准确的"现状"模型，通过图表标注标记问题，并生成了改进的"未来"变体，同时将API成本控制在中小企业友好预算内。研究还分析了延迟来源、模型选择权衡和严格XML模式执行的挑战。

Conclusion: 对话式大语言模型能够降低严格流程文档化的技能和成本障碍，帮助中小企业保存机构知识、增强运营透明度并加速持续改进工作。研究为未来智能化和多模态部署提供了路线图。

Abstract: Small and medium-sized enterprises (SMEs) still depend heavily on tacit, experience-based know-how that rarely makes its way into formal documentation. This paper introduces a large-language-model (LLM)-driven conversational assistant that captures such knowledge on the shop floor and converts it incrementally and interactively into standards-compliant Business Process Model and Notation (BPMN) 2.0 diagrams. Powered by Gemini 2.5 Pro and delivered through a lightweight Gradio front-end with client-side bpmn-js visualisation, the assistant conducts an interview-style dialogue: it elicits process details, supports clarifying dialogue and on-demand analysis, and renders live diagrams that users can refine in real time. A proof-of-concept evaluation in an equipment-maintenance scenario shows that the chatbot produced an accurate "AS-IS" model, flagged issues via on-diagram annotations, and generated an improved "TO-BE" variant, all within about 12-minutes, while keeping API costs within an SME-friendly budget. The study analyses latency sources, model-selection trade-offs, and the challenges of enforcing strict XML schemas, then outlines a roadmap toward agentic and multimodal deployments. The results demonstrate that conversational LLMs can potentially be used to lower the skill and cost barriers to rigorous process documentation, helping SMEs preserve institutional knowledge, enhance operational transparency, and accelerate continuous-improvement efforts.

</details>


### [2] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 提出两种基于信息论和热力学的无监督度量方法，用于评估大语言模型对给定任务的忠实度，通过将LLM建模为二分信息引擎，计算语义忠实度和语义熵产生度量。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型对给定任务的忠实度是一个复杂挑战，需要新的无监督度量方法来量化模型输出与任务要求的一致性程度。

Method: 将LLM建模为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文到答案的转换。将问题-上下文-答案三元组建模为共享主题上的概率分布，使用KL散度计算语义忠实度，并通过凸优化同时推断转换矩阵。还提出基于热力学的语义熵产生度量。

Result: 提出的语义忠实度(SF)和语义熵产生(SEP)度量可以联合或单独用于LLM评估和幻觉控制。在LLM总结公司SEC 10-K文件的案例中展示了该框架的有效性。

Conclusion: 基于信息论和热力学的无监督度量方法能够有效评估LLM的忠实度，高忠实度通常对应低熵产生，这些度量可用于模型评估和幻觉控制。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [3] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: 该论文提出了一种创新的AI和数据科学教学方法，系统性地将传统机器学习技术与现代大语言模型相结合，通过两部分课程设计帮助学生全面理解AI发展并掌握实践技能。


<details>
  <summary>Details</summary>
Motivation: 为了帮助学生全面理解人工智能的发展历程，同时掌握传统机器学习和现代大语言模型的实践技能，更好地适应快速发展的AI行业需求。

Method: 采用两部分课程设计：第一部分教授基础机器学习概念，第二部分专注于当代大语言模型应用。课程包括架构设计、实施策略、评估方法，在夏季课程中分两个七周学期实施。

Result: 这种整合教学方法增强了学生对AI领域的理解，更好地为他们应对行业需求做好准备，在快速演进的人工智能领域中具有更好的适应性。

Conclusion: 将传统机器学习与当代大语言模型系统结合的教学方法是有效的，能够帮助学生建立全面的AI知识体系，为他们在快速发展的AI行业中取得成功奠定基础。

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [4] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: 该论文通过形式化证明指出，任何算法（包括AI模型）都无法创造出初始算法本身不具备的新功能能力，因此无法实现真正的创造力，只能展示现有功能能力的组合与排列。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，人们开始关注何时能实现人工通用智能（AGI）。本研究旨在探讨计算过程的极限，特别是算法能否实现真正的创造力，从而回答AI能否达到人类智能水平的问题。

Method: 采用先前研究中关于AGI的定义（即在某个领域展现创造力并解锁新功能能力），然后通过形式化证明方法，论证任何算法都无法产生初始算法本身不具备的新功能能力。

Result: 形式化证明表明，没有任何算法（包括AI模型）能够在科学、工程、艺术、体育等领域展现真正的创造力，即无法创造出初始算法本身不具备的新功能能力。AI只能展示现有功能能力及其组合与排列。

Conclusion: 该证明对AI发展的未来具有重要意义，表明AI无法实现真正的创造力。同时，这一结论也对人类智能的起源提出了重要问题，暗示人类智能可能具有超越算法计算的特殊性质。

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [5] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: 该论文主张可能性理论是解决Dempster-Shafer理论悖论的根本方案，通过可能性与必要性测度的二元框架，为不确定性处理提供逻辑一致且数学严谨的基础。


<details>
  <summary>Details</summary>
Motivation: Dempster-Shafer理论在处理不确定性时存在悖论和逻辑陷阱，许多尝试修复Dempster规则的方法都未能从根本上解决问题。作者认为需要建立一个逻辑一致且数学严谨的不确定性处理基础框架。

Method: 采用Bychkov文章中发展的公理化方法，基于可能性与必要性测度的二元框架构建不确定性处理的基础。通过比较分析概率论、证据理论和可能性理论三种范式，并以经典医疗诊断困境为例，展示可能性理论如何正确处理矛盾数据。

Result: 可能性理论能够避免DST的逻辑陷阱，正确处理矛盾数据，使形式推理更接近自然智能的逻辑。相比其他修复Dempster规则的尝试，该方法提供了更根本的解决方案。

Conclusion: 可能性理论不仅是DST的替代方案，而是解决DST悖论的根本方案，为不确定性处理提供了逻辑一致且数学严谨的基础框架，使形式推理更接近自然智能的逻辑。

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [6] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: 论文主张将AI研究目标从自我改进转向协同改进，即人类研究者与AI系统合作实现共同超级智能，以加速AI研究并确保安全


<details>
  <summary>Details</summary>
Motivation: 当前AI领域的自我改进目标存在风险且难以完全实现，需要更可行、更安全的研究方向来推动AI发展

Method: 提出"协同改进"框架，专注于提升AI系统与人类研究者的协作能力，从构思到实验的完整研究过程中实现人机共生

Result: 协同改进既能加速AI研究进程，又能通过人机共生赋予双方更安全的超级智能能力

Conclusion: 将人类研究改进纳入循环是更快、更安全实现超级智能的途径，协同改进是比自我改进更可行、更优越的目标

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [7] [MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare](https://arxiv.org/abs/2512.05365)
*Zag ElSayed,Craig Erickson,Ernest Pedapati*

Main category: cs.AI

TL;DR: MCP-AI是一种创新的医疗AI架构，结合模型上下文协议(MCP)实现可解释的临床决策支持，支持长期状态管理、协作推理和符合监管标准的临床工作流。


<details>
  <summary>Details</summary>
Motivation: 传统医疗AI系统难以融合上下文推理、长期状态管理和人类可验证工作流，随着医疗系统日益复杂，迫切需要自主、上下文感知的临床推理框架。

Method: 提出MCP-AI架构，基于模型上下文协议(MCP)构建模块化、可执行的规范，用于编排生成式和描述式AI代理。每个MCP文件捕获临床目标、患者上下文、推理状态和任务逻辑，形成可重用、可审计的记忆对象。

Result: 通过两个用例验证：1)脆性X综合征伴抑郁的诊断建模；2)2型糖尿病和高血压的远程协调。系统支持医生在环验证、简化临床流程，并保证AI责任在医疗提供者之间的安全转移。

Conclusion: MCP-AI为即将到来的临床环境提供了可扩展、可解释、可组合且安全导向的AI基础，连接HL7/FHIR接口并符合HIPAA和FDA SaMD等监管标准。

Abstract: Healthcare AI systems have historically faced challenges in merging contextual reasoning, long-term state management, and human-verifiable workflows into a cohesive framework. This paper introduces a completely innovative architecture and concept: combining the Model Context Protocol (MCP) with a specific clinical application, known as MCP-AI. This integration allows intelligent agents to reason over extended periods, collaborate securely, and adhere to authentic clinical logic, representing a significant shift away from traditional Clinical Decision Support Systems (CDSS) and prompt-based Large Language Models (LLMs). As healthcare systems become more complex, the need for autonomous, context-aware clinical reasoning frameworks has become urgent. We present MCP-AI, a novel architecture for explainable medical decision-making built upon the Model Context Protocol (MCP) a modular, executable specification for orchestrating generative and descriptive AI agents in real-time workflows. Each MCP file captures clinical objectives, patient context, reasoning state, and task logic, forming a reusable and auditable memory object. Unlike conventional CDSS or stateless prompt-based AI systems, MCP-AI supports adaptive, longitudinal, and collaborative reasoning across care settings. MCP-AI is validated through two use cases: (1) diagnostic modeling of Fragile X Syndrome with comorbid depression, and (2) remote coordination for Type 2 Diabetes and hypertension. In either scenario, the protocol facilitates physician-in-the-loop validation, streamlines clinical processes, and guarantees secure transitions of AI responsibilities between healthcare providers. The system connects with HL7/FHIR interfaces and adheres to regulatory standards, such as HIPAA and FDA SaMD guidelines. MCP-AI provides a scalable basis for interpretable, composable, and safety-oriented AI within upcoming clinical environments.

</details>


### [8] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind是一个基于知识图谱增强的推理框架，专门用于处理冗长的集成电路规格文档，通过将电路规格转换为领域特定知识图谱并结合自适应检索机制，显著提升了LLM在硬件设计中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在集成电路开发自动化方面具有巨大潜力，但实际部署受到有限上下文窗口的根本限制。现有的上下文扩展方法难以在广泛复杂的电路规格上实现有效的语义建模和深入的多跳推理。

Method: ChipMind采用知识图谱增强推理框架：1) 通过电路语义感知知识图谱构建方法将电路规格转换为领域特定知识图谱ChipKG；2) 利用ChipKG增强推理机制，结合信息论自适应检索动态追踪逻辑依赖关系，以及意图感知语义过滤去除无关噪声，有效平衡检索完整性和精确性。

Result: 在工业级规格推理基准测试中，ChipMind显著优于现有最先进基线方法，平均提升34.59%（最高达72.73%）。

Conclusion: 该框架填补了学术研究与LLM辅助硬件设计实际工业部署之间的关键空白，为集成电路开发自动化提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [9] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: BEAVER是首个为LLM约束满足提供确定性、可靠概率边界的实用框架，相比基线方法能获得6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从研究原型转向生产系统，从业者需要可靠方法来验证模型输出是否满足所需约束。基于采样的估计只能提供模型行为的直觉，无法提供可靠保证。

Method: BEAVER框架使用新颖的token trie和frontier数据结构，系统性地探索生成空间，为任何前缀封闭的语义约束维护可证明可靠的概率边界。

Result: 在多个最先进LLM上的正确性验证、隐私验证和安全代码生成任务中，BEAVER在相同计算预算下实现了6-8倍更紧的概率边界，识别出3-4倍更多高风险实例。

Conclusion: BEAVER能够提供松散边界或经验评估无法实现的精确特征描述和风险评估，为LLM约束满足提供了首个实用的确定性验证框架。

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [10] [The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems](https://arxiv.org/abs/2512.05449)
*Robert Yang*

Main category: cs.AI

TL;DR: 该论文提出将哲学中的"意志薄弱"(akrasia)概念引入AI系统分析，用于衡量大语言模型知道正确答案但无法坚持执行的矛盾现象，并开发了初步的Akrasia Benchmark来量化评估模型的"自我控制"能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在一种特殊的不一致性：它们"知道"正确答案但无法坚持执行。这种全局判断与局部冲动之间的张力类似于人类哲学中的"意志薄弱"(akrasia)。作者希望将这一哲学概念引入AI系统分析，为理解智能体的不一致性和目标漂移提供理论基础。

Method: 提出了Akrasia Benchmark的初步版本，包含四种结构化提示条件：基线(B)、同义词(S)、时间(T)和诱惑(X)。该基准通过测量模型局部响应与其先前承诺之间的矛盾来量化"自我控制"能力，可用于比较不同模型家族、解码策略和诱惑类型。

Result: 建立了量化评估模型"意志薄弱"程度的基准框架，能够系统比较不同AI系统的自我控制能力。该框架不仅适用于单模型评估，还能分析微观层面的意志薄弱如何导致多智能体系统的宏观不稳定性，可能被解释为"阴谋"或故意错位。

Conclusion: 通过将不一致性重新定义为意志薄弱，这项工作将智能体行为与经典代理理论联系起来，为哲学、心理学和新兴的智能体AI科学之间建立了实证桥梁，为分析AI系统的目标一致性和稳定性提供了新视角。

Abstract: Large language models display a peculiar form of inconsistency: they "know" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of "self-control" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as "scheming" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.

</details>


### [11] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: 提出MIND推理框架，通过"理解->反思->纠正"的类人认知能力，将MLLMs从被动模仿推理转变为主动判别推理，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在推理任务中存在多理性语义建模有限、逻辑鲁棒性不足、易受复杂场景误导等问题，需要提升其认知智能水平。

Method: 提出MIND框架：1) RAD范式自动生成多样化理性，扩展数据集；2) P2CL策略分两阶段进行多理性正向学习和主动逻辑判别纠正；3) MCA优化策略解决多理性语义空间表示纠缠问题。

Result: 在涵盖科学、常识和数学场景的多个公共数据集上实现了最先进的性能，为推进MLLMs向更高认知智能水平提供了新视角。

Conclusion: MIND框架通过类人认知的"理解->反思->纠正"能力，实现了从被动模仿推理到主动判别推理的范式演进，显著提升了多模态大语言模型的推理性能。

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [12] [CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning](https://arxiv.org/abs/2512.05576)
*Ting-Ting Xie,Yixin Zhang*

Main category: cs.AI

TL;DR: 提出Executor-Analyst框架解决临床AI代理的上下文利用失败问题，通过解耦工具执行和临床推理，结合分层集成策略，在无需昂贵微调的情况下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于小型LLM的临床代理（如TxAgent）存在"上下文利用失败"问题：模型能够成功检索生物医学证据，但无法基于这些信息进行诊断推理。需要解决这种推理缺陷。

Method: 提出Executor-Analyst框架，将工具执行的语法精度与临床推理的语义鲁棒性解耦。通过专门的TxAgents（执行器）与长上下文基础模型（分析师）协同工作。采用分层集成策略而非全局池化，保持证据多样性。还研究了上下文长度和工具集扩展的缩放特性。

Result: 在CURE-Bench上实现了最先进的性能，无需昂贵的端到端微调。发现了两个关键缩放见解：1）上下文长度超过12k标记会引入噪声降低准确性（上下文-性能悖论）；2）工具集扩展需要分层检索策略（维度诅咒）。

Conclusion: 通过免训练的架构工程方法，为下一代可信赖的AI驱动治疗提供了可扩展、敏捷的基础。该方法有效解决了临床代理的推理缺陷，展示了模块化架构在医疗AI中的潜力。

Abstract: Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.

</details>


### [13] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: 本文提出了OntoAxiom基准测试，用于评估大型语言模型在识别本体公理方面的性能，发现Axiom-by-Axiom提示策略优于直接方法，但性能因公理类型和本体领域而异。


<details>
  <summary>Details</summary>
Motivation: 本体开发需要大量建模和领域专业知识，虽然本体学习在过去十年有所进展，但识别公理（定义类和属性间逻辑关系的基本组件）仍然是一个挑战。本文旨在评估LLMs在公理识别任务上的能力。

Method: 创建了OntoAxiom基准测试，包含9个中等规模本体（共17,118个三元组，2,771个公理），专注于子类、不相交、子属性、定义域和值域公理。评估了12个LLMs，采用三种shot设置和两种提示策略：直接方法（一次查询所有公理）和Axiom-by-Axiom方法（每个提示只查询一个公理）。

Result: Axiom-by-Axiom提示策略比直接方法获得更高的F1分数。性能因公理类型而异，某些公理更难识别。领域也影响性能：FOAF本体的子类公理得分为0.642，而音乐本体仅为0.218。大型LLMs优于小型模型，但小型模型在资源受限环境下仍可能适用。整体性能不足以完全自动化公理识别。

Conclusion: 虽然LLMs在公理识别方面的整体性能还不够高，无法完全自动化该过程，但它们可以提供有价值的候选公理，支持本体工程师开发和优化本体。Axiom-by-Axiom提示策略更有效，性能差异表明需要针对特定公理类型和领域进行优化。

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [14] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: 提出了一种针对部分最大可满足性(PMS)和加权部分最大可满足性(WPMS)问题的新型子句权重方案DeepDist，首次区分两种问题的权重更新条件，并引入新的初始化方法和消减技术，显著提升了SLS求解器性能。


<details>
  <summary>Details</summary>
Motivation: 现有随机局部搜索(SLS)算法在处理PMS和WPMS问题时，通常采用统一的子句权重更新策略，未能充分考虑两种问题类型的结构差异，导致求解效果受限。

Method: 1) 首次提出区分PMS和WPMS的子句权重更新条件；2) 引入新的权重初始化方法以适应两种实例特性；3) 提出优先满足单元子句和硬子句的消减方法；4) 基于上述方法开发了DeepDist求解器。

Result: 在最近MaxSAT评估的基准测试中，DeepDist优于现有最先进的SLS求解器。与TT-Open-WBO-Inc结合的混合求解器超越了MaxSAT评估2024的获胜者SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS。

Conclusion: 提出的区分PMS和WPMS的子句权重方案及相关技术显著提升了(W)PMS问题的求解性能，证明了考虑问题类型结构差异的重要性，DeepDist成为有效的SLS求解器。

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [15] [KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books](https://arxiv.org/abs/2512.05734)
*Jinfeng Zhong,Emmanuel Bacry,Agathe Guilloux,Jean-François Muzy*

Main category: cs.AI

TL;DR: KANFormer：结合Dilated Causal CNN、Transformer和Kolmogorov-Arnold Networks的深度学习模型，用于预测限价单成交时间，整合市场级和代理级信息，在CAC 40期货数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅依赖限价订单簿的快照序列，忽略了与LOB动态相关的代理行为以及订单在队列中的位置信息，无法有效捕捉执行可能性的模式。需要结合更丰富的市场信号和表达性强的神经网络架构来提高预测准确性和可解释性。

Method: KANFormer结合了Dilated Causal Convolutional网络和Transformer编码器，并通过Kolmogorov-Arnold Networks（KANs）增强非线性逼近能力。模型整合了与LOB动态相关的代理行为以及订单在队列中的位置信息，使用SHAP进行特征重要性随时间变化的分析。

Result: 在CAC 40指数期货带标签订单数据上的评估显示，KANFormer在校准指标（右删失对数似然、集成Brier分数）和判别指标（C指数、时间依赖性AUC）上均优于现有工作。SHAP分析揭示了特征重要性随时间变化的模式。

Conclusion: 结合丰富的市场信号和表达性强的神经网络架构能够实现准确且可解释的成交概率预测。KANFormer通过整合市场级和代理级信息，在限价单成交时间预测任务上取得了显著改进。

Abstract: This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.

</details>


### [16] [A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning](https://arxiv.org/abs/2512.05753)
*Wencheng Cai,Xuchao Gao,Congying Han,Mingqiang Li,Tiande Guo*

Main category: cs.AI

TL;DR: 提出FARDA框架，使用深度强化学习快速部署认知雷达对抗干扰，比传统进化算法快约7000倍，覆盖效果相当。


<details>
  <summary>Details</summary>
Motivation: 现代战争中快速部署认知雷达对抗干扰是关键挑战，现有基于进化算法的方法耗时且易陷入局部最优。

Method: 将雷达部署问题建模为端到端任务，设计深度强化学习算法，开发集成神经模块感知热图信息和新奖励格式。

Result: FARDA达到与进化算法相当的覆盖效果，同时部署速度提升约7000倍，消融实验验证各组件必要性。

Conclusion: FARDA框架通过神经网络的快速推理有效解决了雷达快速部署问题，显著提升了部署效率。

Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.

</details>


### [17] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: 论文提出进化推理优化（ERO）框架，通过进化算法优化LLMs的推理能力，发现GPT-5等最新模型仍存在系统2推理局限，但通过简单进化循环可显著增强较弱模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在特定任务上表现出色，但在通用智能和系统2推理（慢思考）能力方面仍有不足。研究旨在探索机器智能能否像人类一样获得真正的推理能力，而不仅仅是特定技能。

Method: 提出进化推理优化（ERO）框架：1）初始化多个LLMs作为种群；2）采用进化策略优化种群，最大化最佳个体的量化推理分数；3）通过"适者生存"原则搜索具有强大推理能力的个体。

Result: 1）最新LLMs如GPT-5仍表现出有限的系统2推理能力；2）通过ERO的简单进化循环，相对较弱的模型（Qwen-7B）可被增强，涌现出强大的推理能力。

Conclusion: 机器智能可以通过进化方法获得类似人类的推理能力，ERO框架为提升LLMs的通用智能提供了有效途径，表明进化优化是增强模型推理能力的可行方向。

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [18] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 论文认为LLM不是AGI的死胡同，而是必要的模式存储库，真正缺失的是能够选择、约束和绑定这些模式的系统2协调层。论文提出了UCCT理论和MACI架构来实现这种协调。


<details>
  <summary>Details</summary>
Motivation: 针对"LLM只是模式匹配器，无法进行推理或规划"的批评，作者认为这种观点错误地识别了瓶颈。问题不在于LLM本身，而在于缺乏一个能够协调LLM模式存储库的系统2层。

Method: 提出了UCCT理论，将推理建模为由有效支持度、表征不匹配和自适应锚定预算控制的相变过程。然后基于UCCT开发了MACI架构，包含诱饵机制、过滤机制和持久性机制三个核心组件。

Result: 论文理论性地展示了如何通过UCCT理论框架和MACI架构实现从无基础的生成到目标导向推理的转变，将常见的反对意见重新定义为可测试的协调失败。

Conclusion: 通往AGI的道路应该通过LLM而不是绕过它们，关键在于为LLM的模式存储库添加适当的协调层，而不是否定LLM作为系统1基板的价值。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [19] [Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma](https://arxiv.org/abs/2512.05824)
*Hafsa Akebli,Adam Shephard,Vincenzo Della Mea,Nasir Rajpoot*

Main category: cs.AI

TL;DR: 本文提出了一种多模态肿瘤学智能体（MOA），整合基于TITAN基础模型的IDH1突变预测组织学工具，结合临床和基因组数据推理，在低级别胶质瘤中实现准确的IDH1突变预测。


<details>
  <summary>Details</summary>
Motivation: 低级别胶质瘤中IDH1突变具有重要的临床意义，但现有预测方法存在局限性。需要开发能够整合多模态信息（包括组织学、临床和基因组数据）并利用外部生物医学知识的智能系统来提高预测准确性。

Method: 开发多模态肿瘤学智能体（MOA），包含：1）基于TITAN基础模型的IDH1突变预测组织学工具；2）通过PubMed、Google Search和OncoKB对结构化临床和基因组输入进行推理的模块。在TCGA-LGG队列的488名患者上进行评估。

Result: MOA（无组织学工具）F1分数为0.826，优于临床基线（0.798）。融合组织学特征后，MOA达到最高性能，F1分数为0.912，超过组织学基线（0.894）和融合组织学-临床基线（0.897）。

Conclusion: MOA能够通过整合外部生物医学资源捕获互补的突变相关信息，显著提高IDH1突变预测准确性，为低级别胶质瘤的精准医疗提供有力工具。

Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.

</details>


### [20] [Using Large Language Models to Create Personalized Networks From Therapy Sessions](https://arxiv.org/abs/2512.05836)
*Clarissa W. Ong,Hiba Arnaout,Kate Sheehan,Estella Fox,Eugen Owtscharow,Iryna Gurevych*

Main category: cs.AI

TL;DR: 利用LLMs从治疗记录自动生成个性化心理网络，支持临床个案概念化和治疗规划，专家评估显示该方法在临床效用和可解释性方面优于直接提示方法。


<details>
  <summary>Details</summary>
Motivation: 个性化心理治疗需要基于个性化网络选择治疗模块，但传统方法需要密集的纵向数据，难以规模化。本研究旨在利用LLMs解决这一问题，从治疗记录中自动生成临床相关网络。

Method: 开发端到端管道：1) 标注77个治疗记录中的3364个心理过程及其维度；2) 使用上下文学习联合识别心理过程和维度；3) 两步法将过程组织成网络：先聚类成临床有意义的簇，再生成解释增强的簇间关系。

Result: 方法在少量训练示例下表现优异。专家评估显示，多步方法生成的网络在临床效用和可解释性方面优于直接提示方法，90%专家偏好该方法。网络在临床相关性、新颖性和有用性方面得分72-75%。

Conclusion: 研究证明了使用LLMs从治疗记录创建临床相关网络的可行性。该方法优势包括自下而上的个案概念化和潜在主题识别。未来研究应比较这些网络与其他个性化治疗方法对治疗结果的影响。

Abstract: Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.

</details>


### [21] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: 基于GPT-5的论文正确性检查器发现，顶级AI会议和期刊发表的论文中存在显著数量的客观错误，且错误数量随时间增加，人类专家验证的准确率达83.2%，检查器还能为75.8%的错误提出正确修正方案。


<details>
  <summary>Details</summary>
Motivation: 同行评审出版物是构建新研究和知识的基础，但文献中的错误会传播并导致后续研究混乱和可复现性问题。研究加速和同行评审系统压力使得错误更难被发现和避免。

Method: 开发基于GPT-5的论文正确性检查器，系统识别顶级AI会议和期刊已发表论文中的客观错误（如公式、推导、计算、图表错误），排除主观考量，并由人类专家验证AI识别的潜在错误。

Result: 发现已发表论文包含不可忽视的客观错误数量，且平均错误数随时间增加：NeurIPS从2021年的3.8个增至2025年的5.9个（增长55.3%）；ICLR从2018年的4.1个增至2025年的5.2个；TMLR从2022/23年的5.0个增至2025年的5.5个。人类专家验证316个潜在错误中263个为真实错误，精确率83.2%。AI检查器能为75.8%的识别错误提出正确修正。

Conclusion: 前沿大语言模型在检测和修正已发表论文中的客观错误方面具有潜力，有助于建立更坚实的知识基础，减少文献混乱并增强可复现性。

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [22] [PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation](https://arxiv.org/abs/2512.05930)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: PRiSM是一个用于评估视觉语言模型在科学领域推理能力的动态多模态基准测试，包含超过24,750个大学水平的物理和数学问题，通过Python代码生成和验证科学正确性。


<details>
  <summary>Details</summary>
Motivation: 当前评估视觉语言模型在科学领域（如数学和物理）的基准测试存在局限性：缺乏中间推理步骤、对变化的鲁棒性不足、缺少验证科学正确性的机制。这些领域需要概念理解、符号推理和遵守形式化定律，而现有基准无法满足这些要求。

Method: 提出了PRiSM基准测试，使用基于代理的PrismAgent管道生成结构化问题实例。每个问题包含动态文本和视觉输入、生成的图形，以及丰富的结构化输出：用于生成和验证真实值的可执行Python代码，以及详细的逐步推理步骤。

Result: PRiSM包含超过24,750个大学水平的物理和数学问题，支持五个针对性评估任务：泛化能力、符号程序合成、扰动鲁棒性、推理修正和歧义消解。通过对现有视觉语言模型的全面评估，揭示了它们在科学推理方面的局限性。

Conclusion: PRiSM基准测试通过其动态特性和Python驱动的自动真实值生成，能够对多模态视觉语言模型进行细粒度实验审计，揭示失败模式、不确定性行为和科学推理的局限性，为深入理解模型的科学推理能力提供了工具。

Abstract: Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities.

</details>


### [23] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: TRACE框架通过透明推理和一致性评估诊断大视觉语言模型的数学科学推理轨迹，使用辅助推理集分解复杂问题，评估中间步骤，发现标准评估忽略的错误。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在可靠数学和科学推理方面仍面临挑战，标准最终答案评估往往掩盖推理错误，导致无声失败持续存在。

Method: 引入TRACE框架，使用辅助推理集（紧凑的子问题-答案对）分解复杂问题，通过基于一致性的指标评估中间步骤，暴露标准评估忽略的失败。

Result: 实验表明，ARS上的一致性相关于最终答案正确性，帮助精确定位推理步骤中的失败，为模型改进提供可操作信号。TRACE定义置信区域区分可靠与不可靠推理路径。

Conclusion: TRACE框架通过透明推理轨迹评估和一致性分析，为诊断和改进大视觉语言模型的数学科学推理能力提供了有效工具，支持有效的过滤、调试和模型优化。

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [24] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: VQR-DQN将变分量子电路与Rainbow DQN结合，用于人力资源分配问题，相比传统方法获得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 资源分配问题是NP难问题，传统深度强化学习方法受限于经典函数逼近器的表达能力，需要更强大的表示能力来处理组合复杂性

Method: 提出VQR-DQN方法，将环形拓扑变分量子电路与Rainbow DQN集成，利用量子叠加和纠缠特性；将人力资源分配问题建模为基于官员能力、事件调度和转移时间的马尔可夫决策过程

Result: 在四个人力资源分配基准测试中，VQR-DQN相比随机基线减少26.8%的归一化完工时间，比Double DQN和经典Rainbow DQN提升4.9-13.4%

Conclusion: 量子增强的深度强化学习在大规模资源分配中具有潜力，电路表达能力、纠缠和策略质量之间存在理论联系

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [25] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: SymPyBench是一个包含15,045个大学物理问题的大规模合成基准测试，支持无限参数配置，包含三种问题类型和创新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 创建大规模、参数化的物理问题基准测试，以评估语言模型在科学推理方面的能力，解决现有基准测试在多样性、可扩展性和评估指标方面的不足。

Method: 开发了15,045个完全参数化的大学物理问题（90/10训练/测试分割），每个问题都包含结构化逐步推理和可执行Python代码。包含三种问题类型：MC-Symbolic（符号多项选择）、MC-Numerical（数值多项选择）和自由形式（开放式回答）。

Result: 通过最先进的指令调优语言模型实验，揭示了科学推理的优势和局限性。引入了三个新颖的评估指标：一致性分数、失败率和混淆率，用于量化问题变体间的变异性和不确定性。

Conclusion: SymPyBench为开发更鲁棒和可解释的推理系统奠定了基础，通过其动态、代码驱动的特性和多样化的评估指标，能够更全面地评估语言模型的科学推理能力。

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [26] [RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering](https://arxiv.org/abs/2512.05119)
*Rongyang Zhang,Yuqing Huang,Chengqiang Lu,Qimeng Wang,Yan Gao,Yi Wu,Yao Hu,Yin Xu,Wei Wang,Hao Wang,Enhong Chen*

Main category: cs.IR

TL;DR: 论文提出了RAG-IGBench基准，专门用于评估基于检索增强生成的多模态交错生成任务，包含创新的评估指标来测量文本、图像质量及其一致性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，视觉增强的响应能显著提升用户理解和记忆，但现有交错图像-文本生成质量仍面临挑战，且缺乏专门的评估基准和合适的评估指标。

Method: 构建RAG-IGBench基准，基于社交媒体最新公开内容，设计创新评估指标测量文本质量、图像质量及其一致性，将多模态大语言模型与检索机制结合。

Result: 通过实验分析主流MLLM模型的能力与局限，验证评估指标与人工评估高度相关，在训练集上微调的模型在多个基准上表现提升。

Conclusion: RAG-IGBench为交错生成任务提供了全面评估框架，数据集质量和实用性得到验证，有助于推动多模态内容生成研究。

Abstract: In real-world scenarios, providing user queries with visually enhanced responses can considerably benefit understanding and memory, underscoring the great value of interleaved image-text generation. Despite recent progress, like the visual autoregressive model that unifies text and image processing in a single transformer architecture, generating high-quality interleaved content remains challenging. Moreover, evaluations of these interleaved sequences largely remain underexplored, with existing benchmarks often limited by unimodal metrics that inadequately assess the intricacies of combined image-text outputs. To address these issues, we present RAG-IGBench, a thorough benchmark designed specifically to evaluate the task of Interleaved Generation based on Retrieval-Augmented Generation (RAG-IG) in open-domain question answering. RAG-IG integrates multimodal large language models (MLLMs) with retrieval mechanisms, enabling the models to access external image-text information for generating coherent multimodal content. Distinct from previous datasets, RAG-IGBench draws on the latest publicly available content from social platforms and introduces innovative evaluation metrics that measure the quality of text and images, as well as their consistency. Through extensive experiments with state-of-the-art MLLMs (both open-source and proprietary) on RAG-IGBench, we provide an in-depth analysis examining the capabilities and limitations of these models. Additionally, we validate our evaluation metrics by demonstrating their high correlation with human assessments. Models fine-tuned on RAG-IGBench's training set exhibit improved performance across multiple benchmarks, confirming both the quality and practical utility of our dataset. Our benchmark is available at https://github.com/USTC-StarTeam/RAG-IGBench.

</details>


### [27] [The Effect of Document Summarization on LLM-Based Relevance Judgments](https://arxiv.org/abs/2512.05334)
*Samaneh Mohtadi,Kevin Roitero,Stefano Mizzaro,Gianluca Demartini*

Main category: cs.IR

TL;DR: 研究探讨文本摘要如何影响基于大语言模型的检索系统评估可靠性，发现摘要评估在系统排序稳定性上与全文评估相当，但会引入系统性偏差


<details>
  <summary>Details</summary>
Motivation: 人工相关性标注成本高且耗时，大语言模型作为自动评估器显示出潜力。现有研究大多将文档作为固定单元直接输入，需要研究文本摘要如何影响LLM评估的可靠性及其对信息检索评估的下游影响

Method: 使用最先进的大语言模型在多个TREC数据集上，比较基于全文的评估与基于不同长度LLM生成摘要的评估，分析它们与人工标注的一致性、对检索效果评估的影响以及对系统排序稳定性的影响

Result: 摘要评估在系统排序稳定性方面与全文评估相当，但会引入系统性标签分布偏移和偏差，这些偏差因模型和数据集而异

Conclusion: 文本摘要既是实现大规模信息检索评估效率提升的机会，也是影响自动评估可靠性的重要方法论选择，需要谨慎考虑其引入的偏差

Abstract: Relevance judgments are central to the evaluation of Information Retrieval (IR) systems, but obtaining them from human annotators is costly and time-consuming. Large Language Models (LLMs) have recently been proposed as automated assessors, showing promising alignment with human annotations. Most prior studies have treated documents as fixed units, feeding their full content directly to LLM assessors. We investigate how text summarization affects the reliability of LLM-based judgments and their downstream impact on IR evaluation. Using state-of-the-art LLMs across multiple TREC collections, we compare judgments made from full documents with those based on LLM-generated summaries of different lengths. We examine their agreement with human labels, their effect on retrieval effectiveness evaluation, and their influence on IR systems' ranking stability. Our findings show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, while introducing systematic shifts in label distributions and biases that vary by model and dataset. These results highlight summarization as both an opportunity for more efficient large-scale IR evaluation and a methodological choice with important implications for the reliability of automatic judgments.

</details>


### [28] [A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems](https://arxiv.org/abs/2512.05411)
*Pranav Pushkar Mishra,Kranti Prakash Yeole,Ramyashree Keshavamurthy,Mokshit Bharat Surana,Fatemeh Sarayloo*

Main category: cs.IR

TL;DR: 本文提出使用大语言模型进行元数据增强的系统框架，以提升企业环境中RAG系统的文档检索效果。通过比较三种分块策略与嵌入技术组合，发现元数据增强方法显著优于仅基于内容的基线方法。


<details>
  <summary>Details</summary>
Motivation: 在企业环境中，从大型复杂知识库中高效检索相关信息对运营生产力和决策制定至关重要。现有RAG系统在文档检索方面仍有优化空间，特别是文档语义表示和检索准确性方面。

Method: 提出系统化的元数据增强框架，使用LLM动态生成文档片段的元数据。比较三种分块策略（语义分块、递归分块、朴素分块）与先进嵌入技术（TF-IDF加权嵌入、前缀融合等）的组合效果。使用交叉编码器重排序生成基准真值，通过命中率和元数据一致性指标进行严格评估。

Result: 元数据增强方法在所有实验中均优于仅基于内容的基线。递归分块配合TF-IDF加权嵌入达到82.5%的精确率（相比语义仅内容方法的73.3%）。朴素分块配合前缀融合获得最高的Hit Rate@10（0.925）。元数据增强提高了向量聚类质量，同时减少了检索延迟。

Conclusion: 元数据增强是提升RAG系统效果的关键优化方法，能够显著提高文档检索的准确性和效率。该研究为企业部署高性能、可扩展的文档检索解决方案提供了实用见解，证明了元数据增强在不同知识领域中的强大应用价值。

Abstract: In enterprise settings, efficiently retrieving relevant information from large and complex knowledge bases is essential for operational productivity and informed decision-making. This research presents a systematic framework for metadata enrichment using large language models (LLMs) to enhance document retrieval in Retrieval-Augmented Generation (RAG) systems. Our approach employs a comprehensive, structured pipeline that dynamically generates meaningful metadata for document segments, substantially improving their semantic representations and retrieval accuracy. Through extensive experiments, we compare three chunking strategies-semantic, recursive, and naive-and evaluate their effectiveness when combined with advanced embedding techniques. The results demonstrate that metadata-enriched approaches consistently outperform content-only baselines, with recursive chunking paired with TF-IDF weighted embeddings yielding an 82.5% precision rate compared to 73.3% for semantic content-only approaches. The naive chunking strategy with prefix-fusion achieved the highest Hit Rate@10 of 0.925. Our evaluation employs cross-encoder reranking for ground truth generation, enabling rigorous assessment via Hit Rate and Metadata Consistency metrics. These findings confirm that metadata enrichment enhances vector clustering quality while reducing retrieval latency, making it a key optimization for RAG systems across knowledge domains. This work offers practical insights for deploying high-performance, scalable document retrieval solutions in enterprise settings, demonstrating that metadata enrichment is a powerful approach for enhancing RAG effectiveness.

</details>


### [29] [Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms](https://arxiv.org/abs/2512.05967)
*Francesco Granata,Francesco Poggi,Misael Mongiovì*

Main category: cs.IR

TL;DR: 该研究提出了一种增强的RAG架构，通过集成基于实体链接的事实信号来提高意大利语教育问答系统的准确性，在特定领域场景中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型时代，基于语义相似度的RAG系统在专业领域中经常无法确保事实准确性，因为术语歧义会影响检索相关性。特别是在教育问答系统中，需要更可靠的事实基础。

Method: 提出增强的RAG架构，集成基于Wikidata的实体链接模块，并实现三种重排序策略：混合分数加权模型、互惠排名融合和交叉编码器重排序器，结合语义和实体信息。

Result: 在特定领域上下文中，基于互惠排名融合的混合方案显著优于基线和交叉编码器方法；而在通用领域数据集上，交叉编码器获得最佳结果，证实了领域不匹配效应。

Conclusion: 研究强调了领域适应和混合排序策略对提高检索增强生成的事实精度和可靠性的重要性，展示了实体感知RAG系统在教育环境中促进自适应和可靠AI辅导工具的潜力。

Abstract: In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.

</details>


### [30] [EnterpriseEM: Fine-tuned Embeddings for Enterprise Semantic Search](https://arxiv.org/abs/2406.00010)
*Kamalkumar Rathinasamy,Jayarama Nettar,Amit Kumar,Vishal Manchanda,Arun Vijayakumar,Ayush Kataria,Venkateshprasanna Manjunath,Chidambaram GS,Jaskirat Singh Sodhi,Shoeb Shaikh,Wasim Akhtar Khan,Prashant Singh,Tanishq Dattatray Ige,Vipin Tiwari,Rajab Ali Mondal,Harshini K,S Reka,Chetana Amancharla,Faiz ur Rahman,Harikrishnan P A,Indraneel Saha,Bhavya Tiwary,Navin Shankar Patel,Pradeep T S,Balaji A J,Priyapravas,Mohammed Rafee Tarafdar*

Main category: cs.IR

TL;DR: 本文提出了一种将预训练嵌入模型适配到企业环境的综合方法，通过微调提升企业信息检索的准确性和相关性。


<details>
  <summary>Details</summary>
Motivation: 企业面临管理专有非结构化数据的挑战，现有AI检索方案使用的预训练嵌入模型可能无法完全适应企业特定数据特征，导致检索效果不佳。

Method: 提出从数据准备到模型微调和评估的完整流程，通过微调预训练嵌入模型使其更好地适应企业检索任务。

Result: 微调后的嵌入模型在企业环境中显著提高了搜索结果的精确度和相关性。

Conclusion: 通过将预训练嵌入模型适配到企业环境，可以有效提升信息检索解决方案的性能，对企业信息管理具有潜在益处。

Abstract: Enterprises grapple with the significant challenge of managing proprietary unstructured data, hindering efficient information retrieval. This has led to the emergence of AI-driven information retrieval solutions, designed to adeptly extract relevant insights to address employee inquiries. These solutions often leverage pre-trained embedding models and generative models as foundational components. While pre-trained embeddings may exhibit proximity or disparity based on their original training objectives, they might not fully align with the unique characteristics of enterprise-specific data, leading to suboptimal alignment with the retrieval goals of enterprise environments. In this paper, we propose a comprehensive methodology for contextualizing pre-trained embedding models to enterprise environments, covering the entire process from data preparation to model fine-tuning and evaluation. By adapting the embeddings to better suit the retrieval tasks prevalent in enterprises, we aim to enhance the performance of information retrieval solutions. We discuss the process of fine-tuning, its effect on retrieval accuracy, and the potential benefits for enterprise information management. Our findings demonstrate the efficacy of fine-tuned embedding models in improving the precision and relevance of search results in enterprise settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Advanced Unsupervised Learning: A Comprehensive Overview of Multi-View Clustering Techniques](https://arxiv.org/abs/2512.05169)
*Abdelmalik Moujahid,Fadi Dornaika*

Main category: cs.LG

TL;DR: 本文是一篇关于多视图聚类（MVC）的综述性论文，系统性地分类了MVC方法，分析了各种方法的优缺点，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 机器学习技术面临计算约束、单视图学习算法局限性以及处理来自不同领域、来源或视图的大数据集复杂性等挑战。多视图聚类作为无监督多视图学习的一种，能够克服这些挑战，弥补单视图方法的不足，为各种无监督学习任务提供更丰富的数据表示和有效解决方案。

Method: 本文采用系统性综述方法，对140多篇基础和近期文献进行了回顾，将多视图聚类方法系统性地分类为：协同训练、协同正则化、子空间、深度学习、基于核、基于锚点和基于图等策略。同时比较了早期融合、晚期融合和联合学习等集成策略，并结构化地调查了医疗保健、多媒体和社交网络分析等领域的实际用例。

Result: 研究提供了对多视图聚类方法的系统性分类和深入分析，揭示了各种方法的优缺点以及可扩展性和不完整数据等实际挑战。通过整合这些工作，填补了MVC研究中的现有空白，并为该领域的进展提供了可行的见解。

Conclusion: 多视图聚类是一种强大的方法，能够克服单视图学习的局限性。本文通过系统性综述为MVC研究提供了全面的框架，包括方法分类、挑战分析和未来方向展望，旨在推动该领域的进一步发展。

Abstract: Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.

</details>


### [32] [Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models](https://arxiv.org/abs/2512.05216)
*Rajna Fani,Rafi Al Attrach,David Restrepo,Yugang Jia,Leo Anthony Celi,Peter Schüffler*

Main category: cs.LG

TL;DR: 提出CV-Masking方法，根据生物标志物波动性调整掩码概率，改进电子健康记录的MAE预训练效果


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法对电子健康记录使用均匀随机掩码，假设所有特征同等可预测。但实际上实验室测试特征具有显著异质性：一些生物标志物（如钠）稳定，而另一些（如乳酸）波动大且更难建模。临床上，波动性生物标志物常指示急性病理生理变化，需要更复杂的建模来捕捉其时间模式。

Method: 提出波动性感知预训练策略CV-Masking（变异系数掩码），根据每个特征的内在变异性自适应调整掩码概率。结合与临床工作流程对齐的仅值掩码目标，该方法优于随机和基于方差的策略。

Result: 在大规模实验室测试面板上的实验表明，CV-Masking增强了重建效果，改善了下游预测性能，加速了收敛，产生了更稳健且临床意义更强的电子健康记录表示。

Conclusion: CV-Masking方法通过考虑生物标志物的波动性异质性，为电子健康记录的MAE预训练提供了更有效的策略，能更好地捕捉临床相关的时间模式。

Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.

</details>


### [33] [Rethinking Tokenization for Clinical Time Series: When Less is More](https://arxiv.org/abs/2512.05217)
*Rafi Al Attrach,Rajna Fani,David Restrepo,Yugang Jia,Peter Schüffler*

Main category: cs.LG

TL;DR: 本文系统评估了临床时间序列建模中的分词策略，发现时间编码对下游任务无显著帮助，值特征重要性因任务而异，冻结预训练编码器优于可训练版本，且更简单的参数高效方法通常能取得良好性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对电子健康记录处理中分词策略有效性的公平比较，不同分词方法对临床预测任务的影响尚不明确，需要系统评估以指导模型设计。

Method: 使用基于Transformer的架构，在MIMIC-IV数据集上对四个临床预测任务进行控制性消融实验，评估不同分词策略（时间编码、值特征、代码序列等）的效果。

Result: 显式时间编码对评估的下游任务无一致统计显著益处；值特征重要性因任务而异（影响死亡率预测但不影响再入院）；冻结预训练代码编码器显著优于可训练版本且参数更少；更大的临床编码器在所有任务中表现一致更好。

Conclusion: 更简单、参数高效的方法在许多情况下能取得强性能，但最佳分词策略仍取决于具体任务。控制性评估实现了更公平的分词比较，为临床时间序列建模提供了实用指导。

Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.

</details>


### [34] [Mitigating the Antigenic Data Bottleneck: Semi-supervised Learning with Protein Language Models for Influenza A Surveillance](https://arxiv.org/abs/2512.05222)
*Yanhua Xu*

Main category: cs.LG

TL;DR: 结合预训练蛋白质语言模型与半监督学习，可在标记数据稀缺时准确预测流感病毒抗原性，解决传统血凝抑制实验的规模化瓶颈。


<details>
  <summary>Details</summary>
Motivation: 流感病毒抗原性快速进化需要频繁更新疫苗，但传统血凝抑制实验劳动密集、难以规模化，导致基因组数据远超可用表型标记，限制了传统监督模型的有效性。

Method: 评估两种半监督学习策略（自训练和标签传播），与全监督基线比较，使用四种蛋白质语言模型嵌入（ESM-2、ProtVec、ProtT5、ProtBert）应用于血凝素序列。采用嵌套交叉验证框架模拟低标记场景（25%、50%、75%、100%标记可用性），涵盖四种流感亚型。

Result: 半监督学习在标记稀缺条件下持续提升性能。自训练与ProtVec结合产生最大相对增益，表明半监督学习可补偿低分辨率表示。ESM-2保持高度稳健，仅用25%标记数据即可达到0.82以上F1分数。H1N1和H9N2预测准确率高，而高度变异的H3N2亚型仍具挑战性，但半监督学习缓解了性能下降。

Conclusion: 整合蛋白质语言模型与半监督学习可解决抗原性标记瓶颈，更有效地利用未标记监测序列，支持快速变异株优先排序和及时疫苗株选择。

Abstract: Influenza A viruses (IAVs) evolve antigenically at a pace that requires frequent vaccine updates, yet the haemagglutination inhibition (HI) assays used to quantify antigenicity are labor-intensive and unscalable. As a result, genomic data vastly outpace available phenotypic labels, limiting the effectiveness of traditional supervised models. We hypothesize that combining pre-trained Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) can retain high predictive accuracy even when labeled data are scarce. We evaluated two SSL strategies, Self-training and Label Spreading, against fully supervised baselines using four PLM-derived embeddings (ESM-2, ProtVec, ProtT5, ProtBert) applied to haemagglutinin (HA) sequences. A nested cross-validation framework simulated low-label regimes (25%, 50%, 75%, and 100% label availability) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). SSL consistently improved performance under label scarcity. Self-training with ProtVec produced the largest relative gains, showing that SSL can compensate for lower-resolution representations. ESM-2 remained highly robust, achieving F1 scores above 0.82 with only 25% labeled data, indicating that its embeddings capture key antigenic determinants. While H1N1 and H9N2 were predicted with high accuracy, the hypervariable H3N2 subtype remained challenging, although SSL mitigated the performance decline. These findings demonstrate that integrating PLMs with SSL can address the antigenicity labeling bottleneck and enable more effective use of unlabeled surveillance sequences, supporting rapid variant prioritization and timely vaccine strain selection.

</details>


### [35] [Variance Matters: Improving Domain Adaptation via Stratified Sampling](https://arxiv.org/abs/2512.05226)
*Andrea Napoli,Paul White*

Main category: cs.LG

TL;DR: 提出VaRDASS方法，通过分层采样减少无监督域自适应中域差异估计的方差，提升模型在目标域的性能。


<details>
  <summary>Details</summary>
Motivation: 无监督域自适应（UDA）通过最小化域差异来解决域偏移问题，但在随机设置中，域差异估计存在高方差问题，这会阻碍方法理论优势的发挥。

Method: 提出VaRDASS方法，这是首个专门用于UDA的随机方差减少技术。针对相关性对齐和最大均值差异（MMD）两种差异度量，推导了专门的分层采样目标。引入了一种实用的k-means风格优化算法。

Result: 在三个域偏移数据集上的实验表明，该方法提高了差异估计的准确性和目标域性能。证明了在特定假设下，提出的MMD目标在理论上是最优的（即最小化方差）。

Conclusion: VaRDASS通过分层采样有效减少了UDA中域差异估计的方差，为域自适应提供了更稳定可靠的差异度量方法，提升了模型在目标域的泛化能力。

Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.

</details>


### [36] [MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System](https://arxiv.org/abs/2512.05234)
*Felix Mulitze,Herbert Woisetschläger,Hans Arno Jacobsen*

Main category: cs.LG

TL;DR: MAR-FL是一种新型的P2P联邦学习系统，通过迭代分组聚合大幅降低通信开销，同时保持对网络变动的鲁棒性，通信复杂度从O(N²)降低到O(N log N)。


<details>
  <summary>Details</summary>
Motivation: 下一代无线系统与分布式机器学习的融合需要高效且鲁棒的联邦学习方法，但现有的P2P FL方法存在通信复杂度过高的问题，限制了实际可扩展性。

Method: 提出MAR-FL系统，采用迭代分组聚合机制，通过分组方式减少通信开销，同时保持对不可靠客户端和网络变动的鲁棒性，并可集成隐私计算。

Result: MAR-FL将通信复杂度从O(N²)降低到O(N log N)，显著提升了系统可扩展性，特别是在聚合轮次中参与节点数量增加时仍能保持有效性。

Conclusion: MAR-FL通过创新的分组聚合策略成功解决了P2P FL的通信瓶颈问题，为无线网络环境下的分布式机器学习提供了高效且鲁棒的解决方案。

Abstract: The convergence of next-generation wireless systems and distributed Machine Learning (ML) demands Federated Learning (FL) methods that remain efficient and robust with wireless connected peers and under network churn. Peer-to-peer (P2P) FL removes the bottleneck of a central coordinator, but existing approaches suffer from excessive communication complexity, limiting their scalability in practice. We introduce MAR-FL, a novel P2P FL system that leverages iterative group-based aggregation to substantially reduce communication overhead while retaining resilience to churn. MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines, and thereby maintains effectiveness especially as the number of peers in an aggregation round grows. The system is robust towards unreliable FL clients and can integrate private computing.

</details>


### [37] [Edged Weisfeiler-Lehman Algorithm](https://arxiv.org/abs/2512.05238)
*Xiao Yue,Bo Liu,Feng Zhang,Guangzhi Qu*

Main category: cs.LG

TL;DR: 本文提出了一种新的E-WL算法，扩展了经典的1-WL算法以纳入边特征，并基于此构建了EGIN模型，解决了传统GNN忽略边特征的问题。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNNs）广泛采用传播-聚合方法，而Weisfeiler-Lehman（1-WL）算法通过颜色细化测试同构性，但两者都未能充分利用边特征。在许多领域中，边特征包含重要信息，因此需要开发能够有效利用边特征的算法和模型。

Method: 提出了E-WL算法，扩展了原始的1-WL算法以纳入边特征。基于E-WL算法，进一步设计了EGIN模型，专门用于利用图数据的边特征进行图学习。

Result: 在12个具有边特征的基准图数据集上进行评估，与最先进的基线模型比较。实验结果表明，提出的EGIN模型在图分类任务上普遍表现出优越性能。

Conclusion: 通过扩展1-WL算法以纳入边特征，并构建相应的EGIN模型，成功解决了传统GNN忽略边特征的问题，在图分类任务中取得了更好的性能。

Abstract: As a classical approach on graph learning, the propagation-aggregation methodology is widely exploited by many of Graph Neural Networks (GNNs), wherein the representation of a node is updated by aggregating representations from itself and neighbor nodes recursively. Similar to the propagation-aggregation methodology, the Weisfeiler-Lehman (1-WL) algorithm tests isomorphism through color refinement according to color representations of a node and its neighbor nodes. However, 1-WL does not leverage any edge features (labels), presenting a potential improvement on exploiting edge features in some fields. To address this limitation, we proposed a novel Edged-WL algorithm (E-WL) which extends the original 1-WL algorithm to incorporate edge features. Building upon the E-WL algorithm, we also introduce an Edged Graph Isomorphism Network (EGIN) model for further exploiting edge features, which addresses one key drawback in many GNNs that do not utilize any edge features of graph data. We evaluated the performance of proposed models using 12 edge-featured benchmark graph datasets and compared them with some state-of-the-art baseline models. Experimental results indicate that our proposed EGIN models, in general, demonstrate superior performance in graph learning on graph classification tasks.

</details>


### [38] [Bridging quantum and classical computing for partial differential equations through multifidelity machine learning](https://arxiv.org/abs/2512.05241)
*Bruno Jacob,Amanda A. Howard,Panos Stinis*

Main category: cs.LG

TL;DR: 量子PDE求解器受限于硬件约束，本文提出多保真度学习框架，用量子粗解结合少量经典数据训练校正模型，提升精度并实现时间外推。


<details>
  <summary>Details</summary>
Motivation: 量子PDE算法面临近端硬件限制：量子比特数限制空间分辨率，电路深度限制长时间积分精度。这些瓶颈使量子求解器只能提供低保真度解，尽管理论上具有计算加速潜力。

Method: 引入多保真度学习框架，用量子求解器产生的丰富低保真度解训练代理模型，然后通过多保真度神经网络架构学习校正映射，平衡线性和非线性变换。在粘性Burgers方程和不可压缩Navier-Stokes流动等非线性PDE基准上，通过量子格子玻尔兹曼方法进行验证。

Result: 框架成功校正了粗量子预测，实现了远超经典训练窗口的时间外推。该方法减少了昂贵的高保真度模拟需求，同时产生与经典精度相当的预测结果。

Conclusion: 通过弥合硬件受限的量子模拟与应用需求之间的差距，为从当前量子设备中提取计算价值建立了途径，推进了近端量子计算在计算物理中的算法开发和实际部署。

Abstract: Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.

</details>


### [39] [When unlearning is free: leveraging low influence points to reduce computational costs](https://arxiv.org/abs/2512.05254)
*Anat Kleiman,Robert Fisher,Ben Deaner,Udi Wieder*

Main category: cs.LG

TL;DR: 该论文提出了一种高效的机器学习遗忘框架，通过识别对模型输出影响可忽略的训练数据子集，在遗忘前减少数据集规模，从而显著降低计算成本（最高约50%）


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中数据隐私问题的日益突出，从训练模型中遗忘或移除特定数据点的能力变得越来越重要。现有的遗忘方法通常平等对待遗忘集中的所有数据点，但作者质疑那些对模型学习影响可忽略的数据点是否真的需要被移除。

Method: 通过对语言和视觉任务中影响函数的比较分析，识别出对模型输出影响可忽略的训练数据子集。基于这一洞察，提出了一个高效的遗忘框架，在遗忘前减少数据集规模。

Result: 该方法在真实世界实证案例中实现了显著的计算节省，最高可达约50%的计算成本降低。

Conclusion: 并非所有数据点都需要被遗忘，通过识别和移除对模型影响可忽略的数据点，可以构建更高效的机器学习遗忘框架，在保护隐私的同时大幅降低计算开销。

Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.

</details>


### [40] [DMAGT: Unveiling miRNA-Drug Associations by Integrating SMILES and RNA Sequence Structures through Graph Transformer Models](https://arxiv.org/abs/2512.05287)
*Ziqi Zhang*

Main category: cs.LG

TL;DR: DMAGT：基于多层Transformer图神经网络预测药物-miRNA关联的新模型，在三个数据集上达到95.24%的AUC，为miRNA药物开发提供新途径。


<details>
  <summary>Details</summary>
Motivation: miRNA在基因调控中起重要作用，为药物开发提供新靶点，但传统湿实验受效率和成本限制，难以广泛探索药物与miRNA的潜在关联。

Method: 提出DMAGT模型：将药物-miRNA关联转化为图结构，使用Word2Vec嵌入药物分子结构和miRNA碱基结构特征，利用图Transformer模型学习嵌入特征和关系结构，预测药物-miRNA关联。

Result: 在ncDR、RNAInter和SM2miR三个数据集上测试，最高AUC达95.24±0.05；对5-氟尿嘧啶和奥沙利铂两种药物，预测的20个最可能关联中有14个得到验证。

Conclusion: DMAGT在预测药物-miRNA关联方面表现出优异的性能和稳定性，为miRNA药物开发提供了新的快捷途径。

Abstract: MiRNAs, due to their role in gene regulation, have paved a new pathway for pharmacology, focusing on drug development that targets miRNAs. However, traditional wet lab experiments are limited by efficiency and cost constraints, making it difficult to extensively explore potential associations between developed drugs and target miRNAs. Therefore, we have designed a novel machine learning model based on a multi-layer transformer-based graph neural network, DMAGT, specifically for predicting associations between drugs and miRNAs. This model transforms drug-miRNA associations into graphs, employs Word2Vec for embedding features of drug molecular structures and miRNA base structures, and leverages a graph transformer model to learn from embedded features and relational structures, ultimately predicting associations between drugs and miRNAs. To evaluate DMAGT, we tested its performance on three datasets composed of drug-miRNA associations: ncDR, RNAInter, and SM2miR, achieving up to AUC of $95.24\pm0.05$. DMAGT demonstrated superior performance in comparative experiments tackling similar challenges. To validate its practical efficacy, we specifically focused on two drugs, namely 5-Fluorouracil and Oxaliplatin. Of the 20 potential drug-miRNA associations identified as the most likely, 14 were successfully validated. The above experiments demonstrate that DMAGT has an excellent performance and stability in predicting drug-miRNA associations, providing a new shortcut for miRNA drug development.

</details>


### [41] [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出RSA2C算法，将SHAP可解释性方法融入Actor-Critic框架，通过状态归因指导训练，实现高效、稳定且可解释的强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有Actor-Critic方法缺乏可解释性，当前可解释强化学习方法很少利用状态归因来辅助训练，通常平等对待所有状态特征，忽略了不同状态维度对奖励的异质性影响。

Method: 提出RSA2C算法，包含Actor、Value Critic和Advantage Critic三个组件，均基于RKHS实现。Actor使用带马氏距离加权算子值核的向量值RKHS，Critic使用标量RKHS。通过RKHS-SHAP计算状态归因，转换为马氏门控权重来调节Actor梯度和Advantage Critic目标。

Result: 理论上推导了在状态扰动下的全局非渐近收敛界，显示算法通过扰动误差项保持稳定性，通过收敛误差项实现效率。在三个标准连续控制环境中的实验结果表明算法实现了效率、稳定性和可解释性。

Conclusion: RSA2C成功将可解释性融入Actor-Critic框架，通过状态归因指导训练，在保持算法效率的同时提供了可解释性，为强化学习的可解释性研究提供了新思路。

Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.

</details>


### [42] [CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators](https://arxiv.org/abs/2512.05297)
*Xianglong Hou,Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: CFO框架通过流匹配直接学习PDE的右侧项，避免自回归预测的误差累积，实现时间分辨率无关的训练和推理


<details>
  <summary>Details</summary>
Motivation: 传统神经算子代理使用自回归预测方案，在长时间推演中会累积误差，且需要均匀时间离散化。需要一种能够学习连续时间PDE动态而不增加标准连续方法计算负担的方法

Method: CFO框架重新利用流匹配直接学习PDE的右侧项，无需通过ODE求解器反向传播。通过时间样条拟合轨迹数据，使用节点处时间导数的有限差分估计来构建概率路径，其速度场近似真实PDE动态。然后通过流匹配训练神经算子来预测这些解析速度场

Result: 在四个基准测试（Lorenz、1D Burgers、2D扩散反应、2D浅水方程）中，CFO表现出优越的长期稳定性和显著的数据效率。仅使用25%不规则子采样时间点训练的CFO优于使用完整数据训练的自回归基线，相对误差减少高达87%。推理时仅需50%的函数评估即可超越基线，同时支持反向时间推理和任意时间查询

Conclusion: CFO框架提供了一种时间分辨率无关的连续时间PDE学习方法，避免了自回归方法的误差累积问题，在数据效率和长期稳定性方面显著优于传统方法，同时支持灵活的推理能力

Abstract: Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.

</details>


### [43] [Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)](https://arxiv.org/abs/2512.05306)
*Y. Sungtaek Ju*

Main category: cs.LG

TL;DR: SVGP KANs：将稀疏变分高斯过程与Kolmogorov-Arnold网络结合，为科学机器学习提供可解释的不确定性量化框架


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold网络虽具可解释性，但缺乏系统的不确定性量化能力，这在科学应用中至关重要

Method: 将稀疏变分高斯过程推断与Kolmogorov-Arnold拓扑结构集成，通过解析矩匹配在深度加性结构中传播不确定性

Result: 在三个案例研究中成功区分偶然不确定性和认知不确定性：流体流动重建中的异方差测量噪声校准、平流-扩散动力学多步预测中的置信度退化量化、卷积自编码器的分布外检测

Conclusion: SVGP KANs是科学机器学习中具有不确定性感知能力的有前景架构，计算复杂度与样本量呈准线性关系

Abstract: Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning.

</details>


### [44] [The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?](https://arxiv.org/abs/2512.05311)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee*

Main category: cs.LG

TL;DR: 研究评估了最先进机器学习模型区分人类与LLM生成科学想法的能力，发现经过连续改写后检测性能平均下降25.4%，加入研究问题作为上下文可提升2.97%性能，简化改写风格对检测最具挑战性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为研究代理的日益依赖，区分LLM与人类生成的想法对于理解LLM研究能力的认知细微差别变得至关重要。虽然检测LLM生成文本已有广泛研究，但区分人类与LLM生成的科学想法仍是一个未探索的领域。

Method: 系统评估最先进机器学习模型区分人类与LLM生成想法的能力，特别是在连续改写阶段后。研究考察了将研究问题作为上下文信息对检测性能的影响，并分析了不同改写风格对检测难度的影响。

Result: 检测性能在五个连续改写阶段后平均下降25.4%；加入研究问题作为上下文信息可将检测性能提升最多2.97%；检测算法在想法被改写成简化的非专家风格时面临最大困难，这是导致可区分LLM特征消失的主要原因。

Conclusion: 区分人类与LLM生成的科学想法具有挑战性，特别是在经过多次改写后。上下文信息有助于提升检测性能，但简化改写风格会显著削弱检测算法的有效性，这为未来研究LLM作为研究代理的认知特征提供了重要见解。

Abstract: With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.

</details>


### [45] [Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay](https://arxiv.org/abs/2512.05320)
*Mehmet Efe Lorasdagi,Dogan Can Cicek,Furkan Burak Mutlu,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: 本文提出了一种解耦优先经验回放（DPER）方法，通过为Actor和Critic网络分别采样不同的转移批次来改进深度确定性策略梯度算法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的Actor-Critic架构通常使用相同的经验回放批次来训练两个网络，但Actor和Critic的学习目标和更新动态不同，这引发了对统一批次使用是否最优的疑问。

Method: 提出了DPER方法，允许为Actor和Critic独立采样转移批次。该方法可与任何连续控制领域的离策略深度强化学习算法集成，并与最先进的Twin Delayed DDPG算法结合进行评估。

Result: DPER在多个MuJoCo任务中优于传统的经验回放策略（如普通经验回放和优先经验回放）。

Conclusion: 解耦Actor和Critic的经验回放可以改善训练动态和最终策略质量，DPER为广泛的Actor-Critic离策略强化学习算法提供了通用的性能增强机制。

Abstract: Background: Deep Deterministic Policy Gradient-based reinforcement learning algorithms utilize Actor-Critic architectures, where both networks are typically trained using identical batches of replayed transitions. However, the learning objectives and update dynamics of the Actor and Critic differ, raising concerns about whether uniform transition usage is optimal.
  Objectives: We aim to improve the performance of deep deterministic policy gradient algorithms by decoupling the transition batches used to train the Actor and the Critic. Our goal is to design an experience replay mechanism that provides appropriate learning signals to each component by using separate, tailored batches.
  Methods: We introduce Decoupled Prioritized Experience Replay (DPER), a novel approach that allows independent sampling of transition batches for the Actor and the Critic. DPER can be integrated into any off-policy deep reinforcement learning algorithm that operates in continuous control domains. We combine DPER with the state-of-the-art Twin Delayed DDPG algorithm and evaluate its performance across standard continuous control benchmarks.
  Results: DPER outperforms conventional experience replay strategies such as vanilla experience replay and prioritized experience replay in multiple MuJoCo tasks from the OpenAI Gym suite.
  Conclusions: Our findings show that decoupling experience replay for Actor and Critic networks can enhance training dynamics and final policy quality. DPER offers a generalizable mechanism that enhances performance for a wide class of actor-critic off-policy reinforcement learning algorithms.

</details>


### [46] [Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition](https://arxiv.org/abs/2512.05323)
*Adam Lizerbram,Shane Stevenson,Iman Khadir,Matthew Tu,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 测试AI天气预测模型FourCastNetv2对输入噪声的鲁棒性，通过注入高斯噪声和完全随机初始条件来评估其对飓风预测的敏感性


<details>
  <summary>Details</summary>
Motivation: 评估AI天气预测模型在输入噪声和不确定性下的鲁棒性对于极端天气事件（如飓风）的可靠性评估至关重要

Method: 进行两个实验：1) 在飓风Florence的初始条件中注入不同水平的高斯噪声，观察对预测轨迹和风暴强度的影响；2) 使用完全随机初始条件启动模型，观察模型对无意义输入的反应

Result: FCNv2在低到中等噪声下能准确保持飓风特征；即使在高噪声下也能保持风暴轨迹和结构，但位置精度开始下降；模型在所有噪声水平下都一致低估风暴强度和持续性；完全随机初始条件下，模型在几个时间步后能生成平滑连贯的预测

Conclusion: FCNv2表现出良好的鲁棒性，倾向于生成稳定平滑的输出，该方法简单且可移植到其他数据驱动的AI天气预测模型

Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.

</details>


### [47] [Non-Convex Federated Optimization under Cost-Aware Client Selection](https://arxiv.org/abs/2512.05327)
*Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 本文提出了一种新的联邦优化模型，能够量化通信和本地计算复杂度，并区分不同的客户端选择策略。基于该模型，作者开发了一种基于SAGA方差缩减梯度估计器的算法，通过递归梯度技术改进误差界，在非凸优化中实现了目前已知最佳的通信和本地计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦优化算法的比较指标通常不区分不同的客户端选择策略（随机采样、全客户端通信或混合方案），而这些策略在实际中会产生不同的通信成本。为了解决这种差异，需要一个新的模型来量化通信和本地计算复杂度，并明确区分不同策略的成本。

Method: 1. 引入一个简单自然的联邦优化模型，量化通信和本地计算复杂度，支持多种常用客户端选择策略
2. 基于不精确复合梯度方法，构建精心设计的梯度估计器
3. 使用SAGA方差缩减梯度估计器，并推导新的方差界，展示其可利用函数相似性
4. 引入递归梯度技术作为通用方法，改进给定条件无偏梯度估计器的误差界
5. 将递归梯度技术应用于SAGA，得到新的RG-SAGA估计器，具有改进的误差界

Result: 提出的新算法在非凸优化的联邦优化方法中，实现了目前已知最佳的通信和本地计算复杂度。RG-SAGA估计器相比原始SAGA具有改进的误差界，递归梯度技术可应用于SAGA和SVRG等多种梯度估计器。

Conclusion: 本文提出的联邦优化模型能够准确量化不同客户端选择策略的成本差异，基于该模型开发的新算法在通信和计算效率方面达到了当前最优水平。递归梯度技术为改进梯度估计器的误差界提供了通用框架，具有广泛的应用潜力。

Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.

</details>


### [48] [PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering](https://arxiv.org/abs/2512.05336)
*Durga Prasad Maram,Kalpa Gunaratna,Vijay Srinivasan,Haris Jeelani,Srinivas Chappidi*

Main category: cs.LG

TL;DR: PATHFINDER使用蒙特卡洛树搜索生成训练路径轨迹，通过子答案召回和LLM验证过滤错误轨迹，改进多跳问答性能


<details>
  <summary>Details</summary>
Motivation: 现有基于训练的多跳问答方法仍受LLM幻觉和错误推理路径影响，导致性能受限

Method: 1) 使用蒙特卡洛树搜索生成训练路径轨迹；2) 通过子答案召回和LLM作为裁判验证过滤错误和冗长轨迹；3) 重新表述子查询处理检索失败情况

Result: PATHFINDER在公开基准数据集上提高了多跳问答的性能

Conclusion: 通过改进训练数据质量和处理检索失败，PATHFINDER有效提升了多跳问答系统的性能

Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.

</details>


### [49] [Interaction Tensor Shap](https://arxiv.org/abs/2512.05338)
*Hiroki Hasegawa,Yukihiko Okada*

Main category: cs.LG

TL;DR: 提出IT SHAP方法，通过张量网络收缩将高阶Shapley交互表示为多项式时间可计算的形式，解决了传统方法指数级计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型变得越来越深和高维，理解特征如何影响预测变得困难。现有Shapley值方法无法高效计算高阶交互：STII需要指数级枚举，MST仅限于一阶效应。核心问题是缺乏同时保持STII公理精确性又避免指数计算复杂度的框架。

Method: 提出Interaction Tensor SHAP (IT SHAP)，将Shapley Taylor交互指数重新表述为值张量和权重张量的收缩，并假设权重张量具有多项式TT秩的有限状态张量列车表示。在TT结构化模型和分布张量下，将STII的指数复杂度Θ(4^n)降低到NC2并行时间。

Result: IT SHAP能够精确表示高阶Shapley交互为张量网络收缩，在张量列车结构下实现多项式时间和多对数深度计算，为高维模型中的主效应和高阶交互提供了统一、公理化且计算可行的表述。

Conclusion: IT SHAP为可扩展的交互感知可解释AI奠定了基础，对大型黑盒模型具有重要意义，这些模型的组合结构先前使得交互分析不可行。

Abstract: Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.

</details>


### [50] [Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models](https://arxiv.org/abs/2512.05339)
*Mahesh Kumar Nandwana,Youngwan Lim,Joseph Liu,Alex Yang,Varun Notibala,Nishchaie Khanna*

Main category: cs.LG

TL;DR: Roblox Guard 1.0是基于Llama-3.1-8B-Instruct构建的指令微调LLM，通过全面的输入输出审核增强LLM系统安全性，并发布了RobloxGuard-Eval评估基准。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在训练后阶段进行了安全对齐，但仍可能生成不适当的输出，对用户构成潜在风险，因此需要强大的跨输入输出保护机制。

Method: 基于Llama-3.1-8B-Instruct构建，通过指令微调使其能够泛化到未见过的安全分类体系；使用合成和开源安全数据集混合训练，增强思维链推理和输入反转技术以提升上下文理解和决策能力；采用LLM管道增强审核能力。

Result: 模型在领域外安全基准测试中表现出色，能够有效处理未见过的安全分类体系；同时发布了RobloxGuard-Eval评估基准，包含可扩展的安全分类体系来评估LLM护栏和审核框架的有效性。

Conclusion: Roblox Guard 1.0是一个先进的指令微调LLM，通过全面的输入输出审核增强LLM系统安全性，其可扩展的安全分类体系和评估基准为LLM安全研究提供了重要工具。

Abstract: Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.

</details>


### [51] [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)
*Yiwen Liang,Qiufeng Li,Shikai Wang,Weidong Cao*

Main category: cs.LG

TL;DR: 提出针对硬件代码生成的LLM遗忘框架，通过语法保持遗忘策略和细粒度选择性损失，有效移除问题知识而不影响代码生成能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在硬件设计自动化中显示出潜力，但存在可靠性问题：可能记忆专有知识产权、受污染的基准测试和不安全的编码模式，需要解决这些风险

Method: 结合语法保持遗忘策略（保护硬件代码结构完整性）和细粒度floor-aware选择性损失（精确高效移除问题知识）的遗忘框架

Result: 实验表明框架支持最多3倍大的遗忘集，通常只需单次训练周期，同时保持RTL代码的语法正确性和功能完整性

Conclusion: 该工作为可靠的LLM辅助硬件设计开辟了新途径，实现了有效遗忘而不降低代码生成能力

Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.

</details>


### [52] [Enhancing Dimensionality Prediction in Hybrid Metal Halides via Feature Engineering and Class-Imbalance Mitigation](https://arxiv.org/abs/2512.05367)
*Mariia Karabin,Isaac Armstrong,Leo Beck,Paulina Apanel,Markus Eisenbach,David B. Mitzi,Hanna Terletska,Hendrik Heinz*

Main category: cs.LG

TL;DR: 提出机器学习框架预测杂化金属卤化物结构维度，通过特征工程和类别不平衡处理技术提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 杂化金属卤化物（包括有机-无机钙钛矿）的结构维度（0D、1D、2D、3D）预测对材料设计至关重要，但现有数据集存在严重的类别不平衡问题，影响预测模型性能

Method: 采用化学信息特征工程开发相互作用描述符，使用SMOTE技术将数据集从494扩充到1336以缓解类别不平衡，构建多阶段工作流结合特征选择、模型堆叠和性能优化

Result: 显著提高了少数类别的F1分数，在所有维度上都实现了稳健的交叉验证性能

Conclusion: 该框架通过创新的特征工程和类别不平衡处理技术，有效解决了杂化金属卤化物结构维度预测中的不平衡问题，为材料设计提供了可靠的工具

Abstract: We present a machine learning framework for predicting the structural dimensionality of hybrid metal halides (HMHs), including organic-inorganic perovskites, using a combination of chemically-informed feature engineering and advanced class-imbalance handling techniques. The dataset, consisting of 494 HMH structures, is highly imbalanced across dimensionality classes (0D, 1D, 2D, 3D), posing significant challenges to predictive modeling. This dataset was later augmented to 1336 via the Synthetic Minority Oversampling Technique (SMOTE) to mitigate the effects of the class imbalance. We developed interaction-based descriptors and integrated them into a multi-stage workflow that combines feature selection, model stacking, and performance optimization to improve dimensionality prediction accuracy. Our approach significantly improves F1-scores for underrepresented classes, achieving robust cross-validation performance across all dimensionalities.

</details>


### [53] [Text Rationalization for Robust Causal Effect Estimation](https://arxiv.org/abs/2512.05373)
*Lijinghua Zhang,Hengrui Cai*

Main category: cs.LG

TL;DR: CATR框架通过选择稀疏的必要文本标记子集来解决文本数据在因果推断中的挑战，特别是处理高维文本导致的倾向得分极端值和方差膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 高维文本数据在因果推断中虽然能编码丰富的上下文信息，但也带来独特挑战：正性假设在观测层面经常被违反，冗余或虚假的文本特征会膨胀维度，导致极端倾向得分、权重不稳定和效应估计方差膨胀。

Method: 提出Confounding-Aware Token Rationalization (CATR)框架，使用残差独立性诊断选择稀疏的必要标记子集，旨在保留足够用于无混淆性的混杂信息，同时丢弃无关文本。

Result: 在合成数据和MIMIC-III数据库的真实世界研究中，CATR相比现有基线方法产生了更准确、稳定且可解释的因果效应估计。

Conclusion: CATR通过选择稀疏的文本标记子集，有效缓解了观测层面的正性假设违反问题，稳定了下游因果效应估计器，为文本数据在因果推断中的应用提供了更可靠的框架。

Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.

</details>


### [54] [China Regional 3km Downscaling Based on Residual Corrective Diffusion Model](https://arxiv.org/abs/2512.05377)
*Honglu Sun,Hao Jing,Zhixiang Dai,Sa Xiao,Wei Xue,Jian Sun,Qifeng Lu*

Main category: cs.LG

TL;DR: 该研究基于CorrDiff扩散模型框架，将统计降尺度方法应用于中国区域天气预报，将25km全球模式预报降尺度至3km分辨率，相比传统区域模式CMA-MESO在多个变量上取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 数值天气预报中高效生成高分辨率预报是一个基本挑战。传统方法包括动力降尺度和统计降尺度，本研究专注于统计降尺度，利用深度学习建立低分辨率与高分辨率历史数据之间的统计关系，以生成更精细的天气预报。

Method: 采用基于扩散模型的CorrDiff降尺度框架，在原始工作基础上扩展了约20倍的研究区域，不仅考虑地表变量，还增加了六个气压层的高空变量作为目标降尺度变量。添加全局残差连接以提高精度。将训练好的模型应用于CMA-GFS（25km全球网格预报）和SFF（基于球形傅里叶神经算子的数据驱动深度学习天气模型），生成中国区域3km预报。

Result: 实验结果表明，该方法降尺度后的预报在目标变量的平均绝对误差（MAE）方面普遍优于CMA-MESO直接预报。雷达组合反射率预报显示，CorrDiff作为生成模型能够生成精细尺度细节，相比相应的确定性回归模型产生更真实的预测。

Conclusion: 基于扩散模型的统计降尺度方法能够有效提高天气预报分辨率，在更大区域和更多变量类型上表现出良好性能，生成的高分辨率预报在准确性和真实性方面优于传统区域模式。

Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.

</details>


### [55] [Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets](https://arxiv.org/abs/2512.05386)
*Jakub Kopko,David Graber,Saltuk Mustafa Eyrilmez,Stanislav Mazurenko,David Bednar,Jiri Sedlar,Josef Sivic*

Main category: cs.LG

TL;DR: 评估蛋白质-配体评分函数在新型蛋白质靶点上的泛化能力，发现常用基准测试未能反映真实挑战，探索大规模自监督预训练和简单数据利用方法提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在分子设计中日益重要，需要确保可学习的蛋白质-配体评分函数在新蛋白质靶点上的可靠性。虽然许多评分函数在标准基准测试上表现良好，但其在训练数据之外的泛化能力仍然是一个重大挑战。

Method: 1) 在模拟评估有限已知结构和实验亲和力测量靶点的数据集分割上评估最先进评分函数的泛化能力；2) 研究大规模自监督预训练是否能弥合泛化差距；3) 探索利用有限测试靶点数据提升评分函数性能的简单方法。

Result: 分析显示常用基准测试未能反映向新型靶点泛化的真实挑战；提供了大规模自监督预训练潜力的初步证据；研究了利用有限测试数据提升性能的方法。

Conclusion: 研究结果强调了需要更严格的评估协议，并为设计具有扩展到新型蛋白质靶点预测能力的评分函数提供了实用指导。

Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.

</details>


### [56] [Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction](https://arxiv.org/abs/2512.05402)
*Sithumi Wickramasinghe,Bikramjit Das,Dorien Herremans*

Main category: cs.LG

TL;DR: 该研究提出MineROI-Net，一个基于Transformer的模型，用于预测比特币挖矿硬件购买时机，将硬件采购决策转化为时间序列分类任务，预测一年内投资回报率是否盈利。


<details>
  <summary>Details</summary>
Motivation: 比特币挖矿硬件采购面临市场波动、技术快速过时和协议驱动收入周期等挑战，但缺乏何时购买新ASIC硬件的指导，也没有计算框架解决这一决策问题。

Method: 将硬件采购决策制定为时间序列分类任务，预测购买ASIC机器在一年内是否盈利。提出MineROI-Net，一种开源的基于Transformer的架构，旨在捕捉挖矿盈利能力的多尺度时间模式。

Result: 在2015年至2024年间发布的20个ASIC矿机数据上评估，MineROI-Net在多样化市场环境下表现优于LSTM和TSLANet基线，达到83.7%准确率和83.1%宏观F1分数。模型在经济相关性方面表现强劲，检测不盈利时期的精确度为93.6%，盈利时期的精确度为98.5%。

Conclusion: MineROI-Net为挖矿硬件采购时机提供了实用的数据驱动工具，可能降低资本密集型挖矿操作的财务风险。模型已开源提供。

Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.

</details>


### [57] [RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design](https://arxiv.org/abs/2512.05403)
*Gyusam Chang,Jeongyoon Yoon,Shin han yi,JaeHyeok Lee,Sujin Jang,Sangpil Kim*

Main category: cs.LG

TL;DR: RevoNAD是一个反射式进化编排器，通过多轮多专家共识、自适应反射探索和帕累托引导进化选择，有效连接LLM推理与反馈对齐的架构搜索，在多个数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的神经架构设计系统存在挑战：令牌级设计循环是离散且不可微分的，反馈无法平滑指导架构改进。这些方法通常陷入模式崩溃到冗余结构或漂移到不可行设计中，当建设性推理没有良好基础时。

Method: 1. 多轮多专家共识：将孤立的设计规则转化为有意义的架构线索；2. 自适应反射探索：利用奖励方差调整探索程度，在反馈不确定时探索，在稳定性达到时细化；3. 帕累托引导进化选择：有效促进同时优化准确性、效率、延迟、置信度和结构多样性的架构。

Result: 在CIFAR10、CIFAR100、ImageNet16-120、COCO-5K和Cityscape数据集上实现了最先进的性能。消融和迁移研究进一步验证了RevoNAD在实际可靠、可部署的神经架构设计中的有效性。

Conclusion: RevoNAD通过反射式进化编排有效桥接基于LLM的推理与反馈对齐的架构搜索，解决了现有LLM驱动生成方法的局限性，实现了可靠且可部署的神经架构设计。

Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.

</details>


### [58] [Sepsis Prediction Using Graph Convolutional Networks over Patient-Feature-Value Triplets](https://arxiv.org/abs/2512.05416)
*Bozhi Dan,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: Triplet-GCN：一种基于患者-特征-值三元组的图卷积网络模型，用于从复杂稀疏的电子健康记录数据中早期检测脓毒症，在多中心中国队列中优于传统表格基线方法。


<details>
  <summary>Details</summary>
Motivation: 在重症监护环境中，脓毒症是导致患者疾病和死亡的主要原因，但由于电子健康记录数据复杂、稀疏且异质性高，其及时检测面临挑战。现有方法难以有效处理这种数据结构。

Method: 提出Triplet-GCN模型，将每次就诊表示为患者-特征-值三元组，构建二分EHR图。使用图卷积网络学习患者嵌入，后接轻量级多层感知机。采用类型特定的预处理：数值变量用中位数插补和标准化，二元特征用效应编码，罕见分类属性用众数插补和低维嵌入。患者节点用汇总统计初始化，同时在边上保留测量值以保持"谁测量了什么以及测量了多少"的信息。

Result: 在来自三家三级医院的回顾性多中心中国队列（N=648；70/30训练-测试分割）中，Triplet-GCN在区分度和平衡误差指标上持续优于强表格基线（KNN、SVM、XGBoost、随机森林），产生更有利的敏感性-特异性权衡，并提高了早期预警的整体效用。

Conclusion: 将EHR编码为三元组并通过患者-特征图传播信息，比特征无关模型产生更具信息量的患者表示。这为可部署的脓毒症风险分层提供了一个简单、端到端的蓝图。

Abstract: In the intensive care setting, sepsis continues to be a major contributor to patient illness and death; however, its timely detection is hindered by the complex, sparse, and heterogeneous nature of electronic health record (EHR) data. We propose Triplet-GCN, a single-branch graph convolutional model that represents each encounter as patient--feature--value triplets, constructs a bipartite EHR graph, and learns patient embeddings via a Graph Convolutional Network (GCN) followed by a lightweight multilayer perceptron (MLP). The pipeline applies type-specific preprocessing -- median imputation and standardization for numeric variables, effect coding for binary features, and mode imputation with low-dimensional embeddings for rare categorical attributes -- and initializes patient nodes with summary statistics, while retaining measurement values on edges to preserve "who measured what and by how much". In a retrospective, multi-center Chinese cohort (N = 648; 70/30 train--test split) drawn from three tertiary hospitals, Triplet-GCN consistently outperforms strong tabular baselines (KNN, SVM, XGBoost, Random Forest) across discrimination and balanced error metrics, yielding a more favorable sensitivity--specificity trade-off and improved overall utility for early warning. These findings indicate that encoding EHR as triplets and propagating information over a patient--feature graph produce more informative patient representations than feature-independent models, offering a simple, end-to-end blueprint for deployable sepsis risk stratification.

</details>


### [59] [TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning](https://arxiv.org/abs/2512.05419)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: 提出TS-Hint框架，结合时序基础模型与思维链推理，通过注意力提示改进半导体制造中材料去除率的预测，解决传统方法丢失时序动态和需要大量数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法从时间序列中提取静态特征来近似半导体制造过程（如化学机械抛光）的材料去除率，但这会导致时序动态信息的丢失。此外，这些方法需要大量数据进行有效训练。

Method: 提出TS-Hint框架，这是一个时序基础模型框架，集成了思维链推理机制。该框架基于注意力机制数据和显著性数据，在训练过程中提供注意力提示。

Result: 实验结果表明，该模型在有限数据设置下通过少样本学习表现出有效性，并且能够直接从多元时间序列特征中学习。

Conclusion: TS-Hint框架通过结合时序基础模型与思维链推理，解决了传统方法在时序动态信息保留和训练数据需求方面的局限性，为半导体制造过程预测提供了更有效的解决方案。

Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.

</details>


### [60] [IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?](https://arxiv.org/abs/2512.05442)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: 提出IdealTSF框架，利用非理想负样本增强时间序列预测，通过预训练、训练和优化三阶段处理，特别适用于噪声样本或低质量数据场景。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中，缺失值和异常值等非理想数据阻碍深度学习发展。以往研究主要关注从序列数据提取特征或将这些问题数据作为正样本进行知识迁移，但更有效的方法是利用这些非理想负样本来增强事件预测。

Method: 提出IdealTSF框架，包含三个渐进步骤：1) 预训练阶段从负样本数据提取知识；2) 训练阶段将序列数据转化为理想正样本；3) 应用带有对抗扰动的负优化机制。该框架整合理想正样本和非理想负样本进行时间序列预测。

Result: 大量实验表明，负样本数据在基础注意力架构中释放了显著的时间序列预测潜力。IdealTSF特别适用于具有噪声样本或低质量数据的应用场景。

Conclusion: IdealTSF框架通过有效利用非理想负样本，显著提升了时间序列预测性能，为处理噪声数据和低质量数据的预测任务提供了有效解决方案。

Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.

</details>


### [61] [How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data](https://arxiv.org/abs/2512.05469)
*Zubair Ahmed Mohammad*

Main category: cs.LG

TL;DR: 该研究探讨了集成模型如何在保持高准确率的同时控制过拟合，通过四个表格分类任务分析不同集成方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 集成模型通常比单一学习器获得更高的准确率，但其保持较小泛化差距的能力尚未得到充分理解。本研究旨在探究集成模型如何在准确率和过拟合之间取得平衡。

Method: 使用重复分层交叉验证和统计显著性检验，在四个表格分类任务（乳腺癌、心脏病、皮马糖尿病、信用卡欺诈）上比较线性模型、单一决策树和九种集成方法。同时计算数据集复杂度指标（线性度评分、Fisher比率、噪声估计）来解释集成效果。

Result: 1. 在线性且干净的数据上，线性模型已经泛化得很好，集成方法提供额外收益有限；2. 在有意义的非线性结构数据集上，基于树的集成方法将测试准确率提高5-7个百分点，同时保持泛化差距低于3%；3. 在噪声大或高度不平衡的数据集上，集成方法仍具竞争力但需要正则化以避免过拟合噪声或多数类模式。

Conclusion: 集成模型通过平均或受控提升来减少方差，可以在保持高准确率的同时控制过拟合。数据集复杂度指标（线性度评分、Fisher比率、噪声估计）能够有效预测集成方法何时能有效控制方差。该研究为实际表格应用中的模型选择提供了实用指导。

Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.

</details>


### [62] [PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning](https://arxiv.org/abs/2512.05475)
*Saumya Biswas,Jiten Oswal*

Main category: cs.LG

TL;DR: 比较不同几何量子机器学习模型在分子几何结构上的性能，发现置换对称嵌入是最具泛化性的几何学习量子模型


<details>
  <summary>Details</summary>
Motivation: 研究不同对称性等变性的量子机器学习模型在分子几何结构数据集上的性能差异，为几何学习选择合适模型提供标准

Method: 使用两个分子数据集（线性LiH分子和三角锥形NH3分子），比较四种模型：无对称等变性模型、旋转和置换等变性模型、图嵌入置换等变性模型，以及经典等变模型作为基线

Result: 图嵌入特征被证明是提高几何数据集可训练性的有效途径，置换对称嵌入被发现是几何学习中最具泛化性的量子机器学习模型

Conclusion: 分子几何结构与模型性能差异揭示了选择泛化性模型的标准，置换对称嵌入量子模型在几何学习中表现最佳

Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.

</details>


### [63] [Turbulence Regression](https://arxiv.org/abs/2512.05483)
*Yingang Fan,Binjie Ding,Baiyi Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于离散化数据的NeuTucker分解模型，用于解决仅使用风廓线雷达数据时传统方法难以准确预测低空湍流状态的问题。


<details>
  <summary>Details</summary>
Motivation: 低空湍流受多种复杂因素影响，结果复杂多变。在当前观测条件下，特别是仅使用风廓线雷达数据时，传统方法难以准确预测湍流状态，需要新的建模方法。

Method: 1) 对连续输入数据进行离散化处理，以适应需要离散数据输入的NeuTucF等模型；2) 构建四维Tucker交互张量，表示不同高度和三维风速之间所有可能的时空交互作用；3) 基于Tucker神经网络构建低秩Tucker分解模型，捕捉三维风场数据中的潜在交互关系。

Result: 在真实数据集上估计缺失观测值时，离散化的NeuTucF模型相比各种常见回归模型表现出更优越的性能。

Conclusion: 提出的离散化NeuTucker分解模型能够有效处理连续稀疏的三维风场数据，通过捕捉时空交互关系，在湍流状态预测方面优于传统方法。

Abstract: Air turbulence refers to the disordered and irregular motion state generated by drastic changes in velocity, pressure, or direction during airflow. Various complex factors lead to intricate low-altitude turbulence outcomes. Under current observational conditions, especially when using only wind profile radar data, traditional methods struggle to accurately predict turbulence states. Therefore, this paper introduces a NeuTucker decomposition model utilizing discretized data. Designed for continuous yet sparse three-dimensional wind field data, it constructs a low-rank Tucker decomposition model based on a Tucker neural network to capture the latent interactions within the three-dimensional wind field data. Therefore, two core ideas are proposed here: 1) Discretizing continuous input data to adapt to models like NeuTucF that require discrete data inputs. 2) Constructing a four-dimensional Tucker interaction tensor to represent all possible spatio-temporal interactions among different elevations and three-dimensional wind speeds. In estimating missing observations in real datasets, this discretized NeuTucF model demonstrates superior performance compared to various common regression models.

</details>


### [64] [GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop](https://arxiv.org/abs/2512.05502)
*Omid Bazgir,Vineeth Manthapuri,Ilia Rattsev,Mohammad Jafarnejad*

Main category: cs.LG

TL;DR: GRASP是一个基于图推理的多智能体框架，通过人机对话界面将定量系统药理学模型编码为类型化生物知识图谱，并编译为可执行的MATLAB/SimBiology代码，同时保持单位、质量平衡和生理约束。


<details>
  <summary>Details</summary>
Motivation: 定量系统药理学建模对药物开发至关重要，但需要大量时间投入，限制了领域专家的吞吐量。需要一种方法使领域专家能够用自然语言指定机制，同时不牺牲生物医学保真度。

Method: GRASP采用两阶段工作流程：理解阶段（对遗留代码进行图重建）和行动阶段（约束检查、语言驱动的修改）。该框架使用状态机进行协调，具有迭代验证功能。采用广度优先参数对齐方法围绕新实体进行参数对齐，发现依赖量并建议生物学合理的默认值。

Result: 在LLM作为评判的头对头评估中，GRASP在生物合理性、数学正确性、结构保真度和代码质量方面优于SME引导的CoT和ToT基线（约9-10/10 vs. 5-7/10）。BFS对齐在依赖发现、单位和范围方面达到F1 = 0.95。

Conclusion: 图结构化的智能体工作流程可以使QSP模型开发既易于访问又严谨，使领域专家能够用自然语言指定机制，同时不牺牲生物医学保真度。

Abstract: Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \textsc{Understanding} (graph reconstruction of legacy code) and \textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\(\approx\)9--10/10 vs.\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity.

</details>


### [65] [Credal and Interval Deep Evidential Classifications](https://arxiv.org/abs/2512.05526)
*Michele Caprio,Shireen K. Manchingal,Fabio Cuzzolin*

Main category: cs.LG

TL;DR: 论文提出CDEC和IDEC两种深度证据分类方法，通过概率集合和区间证据分布来量化不确定性，在可接受不确定性范围内提供带概率保证的标签集合，超出阈值时则拒绝分类。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化是人工智能领域的关键挑战，直接影响决策制定、风险评估和模型可靠性。现有方法存在局限性，需要更系统的不确定性评估方法。

Method: 提出CDEC（基于概率集合）和IDEC（基于区间证据分布）两种深度证据分类方法，使用标准反向传播和基于证据理论的损失函数进行训练，能够同时评估认知不确定性和随机不确定性。

Result: 在MNIST、CIFAR-10、CIFAR-100及其OoD数据集上的实验表明，CDEC和IDEC具有竞争力的预测准确性、最先进的OoD检测性能，以及紧致且校准良好的预测区域，在分布偏移时能可靠扩展。

Conclusion: CDEC和IDEC克服了先前方法的不足，扩展了当前证据深度学习文献，能够提供可靠的不确定性量化，并在不确定性超过阈值时拒绝分类，为AI系统的可靠决策提供了新方法。

Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.

</details>


### [66] [IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection](https://arxiv.org/abs/2512.05531)
*Yang Xu,Yixiao Ma,Kaifeng Zhang,Zuliang Yang,Kai Ming Ting*

Main category: cs.LG

TL;DR: IDK-S是一种用于数据流异常检测的新型增量分布核方法，通过动态核均值嵌入表示，在保持高检测精度的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 数据流异常检测面临两大挑战：需要在分布不断演变的情况下保持高检测精度，同时确保实时效率。现有方法往往难以同时满足这两个要求。

Method: IDK-S基于隔离分布核（Isolation Distributional Kernel）框架，采用轻量级增量更新机制。它继承了离线检测器IDK的优势，使用数据依赖核，同时通过增量更新避免完全模型重训练，显著降低计算开销。

Result: 在13个基准测试上的实验表明，IDK-S在检测精度上优于现有最先进方法，同时在运行速度上显著更快，在许多情况下快一个数量级。该方法在统计上等效于完全重训练的模型。

Conclusion: IDK-S成功解决了数据流异常检测中精度与效率的平衡问题，通过创新的增量分布核方法实现了高精度检测和实时性能的兼顾。

Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.

</details>


### [67] [On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability](https://arxiv.org/abs/2512.05534)
*Yiming Tang,Harshvardhan Saini,Yizhen Liao,Dianbo Liu*

Main category: cs.LG

TL;DR: 该论文提出了首个统一的稀疏字典学习理论框架，分析了多种SDL方法的优化景观，并首次从理论上解释了特征吸收、死亡神经元等经验现象。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力增强，理解其内部表示和处理机制变得至关重要。虽然稀疏字典学习方法在经验上取得了成功，但缺乏统一的理论基础，现有理论仅限于特定约束的稀疏自编码器，需要为更广泛的SDL方法提供理论支撑。

Method: 开发了统一的优化问题理论框架，将多种SDL方法（稀疏自编码器、转码器、交叉编码器）纳入同一分析体系，对优化景观进行严格分析，并通过受控实验验证理论结果。

Result: 首次从理论上解释了特征吸收、死亡神经元和神经元重采样技术等经验观察现象，为SDL方法提供了形式化理论基础。

Conclusion: 该研究为稀疏字典学习方法建立了首个统一的理论框架，填补了该领域理论理解的空白，为更可靠的AI模型解释提供了理论基础。

Abstract: As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.

</details>


### [68] [SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection](https://arxiv.org/abs/2512.05540)
*Yang Xu,Hang Zhang,Yixiao Ma,Ye Zhu,Kai Ming Ting*

Main category: cs.LG

TL;DR: SCoNE是一种新型多视图异常检测方法，通过直接使用多视图实例表示一致邻域，无需中间表示，具有数据依赖的邻域特性，在稀疏区域使用大邻域、密集区域使用小邻域，实现了O(N)时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有多视图异常检测方法存在两个关键问题：1) 无法保证在不同视图密度变化区域有效捕获一致邻域，导致检测精度低；2) 学习过程计算复杂度为O(N²)，不适用于大规模数据集。

Method: 提出SCoNE方法，具有两个独特特征：a) 直接使用多视图实例表示一致邻域，无需现有方法中的中间表示；b) 邻域具有数据依赖特性，在稀疏区域使用大邻域，在密集区域使用小邻域。

Result: SCoNE在检测精度上优于现有方法，在大规模数据集上运行速度比现有方法快几个数量级。

Conclusion: SCoNE通过直接表示数据依赖的一致邻域，无需学习过程，解决了现有方法在检测精度和计算效率方面的局限性，实现了高效准确的多视图异常检测。

Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.

</details>


### [69] [RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs](https://arxiv.org/abs/2512.05542)
*Jonathan Geuter,Gregor Kornhardt*

Main category: cs.LG

TL;DR: RoBoN是一种多模型推理方法，通过在线路由机制在多个LLM间分配生成任务，相比传统单模型best-of-n方法能获得更好的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在不同任务上表现出互补优势，但传统的best-of-n方法仅使用单一模型生成响应，未能充分利用多模型多样性带来的好处。

Method: RoBoN采用顺序路由机制，基于奖励模型评分和响应一致性信号，在多个模型间逐个路由生成任务，无需额外训练，保持计算对等性，可与任何插件式奖励模型配合使用。

Result: 在多个推理基准测试（MATH500、OlympiadBench、MinervaMath、GSM8K、MMLU）中，RoBoN在较大n值时始终优于标准单模型best-of-n方法，绝对准确率提升高达3.4%，也优于均匀多模型组合基线。

Conclusion: 模型间的多样性可以在推理阶段被利用来提升best-of-n性能，超过任何单个组成模型，为多LLM测试时扩展提供了一种简单、无需训练的路径。

Abstract: Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.

</details>


### [70] [Improving Local Fidelity Through Sampling and Modeling Nonlinearity](https://arxiv.org/abs/2512.05556)
*Sanjeev Shrestha,Rahul Dubey,Hui Liu*

Main category: cs.LG

TL;DR: 提出一种基于MARS的非线性局部解释方法，相比LIME能更好地捕捉黑盒模型的非线性决策边界，通过N-ball采样技术提高解释的忠实度。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒机器学习模型在关键领域的应用增加，需要提供可靠的解释。现有LIME方法假设局部决策边界是线性的，无法捕捉非线性关系，导致解释不准确。

Method: 使用多元自适应回归样条(MARS)建模非线性局部边界，捕捉参考模型的底层行为；采用N-ball采样技术直接从期望分布采样，而不是像LIME那样重新加权样本。

Result: 在三个UCI数据集上评估，相比基线方法，新方法能生成更忠实的解释，平均减少37%的均方根误差，显著提高了局部忠实度。

Conclusion: 提出的基于MARS的非线性局部解释方法能有效捕捉黑盒模型的非线性决策边界，通过改进采样技术提高了解释的忠实度，为复杂模型提供更可靠的可解释性。

Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.

</details>


### [71] [Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection](https://arxiv.org/abs/2512.05567)
*Antoine Blais,Nicolas Couëllan*

Main category: cs.LG

TL;DR: 提出基于最优传输的半监督学习方法，使用Wasserstein距离作为图像相似性度量，在GNSS多径干扰检测应用中显著提升分类准确率


<details>
  <summary>Details</summary>
Motivation: 解决标记图像数据稀缺的问题，通过半监督学习利用未标记数据提升深度学习模型性能

Method: 基于最优传输的隐式图半监督学习方法，使用Wasserstein距离作为图像相似性度量，在标签传播机制中应用该距离度量

Result: 在GNSS多径干扰检测应用中，通过调整半监督程度和度量敏感度的超参数，分类准确率显著优于全监督训练方法

Conclusion: 基于最优传输的半监督学习方法能有效利用稀缺标记数据，在特定超参数设置下显著提升分类性能，特别适用于GNSS多径干扰检测等实际应用

Abstract: The main objective of this study is to propose an optimal transport based semi-supervised approach to learn from scarce labelled image data using deep convolutional networks. The principle lies in implicit graph-based transductive semi-supervised learning where the similarity metric between image samples is the Wasserstein distance. This metric is used in the label propagation mechanism during learning. We apply and demonstrate the effectiveness of the method on a GNSS real life application. More specifically, we address the problem of multi-path interference detection. Experiments are conducted under various signal conditions. The results show that for specific choices of hyperparameters controlling the amount of semi-supervision and the level of sensitivity to the metric, the classification accuracy can be significantly improved over the fully supervised training method.

</details>


### [72] [Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning](https://arxiv.org/abs/2512.05591)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Tiehua Mei,Zijia Lin,Yuntao Li,Wenping Hu,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 提出熵比率裁剪(ERC)机制，通过约束当前策略与先前策略的熵比率来稳定强化学习训练，解决PPO-Clip无法调节未采样动作概率偏移的问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型后训练依赖强化学习提升能力和对齐质量，但离策略训练范式引入分布偏移，导致策略超出信任区域，表现为策略熵波动和梯度不稳定。PPO-Clip通过重要性裁剪缓解此问题，但仍忽视动作的全局分布偏移

Method: 提出使用当前策略与先前策略的熵比率作为新的全局度量，量化策略探索的相对变化。基于此提出熵比率裁剪(ERC)机制，对熵比率施加双向约束，在全局分布层面稳定策略更新，补偿PPO-clip无法调节未采样动作概率偏移的不足。将ERC集成到DAPO和GPPO强化学习算法中

Result: 在多个基准测试上的实验表明，ERC持续提升性能

Conclusion: ERC机制通过熵比率约束有效稳定强化学习训练，解决了PPO-Clip的局限性，在多个强化学习算法中均表现出性能提升

Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.

</details>


### [73] [Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales](https://arxiv.org/abs/2512.05620)
*Shikai Qiu,Zixi Chen,Hoang Phan,Qi Lei,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 该研究探讨了如何通过超参数迁移来扩展预条件优化器（如Shampoo、SOAP、Muon）的规模，发现遵循μP缩放学习率并采用1/宽度缩放权重衰减能显著提升优化器性能，在Llama架构语言模型上实现1.3-1.4倍加速。


<details>
  <summary>Details</summary>
Motivation: 近期基于矩阵级预条件的深度学习优化器在小规模实验中显示出优于AdamW的加速效果，但其大规模验证结果不一致。本研究旨在理解如何通过超参数迁移来有效扩展这些预条件优化器，建立可靠的规模化比较方法。

Method: 研究基于μP理论框架，系统分析了学习率和权重衰减如何随模型宽度和深度缩放。研究了Shampoo、SOAP、Muon等多种优化器，考虑了分块和嫁接等常用技术的影响。通过有限宽度偏差分析和显式谱归一化来缓解学习率漂移问题。

Result: 发现按μP缩放学习率能改善迁移效果，但仍有显著有限宽度偏差导致最优学习率漂移，可通过分块和显式谱归一化缓解。对于计算最优缩放，1/宽度缩放的独立权重衰减近乎最优。应用这些规则后，Muon和Shampoo在190M到1.4B参数的Llama架构语言模型上分别实现1.4倍和1.3倍相对于AdamW的加速。

Conclusion: 研究超参数最优迁移对于在现实调优预算下可靠比较大规模优化器至关重要。正确的缩放规则能保持优化器优势，而错误缩放会导致加速效果随规模迅速消失。

Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $μ$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $μ$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.

</details>


### [74] [Bounded Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2512.05623)
*Kibidi Neocosmos,Diego Baptista,Nicole Ludwig*

Main category: cs.LG

TL;DR: 该论文提出了一种灵活控制GNN社区检测中社区数量的框架，允许用户指定合理范围或精确数量，解决了传统GNN方法难以准确返回指定社区数量的问题。


<details>
  <summary>Details</summary>
Motivation: 传统社区检测方法需要预先指定聚类数量，而GNN方法即使指定了期望数量也常常无法准确返回该数量。现有方法缺乏灵活控制社区数量的机制，限制了GNN在社区检测中的应用。

Method: 提出了一个灵活控制GNN社区检测中社区数量的框架。用户可指定社区数量的合理范围，并在训练过程中强制执行这些边界约束。如果用户需要精确的社区数量，也可以指定并可靠返回。

Result: 该方法能够在用户指定的范围内可靠地控制社区数量，同时支持精确指定社区数量。相比传统GNN方法，能够更准确地返回用户期望的社区数量。

Conclusion: 该框架为GNN社区检测提供了灵活控制社区数量的能力，解决了传统方法在社区数量控制方面的局限性，增强了GNN在社区检测任务中的实用性和可靠性。

Abstract: In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.

</details>


### [75] [Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability](https://arxiv.org/abs/2512.05638)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 论文提出"模块化喷射"框架，用于评估回归和分类流水线的内部模块分解是否可由数据唯一确定，区分"幻象"和"可识别"两种状态。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习主要通过预测风险评估模型，但无法判断模型的内部模块分解是否由数据和评估设计唯一确定。需要一种方法来量化模块级表示如何响应输入的结构化扰动。

Method: 提出模块化喷射框架：1) 定义任务流形、模块分解和模块级表示；2) 估计经验喷射（局部线性响应映射），描述每个模块对输入小扰动的响应；3) 提出MoJet算法进行经验喷射估计和幻象诊断。

Result: 1) 在双模块线性回归流水线中证明了喷射可识别性定理：在温和的秩假设和模块级喷射可访问条件下，内部分解是唯一确定的；2) 仅基于风险的评估则允许大量幻象分解；3) 在线性和深度回归以及流水线分类中展示了框架的应用。

Conclusion: 模块化喷射框架超越了传统风险评估，能够诊断模型内部分解的可识别性，区分幻象和可识别状态，为理解模型内部结构提供了新工具。

Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.

</details>


### [76] [Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs](https://arxiv.org/abs/2512.05648)
*Igor Shilov,Alex Cloud,Aryo Pradipta Gema,Jacob Goldman-Wetzler,Nina Panickssery,Henry Sleight,Erik Jones,Cem Anil*

Main category: cs.LG

TL;DR: SGTM（选择性梯度掩码）是一种改进的梯度路由技术，通过在预训练时对选定梯度进行零掩码，将目标知识定位到专用参数子集中，从而在存在标签噪声的情况下更好地实现知识保留/遗忘平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具有双重用途风险，数据过滤作为预训练缓解措施面临挑战：大规模标注有害数据成本高，且随着模型样本效率提升，即使少量错误标注也可能导致危险能力。需要解决错误标注有害内容带来的风险。

Method: 提出SGTM（选择性梯度掩码）技术，改进梯度路由方法。通过零掩码选定梯度，使目标领域示例仅更新其专用参数，将目标知识定位到特定参数子集中，便于后续移除。

Result: 在两个应用中验证SGTM效果：1）从双语合成数据集训练的模型中移除一种语言知识；2）从英文维基百科训练的模型中移除生物学知识。SGTM在存在标签错误时相比数据过滤和先前梯度路由方法提供更好的保留/遗忘平衡。SGTM对对抗性微调表现出强鲁棒性，需要比基于微调的遗忘方法（RMU）多7倍的微调步骤才能达到遗忘集基线性能。

Conclusion: SGTM为现有安全缓解措施提供了有前景的预训练补充，特别是在标签噪声不可避免的场景中。相比浅层遗忘方法，SGTM更难通过微调快速恢复，提供了更强的安全性保障。

Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.

</details>


### [77] [Feasibility of AI-Assisted Programming for End-User Development](https://arxiv.org/abs/2512.05666)
*Irene Weber*

Main category: cs.LG

TL;DR: AI辅助的终端用户编码作为终端用户开发的新范式，通过自然语言与AI助手交互来创建应用，相比传统的低代码/无代码平台具有更大灵活性、更广适用性、更快开发速度等优势。


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助的终端用户编码是否可以作为终端用户开发的可行范式，可能补充甚至替代现有的低代码/无代码平台模型。随着生成式AI和大型语言模型的发展，终端用户能够通过自然语言提示直接生成和优化编程代码，这为终端用户开发带来了新的可能性。

Method: 通过案例研究，让非程序员参与者通过与AI助手交互来开发基本的Web应用程序，研究AI辅助的终端用户编码的可行性。

Result: 大多数研究参与者在合理时间内成功完成了任务，并表达了对AI辅助的终端用户编码作为终端用户开发可行方法的支持。

Conclusion: AI辅助的终端用户编码是终端用户开发的可行范式，可能在未来补充甚至替代传统的低代码/无代码平台模型，具有重要的实践意义和研究价值。

Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.

</details>


### [78] [Mechanistic Interpretability of Antibody Language Models Using SAEs](https://arxiv.org/abs/2512.05794)
*Rebonto Haque,Oliver M. Turnbull,Anisha Parsan,Nithin Parsan,John J. Yang,Charlotte M. Deane*

Main category: cs.LG

TL;DR: TopK和Ordered稀疏自编码器用于分析抗体语言模型p-IgGen，TopK能揭示生物学特征但控制生成效果有限，Ordered能可靠识别可操控特征但可解释性较差。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏自编码器在蛋白质语言模型中的机制可解释性，特别是如何利用SAEs来理解和操控抗体语言模型的生成过程。

Method: 使用TopK和Ordered两种稀疏自编码器分析自回归抗体语言模型p-IgGen，比较它们在特征识别和生成操控方面的表现。

Result: TopK SAEs能揭示有生物学意义的潜在特征，但高特征概念相关性不能保证对生成的因果控制；Ordered SAEs通过层次结构能可靠识别可操控特征，但激活模式更复杂、可解释性较差。

Conclusion: TopK SAEs适合将潜在特征映射到概念，而Ordered SAEs在需要精确生成操控时更优，这推进了领域特定蛋白质语言模型的机制可解释性。

Abstract: Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.

</details>


### [79] [Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks](https://arxiv.org/abs/2512.05680)
*Alexander Mattick,George Yammine,Georgios Kontes,Setareh Maghsudi,Christopher Mutschler*

Main category: cs.LG

TL;DR: 该论文提出了一种基于部分可观测马尔可夫决策过程（POMDP）的波束选择方法，将波束选择问题建模为在线搜索过程，相比传统的监督学习方法能更好地处理移动用户轨迹和环境变化。


<details>
  <summary>Details</summary>
Motivation: 在5G/6G网络中，具有波束成形能力的大规模天线阵列需要高效的波束管理。传统基于预配置码本的模拟波束成形面临挑战：大码本、反射效应和波束阻塞使得最优波束选择困难。现有方法多采用监督学习基于历史波束选择预测最优波束，但难以处理新轨迹和环境变化。

Method: 将波束选择问题建模为部分可观测马尔可夫决策过程（POMDP），将环境建模为码本本身。在每个时间步，基于不可观测最优波束的信念状态和先前探测的波束选择候选波束。这种方法将波束选择问题框架化为在线搜索过程，定位移动的最优波束。

Result: 该方法能够处理新的或不可预见的用户轨迹和物理环境变化，相比先前工作性能提升数个数量级。

Conclusion: 基于POMDP的波束选择方法相比传统监督学习方法具有更好的适应性和性能，能够有效应对移动用户和动态环境带来的挑战，为5G/6G网络中的波束管理提供了更优的解决方案。

Abstract: Beamforming-capable antenna arrays with many elements enable higher data rates in next generation 5G and 6G networks. In current practice, analog beamforming uses a codebook of pre-configured beams with each of them radiating towards a specific direction, and a beam management function continuously selects \textit{optimal} beams for moving user equipments (UEs). However, large codebooks and effects caused by reflections or blockages of beams make an optimal beam selection challenging. In contrast to previous work and standardization efforts that opt for supervised learning to train classifiers to predict the next best beam based on previously selected beams we formulate the problem as a partially observable Markov decision process (POMDP) and model the environment as the codebook itself. At each time step, we select a candidate beam conditioned on the belief state of the unobservable optimal beam and previously probed beams. This frames the beam selection problem as an online search procedure that locates the moving optimal beam. In contrast to previous work, our method handles new or unforeseen trajectories and changes in the physical environment, and outperforms previous work by orders of magnitude.

</details>


### [80] [Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization](https://arxiv.org/abs/2512.05825)
*Shuhei Watanabe*

Main category: cs.LG

TL;DR: 本文填补了多目标贝叶斯优化中HV近似算法文献空白，提供了完整的数学和算法描述


<details>
  <summary>Details</summary>
Motivation: 超体积贝叶斯优化中，HV改进计算的计算成本是主要瓶颈。虽然HV盒分解可以应对频繁的精确改进计算，但其在最坏情况下具有超多项式内存复杂度。现有近似算法缺乏严格的算法描述。

Method: 提供了Couckuyt等人(2012)提出的超体积近似算法的全面数学和算法细节描述，填补了文献中的空白。

Result: 为多目标贝叶斯优化中的超体积近似算法提供了完整的数学基础和算法实现细节，使该算法在文献中有了严格的描述。

Conclusion: 本文填补了多目标决策中HV近似算法文献的重要空白，为超体积贝叶斯优化的计算效率提升提供了完整的算法基础。

Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.

</details>


### [81] [BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language](https://arxiv.org/abs/2512.05721)
*Nitin Priyadarshini Shankar,Vaibhav Singh,Sheetal Kalyani,Christian Maciocco*

Main category: cs.LG

TL;DR: BERTO是一个基于BERT的交通预测和能源优化框架，通过自然语言提示平衡节能与性能，在真实数据集上减少4.13%的MSE


<details>
  <summary>Details</summary>
Motivation: 在蜂窝网络中，交通预测和能源优化存在节能与性能之间的权衡问题，需要一种灵活的方法来平衡这两个竞争目标

Method: 基于Transformer架构构建BERTO框架，采用平衡损失函数和基于提示的自定义方法，通过自然语言提示指导模型管理预测偏差

Result: 在真实数据集上，BERTO相比现有模型减少4.13%的MSE，能够在1.4kW功率范围和9倍服务质量变化范围内灵活操作

Conclusion: BERTO通过自然语言提示有效平衡节能与性能，适用于智能无线接入网络部署，为运营商提供了灵活的优化工具

Abstract: We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\times$ variation in service quality, making it well suited for intelligent RAN deployments.

</details>


### [82] [NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation](https://arxiv.org/abs/2512.05844)
*Daniel Rose,Roxane Axel Jacob,Johannes Kirchmair,Thierry Langer*

Main category: cs.LG

TL;DR: NEAT是一种基于邻域引导的高效自回归集合Transformer，用于3D分子结构生成，通过将分子图视为原子集合并学习图边界上可接受标记的顺序无关分布，实现了原子级置换不变性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型是扩散模型在3D分子生成中的有前景替代方案，但存在标记顺序假设的限制。分子图中的下一个标记预测应该对原子置换保持不变，而文本具有自然顺序。先前工作通过使用规范顺序或焦点原子来回避这种不匹配，作者认为这是不必要的。

Method: NEAT（邻域引导、高效、自回归、集合Transformer）将分子图视为原子集合，使用自回归流模型学习图边界上可接受标记的顺序无关分布。该方法具有原子级置换不变性。

Result: NEAT在3D分子生成方面达到了最先进的性能，同时具有高计算效率和原子级置换不变性，为可扩展分子设计建立了实用基础。

Conclusion: NEAT通过将分子视为原子集合并学习顺序无关分布，解决了自回归模型中标记顺序假设与分子置换不变性之间的不匹配问题，为3D分子生成提供了高效实用的解决方案。

Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.

</details>


### [83] [Teaching Language Models Mechanistic Explainability Through Arrow-Pushing](https://arxiv.org/abs/2512.05722)
*Théo A. Neukomm,Zlatko Jončev,Philippe Schwaller*

Main category: cs.LG

TL;DR: 该研究开发了一个基于语言模型的化学机理预测框架，使用MechSMILES格式编码分子结构和电子流动，在机理预测任务上达到高准确率，并应用于合成规划验证、原子映射和催化剂识别。


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助合成规划系统缺乏机理基础，无法提供化学合成的机理洞察。需要开发能够理解和预测化学反应机理的计算框架，以提高合成规划的可解释性和化学有效性。

Method: 提出MechSMILES格式编码分子结构和电子流动，使用语言模型在四个复杂度递增的机理预测任务上进行训练，利用mech-USPTO-31k和FlowER等机理反应数据集。

Result: 模型在基本步骤预测上达到超过95%的top-3准确率，在mech-USPTO-31k数据集上超过73%，在FlowER数据集上达到93%的完整反应机理检索准确率。

Conclusion: 该工作通过基于物理意义的电子流动预测，为计算合成规划提供了更可解释和化学有效的途径，同时为机理预测提供了架构无关的基准测试框架，并实现了三个关键应用：合成规划验证、原子映射和催化剂识别。

Abstract: Chemical reaction mechanisms provide crucial insight into synthesizability, yet current Computer-Assisted Synthesis Planning (CASP) systems lack mechanistic grounding. We introduce a computational framework for teaching language models to predict chemical reaction mechanisms through arrow pushing formalism, a century-old notation that tracks electron flow while respecting conservation laws. We developed MechSMILES, a compact textual format encoding molecular structure and electron flow, and trained language models on four mechanism prediction tasks of increasing complexity using mechanistic reaction datasets, such as mech-USPTO-31k and FlowER. Our models achieve more than 95\% top-3 accuracy on elementary step prediction and scores that surpass 73\% on mech-USPTO-31k, and 93\% on FlowER dataset for the retrieval of complete reaction mechanisms on our hardest task. This mechanistic understanding enables three key applications. First, our models serve as post-hoc validators for CASP systems, filtering chemically implausible transformations. Second, they enable holistic atom-to-atom mapping that tracks all atoms, including hydrogens. Third, they extract catalyst-aware reaction templates that distinguish recycled catalysts from spectator species. By grounding predictions in physically meaningful electron moves that ensure conservation of mass and charge, this work provides a pathway toward more explainable and chemically valid computational synthesis planning, while providing an architecture-agnostic framework for the benchmarking of mechanism prediction.

</details>


### [84] [Sparse Attention Post-Training for Mechanistic Interpretability](https://arxiv.org/abs/2512.05865)
*Florent Draye,Anson Lei,Ingmar Posner,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 提出一种简单的后训练方法，使Transformer注意力稀疏化而不损失性能，可将注意力连接减少到约0.3%，同时保持预训练损失不变


<details>
  <summary>Details</summary>
Motivation: 探索Transformer注意力中的冗余计算，通过稀疏化作为结构先验，使模型更结构化、可解释，同时保持原有能力

Method: 采用简单的后训练方法，在约束损失目标下应用灵活的稀疏正则化，使Transformer注意力稀疏化

Result: 在高达10亿参数的模型上，可将注意力连接减少到约0.3%，同时保持原始预训练损失；局部稀疏性级联为全局电路简化，任务特定电路涉及的组件和连接边减少达100倍

Conclusion: Transformer注意力可以变得稀疏数个数量级，表明其大部分计算是冗余的，稀疏性可作为构建更结构化、可解释模型的指导原则

Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.

</details>


### [85] [Towards agent-based-model informed neural networks](https://arxiv.org/abs/2512.05764)
*Nino Antulov-Fantulin*

Main category: cs.LG

TL;DR: 提出ABM-NNs框架，将基于主体模型的约束融入神经网络设计，确保学习到的动力学保持物理不变性等结构特性，在生态、传染病、宏观经济三个案例中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 标准神经微分方程在建模复杂系统时存在局限，缺乏物理不变性（如能量守恒），但实际系统往往需要满足其他约束（如质量守恒、网络局部性、有限理性等）。需要开发既能学习复杂动力学又能保持底层模型结构特性的神经网络框架。

Method: 提出Agent-Based-Model informed Neural Networks (ABM-NNs)框架，利用受限图神经网络和层次分解技术，学习可解释且保持结构特性的动力学。通过约束神经网络架构，确保学习到的模型符合基于主体模型的基本原理。

Result: 在三个复杂度递增的案例中验证：1）广义Lotka-Volterra系统：从短轨迹中恢复真实参数，即使在干预存在的情况下；2）基于图的SIR传染病模型：在样本外预测和噪声鲁棒性方面优于GCN、GraphSAGE、Graph Transformer等图学习基线；3）十大经济体宏观经济模型：从经验数据中学习耦合GDP动力学，并实现基于梯度的政策干预反事实分析。

Conclusion: ABM-NNs框架成功地将基于主体模型的约束融入神经网络设计，能够在保持底层模型结构特性的同时学习复杂系统动力学，为复杂系统的可解释建模提供了有效工具。

Abstract: In this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka--Volterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.

</details>


### [86] [Neural Coherence : Find higher performance to out-of-distribution tasks from few samples](https://arxiv.org/abs/2512.05880)
*Simon Guiroy,Mats Richter,Sarath Chandar,Christopher Pal*

Main category: cs.LG

TL;DR: 提出基于"神经一致性"的新方法，仅需少量无标签目标域样本即可有效选择预训练模型检查点，在数据稀缺、无标签、分布外场景下优于现有基线


<details>
  <summary>Details</summary>
Motivation: 当目标任务数据稀缺、无标签且分布外时，传统基于验证集的方法不可靠或不适用，需要一种仅需少量无标签目标域样本就能可靠选择预训练模型检查点的方法

Method: 提出"神经一致性"概念，通过表征模型在源域和目标域的激活统计特性，设计高数据效率的模型选择方法，仅需少量无标签目标域样本即可工作

Result: 在ImageNet1K预训练模型上，针对Food-101、PlantNet-300K、iNaturalist等目标域以及元学习设置进行实验，相比现有基线显著提升了泛化性能，并展示了神经一致性在训练数据选择中的有效性

Conclusion: 神经一致性是一个强大的原理，能够仅用少量无标签目标域样本实现可靠的模型选择，在数据稀缺、分布外场景下具有重要应用价值

Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.

</details>


### [87] [Learnability Window in Gated Recurrent Neural Networks](https://arxiv.org/abs/2512.05790)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，解释门控机制如何决定循环神经网络的可学习窗口，即梯度信息保持统计可恢复的最大时间范围。研究发现可学习性由有效学习率控制，而不是传统的雅可比乘积数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统分析强调雅可比乘积的数值稳定性，但作者认为这不足以解释门控循环网络学习长程时间依赖的能力。需要建立一个理论框架来理解门控机制如何影响梯度信息的统计可恢复性。

Method: 开发了一个理论框架，通过有效学习率μ_{t,ℓ}来分析门控机制对梯度传输的影响。这些有效学习率来自门控诱导雅可比乘积在时间反向传播中的一阶展开。在重尾（α-稳定）梯度噪声假设下，推导了检测滞后ℓ依赖所需的最小样本量公式。

Result: 证明了最小样本量N(ℓ)∝f(ℓ)^{-α}，其中f(ℓ)=‖μ_{t,ℓ}‖_1是有效学习率包络。这导出了可学习窗口H_N的显式公式，以及对数、多项式和指数衰减的闭式标度律。更宽或更异构的门谱产生更慢的f(ℓ)衰减和更大的可学习窗口，而更重的尾部噪声会压缩H_N。

Conclusion: 有效学习率是控制门控循环网络何时以及多长时间可以学习长程时间依赖的基本量。该框架将门控诱导的时间尺度结构、梯度噪声和样本复杂度联系起来，为理解门控机制如何决定可学习性提供了理论基础。

Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $μ_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($α$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-α}$, where $f(\ell)=\|μ_{t,\ell}\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory predicts that broader or more heterogeneous gate spectra produce slower decay of $f(\ell)$ and hence larger learnability windows, whereas heavier-tailed noise compresses $\mathcal{H}_N$ by slowing statistical concentration. By linking gate-induced time-scale structure, gradient noise, and sample complexity, the framework identifies the effective learning rates as the fundamental quantities that govern when -- and for how long -- gated recurrent networks can learn long-range temporal dependencies.

</details>


### [88] [Impugan: Learning Conditional Generative Models for Robust Data Imputation](https://arxiv.org/abs/2512.05950)
*Zalish Mahmud,Anantaa Kotal,Aritran Piplai*

Main category: cs.LG

TL;DR: Impugan是一种基于条件生成对抗网络(cGAN)的缺失值填补方法，能够处理异构数据集并捕获非线性、多模态关系，相比传统方法显著提升了填补质量。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据普遍存在缺失值问题，传感器故障、记录不一致、不同来源数据集的尺度、采样率和质量差异都会导致缺失值。传统填补方法如回归模型、期望最大化、多重填补等依赖线性和独立性假设，这些假设在复杂或异构数据中很少成立，可能导致有偏或过度平滑的估计。

Method: 提出Impugan模型，这是一种条件生成对抗网络(cGAN)。模型在完整样本上训练，学习缺失变量如何依赖于观测变量。在推理阶段，生成器从可用特征重建缺失条目，判别器通过区分真实数据和填补数据来强制实现真实性。这种对抗过程使Impugan能够捕获传统方法无法表示的非线性和多模态关系。

Result: 在基准数据集和多源集成任务上的实验表明，Impugan相比领先基线方法实现了高达82%的地球移动距离(EMD)降低和70%的互信息偏差(MI)降低。这些结果表明对抗训练的生成模型为填补和合并不完整、异构数据提供了可扩展且有原则的方法。

Conclusion: Impugan通过条件生成对抗网络有效解决了异构数据集的缺失值填补问题，能够捕获复杂的非线性关系，显著优于传统填补方法，为处理现实世界不完整数据提供了强大的解决方案。

Abstract: Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025

</details>


### [89] [MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution](https://arxiv.org/abs/2512.05958)
*Sara Patel,Mingxun Zhou,Giulia Fanti*

Main category: cs.LG

TL;DR: MaxShapley是一种用于生成式搜索引擎的高效公平归因算法，基于可分解的最大和效用函数，在保持与Shapley值相当归因质量的同时，将计算复杂度从指数级降低到线性级。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的生成式搜索引擎取代传统搜索，信息提供者的补偿机制需要改变。为了维持这个生态系统，需要公平的机制来根据内容提供者对生成答案的贡献进行归因和补偿。

Method: MaxShapley是一种基于检索增强生成（RAG）管道的公平归因算法，它是著名Shapley值的一个特例。通过利用可分解的最大和效用函数，该算法能够在文档数量上实现线性计算复杂度，而不是Shapley值的指数复杂度。

Result: 在三个多跳问答数据集（HotPotQA、MuSiQUE、MS MARCO）上的评估显示，MaxShapley在保持与精确Shapley计算相当的归因质量的同时，显著减少了资源消耗。例如，在相同归因准确度下，相比先前最先进方法，资源消耗减少了高达8倍。

Conclusion: MaxShapley为生成式搜索引擎提供了一种高效、公平的内容归因机制，能够在保持高质量归因的同时大幅降低计算成本，有助于建立可持续的生成式搜索生态系统。

Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.

</details>


### [90] [Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws](https://arxiv.org/abs/2512.05817)
*Zhengquan Luo,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 论文提出统一理论框架分析数据集蒸馏，揭示了性能随样本量增长的缩放规律和配置多样性对样本需求的覆盖规律，证明不同匹配方法是可互换的代理目标。


<details>
  <summary>Details</summary>
Motivation: 数据集蒸馏虽然取得快速经验进展，但缺乏统一理论框架。现有方法基于异质的代理目标和优化假设，难以分析共同原理或提供通用保证。同时不清楚在训练配置变化时蒸馏数据如何保持有效性。

Method: 提出配置-动态-误差分析统一理论框架，将主要DD方法重新表述在通用泛化误差视角下。推导出缩放定律和覆盖定律，并通过实验验证。

Result: 获得两个主要理论结果：1）缩放定律提供单配置误差上界，解释性能饱和现象；2）覆盖定律显示所需蒸馏样本量与配置多样性呈线性关系。实验证实了这些定律。

Conclusion: 统一分析揭示了不同匹配方法是可互换的代理目标，为DD提供了理论基础，并支持理论驱动的紧凑、配置鲁棒的数据集蒸馏设计。

Abstract: Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradient, distribution, trajectory matching) are built on heterogeneous surrogate objectives and optimization assumptions, which makes it difficult to analyze their common principles or provide general guarantees. Moreover, it is still unclear under what conditions distilled data can retain the effectiveness of full datasets when the training configuration, such as optimizer, architecture, or augmentation, changes. To answer these questions, we propose a unified theoretical framework, termed configuration--dynamics--error analysis, which reformulates major DD approaches under a common generalization-error perspective and provides two main results: (i) a scaling law that provides a single-configuration upper bound, characterizing how the error decreases as the distilled sample size increases and explaining the commonly observed performance saturation effect; and (ii) a coverage law showing that the required distilled sample size scales linearly with configuration diversity, with provably matching upper and lower bounds. In addition, our unified analysis reveals that various matching methods are interchangeable surrogates, reducing the same generalization error, clarifying why they can all achieve dataset distillation and providing guidance on how surrogate choices affect sample efficiency and robustness. Experiments across diverse methods and configurations empirically confirm the derived laws, advancing a theoretical foundation for DD and enabling theory-driven design of compact, configuration-robust dataset distillation.

</details>


### [91] [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962)
*Germán Kruszewski,Pierre Erbacher,Jos Rozen,Marc Dymetman*

Main category: cs.LG

TL;DR: 该论文提出了一种新的强化学习方法，通过α-散度家族来优化LLM，在保持多样性的同时提高精度，解决了传统RL方法导致的多样性损失问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在调优大语言模型时会导致显著的多样性损失，这是因为RL隐式地优化了"模式寻求"或"零强制"的反向KL散度，使模型集中在目标分布的高概率区域而忽略其他区域。

Method: 从显式目标分布出发（通过过滤错误答案同时保留正确答案的相对概率），使用α-散度家族来近似这个目标分布，通过插值在模式寻求和质量覆盖散度之间实现精度-多样性权衡的直接控制。

Result: 在Lean定理证明基准测试中，该方法在覆盖精度帕累托前沿上实现了最先进的性能，在覆盖轴上优于所有先前方法。

Conclusion: 通过α-散度家族的方法能够有效解决RL调优LLM时的多样性损失问题，在精度和覆盖范围之间实现更好的平衡。

Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.

</details>


### [92] [Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks](https://arxiv.org/abs/2512.05868)
*Brian Ezinwoke,Oliver Rhodes*

Main category: cs.LG

TL;DR: 该研究将脉冲神经网络应用于高频价格尖峰预测，通过贝叶斯优化和惩罚性尖峰准确率目标函数提升性能，在模拟交易中显著超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现代高频交易环境中的价格尖峰现象既带来风险也蕴含机会，但传统金融模型难以捕捉所需的精细时间结构。脉冲神经网络具有处理离散事件和保持毫秒级时间精度的天然优势，适合应对这一挑战。

Method: 将高频股票数据转换为脉冲序列，评估三种架构：已建立的STDP训练的无监督SNN、具有显式抑制竞争的新型SNN、以及监督反向传播网络。使用贝叶斯优化进行超参数调优，采用新颖的惩罚性尖峰准确率目标函数确保预测尖峰率与经验事件率一致。

Result: 使用PSA优化的模型在模拟交易中持续优于SA调优的对应模型和基线。扩展的SNN模型在简单回测中实现了最高的累计回报（76.8%），显著超过监督替代方案（42.54%回报）。

Conclusion: 研究验证了脉冲神经网络在高频价格尖峰预测中的潜力，特别是当使用任务特定目标函数进行稳健调优时，能够有效应用于高频交易环境。

Abstract: Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.

</details>


### [93] [Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning](https://arxiv.org/abs/2512.05870)
*Daniel Miliate,Ashlie Martini*

Main category: cs.LG

TL;DR: 使用机器学习方法预测蒸汽压，筛选适用于太空环境的液体润滑剂


<details>
  <summary>Details</summary>
Motivation: 太空环境中移动机械组件需要液体润滑剂，但现有适合真空条件的液体润滑剂种类有限且各有局限性，限制了机械设计

Method: 采用数据驱动的机器学习方法，结合高通量分子动力学模拟和实验数据库数据训练模型，注重模型的可解释性以识别化学结构与蒸汽压之间的关系

Result: 开发了能够预测蒸汽压的机器学习模型，并基于模型洞察提出了几种有潜力的候选分子

Conclusion: 机器学习方法能够有效筛选适用于太空环境的液体润滑剂，为移动机械组件设计提供了新的候选材料

Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.

</details>


### [94] [DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints](https://arxiv.org/abs/2512.05881)
*Rahul Golder,Bimol Nath Roy,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: DAE-HardNet是一种物理约束神经网络，通过可微分投影层同时学习函数及其导数，严格满足微分代数方程约束，相比传统PINNs和MLPs显著降低物理损失。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINNs）通常以软约束方式最小化物理约束违反，难以严格满足包含微分算子的约束。数据驱动模型将原始函数视为黑盒，其导数只能在函数评估后获得，这使得严格嵌入微分代数方程（DAEs）约束具有挑战性。

Method: 提出DAE-HardNet，通过可微分投影层将模型预测投影到约束流形上，同时学习函数及其导数，强制实施代数和微分约束。该方法能够同时学习函数值和导数值，确保严格满足物理约束。

Result: 相比多层感知机（MLPs）和PINNs，DAE-HardNet实现了数量级级别的物理损失降低，同时保持预测精度。该方法还能学习导数，改进了投影层之前骨干神经网络的约束学习。对于特定问题，可以绕过投影层实现更快推理。

Conclusion: DAE-HardNet是一种物理约束而非仅仅物理信息的神经网络，能够严格满足微分代数方程约束，在多个测试问题中表现出优越性能，包括动态Lotka-Volterra捕食者-猎物系统和瞬态热传导问题，并能用于参数估计。

Abstract: Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.

</details>


### [95] [NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process](https://arxiv.org/abs/2512.05893)
*Neha Gupta,Aditya Maheshwari*

Main category: cs.LG

TL;DR: 提出基于LSTM的循环神经网络框架，用于估计分数泊松过程的参数μ和β，相比传统矩估计法将均方误差降低约55.3%，并在真实高频数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 分数泊松过程能够建模具有记忆性和长程依赖的事件到达，但传统参数估计方法存在局限性。需要开发能够有效捕捉时间依赖性的新方法来估计FPP的关键参数。

Method: 使用长短期记忆网络构建循环神经网络框架，从到达时间间隔序列中估计分数泊松过程的参数μ（>0）和β（0,1）。LSTM能够有效建模时间依赖性。

Result: 在合成数据上，相比传统矩估计法，该方法将均方误差降低约55.3%。在真实数据（蒙哥马利县紧急呼叫记录和AAPL股票交易数据）上，LSTM能有效跟踪日模式和参数变化。

Conclusion: 提出的LSTM框架能够有效估计分数泊松过程参数，在合成和真实数据上都表现出优越性能，特别适用于具有复杂时间依赖性的现实世界数据。

Abstract: In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $μ>0$ and $β\in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.

</details>


### [96] [LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction](https://arxiv.org/abs/2512.05915)
*Marius F. R. Juston,Ramavarapu S. Sreenivas,Dustin Nottage,Ahmet Soylemezoglu*

Main category: cs.LG

TL;DR: 本文提出了一种基于线性矩阵不等式框架的Lipschitz深度残差网络设计方法，通过LDL分解扩展了Lipschitz约束到任意非线性架构，在UCI数据集上相比SLL Layers获得了3%-13%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 深度残差网络在计算机视觉任务中表现出色，但需要控制网络的Lipschitz常数以增强对抗鲁棒性和网络可验证性。现有方法在构建Lipschitz约束网络方面存在局限性。

Method: 将ResNet架构重新表述为循环三对角LMI，推导网络参数的闭式约束以确保Lipschitz连续性；使用新的LDL分解方法验证LMI可行性，将Lipschitz网络构造扩展到任意非线性架构；采用Cholesky分解进行高效参数化。

Result: 提出的LDL公式是SDP基网络的紧密松弛，保持了完全表达能力；在121个UCI数据集上相比SLL Layers获得了3%-13%的准确率提升。

Conclusion: 该方法为构建Lipschitz约束的残差网络和其他分层架构提供了可证明的参数化方法，适用于对抗鲁棒性、认证训练和控制系统等应用。

Abstract: Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\%-13\% accuracy gains over SLL Layers on 121 UCI data sets.

</details>


### [97] [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916)
*Damien Lesens,Beheshteh T. Rakhshan,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: KQ-SVD：一种通过闭式解直接对注意力矩阵进行最优低秩分解的KV缓存压缩方法，相比现有方法能更准确地保持注意力输出


<details>
  <summary>Details</summary>
Motivation: 随着序列长度和批处理大小的增长，Transformer大语言模型中的键值缓存成为主要内存瓶颈。现有压缩方法通常只对键进行低秩分解或尝试联合嵌入查询和键，但都忽略了注意力机制本质上依赖于它们的内积这一事实

Method: 提出KQ-SVD方法，通过闭式解直接对注意力矩阵进行最优低秩分解，针对冗余的真实来源，在压缩下能更高保真度地保持注意力输出

Result: 在LLaMA和Mistral模型上的广泛评估表明，该方法在投影质量方面始终优于现有方法

Conclusion: KQ-SVD是一种简单且计算高效的方法，通过直接分解注意力矩阵来更有效地压缩KV缓存，解决了现有方法的次优问题

Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.

</details>


### [98] [On the Bayes Inconsistency of Disagreement Discrepancy Surrogates](https://arxiv.org/abs/2512.05931)
*Neil G. Marchant,Andrew C. Cullen,Feng Liu,Sarah M. Erfani*

Main category: cs.LG

TL;DR: 论文针对深度神经网络在分布偏移下的失效问题，提出了一种新的分歧损失函数，该函数与交叉熵结合可提供对分歧差异的一致代理估计，解决了现有代理损失函数缺乏贝叶斯一致性的根本缺陷。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在现实世界部署中常因分布偏移而失效，这是构建安全可靠系统的关键障碍。现有方法依赖于分歧差异来衡量模型间分歧在分布变化下的变化，但优化过程中涉及不可微的0-1损失，需要使用代理损失函数。研究发现现有代理损失函数缺乏贝叶斯一致性，这是根本缺陷。

Method: 通过理论分析为现有代理损失函数的最优性差距提供上下界，并基于此理论指导提出一种新的分歧损失函数。该分歧损失与交叉熵结合，构成了对分歧差异的可证明一致代理估计方法。

Result: 在多种基准测试上的实证评估表明，该方法比现有方法能提供更准确、更稳健的分歧差异估计，特别是在具有挑战性的对抗条件下表现更优。

Conclusion: 论文通过理论分析和实证验证，提出了一种新的分歧损失函数，解决了现有代理损失函数缺乏贝叶斯一致性的问题，为在分布偏移下构建更可靠的深度学习系统提供了更有效的工具。

Abstract: Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models changes under a shifting distribution. The process of maximizing this measure has seen applications in bounding error under shifts, testing for harmful shifts, and training more robust models. However, this optimization involves the non-differentiable zero-one loss, necessitating the use of practical surrogate losses. We prove that existing surrogates for disagreement discrepancy are not Bayes consistent, revealing a fundamental flaw: maximizing these surrogates can fail to maximize the true disagreement discrepancy. To address this, we introduce new theoretical results providing both upper and lower bounds on the optimality gap for such surrogates. Guided by this theory, we propose a novel disagreement loss that, when paired with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. Empirical evaluations across diverse benchmarks demonstrate that our method provides more accurate and robust estimates of disagreement discrepancy than existing approaches, particularly under challenging adversarial conditions.

</details>


### [99] [Developing synthetic microdata through machine learning for firm-level business surveys](https://arxiv.org/abs/2512.05948)
*Jorge Cisneros Paz,Timothy Wojan,Matthew Williams,Jennifer Ozawa,Robert Chew,Kimberly Janda,Timothy Navarro,Michael Floyd,Christine Task,Damon Streat*

Main category: cs.LG

TL;DR: 该论文介绍了使用机器学习模型为美国年度商业调查（ABS）创建合成公共使用微数据样本（PUMS）的方法，以解决商业数据匿名化挑战，并通过复制已有研究验证了合成数据的真实性。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力提升和大数据可用性增加，传统匿名化数据面临重新识别的风险，可能违反调查对象的保密承诺。特别是商业数据面临独特挑战，因为企业缺乏匿名性且某些行业在特定地理区域容易被识别。

Method: 使用机器学习模型构建基于年度商业调查（ABS）的合成PUMS，生成保留原始数据关键统计特征但不包含任何真实个体或企业记录的数据。论文还讨论了各种质量评估指标。

Result: 虽然ABS PUMS仍在完善中且结果保密，但作者展示了为2007年企业主调查开发的合成PUMS。通过复制《小企业经济学》中已发表的高影响力分析，验证了合成数据与真实数据的相似性。

Conclusion: 合成数据方法能够有效保护调查对象的隐私，同时保留数据的分析价值。论文讨论了ABS合成数据的潜在应用场景，为商业调查数据的公开使用提供了可行的技术解决方案。

Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.

</details>
