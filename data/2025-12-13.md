<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 12]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 线性模型（特别是DLinear）在仅使用历史室内温度数据的长期预测任务中，比复杂的Transformer系列模型表现更好


<details>
  <summary>Details</summary>
Motivation: 研究在仅使用历史温度数据（外生变量）的挑战性单变量设置下，线性模型和Transformer系列模型在长期温度预测中的表现对比

Method: 使用Linear、NLinear、DLinear、Transformer、Informer和Autoformer模型，在标准化的训练、验证和测试分割下进行长期外生温度预测评估

Result: 线性基线模型（Linear、NLinear、DLinear）在所有分割中始终优于更复杂的Transformer系列架构，其中DLinear在所有分割中实现了最佳的整体准确性

Conclusion: 精心设计的线性模型在仅使用外生变量的挑战性时间序列预测场景中仍然是强大的基线方法

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [2] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: 提出GTL方法，在不修改预训练去噪器的情况下，通过引导实现离散扩散模型的迁移学习，并开发高效采样器降低计算成本


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在语言等离散领域表现优异，但需要大量训练数据，在新领域适应成本高。现有迁移学习方法需要微调大型扩散模型，计算昂贵且不实用

Method: 基于连续扩散的比率迁移学习，提出GTL方法，通过引导从目标分布采样而不修改预训练去噪器。方法适用于离散时间扩散和连续时间基于分数的离散扩散。进一步开发高效引导采样器，集中评估规划选择的位置和候选标记

Result: GTL在序列数据（包括合成马尔可夫链和语言建模）上进行了评估，提供了其行为的实证分析。高效采样器使大规模语言建模在大型词汇表和长序列中变得实用

Conclusion: GTL为离散扩散模型提供了一种有效的迁移学习方法，无需微调预训练模型，并通过高效采样器解决了大规模应用的计算挑战

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [3] [Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes](https://arxiv.org/abs/2512.10878)
*Xuan Zhao,Zhuo Cao,Arya Bangun,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: 该论文提出了一种利用反事实解释改进模型重构的方法，通过将原始数据与反事实样本结合，使用Wasserstein重心近似类别原型，从而提高代理模型的质量并减少决策边界偏移问题。


<details>
  <summary>Details</summary>
Motivation: 反事实解释通常位于决策边界附近，可以作为信息丰富的样本用于模型重构，特别是在标记数据有限的情况下。然而，直接将反事实作为普通训练实例使用会导致决策边界偏移问题，需要更有效的方法来利用这些样本。

Method: 提出了一种将原始数据样本与反事实样本相结合的方法，使用Wasserstein重心来近似每个类别的原型，从而保持类别底层分布结构。这种方法能够更好地利用反事实样本的信息价值，同时避免决策边界偏移。

Result: 在多个数据集上的实证结果表明，该方法显著提高了代理模型与目标模型之间的保真度，验证了其有效性。相比直接将反事实作为普通训练实例的方法，该方法在模型重构质量上有明显改进。

Conclusion: 通过将反事实样本与原始数据结合，并使用Wasserstein重心近似类别原型，可以显著改进模型重构的质量。这种方法特别适用于标记数据有限的场景，能够有效利用反事实样本的信息价值，同时避免决策边界偏移问题。

Abstract: Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.

</details>


### [4] [Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields](https://arxiv.org/abs/2512.10886)
*Stefan Matthes,Markus Schramm*

Main category: cs.LG

TL;DR: 提出基于物理信息学习的框架，从常规运行数据中推断槽式光热电站的回路质量流量比和随时间变化的接收器传热系数，解决液压不平衡和接收器退化诊断问题。


<details>
  <summary>Details</summary>
Motivation: 槽式光热电站的大型液压收集器回路网络需要提供均匀的出口温度，但由于空间异质性的光学性能、热损失和压降，难以实现。回路温度可测量，但回路级质量流量和接收器热损失参数无法观测，导致无法使用标准监测工具诊断液压不平衡或接收器退化。

Method: 提出物理信息学习框架，利用夜间均匀化时段（热油在非辐照场中循环）来分离液压和热损失效应。将可微共轭传热模型离散化并嵌入端到端学习管道，使用50MW Andasol 3太阳能场的历史数据进行优化。

Result: 模型准确重建回路温度（RMSE <2°C），产生物理上有意义的回路不平衡和接收器热损失估计。与基于无人机的红外热成像（QScan）比较显示强相关性，正确识别所有高损失接收器区域。

Conclusion: 嘈杂的真实世界光热电站运行数据包含足够信息来恢复潜在物理参数，当与适当的建模和可微优化结合时。该方法为诊断液压不平衡和接收器退化提供了有效工具。

Abstract: Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.
  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.
  The model accurately reconstructs loop temperatures (RMSE $<2^\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.

</details>


### [5] [SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale](https://arxiv.org/abs/2512.10922)
*Max Zimmer,Christophe Roux,Moritz Wagner,Deborah Hendrych,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 提出一种针对大语言模型的高效剪枝方法，通过行级等稀疏约束和1-swap优化，显著降低剪枝误差，无需完整重训练。


<details>
  <summary>Details</summary>
Motivation: 大语言模型剪枝面临挑战：完整重训练成本过高，传统全局幅度剪枝对Transformer架构不优，现有方法依赖近似或启发式算法，整数规划求解组合优化问题计算不可行。

Method: 1. 通过强制每行相等稀疏度解耦行间依赖；2. 推导可高效计算的Gram矩阵最优1-swap（交换一个保留权重和一个剪枝权重）；3. 提出从任意剪枝掩码开始、在GPU上高效运行、基本无超参数的1-swap算法。

Result: 相比Wanda方法减少每层剪枝误差达60%，在先进GPT架构上持续改善困惑度和零样本准确率。

Conclusion: 通过行级等稀疏约束和高效1-swap算法，使大语言模型规模的掩码选择问题变得可行，提供了一种简单、高效、无超参数的剪枝解决方案。

Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

</details>


### [6] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 本文提出基于PPO算法的深度强化学习方法，用于BlueROV2水下机器人在无GPS、能见度差且有障碍物的环境中的自主导航，相比传统DWA方法在复杂环境中表现更优。


<details>
  <summary>Details</summary>
Motivation: 水下环境自主导航面临三大挑战：GPS信号缺失、能见度降低以及水下障碍物存在。这些因素使得传统导航方法效果受限，需要更智能的解决方案。

Method: 采用基于PPO算法的深度强化学习方法，观测空间结合了目标导向导航信息、虚拟占用网格和操作区域边界的射线投射。通过与DWA确定性运动规划器的对比评估，在仿真环境中训练并在物理BlueROV2上验证，使用测试场地的3D数字孪生进行监督。

Result: PPO策略在高度杂乱环境中持续优于DWA方法，主要体现在更好的局部适应性和更少的碰撞次数。实验证明了从仿真到真实世界的学习行为可迁移性。

Conclusion: 深度强化学习对于水下机器人自主导航具有实际应用价值，PPO算法在复杂水下环境中展现出优于传统规划方法的性能，且仿真到现实的迁移效果良好。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [7] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 提出一种解耦批评家与策略块长度的方法，通过蒸馏部分动作块的乐观价值估计，解决长动作块策略学习的挑战，在长时域离线目标条件任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间差分方法存在自举偏差问题，块状批评家虽然加速了价值备份，但要求策略以开环方式输出整个动作块，这在需要策略反应性的环境中可能次优，且长动作块建模困难。

Method: 提出解耦批评家与策略块长度的算法：通过从原始块状批评家乐观回推，蒸馏出部分动作块的价值估计，近似当部分动作块扩展为完整块时可达到的最大价值，从而优化策略。

Result: 在具有挑战性的长时域离线目标条件任务上评估，该方法可靠地优于先前方法。

Conclusion: 该方法保留了多步价值传播的优势，同时避免了开环次优性和学习长动作块策略的困难，为块状批评家与策略的协调提供了有效解决方案。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [8] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 该研究提出了一种让大语言模型能够同时思考、听取和生成输出的方法，解决了传统推理模型需要停止思考才能响应的延迟问题，实现了实时交互。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理时需要先完成思考才能给出答案，这种顺序交互方式不适合需要实时响应的应用场景（如语音助手）。人类可以异步地听、想、说，而现有LLM无法做到这一点。

Method: 利用旋转嵌入的特性，使原本设计用于顺序交互的LLM能够同时进行思考、听取和生成输出，无需额外训练。

Result: 在数学推理、常识推理和安全推理任务上评估，该方法能够实时生成准确的思考增强答案，将第一个非思考token的生成时间从几分钟减少到≤5秒，整体实时延迟降低了6-11倍。

Conclusion: 通过旋转嵌入技术，可以使推理型LLM实现类似人类的异步听、想、说能力，显著提升实时交互性能，适用于需要实时响应的应用场景。

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [9] [Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks](https://arxiv.org/abs/2512.10936)
*Kristina Korotkova,Aleksandr Katrutsa*

Main category: cs.LG

TL;DR: 该论文提出使用改进的Frank-Wolfe方法（无投影优化算法）构建高效的白盒对抗攻击，从数值优化角度提升对抗攻击的构造效率


<details>
  <summary>Details</summary>
Motivation: 神经网络在各类服务中部署时面临对抗攻击的挑战，需要快速有效的对抗攻击构建方法来评估神经网络的对抗鲁棒性。由于对抗攻击构建本质上是一个优化问题，作者希望从数值优化角度寻找更高效的解决方案。

Method: 采用改进的Frank-Wolfe方法（无投影优化算法）构建白盒对抗攻击，避免了传统方法中的投影操作。在MNIST和CIFAR-10数据集上，对多类逻辑回归模型、卷积神经网络（CNN）和视觉变换器（ViT）进行了实验验证。

Result: 通过理论分析和数值实验评估了改进的Frank-Wolfe方法，并与基于投影操作或几何直觉的标准方法进行了比较。实验结果表明该方法在对抗攻击构建方面具有效率和效果优势。

Conclusion: 从数值优化角度出发，改进的Frank-Wolfe方法为构建高效有效的对抗攻击提供了一种有前景的解决方案，能够更好地评估神经网络的对抗鲁棒性。

Abstract: The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).

</details>


### [10] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的点状函数Derf，作为归一化层的替代方案，在多个领域超越了LayerNorm、RMSNorm和DyT等现有方法。


<details>
  <summary>Details</summary>
Motivation: 虽然归一化层长期以来被视为深度学习架构中不可或缺的组件，但最近引入的Dynamic Tanh (DyT)表明存在替代方案。DyT通过约束极值实现稳定收敛并达到归一化级别的性能，本研究旨在寻找能够超越DyT的函数设计。

Method: 首先研究点状函数的内在特性如何影响训练和性能，基于这些发现进行大规模搜索以寻找更有效的函数设计。通过探索，引入了Derf(x) = erf(αx + s)，其中erf(x)是重新缩放的高斯累积分布函数，并将其确定为性能最佳的设计。

Result: Derf在多个领域超越了LayerNorm、RMSNorm和DyT，包括视觉（图像识别和生成）、语音表示和DNA序列建模。性能提升主要源于其改进的泛化能力而非更强的拟合能力。

Conclusion: Derf的简单性和更强的性能使其成为无归一化Transformer架构的实用选择，为深度学习架构设计提供了新的方向。

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [11] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: DaSH方法通过层次化建模数据集和组级效用，在资源约束下高效选择数据集以提升下游任务性能


<details>
  <summary>Details</summary>
Motivation: 现实世界中数据通常以离散数据集形式组织，不同数据集在相关性、质量和效用上存在差异。现有方法通常选择单个样本且将所有数据视为同等相关，忽略了数据集及其来源之间的差异。因此需要一种能够选择整个数据集的方法来改善下游性能。

Method: 提出Dataset Selection via Hierarchies (DaSH)方法，在数据集和组（如集合、机构）两个层次上建模效用，使系统能够从有限观察中高效泛化。该方法适用于资源约束条件下的数据集选择。

Result: 在两个公共基准测试（Digit-Five和DomainNet）上，DaSH比最先进的数据选择基线方法在准确率上提升了高达26.2%，同时需要显著更少的探索步骤。消融实验表明DaSH在低资源设置和缺乏相关数据集的情况下具有鲁棒性。

Conclusion: DaSH方法适用于实际多源学习工作流中的可扩展和自适应数据集选择，能够有效解决从异构数据池中选择数据集的问题。

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>


### [12] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: BiFlow提出了一种双向归一化流框架，通过近似逆映射而非精确解析逆，解决了自回归流中因果解码的瓶颈问题，在ImageNet上实现了生成质量提升和采样速度两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 传统归一化流需要精确解析逆变换，限制了架构灵活性。TARFlow等基于Transformer的自回归流虽然复兴了NF方法，但暴露了因果解码作为主要瓶颈的问题。需要一种既能保持NF原则性框架，又能突破因果解码限制的方法。

Method: 提出双向归一化流(BiFlow)框架，放弃对精确解析逆的要求，学习一个近似噪声到数据的逆映射模型。这使得可以使用更灵活的损失函数和架构设计，不再受因果解码约束。

Result: 在ImageNet上的实验表明，相比因果解码对应方法，BiFlow提高了生成质量，同时将采样速度加速了高达两个数量级。在基于NF的方法中达到最先进水平，在单次评估("1-NFE")方法中具有竞争力。

Conclusion: BiFlow通过近似逆映射而非精确解析逆，解决了归一化流中的因果解码瓶颈，为NF框架提供了更大的灵活性，有望进一步推动这一经典范式的发展。

Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: LLMs可用于大型用户设施的提案评审，通过成对偏好方法提供比传统人工评分更一致、可扩展且成本更低的替代方案，在识别高发表潜力提案方面表现不亚于人类评审，成本降低两个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统人工提案评审存在提案间相关性弱、评审者偏见和不一致性问题。成对偏好方法在逻辑上更优越，但二次方工作量使其对人工评审不切实际，需要利用LLMs解决这一限制。

Method: 利用LLMs进行成对偏好评估，基于美国橡树岭国家实验室散裂中子源三个束线的精心策划提案和发表记录，比较LLM排名与人工排名的相关性，并通过嵌入模型定量评估提案相似性。

Result: LLM排名与人工排名强相关（Spearman ρ≈0.2-0.8，去除10%异常值后≥0.5）。LLM在识别高发表潜力提案方面表现不亚于人类评审，成本降低两个数量级以上，并能进行人类难以完成的提案相似性分析。

Conclusion: LLMs为大型用户设施的提案选择提供了可扩展、一致且成本效益高的替代方案，不仅能有效排名提案，还能提供对评审委员会至关重要的高级分析能力。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [14] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 本文提出了一种节点级剪枝框架用于电路发现，解决了现有方法在可扩展性和粒度上的限制，通过多粒度可学习掩码和粒度特定稀疏惩罚，在单次微调中实现全面压缩。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法主要依赖迭代边剪枝，计算成本高且仅限于粗粒度单元（如注意力头或MLP块），忽略了神经元级别的细粒度结构。需要解决可扩展性和粒度限制问题。

Method: 提出节点级剪枝框架，引入跨多个粒度级别的可学习掩码（从整个块到单个神经元），在统一优化目标中使用粒度特定稀疏惩罚指导剪枝过程，实现单次微调中的全面压缩。

Result: 该方法发现的电路节点数少于先前方法，证明许多粗粒度方法认为重要的神经元实际上无关紧要，同时保持任务性能。内存占用显著降低5-10倍，因为不需要在内存中保留中间激活。

Conclusion: 提出的节点级剪枝框架在电路发现中实现了更好的可扩展性和粒度控制，能够识别更精细的电路结构，同时大幅降低计算资源需求。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


### [15] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: 该论文建立了部分可观测马尔可夫决策过程（POMDP）中的智能体与单输入过程函数（高阶量子操作的经典极限）之间的精确对应关系。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能中的决策理论与物理学中量子操作理论之间的深层联系，为理解智能体决策过程提供新的数学框架。

Method: 通过将智能体的策略和记忆更新组合成过程函数w，使用链接积与POMDP环境交互，建立智能体与过程函数之间的对应关系。

Result: 成功建立了POMDP智能体与单输入过程函数之间的精确对应，并发现这种对应具有双重解释：从物理视角看，过程函数作为环境；从AI视角看，过程函数编码智能体。

Conclusion: 该对应关系为理解智能体决策提供了新的理论框架，并可扩展到多智能体系统，将观测无关的分散POMDP识别为多输入过程函数的自然领域。

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>
