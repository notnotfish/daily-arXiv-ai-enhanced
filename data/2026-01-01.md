<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 92]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models](https://arxiv.org/abs/2512.23850)
*Rahul Baxi*

Main category: cs.AI

TL;DR: 该研究提出了DDFT评估框架，用于测量语言模型在语义压缩和对抗性攻击下的认知稳健性，发现模型稳健性与参数规模无关，而主要取决于错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型评估主要测量理想条件下的知识掌握，但无法评估模型在现实压力下的稳健性。静态基准测试无法区分模型是缺乏知识还是验证机制在信息退化或对抗性攻击下崩溃。

Method: 引入DDFT（Drill-Down and Fabricate Test）协议，通过渐进语义压缩和对抗性伪造来测量认知稳健性。提出两系统认知模型：语义系统生成流畅文本，认知验证器验证事实准确性。评估了9个前沿模型在8个知识领域和5个压缩级别（共1800次轮级评估）。

Result: 认知稳健性与传统设计范式正交：参数数量（r=0.083, p=0.832）和架构类型（r=0.153, p=0.695）对稳健性无显著预测作用。错误检测能力强烈预测整体稳健性（rho=-0.817, p=0.007）。旗舰模型尽管规模大但表现脆弱，而较小模型可能达到稳健性能。

Conclusion: 认知稳健性主要取决于训练方法和验证机制，而非模型规模。DDFT框架为关键应用部署前评估认知稳健性提供了理论基础和实用工具，挑战了模型规模与可靠性关系的传统假设。

Abstract: Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.

</details>


### [2] [CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution](https://arxiv.org/abs/2512.23880)
*Xu Huang,Junwu Chen,Yuxing Fei,Zhuohan Li,Philippe Schwaller,Gerbrand Ceder*

Main category: cs.AI

TL;DR: CASCADE是一个自演化的LLM智能体框架，通过持续学习和自我反思等元技能，使智能体能够掌握复杂外部工具并编码知识，在科学任务上实现93.3%的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体依赖预定义工具或脆弱的工具生成，限制了其在复杂科学任务中的能力和适应性。需要从"LLM+工具使用"向"LLM+技能获取"过渡。

Method: CASCADE框架通过两种元技能实现：1) 持续学习（通过网络搜索和代码提取）；2) 自我反思（通过内省和知识图谱探索）。框架支持人类-智能体协作和记忆巩固。

Result: 在SciSkillBench基准测试（116个材料科学和化学研究任务）上，CASCADE使用GPT-5实现了93.3%的成功率，而没有演化机制的基线只有35.4%。展示了在计算分析、自主实验室实验和论文选择性复现等实际应用。

Conclusion: CASCADE能够积累可执行的技能，这些技能可以在智能体和科学家之间共享，推动可扩展的AI辅助科学研究，代表了向"LLM+技能获取"范式的重要转变。

Abstract: Large language model (LLM) agents currently depend on predefined tools or brittle tool generation, constraining their capability and adaptability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from "LLM + tool use" to "LLM + skill acquisition". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search and code extraction, and self-reflection via introspection and knowledge graph exploration, among others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.

</details>


### [3] [A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming](https://arxiv.org/abs/2512.23932)
*Ioanna Gemou,Evangelos Lamprou*

Main category: cs.AI

TL;DR: McCoy框架结合大语言模型和答案集编程，通过将医学文献转化为ASP代码并与患者数据结合，实现可解释的疾病预测系统。


<details>
  <summary>Details</summary>
Motivation: 准确的疾病预测对于及时干预、有效治疗和减少医疗并发症至关重要。虽然符号AI已应用于医疗保健，但由于构建高质量知识库需要大量努力，其采用仍然有限。

Method: McCoy框架结合大型语言模型（LLMs）和答案集编程（ASP），通过LLM将医学文献翻译成ASP代码，将其与患者数据结合，并使用ASP求解器处理以得出最终诊断。

Result: 初步结果显示，McCoy在小规模疾病诊断任务上表现出强大的性能。

Conclusion: 这种整合产生了一个稳健、可解释的预测框架，充分利用了两种范式的优势，克服了传统符号AI在医疗领域应用中的障碍。

Abstract: Accurate disease prediction is vital for timely intervention, effective treatment, and reducing medical complications. While symbolic AI has been applied in healthcare, its adoption remains limited due to the effort required for constructing high-quality knowledge bases. This work introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to overcome this barrier. McCoy orchestrates an LLM to translate medical literature into ASP code, combines it with patient data, and processes it using an ASP solver to arrive at the final diagnosis. This integration yields a robust, interpretable prediction framework that leverages the strengths of both paradigms. Preliminary results show McCoy has strong performance on small-scale disease diagnosis tasks.

</details>


### [4] [SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing](https://arxiv.org/abs/2512.24008)
*Gaurab Chhetri,Subasish Das,Tausif Islam Chowdhury*

Main category: cs.AI

TL;DR: SPARK框架通过协调基于角色的LLM代理实现个性化搜索，这些代理具备专门检索能力和协作机制，能够动态响应用户多维度信息需求。


<details>
  <summary>Details</summary>
Motivation: 传统搜索系统受限于静态用户档案和单一检索流程，难以捕捉用户动态、多维度的信息需求变化，需要更灵活、个性化的搜索解决方案。

Method: SPARK框架定义角色、专业知识、任务上下文和领域构成的人物空间，通过人物协调器动态激活相关专业代理。每个代理执行独立的检索增强生成过程，拥有长短时记忆存储和上下文感知推理模块。代理间通过共享内存库、迭代辩论和中继式知识转移等结构化通信协议协作。

Result: 该框架产生了关于协调效率、个性化质量和认知负载分布的可测试预测，同时包含自适应学习机制用于持续人物优化。通过整合细粒度代理专业化和协作检索，SPARK为下一代搜索系统提供了理论框架。

Conclusion: SPARK框架展示了如何通过分布式代理行为和最小协调规则实现个性化搜索的涌现特性，为捕捉人类信息寻求行为的复杂性、流动性和上下文敏感性提供了新思路。

Abstract: Personalized search demands the ability to model users' evolving, multi-dimensional information needs; a challenge for systems constrained by static profiles or monolithic retrieval pipelines. We present SPARK (Search Personalization via Agent-Driven Retrieval and Knowledge-sharing), a framework in which coordinated persona-based large language model (LLM) agents deliver task-specific retrieval and emergent personalization. SPARK formalizes a persona space defined by role, expertise, task context, and domain, and introduces a Persona Coordinator that dynamically interprets incoming queries to activate the most relevant specialized agents. Each agent executes an independent retrieval-augmented generation process, supported by dedicated long- and short-term memory stores and context-aware reasoning modules. Inter-agent collaboration is facilitated through structured communication protocols, including shared memory repositories, iterative debate, and relay-style knowledge transfer. Drawing on principles from cognitive architectures, multi-agent coordination theory, and information retrieval, SPARK models how emergent personalization properties arise from distributed agent behaviors governed by minimal coordination rules. The framework yields testable predictions regarding coordination efficiency, personalization quality, and cognitive load distribution, while incorporating adaptive learning mechanisms for continuous persona refinement. By integrating fine-grained agent specialization with cooperative retrieval, SPARK provides insights for next-generation search systems capable of capturing the complexity, fluidity, and context sensitivity of human information-seeking behavior.

</details>


### [5] [ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment](https://arxiv.org/abs/2512.24040)
*Natchaya Temyingyong,Daman Jain,Neeraj Kumarsahu,Prabhat Kumar,Rachata Phondi,Wachiravit Modecrua,Krittanon Kaewtawee,Krittin Pachtrachai,Touchapon Kraisingkorn*

Main category: cs.AI

TL;DR: ROAD是一个无需标注数据集的自适应提示优化框架，通过多智能体架构将失败日志转化为结构化决策树协议，在冷启动场景下显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现实软件工程中，LLM代理开发初期通常缺乏标注数据集，只有混乱的生产日志和不断演变的失败模式，传统基于标注数据的自动提示优化方法难以适用。

Method: ROAD采用多智能体架构：分析器进行根因分析，优化器进行模式聚合，教练进行策略整合，将非结构化失败日志转化为结构化决策树协议，模拟人类工程循环。

Result: 在学术基准和实际知识管理引擎中，ROAD仅需3次自动迭代就使成功率提升5.6%（73.6%到79.2%），搜索准确率提升3.8%；在零售领域复杂推理任务中，代理性能相对基线提升约19%。

Conclusion: 模拟人类工程循环的失败分析和补丁方法为部署可靠LLM代理提供了数据高效且可行的替代方案，避免了资源密集的强化学习训练。

Abstract: Automatic Prompt Optimization (APO) has emerged as a critical technique for enhancing Large Language Model (LLM) performance, yet current state-of-the-art methods typically rely on large, labeled gold-standard development sets to compute fitness scores for evolutionary or Reinforcement Learning (RL) approaches. In real-world software engineering, however, such curated datasets are rarely available during the initial cold start of agent development, where engineers instead face messy production logs and evolving failure modes. We present ROAD (Reflective Optimization via Automated Debugging), a novel framework that bypasses the need for refined datasets by treating optimization as a dynamic debugging investigation rather than a stochastic search. Unlike traditional mutation strategies, ROAD utilizes a specialized multi-agent architecture, comprising an Analyzer for root-cause analysis, an Optimizer for pattern aggregation, and a Coach for strategy integration, to convert unstructured failure logs into robust, structured Decision Tree Protocols. We evaluated ROAD across both a standardized academic benchmark and a live production Knowledge Management engine. Experimental results demonstrate that ROAD is highly sample-efficient, achieving a 5.6 percent increase in success rate (73.6 percent to 79.2 percent) and a 3.8 percent increase in search accuracy within just three automated iterations. Furthermore, on complex reasoning tasks in the retail domain, ROAD improved agent performance by approximately 19 percent relative to the baseline. These findings suggest that mimicking the human engineering loop of failure analysis and patching offers a viable, data-efficient alternative to resource-intensive RL training for deploying reliable LLM agents.

</details>


### [6] [LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm](https://arxiv.org/abs/2512.24077)
*Chunhui Wan,Xunan Dai,Zhuo Wang,Minglei Li,Yanpeng Wang,Yinan Mao,Yu Lan,Zhiwen Xiao*

Main category: cs.AI

TL;DR: LoongFlow是一个自进化代理框架，通过将LLM集成到"计划-执行-总结"认知范式中，解决了传统进化方法在代码空间中的结构推理不足问题，显著提高了进化效率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 从静态大语言模型向自改进代理的转变受到传统进化方法缺乏结构化推理的阻碍。现有方法在高维代码空间中经常面临早熟收敛和低效探索的问题。

Method: LoongFlow将LLM集成到认知"计划-执行-总结"范式中，将进化搜索映射为推理密集型过程。采用混合进化记忆系统，结合多岛模型、MAP-Elites和自适应玻尔兹曼选择，平衡探索-利用权衡。框架实例化为通用代理（算法发现）和机器学习代理（管道优化）。

Result: 在AlphaEvolve基准测试和Kaggle竞赛中的广泛评估显示，LoongFlow在进化效率上比领先基线（如OpenEvolve、ShinkaEvolve）高出60%，同时发现更优解决方案。

Conclusion: LoongFlow标志着自主科学发现的重要进展，能够以降低的计算开销生成专家级解决方案，为自进化代理系统提供了有效的框架。

Abstract: The transition from static Large Language Models (LLMs) to self-improving agents is hindered by the lack of structured reasoning in traditional evolutionary approaches. Existing methods often struggle with premature convergence and inefficient exploration in high-dimensional code spaces. To address these challenges, we introduce LoongFlow, a self-evolving agent framework that achieves state-of-the-art solution quality with significantly reduced computational costs. Unlike "blind" mutation operators, LoongFlow integrates LLMs into a cognitive "Plan-Execute-Summarize" (PES) paradigm, effectively mapping the evolutionary search to a reasoning-heavy process. To sustain long-term architectural coherence, we incorporate a hybrid evolutionary memory system. By synergizing Multi-Island models with MAP-Elites and adaptive Boltzmann selection, this system theoretically balances the exploration-exploitation trade-off, maintaining diverse behavioral niches to prevent optimization stagnation. We instantiate LoongFlow with a General Agent for algorithmic discovery and an ML Agent for pipeline optimization. Extensive evaluations on the AlphaEvolve benchmark and Kaggle competitions demonstrate that LoongFlow outperforms leading baselines (e.g., OpenEvolve, ShinkaEvolve) by up to 60% in evolutionary efficiency while discovering superior solutions. LoongFlow marks a substantial step forward in autonomous scientific discovery, enabling the generation of expert-level solutions with reduced computational overhead.

</details>


### [7] [CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation](https://arxiv.org/abs/2512.24113)
*Jiaxin Hu,Tao Wang,Bingsan Yang,Hongrun Wang*

Main category: cs.AI

TL;DR: CogRec是一个结合大语言模型和Soar认知架构的新型认知推荐代理，通过感知-认知-行动循环实现可解释的推荐，并利用LLM进行知识初始化和动态查询解决僵局，通过chunking机制实现在线学习。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推荐系统中虽然能理解用户偏好，但存在黑盒特性、知识幻觉和有限在线学习能力等问题，影响可信度和适应性。而认知架构如Soar虽然提供结构化可解释推理，但知识获取过程繁琐。需要结合两者优势解决互补挑战。

Method: 提出CogRec认知推荐代理，以Soar作为核心符号推理引擎，利用LLM进行知识初始化填充工作记忆中的产生式规则。采用感知-认知-行动循环，当遇到僵局时动态查询LLM获取推理解决方案，然后通过Soar的chunking机制将解决方案转化为新的符号产生式规则。

Result: 在三个公共数据集上的广泛评估表明，CogRec在推荐准确性、可解释性以及解决长尾问题方面表现出显著优势。

Conclusion: CogRec成功结合了LLM和Soar认知架构的优势，实现了可解释的推荐和强大的在线学习能力，解决了传统LLM推荐系统的黑盒特性和知识获取难题。

Abstract: Large Language Models (LLMs) have demonstrated a remarkable capacity in understanding user preferences for recommendation systems. However, they are constrained by several critical challenges, including their inherent "Black-Box" characteristics, susceptibility to knowledge hallucination, and limited online learning capacity. These factors compromise their trustworthiness and adaptability. Conversely, cognitive architectures such as Soar offer structured and interpretable reasoning processes, yet their knowledge acquisition is notoriously laborious. To address these complementary challenges, we propose a novel cognitive recommender agent called CogRec which synergizes the strengths of LLMs with the Soar cognitive architecture. CogRec leverages Soar as its core symbolic reasoning engine and leverages an LLM for knowledge initialization to populate its working memory with production rules. The agent operates on a Perception-Cognition-Action(PCA) cycle. Upon encountering an impasse, it dynamically queries the LLM to obtain a reasoned solution. This solution is subsequently transformed into a new symbolic production rule via Soar's chunking mechanism, thereby enabling robust online learning. This learning paradigm allows the agent to continuously evolve its knowledge base and furnish highly interpretable rationales for its recommendations. Extensive evaluations conducted on three public datasets demonstrate that CogRec demonstrates significant advantages in recommendation accuracy, explainability, and its efficacy in addressing the long-tail problem.

</details>


### [8] [Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks](https://arxiv.org/abs/2512.24156)
*Evgenii Rudakov,Jonathan Shock,Benjamin Ultan Cowley*

Main category: cs.AI

TL;DR: 提出了一种无需训练、基于图的方法来解决ARC-AGI-3基准中的交互式推理任务，该方法通过视觉处理和系统化状态空间探索，在稀疏反馈环境中显著优于前沿LLM


<details>
  <summary>Details</summary>
Motivation: ARC-AGI-3基准包含类游戏任务，需要智能体通过有限交互推断任务机制并适应递增复杂度。当前最先进的LLM无法可靠解决这些任务，因此需要探索新的方法

Method: 结合基于视觉的帧处理与系统化状态空间探索，使用图结构表示。方法包括：将视觉帧分割为有意义的组件、基于视觉显著性优先化动作、维护已探索状态和转移的有向图，通过跟踪访问状态和测试动作，优先选择到未测试状态-动作对的最短路径

Result: 在ARC-AGI-3预览挑战中，该方法解决了52个关卡中位数为30个，在私有排行榜上排名第3，显著优于前沿LLM智能体

Conclusion: 即使无需学习，显式的图结构探索也可以作为交互式推理的强大基线，强调了在稀疏反馈环境中系统化状态跟踪和动作优先化的重要性，这些是当前LLM未能捕捉任务动态的领域

Abstract: We present a training-free graph-based approach for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. ARC-AGI-3 comprises game-like tasks where agents must infer task mechanics through limited interactions, and adapt to increasing complexity as levels progress. Success requires forming hypotheses, testing them, and tracking discovered mechanics. The benchmark has revealed that state-of-the-art LLMs are currently incapable of reliably solving these tasks. Our method combines vision-based frame processing with systematic state-space exploration using graph-structured representations. It segments visual frames into meaningful components, prioritizes actions based on visual salience, and maintains a directed graph of explored states and transitions. By tracking visited states and tested actions, the agent prioritizes actions that provide the shortest path to untested state-action pairs. On the ARC-AGI-3 Preview Challenge, this structured exploration strategy solves a median of 30 out of 52 levels across six games and ranks 3rd on the private leaderboard, substantially outperforming frontier LLM-based agents. These results demonstrate that explicit graph-structured exploration, even without learning, can serve as a strong baseline for interactive reasoning and underscore the importance of systematic state tracking and action prioritization in sparse-feedback environments where current LLMs fail to capture task dynamics. The code is open source and available at https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore.

</details>


### [9] [SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents](https://arxiv.org/abs/2512.24189)
*Yankai Jiang,Wenjie Lou,Lilong Wang,Zhenyu Tang,Shiyang Feng,Jiaxuan Lu,Haoran Sun,Yaning Pan,Shuang Gu,Haoyang Su,Feng Liu,Wangxu Wei,Pan Tan,Dongzhan Zhou,Fenghua Ling,Cheng Tan,Bo Zhang,Xiaosong Wang,Lei Bai,Bowen Zhou*

Main category: cs.AI

TL;DR: SCP（科学上下文协议）是一个开源标准，旨在通过构建自主科学代理的全球网络来加速科学发现。它提供统一的资源集成规范和实验生命周期管理架构，支持大规模异构AI系统与人类研究者的安全协作。


<details>
  <summary>Details</summary>
Motivation: 当前科学研究面临资源分散、平台异构、集成成本高、可重复性差等问题。不同研究机构和平台之间的工具、模型、数据集和物理仪器难以无缝集成和协作，阻碍了科学发现的效率和规模。

Method: SCP基于两大支柱：1）统一资源集成：提供描述和调用科学资源的通用规范，涵盖软件工具、模型、数据集和物理仪器；2）编排的实验生命周期管理：包含中心化SCP Hub和联邦式SCP Server的安全服务架构，管理实验注册、规划、执行、监控和归档的完整生命周期。

Result: 基于SCP构建的科学发现平台已集成超过1600个工具资源。在不同用例中，SCP促进了异构AI系统与人类研究者之间的大规模安全协作，显著降低了集成开销并增强了可重复性。

Conclusion: 通过在协议层面标准化科学上下文和工具编排，SCP为可扩展、多机构、代理驱动的科学研究建立了必要的基础设施，能够加速科学发现进程。

Abstract: We introduce SCP: the Science Context Protocol, an open-source standard designed to accelerate discovery by enabling a global network of autonomous scientific agents. SCP is built on two foundational pillars: (1) Unified Resource Integration: At its core, SCP provides a universal specification for describing and invoking scientific resources, spanning software tools, models, datasets, and physical instruments. This protocol-level standardization enables AI agents and applications to discover, call, and compose capabilities seamlessly across disparate platforms and institutional boundaries. (2) Orchestrated Experiment Lifecycle Management: SCP complements the protocol with a secure service architecture, which comprises a centralized SCP Hub and federated SCP Servers. This architecture manages the complete experiment lifecycle (registration, planning, execution, monitoring, and archival), enforces fine-grained authentication and authorization, and orchestrates traceable, end-to-end workflows that bridge computational and physical laboratories. Based on SCP, we have constructed a scientific discovery platform that offers researchers and agents a large-scale ecosystem of more than 1,600 tool resources. Across diverse use cases, SCP facilitates secure, large-scale collaboration between heterogeneous AI systems and human researchers while significantly reducing integration overhead and enhancing reproducibility. By standardizing scientific context and tool orchestration at the protocol level, SCP establishes essential infrastructure for scalable, multi-institution, agent-driven science.

</details>


### [10] [Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem](https://arxiv.org/abs/2512.24251)
*Pengfu Wan,Jiawei Chen,Gangyan Xu*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度强化学习的方法来解决车队规模与混合车辆路径问题，能够在几秒内生成接近最优解，特别适用于大规模和时间受限的场景。


<details>
  <summary>Details</summary>
Motivation: FSMVRP问题需要同时考虑车队组成和路径规划，在实际应用中非常重要（如短期车辆租赁和按需物流），但其复杂性给大规模和时间受限环境带来了显著挑战。

Method: 将问题建模为马尔可夫决策过程，开发了名为FRIPN的新型策略网络，整合车队组成和路径决策，采用专门设计的输入嵌入（包括剩余图嵌入）来支持有效的车辆使用决策。

Result: 在随机生成实例和基准数据集上的实验表明，该方法在计算效率和可扩展性方面具有显著优势，特别是在大规模和时间受限的场景中。

Conclusion: 该方法在实际应用中具有潜力，并为将基于DRL的技术扩展到其他VRP变体提供了有价值的启发。

Abstract: The Fleet Size and Mix Vehicle Routing Problem (FSMVRP) is a prominent variant of the Vehicle Routing Problem (VRP), extensively studied in operations research and computational science. FSMVRP requires simultaneous decisions on fleet composition and routing, making it highly applicable to real-world scenarios such as short-term vehicle rental and on-demand logistics. However, these requirements also increase the complexity of FSMVRP, posing significant challenges, particularly in large-scale and time-constrained environments. In this paper, we propose a deep reinforcement learning (DRL)-based approach for solving FSMVRP, capable of generating near-optimal solutions within a few seconds. Specifically, we formulate the problem as a Markov Decision Process (MDP) and develop a novel policy network, termed FRIPN, that seamlessly integrates fleet composition and routing decisions. Our method incorporates specialized input embeddings designed for distinctdecision objectives, including a remaining graph embedding to facilitate effective vehicle employment decisions. Comprehensive experiments are conducted on both randomly generated instances and benchmark datasets. The experimental results demonstrate that our method exhibits notable advantages in terms of computational efficiency and scalability, particularly in large-scale and time-constrained scenarios. These strengths highlight the potential of our approach for practical applications and provide valuable inspiration for extending DRL-based techniques to other variants of VRP.

</details>


### [11] [Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment](https://arxiv.org/abs/2512.24263)
*Lijun Zhang,Lin Li,Wei Wei,Yajie Qi,Huizhong Song,Jun Wang,Yaodong Yang,Jiye Liang*

Main category: cs.AI

TL;DR: 本文提出了一种风险感知的逐步对齐方法（RSA），通过引入嵌套风险度量来增强语言模型安全对齐过程中的风险控制能力，有效抑制低概率高危害行为。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法（如Safe RLHF和SACPO）通常采用风险中性范式，无法充分应对参考策略偏离带来的风险，且对罕见但可能灾难性的有害行为缺乏鲁棒性。需要一种能够明确纳入风险意识的对齐方法。

Method: 提出风险感知逐步对齐（RSA）方法：1）将安全对齐建模为令牌级风险感知约束策略优化问题；2）通过逐步对齐过程解决该问题，利用嵌套风险度量推导令牌级策略更新；3）设计旨在缓解模型过度偏离参考策略的风险，并明确抑制低概率高危害行为。

Result: 实验结果表明，该方法在保持高帮助性的同时确保了强大的安全性，并显著抑制了尾部风险（即低概率但高影响的不安全响应）。理论分析在温和假设下证明了策略的最优性。

Conclusion: RSA方法通过将风险意识明确纳入策略优化过程，提供了一种更安全、更稳健的语言模型对齐框架，能够有效控制模型偏离风险和罕见有害行为，为安全可信的语言模型部署提供了新思路。

Abstract: When fine-tuning pre-trained Language Models (LMs) to exhibit desired behaviors, maintaining control over risk is critical for ensuring both safety and trustworthiness. Most existing safety alignment methods, such as Safe RLHF and SACPO, typically operate under a risk-neutral paradigm that is insufficient to address the risks arising from deviations from the reference policy and offers limited robustness against rare but potentially catastrophic harmful behaviors. To address this limitation, we propose Risk-aware Stepwise Alignment (RSA), a novel alignment method that explicitly incorporates risk awareness into the policy optimization process by leveraging a class of nested risk measures. Specifically, RSA formulates safety alignment as a token-level risk-aware constrained policy optimization problem and solves it through a stepwise alignment procedure that yields token-level policy updates derived from the nested risk measures. This design offers two key benefits: (1) it mitigates risks induced by excessive model shift away from a reference policy, and (2) it explicitly suppresses low-probability yet high-impact harmful behaviors. Moreover, we provide theoretical analysis on policy optimality under mild assumptions. Experimental results demonstrate that our method achieves high levels of helpfulness while ensuring strong safety and significantly suppresses tail risks, namely low-probability yet high-impact unsafe responses.

</details>


### [12] [Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents](https://arxiv.org/abs/2512.24461)
*Seohui Bae,Jeonghye Kim,Youngchul Sung,Woohyung Lim*

Main category: cs.AI

TL;DR: 提出一种无需梯度更新或额外训练、通过后验引导信念精化的测试时自适应智能体，在部分可观测环境下通过最大化信念空间信息增益来选择动作，显著降低集成开销并优于现有推理时扩展方法。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境下，LLM智能体需要准确对齐潜在世界状态，但现有方法依赖梯度更新或额外训练，集成开销大。需要一种轻量级、无需额外训练的方法来提升智能体在部分可观测环境中的推理能力。

Method: 1) 维护外部结构化环境状态信念；2) 通过动作条件观测迭代更新信念；3) 使用轻量级LLM代理估计信息增益；4) 通过最大化信念空间信息增益选择动作；5) 引入量化后验信念与真实环境配置一致性的新颖奖励机制。

Result: 实验表明，该方法在潜在世界状态对齐方面优于推理时扩展基线（如提示增强或检索增强LLM），且集成开销显著降低。

Conclusion: 提出的测试时自适应智能体通过后验引导信念精化，无需梯度更新或额外训练，在部分可观测环境中有效提升LLM智能体的推理能力，为轻量级自适应智能体设计提供了新思路。

Abstract: In this paper, we propose a test-time adaptive agent that performs exploratory inference through posterior-guided belief refinement without relying on gradient-based updates or additional training for LLM agent operating under partial observability. Our agent maintains an external structured belief over the environment state, iteratively updates it via action-conditioned observations, and selects actions by maximizing predicted information gain over the belief space. We estimate information gain using a lightweight LLM-based surrogate and assess world alignment through a novel reward that quantifies the consistency between posterior belief and ground-truth environment configuration. Experiments show that our method outperforms inference-time scaling baselines such as prompt-augmented or retrieval-enhanced LLMs, in aligning with latent world states with significantly lower integration overhead.

</details>


### [13] [What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?](https://arxiv.org/abs/2512.24497)
*Basile Terver,Tsung-Yen Yang,Jean Ponce,Adrien Bardes,Yann LeCun*

Main category: cs.AI

TL;DR: 该研究系统分析了基于世界模型表示空间规划的JEPA-WM方法，通过实验确定了最优架构、训练目标和规划算法组合，在导航和操作任务上超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: AI领域长期面临开发能解决广泛物理任务并泛化到新任务的智能体的挑战。现有方法通常在输入空间进行规划，而基于世界模型表示空间规划的方法有望通过抽象无关细节实现更高效的规划，但这类方法的技术选择尚未得到系统研究。

Method: 将这类方法定义为JEPA-WM，系统研究模型架构、训练目标和规划算法等关键组件。在模拟环境和真实机器人数据上进行实验，分析各组件对规划成功率的影响，最终提出最优组合方案。

Result: 提出的模型在导航和操作任务上超越了DINO-WM和V-JEPA-2-AC两个现有基线。研究提供了代码、数据和检查点供公开使用。

Conclusion: 通过系统分析JEPA-WM方法的关键技术选择，确定了最优的架构、训练和规划组合，为基于表示空间规划的世界模型方法提供了实证指导，推动了高效物理任务解决智能体的发展。

Abstract: A long-standing challenge in AI is to develop agents capable of solving a wide range of physical tasks and generalizing to new, unseen tasks and environments. A popular recent approach involves training a world model from state-action trajectories and subsequently use it with a planning algorithm to solve new tasks. Planning is commonly performed in the input space, but a recent family of methods has introduced planning algorithms that optimize in the learned representation space of the world model, with the promise that abstracting irrelevant details yields more efficient planning. In this work, we characterize models from this family as JEPA-WMs and investigate the technical choices that make algorithms from this class work. We propose a comprehensive study of several key components with the objective of finding the optimal approach within the family. We conducted experiments using both simulated environments and real-world robotic data, and studied how the model architecture, the training objective, and the planning algorithm affect planning success. We combine our findings to propose a model that outperforms two established baselines, DINO-WM and V-JEPA-2-AC, in both navigation and manipulation tasks. Code, data and checkpoints are available at https://github.com/facebookresearch/jepa-wms.

</details>


### [14] [Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments](https://arxiv.org/abs/2512.24504)
*Zhiwei Wei,Yuxing Liu,Hua Liao,Wenjia Xu*

Main category: cs.AI

TL;DR: 该研究提出了一个交互式评估框架，用于分析基础模型代理在符号地图环境中的空间理解能力，包括探索、记忆和推理三个方面，揭示了不同组件在空间理解中的功能角色。


<details>
  <summary>Details</summary>
Motivation: 现有对基础模型空间能力的评估大多依赖静态地图输入或文本查询，忽视了空间理解的交互性和经验驱动特性。需要一种更全面的评估方法来理解FM代理如何在动态地图环境中探索、记忆和推理。

Method: 提出了一个交互式评估框架，让FM代理在部分可观察的网格地图中增量探索（包含道路、交叉口和兴趣点），每步只接收局部观察。通过六种空间任务评估空间理解能力，并系统性地改变探索策略、记忆表示和推理方案。

Result: 探索主要影响经验获取但对最终推理准确性影响有限；记忆表示在整合空间经验中起核心作用，结构化记忆（特别是序列和图表示）显著提升路径规划等结构密集型任务性能；推理方案影响存储空间知识的使用方式，高级提示支持更有效的多步推理；空间推理性能在模型版本和规模超过一定阈值后趋于饱和。

Conclusion: 提升基于地图的空间理解能力需要专门针对空间表示和推理的机制，而不仅仅是模型规模扩展。结构化记忆表示和高级推理方案对空间任务性能有显著影响，探索策略的影响相对有限。

Abstract: Map environments provide a fundamental medium for representing spatial structure. Understanding how foundation model (FM) agents understand and act in such environments is therefore critical for enabling reliable map-based reasoning and applications. However, most existing evaluations of spatial ability in FMs rely on static map inputs or text-based queries, overlooking the interactive and experience-driven nature of spatial understanding.In this paper, we propose an interactive evaluation framework to analyze how FM agents explore, remember, and reason in symbolic map environments. Agents incrementally explore partially observable grid-based maps consisting of roads, intersections, and points of interest (POIs), receiving only local observations at each step. Spatial understanding is then evaluated using six kinds of spatial tasks. By systematically varying exploration strategies, memory representations, and reasoning schemes across multiple foundation models, we reveal distinct functional roles of these components. Exploration primarily affects experience acquisition but has a limited impact on final reasoning accuracy. In contrast, memory representation plays a central role in consolidating spatial experience, with structured memories particularly sequential and graph-based representations, substantially improving performance on structure-intensive tasks such as path planning. Reasoning schemes further shape how stored spatial knowledge is used, with advanced prompts supporting more effective multi-step inference. We further observe that spatial reasoning performance saturates across model versions and scales beyond a certain capability threshold, indicating that improvements in map-based spatial understanding require mechanisms tailored to spatial representation and reasoning rather than scaling alone.

</details>


### [15] [Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems](https://arxiv.org/abs/2512.24505)
*Samuel Golladay,Majid Bani-Yaghoub*

Main category: cs.AI

TL;DR: 本研究分析了三个主流大语言模型（GPT-4o-mini、Gemini-2.0-Flash、DeepSeek-V3）在密苏里大学数学竞赛问题上的表现，发现DeepSeek-V3在所有数学类别中表现最佳，但所有模型在几何问题上都表现较弱。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多使用相同的数据集评估大语言模型的数学推理能力，这限制了研究结果的普适性，可能无法完全捕捉数学任务中的多样化挑战。本研究旨在通过分析大语言模型在代表性不足的数学竞赛问题上的表现，更全面地评估其数学推理能力。

Method: 研究使用密苏里大学数学竞赛中的微积分、解析几何和离散数学问题，对GPT-4o-mini、Gemini-2.0-Flash和DeepSeek-V3三个大语言模型进行测试。将模型的回答与已知正确答案进行比较，确定每个问题领域的准确性，并分析模型的推理过程以探索不同问题类型和模型之间的错误模式。

Result: DeepSeek-V3在微积分、解析几何和离散数学三个类别中表现最佳，无论是推理过程还是最终答案的正确性。所有三个大语言模型在几何问题上都表现出明显的薄弱。DeepSeek-V3的主要错误是计算和逻辑错误，GPT-4o-mini经常出现逻辑和方法相关的错误，而Gemini则倾向于推理不完整和草率得出结论。

Conclusion: 在代表性不足的数学竞赛数据集上评估大语言模型，可以更深入地了解它们独特的错误模式，并突出结构化推理方面的持续挑战，特别是在几何领域。这表明需要更多样化的评估基准来全面评估大语言模型的数学推理能力。

Abstract: Understanding the limitations of Large Language Models, or LLMs, in mathematical reasoning has been the focus of several recent studies. However, the majority of these studies use the same datasets for benchmarking, which limits the generalizability of their findings and may not fully capture the diverse challenges present in mathematical tasks. The purpose of the present study is to analyze the performance of LLMs on underrepresented mathematics competition problems. We prompted three leading LLMs, namely GPT-4o-mini, Gemini-2.0-Flash, and DeepSeek-V3, with the Missouri Collegiate Mathematics Competition problems in the areas of Calculus, Analytic Geometry, and Discrete Mathematics. The LLMs responses were then compared to the known correct solutions in order to determine the accuracy of the LLM for each problem domain. We also analyzed the LLMs reasoning to explore patterns in errors across problem types and models. DeepSeek-V3 has the best performance in all three categories of Calculus, Analytic Geometry, and Discrete Mathematics, both in reasoning and correct final answers. All three LLMs exhibited notably weak performance in Geometry. The majority of errors made by DeepSeek-V3 were attributed to computational and logical mistakes, whereas GPT-4o-mini frequently exhibited logical and approach-related errors. Gemini, on the other hand, tended to struggle with incomplete reasoning and drawing rushed conclusions. In conclusion, evaluating LLMs on underrepresented mathematics competition datasets can provide deeper insights into their distinct error patterns and highlight ongoing challenges in structured reasoning, particularly within the domain of Geometry.

</details>


### [16] [From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning](https://arxiv.org/abs/2512.24532)
*Amir Tahmasbi,Sadegh Majidi,Kazem Taram,Aniket Bera*

Main category: cs.AI

TL;DR: 论文提出两阶段方法提升大语言模型的空间推理能力：先通过监督微调学习基本空间变换（旋转、平移、缩放），再使用GRPO框架训练LoRA适配器进行多步规划，在ASCII艺术环境中验证效果优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用语言能力上表现强大，但在结构化环境中的空间变换和多步规划方面仍然存在困难，这限制了其在导航和规划等应用中的表现。

Method: 采用两阶段方法：1）对基本空间变换（旋转、平移、缩放）进行监督微调，使模型具备基础空间物理知识；2）冻结该物理感知模型，在GRPO框架内训练轻量级LoRA适配器，学习将这些构建块组合用于谜题环境中的多步规划。为此合成了ASCII艺术数据集并构建相应的强化学习环境。

Result: 该方法在动态环境（有显式状态更新）和静态环境（模型必须依赖内部状态）中均一致优于基线模型，包括通用骨干模型、物理感知模型和端到端RL模型。此外，该方法收敛更快，训练更稳定，注意力模式分析显示微调确实改善了空间理解能力。

Conclusion: 通过将空间推理分解为原子构建块及其组合的两阶段方法，可以有效提升大语言模型在结构化环境中的空间推理能力，为导航和规划应用提供了有前景的解决方案。

Abstract: Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.

</details>


### [17] [MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use](https://arxiv.org/abs/2512.24565)
*Wenrui Liu,Zixiang Liu,Elsie Dai,Wenhan Yu,Lei Yu,Tong Yang*

Main category: cs.AI

TL;DR: MCPAgentBench：基于真实MCP定义构建的基准测试，用于评估LLM智能体的工具使用能力，包含真实任务和模拟工具，采用动态沙盒环境和综合指标。


<details>
  <summary>Details</summary>
Motivation: 当前MCP评估集存在依赖外部MCP服务和缺乏难度感知的问题，需要构建更全面的基准来评估LLM智能体的工具使用能力。

Method: 构建包含真实任务和模拟MCP工具的数据集，采用动态沙盒环境，向智能体提供包含干扰项的工具候选列表，测试其工具选择和辨别能力。

Result: 在多种最新主流大语言模型上的实验显示，在处理复杂多步骤工具调用时存在显著性能差异。

Conclusion: MCPAgentBench为评估LLM智能体的工具使用能力提供了有效的基准测试框架，所有代码已在GitHub开源。

Abstract: Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.

</details>


### [18] [Recursive Language Models](https://arxiv.org/abs/2512.24601)
*Alex L. Zhang,Tim Kraska,Omar Khattab*

Main category: cs.AI

TL;DR: 本文提出递归语言模型（RLMs），通过让LLM将长提示视为外部环境，以编程方式检查、分解并递归调用自身处理提示片段，从而突破模型上下文窗口限制，处理长至两个数量级的输入。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型受限于固定的上下文窗口长度，无法处理任意长度的提示。需要一种方法让LLM能够处理超出其原始上下文窗口的长输入，同时保持高质量的输出。

Method: 提出递归语言模型（RLMs）推理策略：将长提示视为外部环境，让LLM以编程方式检查、分解提示，并递归调用自身处理提示片段。这种方法允许模型通过多次调用逐步处理超长输入。

Result: RLMs能够成功处理超出模型上下文窗口两个数量级的输入。即使在较短的提示上，RLMs在四个不同的长上下文任务中也显著优于基础LLM和常见的长上下文框架，同时每个查询的成本相当或更低。

Conclusion: 递归语言模型提供了一种有效的推理时扩展策略，使LLM能够处理任意长度的提示，在保持成本效益的同时显著提升长上下文任务的处理质量。

Abstract: We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.

</details>


### [19] [Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization](https://arxiv.org/abs/2512.24609)
*Dong Qiu,Duo Xu,Limengxi Yue*

Main category: cs.AI

TL;DR: 提出强化学习增强的LLM多智能体协作框架，采用集中训练分散执行机制，在协作写作和编程任务中显著提升效率和一致性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单智能体任务中表现良好，但在多智能体协作场景中缺乏协作意识，难以优化全局性能，需要专门的协作框架

Method: 将协作建模为分散部分可观测马尔可夫决策过程，采用集中训练分散执行机制，引入群体相对策略优化算法，使用平衡任务质量、速度和协调成本的简化联合奖励函数

Result: 在协作写作和编程基准测试中，任务处理速度比单智能体基线提高3倍，写作结构/风格一致性达98.7%，编程测试通过率达74.6%，优于现有多智能体LLM基线

Conclusion: 该框架为复杂工作流中的可靠协作提供了实用路径，显著提升了多智能体LLM的协作效率和一致性

Abstract: Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.

</details>


### [20] [Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning](https://arxiv.org/abs/2512.24613)
*Zheyu Shi,Dong Qiu,Shanlong Yu*

Main category: cs.AI

TL;DR: 提出基于群体审议的多智能体对话模型，通过三层角色架构提升复杂推理任务的准确性和一致性


<details>
  <summary>Details</summary>
Motivation: 单个大型语言模型在复杂推理任务中存在局限性，需要多智能体协作来提升推理能力和事实一致性

Method: 采用三层角色架构：生成、验证和整合；引入自博弈机制扩展多路径推理轨迹；检索增强模块动态补充外部知识；设计结合事实一致性和逻辑连贯性的复合奖励函数；应用改进的近端策略优化策略进行协作训练

Result: 在HotpotQA上多跳推理准确率提升16.8%，2WikiMultihopQA提升14.3%，MeetingBank提升19.2%；一致性提升21.5%；推理效率优于主流多智能体方法

Conclusion: 该模型为复杂推理任务提供了有效且稳定的解决方案，通过多智能体协作显著提升了推理准确性和一致性

Abstract: This paper proposes a group deliberation oriented multi-agent conversational model to address the limitations of single large language models in complex reasoning tasks. The model adopts a three-level role division architecture consisting of generation, verification, and integration. An opinion generation agent produces diverse reasoning perspectives, an evidence verification agent retrieves external knowledge and quantifies factual support, and a consistency arbitration agent integrates logically coherent conclusions. A self-game mechanism is introduced to expand multi-path reasoning trajectories, while a retrieval enhancement module dynamically supplements external knowledge. A composite reward function combining factual consistency and logical coherence is designed, and an improved proximal policy optimization strategy is applied for collaborative training. Experimental results show that the proposed model improves multi-hop reasoning accuracy by 16.8 percent on HotpotQA, 14.3 percent on 2WikiMultihopQA, and 19.2 percent on MeetingBank, while improving consistency by 21.5 percent. The model achieves higher reasoning efficiency than mainstream multi-agent approaches, providing an effective and stable solution for complex reasoning tasks.

</details>


### [21] [Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization](https://arxiv.org/abs/2512.24615)
*Yuchen Shi,Yuzheng Cai,Siqi Cai,Zihan Xu,Lichao Chen,Yulei Qin,Zhijian Zhou,Xiang Fei,Chaofan Qiu,Xiaoyu Tan,Gang Li,Zongyi Li,Haojia Lin,Guocan Cai,Yong Mao,Yunsheng Wu,Ke Li,Xing Sun*

Main category: cs.AI

TL;DR: Youtu-Agent是一个模块化LLM代理框架，通过自动化生成和持续进化解决现有框架配置成本高、能力静态的问题，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理框架面临两大挑战：1）高配置成本，构建高质量代理需要大量手动工具集成和提示工程；2）静态能力，已部署代理难以适应动态环境，需要昂贵的微调。

Method: 提出模块化框架Youtu-Agent，包含：1）结构化配置系统，解耦执行环境、工具包和上下文管理；2）两种生成范式：Workflow模式用于标准任务，Meta-Agent模式用于复杂需求；3）混合策略优化系统：Agent Practice模块通过上下文优化积累经验，Agent RL模块集成分布式训练框架进行端到端强化学习。

Result: 在WebWalkerQA（71.47%）和GAIA（72.8%）上达到SOTA性能；自动化生成管道工具合成成功率超过81%；Practice模块在AIME 2024/2025上分别提升+2.7%和+5.4%；Agent RL训练在7B LLMs上实现40%加速，在数学和通用/多跳QA基准上分别提升35%和21%。

Conclusion: Youtu-Agent通过自动化生成和持续进化机制有效解决了LLM代理框架的高配置成本和静态能力问题，为构建自适应、可扩展的智能代理提供了系统化解决方案。

Abstract: Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \textbf{Workflow} mode for standard tasks and a \textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\%) and GAIA (72.8\%) using open-weight models. Our automated generation pipeline achieves over 81\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\% and +5.4\% respectively. Moreover, our Agent RL training achieves 40\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\% and 21\% on Maths and general/multi-hop QA benchmarks.

</details>


### [22] [Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions](https://arxiv.org/abs/2512.24679)
*Pengcheng Xia,Yixiang Huang,Chengjin Qin,Chengliang Liu*

Main category: cs.AI

TL;DR: 提出多模态跨域混合融合模型，通过双重解耦框架分离模态不变/特定特征和域不变/特定表示，结合跨域混合融合策略和三模态融合机制，提升电机故障诊断在未见工况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有智能故障诊断方法在真实场景中面临性能下降问题：1) 模型在未见工况下泛化能力差；2) 域适应方法依赖目标域样本；3) 多数研究依赖单模态信号，忽略了多模态信息的互补性。

Method: 1) 双重解耦框架：分离模态不变特征和模态特定特征，以及域不变表示和域特定表示；2) 跨域混合融合策略：随机混合跨域的模态信息以增强模态和域多样性；3) 三模态融合机制：自适应集成多模态异构信息。

Result: 在感应电机故障诊断任务中，针对未见恒定工况和时变工况进行了大量实验。结果表明，该方法始终优于先进方法，全面的消融研究进一步验证了每个提出组件和多模态融合的有效性。

Conclusion: 提出的多模态跨域混合融合模型通过双重解耦和跨域融合策略，有效解决了现有故障诊断方法在未见工况下的泛化问题，充分利用了多模态信息的互补性，显著提升了诊断性能。

Abstract: Intelligent fault diagnosis has become an indispensable technique for ensuring machinery reliability. However, existing methods suffer significant performance decline in real-world scenarios where models are tested under unseen working conditions, while domain adaptation approaches are limited to their reliance on target domain samples. Moreover, most existing studies rely on single-modal sensing signals, overlooking the complementary nature of multi-modal information for improving model generalization. To address these limitations, this paper proposes a multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis. A dual disentanglement framework is developed to decouple modality-invariant and modality-specific features, as well as domain-invariant and domain-specific representations, enabling both comprehensive multi-modal representation learning and robust domain generalization. A cross-domain mixed fusion strategy is designed to randomly mix modality information across domains for modality and domain diversity augmentation. Furthermore, a triple-modal fusion mechanism is introduced to adaptively integrate multi-modal heterogeneous information. Extensive experiments are conducted on induction motor fault diagnosis under both unseen constant and time-varying working conditions. The results demonstrate that the proposed method consistently outperforms advanced methods and comprehensive ablation studies further verify the effectiveness of each proposed component and multi-modal fusion. The code is available at: https://github.com/xiapc1996/MMDG.

</details>


### [23] [BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis](https://arxiv.org/abs/2512.24686)
*Songqi Zhou,Ruixue Liu,Boman Su,Jiazhou Wang,Yixing Wang,Benben Jiang*

Main category: cs.AI

TL;DR: 本文提出BatteryAgent框架，将物理知识特征与大语言模型推理能力结合，实现锂离子电池故障的智能诊断，显著提升检测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法的"黑盒"特性限制了可解释性，且受限于二元分类范式，难以提供根本原因分析和维护建议，需要解决这些局限性。

Method: 提出三层框架：1) 物理感知层使用10个基于电化学原理的机制特征；2) 检测与归因层采用梯度提升决策树和SHAP量化特征贡献；3) 推理与诊断层利用LLM作为智能体核心，构建"数值-语义"桥梁。

Result: 实验结果显示BatteryAgent有效纠正硬边界样本的误分类，AUROC达到0.986，显著优于当前最先进方法，并将传统二元检测扩展到多类型可解释诊断。

Conclusion: 该框架为电池安全管理提供了从"被动检测"到"智能诊断"的新范式转变，实现了故障类型、根本原因分析和维护建议的全面诊断报告生成。

Abstract: Fault diagnosis of lithium-ion batteries is critical for system safety. While existing deep learning methods exhibit superior detection accuracy, their "black-box" nature hinders interpretability. Furthermore, restricted by binary classification paradigms, they struggle to provide root cause analysis and maintenance recommendations. To address these limitations, this paper proposes BatteryAgent, a hierarchical framework that integrates physical knowledge features with the reasoning capabilities of Large Language Models (LLMs). The framework comprises three core modules: (1) A Physical Perception Layer that utilizes 10 mechanism-based features derived from electrochemical principles, balancing dimensionality reduction with physical fidelity; (2) A Detection and Attribution Layer that employs Gradient Boosting Decision Trees and SHAP to quantify feature contributions; and (3) A Reasoning and Diagnosis Layer that leverages an LLM as the agent core. This layer constructs a "numerical-semantic" bridge, combining SHAP attributions with a mechanism knowledge base to generate comprehensive reports containing fault types, root cause analysis, and maintenance suggestions. Experimental results demonstrate that BatteryAgent effectively corrects misclassifications on hard boundary samples, achieving an AUROC of 0.986, which significantly outperforms current state-of-the-art methods. Moreover, the framework extends traditional binary detection to multi-type interpretable diagnosis, offering a new paradigm shift from "passive detection" to "intelligent diagnosis" for battery safety management.

</details>


### [24] [Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences](https://arxiv.org/abs/2512.24829)
*Emmanuel Fashae,Michael Burke,Leimin Tian,Lingheng Meng,Pamela Carreno-Medrano*

Main category: cs.AI

TL;DR: 该研究提出了一个可解释的物体排列偏好框架，包含四个明确维度，并通过问卷验证其心理区分度，最后将其集成到MCTS规划器中生成符合人类偏好的排列方案。


<details>
  <summary>Details</summary>
Motivation: 当前基于隐式偏好模型的机器人系统虽然能预测人类偏好，但缺乏对指导人类决策的可解释因素的理解。研究旨在开发一个明确、可解释的物体排列偏好框架。

Method: 1. 提出四个可解释的偏好构建维度：空间实用性、习惯便利性、语义连贯性、常识适当性；2. 设计并验证自报告问卷（63名参与者在线研究）；3. 将偏好构建集成到蒙特卡洛树搜索规划器中。

Result: 1. 问卷研究证实了四个构建维度的心理区分度；2. 这些维度在两个场景（厨房和客厅）中具有解释力；3. 基于参与者偏好的MCTS规划器能生成与参与者生成排列高度一致的合理排列方案。

Conclusion: 该研究贡献了一个紧凑、可解释的物体排列偏好公式化框架，并展示了如何将其操作化用于机器人规划，为理解人类物体排列决策提供了新的可解释视角。

Abstract: Robotic systems for household object rearrangement often rely on latent preference models inferred from human demonstrations. While effective at prediction, these models offer limited insight into the interpretable factors that guide human decisions. We introduce an explicit formulation of object arrangement preferences along four interpretable constructs: spatial practicality (putting items where they naturally fit best in the space), habitual convenience (making frequently used items easy to reach), semantic coherence (placing items together if they are used for the same task or are contextually related), and commonsense appropriateness (putting things where people would usually expect to find them). To capture these constructs, we designed and validated a self-report questionnaire through a 63-participant online study. Results confirm the psychological distinctiveness of these constructs and their explanatory power across two scenarios (kitchen and living room). We demonstrate the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner and show that when guided by participant-derived preferences, our planner can generate reasonable arrangements that closely align with those generated by participants. This work contributes a compact, interpretable formulation of object arrangement preferences and a demonstration of how it can be operationalized for robot planning.

</details>


### [25] [GenZ: Foundational models as latent variable generators within traditional statistical models](https://arxiv.org/abs/2512.24834)
*Marko Jojic,Nebojsa Jojic*

Main category: cs.AI

TL;DR: GenZ是一种混合模型，通过可解释的语义特征连接基础模型和统计建模。该方法通过迭代过程发现语义特征描述，利用统计建模误差对比项目组，而非仅依赖基础模型的领域知识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然拥有广泛的领域知识，但往往无法捕捉对预测任务至关重要的数据集特定模式。现有方法过度依赖基础模型的领域理解，而忽略了数据集特有的统计规律。

Method: 提出一种广义EM算法，联合优化语义特征描述符和统计模型参数。通过迭代过程发现语义特征：基于统计建模误差对比项目组，然后提示冻结的基础模型根据发现的特征对项目进行分类，将这些判断视为潜在二元特征的噪声观测，这些特征通过学习的统计关系预测实值目标。

Result: 在房价预测中，使用多模态列表数据发现的语义特征实现了12%的中位数相对误差，显著优于依赖LLM一般领域知识的GPT-5基线（38%误差）。在Netflix电影嵌入预测中，仅从语义描述就能以0.59余弦相似度预测协同过滤表示，这相当于传统协同过滤需要约4000个用户评分才能达到的性能。

Conclusion: GenZ成功地将基础模型的语义理解与统计建模的数据特定模式相结合，发现了数据集特有的模式（如预测本地房地产市场的建筑细节、预测用户偏好的系列电影成员关系），这些模式与模型单独基于领域知识得出的结论不同。该方法为结合符号推理和统计学习提供了有前景的途径。

Abstract: We present GenZ, a hybrid model that bridges foundational models and statistical modeling through interpretable semantic features. While large language models possess broad domain knowledge, they often fail to capture dataset-specific patterns critical for prediction tasks. Our approach addresses this by discovering semantic feature descriptions through an iterative process that contrasts groups of items identified via statistical modeling errors, rather than relying solely on the foundational model's domain understanding. We formulate this as a generalized EM algorithm that jointly optimizes semantic feature descriptors and statistical model parameters. The method prompts a frozen foundational model to classify items based on discovered features, treating these judgments as noisy observations of latent binary features that predict real-valued targets through learned statistical relationships. We demonstrate the approach on two domains: house price prediction (hedonic regression) and cold-start collaborative filtering for movie recommendations. On house prices, our model achieves 12\% median relative error using discovered semantic features from multimodal listing data, substantially outperforming a GPT-5 baseline (38\% error) that relies on the LLM's general domain knowledge. For Netflix movie embeddings, our model predicts collaborative filtering representations with 0.59 cosine similarity purely from semantic descriptions -- matching the performance that would require approximately 4000 user ratings through traditional collaborative filtering. The discovered features reveal dataset-specific patterns (e.g., architectural details predicting local housing markets, franchise membership predicting user preferences) that diverge from the model's domain knowledge alone.

</details>


### [26] [A study on constraint extraction and exception exclusion in care worker scheduling](https://arxiv.org/abs/2512.24853)
*Koki Suenaga,Tomohiro Furuta,Satoshi Ono*

Main category: cs.AI

TL;DR: 提出一种基于约束模板的方法，用于从养老机构管理者访谈中提取设施特定的排班约束条件，并排除异常约束，从而生成符合实际需求的护工排班表。


<details>
  <summary>Details</summary>
Motivation: 养老机构的排班条件因设施而异，需要通过与制定排班的管理者访谈来设计设施特定的约束条件。现有约束提取技术缺乏排除异常约束的机制。

Method: 使用约束模板提取各种组件的组合，如连续工作日的班次模式或员工组合。模板可通过改变关注的天数和员工数量，以及将提取重点调整为模式或频率来提取多种约束。该方法还包含排除异常约束的机制。

Result: 实验表明，该方法成功创建了满足所有硬约束的排班表，并通过避免提取异常约束，减少了软约束的违反次数。

Conclusion: 提出的约束模板方法能够有效提取养老机构特定的排班约束条件，并排除异常约束，从而生成更符合实际需求的护工排班表。

Abstract: Technologies for automatically generating work schedules have been extensively studied; however, in long-term care facilities, the conditions vary between facilities, making it essential to interview the managers who create shift schedules to design facility-specific constraint conditions. The proposed method utilizes constraint templates to extract combinations of various components, such as shift patterns for consecutive days or staff combinations. The templates can extract a variety of constraints by changing the number of days and the number of staff members to focus on and changing the extraction focus to patterns or frequency. In addition, unlike existing constraint extraction techniques, this study incorporates mechanisms to exclude exceptional constraints. The extracted constraints can be employed by a constraint programming solver to create care worker schedules. Experiments demonstrated that our proposed method successfully created schedules that satisfied all hard constraints and reduced the number of violations for soft constraints by circumventing the extraction of exceptional constraints.

</details>


### [27] [Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem](https://arxiv.org/abs/2512.24873)
*Weixun Wang,XiaoXiao Xu,Wanhe An,Fangwen Dai,Wei Gao,Yancheng He,Ju Huang,Qiang Ji,Hanqi Jin,Xiaoyang Li,Yang Li,Zhongwen Li,Shirong Lin,Jiashun Liu,Zenan Liu,Tao Luo,Dilxat Muhtar,Yuanbin Qu,Jiaqiang Shi,Qinghui Sun,Yingshui Tan,Hao Tang,Runze Wang,Yi Wang,Zhaoguo Wang,Yanan Wu,Shaopan Xiong,Binchen Xu,Xander Xu,Yuchi Xu,Qipeng Zhang,Xixia Zhang,Haizhou Zhao,Jie Zhao,Shuaibing Zhao,Baihui Zheng,Jianhui Zheng,Suhang Zheng,Yanni Zhu,Mengze Cai,Kerui Cao,Xitong Chen,Yue Dai,Lifan Du,Tao Feng,Tao He,Jin Hu,Yijie Hu,Ziyu Jiang,Cheng Li,Xiang Li,Jing Liang,Chonghuan Liu,ZhenDong Liu,Haodong Mi,Yanhu Mo,Junjia Ni,Shixin Pei,Jingyu Shen,XiaoShuai Song,Cecilia Wang,Chaofan Wang,Kangyu Wang,Pei Wang,Tao Wang,Wei Wang,Ke Xiao,Mingyu Xu,Tiange Xu,Nan Ya,Siran Yang,Jianan Ye,Yaxing Zang,Duo Zhang,Junbo Zhang,Boren Zheng,Wanxi Deng,Ling Pan,Lin Qu,Wenbo Su,Jiamang Wang,Wei Wang,Hu Wei,Minggang Wu,Cheng Yu,Bing Zhao,Zhicheng Zheng,Bo Zheng*

Main category: cs.AI

TL;DR: ALE是一个端到端的智能体学习生态系统，包含ROLL权重优化框架、ROCK沙盒环境管理和iFlow CLI上下文工程工具，并发布了基于此的ROME开源智能体模型


<details>
  <summary>Details</summary>
Motivation: 开源社区缺乏一个原则性的端到端生态系统来简化智能体开发，需要优化智能体LLM的生产流程

Method: 1) ALE基础设施包含ROLL（权重优化）、ROCK（轨迹生成沙盒）、iFlow CLI（上下文工程）；2) 数据组合协议合成复杂行为；3) 新颖的基于交互的策略对齐算法（IPA），在语义交互块而非单个token上分配信用

Result: 发布了ROME开源智能体模型，在超过100万条轨迹上训练，在SWE-bench Verified和Terminal Bench等基准测试中表现出色，证明了ALE基础设施的有效性

Conclusion: ALE提供了一个优化的智能体LLM生产管道，ROME的成功证明了该基础设施的有效性，为开源智能体开发提供了系统化解决方案

Abstract: Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.

</details>


### [28] [Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing](https://arxiv.org/abs/2512.24896)
*Andrii Gamalii,Daniel Górniak,Robert Nowak,Bartłomiej Olber,Krystian Radlak,Jakub Winter*

Main category: cs.AI

TL;DR: DARTS项目开发了一个半自动数据标注流水线，用于创建波兰驾驶场景的大规模多模态数据集，通过人机协作降低标注成本和时间。


<details>
  <summary>Details</summary>
Motivation: 手动标注异构驾驶数据成本高、耗时久，需要更高效的标注方法来支持波兰自动驾驶研究。

Method: 采用人在回路方法，结合AI与人工专业知识，包括自动生成初始标注、迭代模型重训练、数据匿名化和领域适应技术，核心使用3D目标检测算法。

Result: 显著节省标注时间，确保跨不同传感器模态的一致高质量标注，加速了DARTS项目标准化格式的大规模标注数据集准备。

Conclusion: 开发的工具和方法成功降低了标注成本和时间，为波兰自动驾驶研究提供了技术基础，支持了DARTS项目的目标实现。

Abstract: This report presents the design and implementation of a semi-automated data annotation pipeline developed within the DARTS project, whose goal is to create a large-scale, multimodal dataset of driving scenarios recorded in Polish conditions. Manual annotation of such heterogeneous data is both costly and time-consuming. To address this challenge, the proposed solution adopts a human-in-the-loop approach that combines artificial intelligence with human expertise to reduce annotation cost and duration. The system automatically generates initial annotations, enables iterative model retraining, and incorporates data anonymization and domain adaptation techniques. At its core, the tool relies on 3D object detection algorithms to produce preliminary annotations. Overall, the developed tools and methodology result in substantial time savings while ensuring consistent, high-quality annotations across different sensor modalities. The solution directly supports the DARTS project by accelerating the preparation of large annotated dataset in the project's standardized format, strengthening the technological base for autonomous vehicle research in Poland.

</details>


### [29] [Iterative Deployment Improves Planning Skills in LLMs](https://arxiv.org/abs/2512.24940)
*Augusto B. Corrêa,Yoav Gelberg,Luckeciano C. Melo,Ilia Shumailov,André G. Pereira,Yarin Gal*

Main category: cs.AI

TL;DR: 迭代部署LLM并通过用户数据筛选微调，能显著改变模型特性，在规划任务中实现能力提升和泛化，这本质上是一种隐式强化学习过程。


<details>
  <summary>Details</summary>
Motivation: 研究迭代部署大型语言模型时，用户从先前模型部署中精心筛选数据用于微调，这种机制如何影响模型特性变化，特别是探索其与强化学习的理论联系。

Method: 在多个规划领域测试迭代部署机制：每个新模型基于用户从前一模型部署中精心筛选的数据进行微调，观察模型规划能力的变化，并进行理论分析。

Result: 迭代部署显著提升了模型的规划技能，后续模型展现出涌现的泛化能力，能够发现比初始模型长得多的规划方案。理论分析表明迭代部署本质上实现了外层强化学习训练。

Conclusion: 迭代部署机制是一种隐式强化学习过程，对AI安全有重要启示（奖励函数未明确定义），也可作为显式强化学习的替代训练方案，依赖数据筛选而非明确奖励。

Abstract: We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.

</details>


### [30] [AMAP Agentic Planning Technical Report](https://arxiv.org/abs/2512.24957)
*Yulan Hu,Xiangwen Zhang,Sheng Ouyang,Hao Yi,Lu Xu,Qinglin Lang,Lide Tan,Xiang Cheng,Tianchen Ye,Zhicong Li,Ge Chen,Wenjin Yang,Zheng Pan,Shaopan Xiong,Siran Yang,Ju Huang,Yan Zhang,Jiamang Wang,Yong Liu,Yinfeng Huang,Tucheng Lin,Xin Li,Ning Guo*

Main category: cs.AI

TL;DR: STAgent是一个专门用于时空理解的智能大语言模型，通过工具交互和分层训练方法，在保持通用能力的同时解决复杂的时空任务。


<details>
  <summary>Details</summary>
Motivation: 开发一个专门针对时空理解的智能体模型，能够处理复杂的时空任务，如受限兴趣点发现和行程规划，同时保持模型的通用能力。

Method: 1. 构建包含10个领域特定工具的稳定工具环境，支持异步训练；2. 分层数据筛选框架，以1:10,000的比例筛选高质量查询数据；3. 级联训练方法：种子SFT阶段作为难度评估器，第二阶段SFT针对高确定性查询，最终RL阶段利用低确定性数据。

Result: STAgent在TravelBench基准测试中表现出色，同时在广泛的通用基准测试中保持了其通用能力，证明了所提出的智能体模型的有效性。

Conclusion: STAgent通过专门的工具环境、高质量数据筛选和级联训练方法，成功创建了一个在时空任务上表现优异且保持通用能力的智能体模型，为复杂时空推理任务提供了有效解决方案。

Abstract: We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.

</details>


### [31] [Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings](https://arxiv.org/abs/2512.25055)
*Tianzhi He,Farrokh Jazizadeh*

Main category: cs.AI

TL;DR: 本文提出了一个基于大语言模型的建筑能源管理系统AI代理框架，通过自然语言交互实现智能建筑的上下文感知能源管理，并在真实数据集上评估了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有能源管理系统存在局限性，需要更智能、上下文感知的解决方案。通过利用LLM的自主数据分析能力，开发能够理解用户自然语言查询并提供智能能源管理的AI代理。

Method: 提出包含感知、中央控制和行动三个模块的框架，形成闭环反馈系统。使用120个用户查询在四个真实住宅能源数据集上评估原型，采用延迟、功能、能力、准确性和成本效益等指标，并通过ANOVA测试验证框架的通用性。

Result: 原型在设备控制（86%）、记忆相关任务（97%）、调度自动化（74%）和能源分析（77%）方面表现出色，但成本估算任务准确率较低（49%）。研究揭示了响应准确性与计算效率之间的权衡。

Conclusion: 该研究为基于LLM的BEMS AI代理的评估提供了框架，展示了其在智能建筑能源管理中的潜力，同时指出了复杂任务（如成本估算）需要改进的方向，为未来研究奠定了基础。

Abstract: This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype's performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [32] [An Comparative Analysis about KYC on a Recommendation System Toward Agentic Recommendation System](https://arxiv.org/abs/2512.23961)
*Junjie H. Xu*

Main category: cs.IR

TL;DR: 该研究提出了一个基于智能代理AI的KYC推荐系统，并在广告、新闻、八卦、用户生成内容和技术五个垂直领域进行评估，通过nDCG指标对比不同KYC使用强度组的性能表现。


<details>
  <summary>Details</summary>
Motivation: 在金融领域的KYC（了解你的客户）场景中，需要开发能够处理多领域内容的智能推荐系统，以提升用户体验和推荐准确性，同时满足金融合规要求。

Method: 使用智能代理AI构建推荐系统，在五个内容垂直领域（广告、新闻、八卦、用户生成内容、技术）进行实验评估，将用户分为四个基于KYC使用强度的实验组，采用nDCG@1、nDCG@3、nDCG@5作为评估指标，并参考百度和小红书等行业基准。

Result: 研究展示了不同KYC使用强度组在五个垂直领域的推荐性能表现，通过nDCG指标量化了推荐系统的效果，为大规模智能代理推荐系统的工程实现提供了实证数据。

Conclusion: 该研究证明了智能代理AI在KYC推荐系统中的有效性，为金融领域多内容垂直推荐系统的设计和优化提供了理论框架和实践指导。

Abstract: This research presents a cutting-edge recommendation system utilizing agentic AI for KYC (Know Your Customer in the financial domain), and its evaluation across five distinct content verticals: Advertising (Ad), News, Gossip, Sharing (User-Generated Content), and Technology (Tech). The study compares the performance of four experimental groups, grouping by the intense usage of KYC, benchmarking them against the Normalized Discounted Cumulative Gain (nDCG) metric at truncation levels of $k=1$, $k=3$, and $k=5$. By synthesizing experimental data with theoretical frameworks and industry benchmarks from platforms such as Baidu and Xiaohongshu, this research provides insight by showing experimental results for engineering a large-scale agentic recommendation system.

</details>


### [33] [Time-Aware Adaptive Side Information Fusion for Sequential Recommendation](https://arxiv.org/abs/2512.24246)
*Jie Luo,Wenyu Zhang,Xinming Zhang,Yuan Fang*

Main category: cs.IR

TL;DR: TASIF框架通过时间感知、自适应噪声过滤和高效融合机制，解决了序列推荐中忽略时间动态、易受噪声影响和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前序列推荐模型存在三个主要局限：忽略时间戳的细粒度动态特性、对用户交互序列中的噪声敏感、以及依赖计算昂贵的融合架构。

Method: 提出TASIF框架，包含三个协同组件：1) 简单即插即用的时间跨度划分机制捕捉全局时间模式；2) 自适应频率过滤器使用可学习门控自适应去噪特征序列；3) 高效自适应侧信息融合层采用"引导而非混合"架构，属性引导注意力机制但不混入内容表示的项目嵌入。

Result: 在四个公共数据集上的广泛实验表明，TASIF显著优于最先进的基线方法，同时在训练中保持出色的效率。

Conclusion: TASIF框架系统性地解决了序列推荐中的时间动态、噪声鲁棒性和计算效率问题，为结合项目侧信息的推荐系统提供了有效的解决方案。

Abstract: Incorporating item-side information, such as category and brand, into sequential recommendation is a well-established and effective approach for improving performance. However, despite significant advancements, current models are generally limited by three key challenges: they often overlook the fine-grained temporal dynamics inherent in timestamps, exhibit vulnerability to noise in user interaction sequences, and rely on computationally expensive fusion architectures. To systematically address these challenges, we propose the Time-Aware Adaptive Side Information Fusion (TASIF) framework. TASIF integrates three synergistic components: (1) a simple, plug-and-play time span partitioning mechanism to capture global temporal patterns; (2) an adaptive frequency filter that leverages a learnable gate to denoise feature sequences adaptively, thereby providing higher-quality inputs for subsequent fusion modules; and (3) an efficient adaptive side information fusion layer, this layer employs a "guide-not-mix" architecture, where attributes guide the attention mechanism without being mixed into the content-representing item embeddings, ensuring deep interaction while ensuring computational efficiency. Extensive experiments on four public datasets demonstrate that TASIF significantly outperforms state-of-the-art baselines while maintaining excellent efficiency in training. Our source code is available at https://github.com/jluo00/TASIF.

</details>


### [34] [RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation](https://arxiv.org/abs/2512.24268)
*Pankayaraj Pathmanathan,Michael-Andrei Panaitescu-Liess,Cho-Yu Jason Chiang,Furong Huang*

Main category: cs.IR

TL;DR: 提出两种检索增强生成（RAG）的检索阶段防御方法RAGPart和RAGMask，对抗恶意文档注入攻击，保护RAG系统免受语料库污染影响。


<details>
  <summary>Details</summary>
Motivation: RAG系统虽然能增强大语言模型的外部知识并减少幻觉，但存在语料库污染漏洞，攻击者可通过注入恶意文档操控模型输出，需要轻量级防御方案。

Method: 提出两种互补的检索阶段防御：RAGPart利用密集检索器的训练动态和文档分区来缓解中毒点影响；RAGMask通过目标令牌掩码下的显著相似度偏移来识别可疑令牌。

Result: 在两个基准测试、四种中毒策略和四种最先进检索器上，防御方法能持续降低攻击成功率，同时在良性条件下保持效用。还引入了可解释攻击来压力测试防御效果。

Conclusion: 研究展示了检索阶段防御的潜力和局限性，为鲁棒的RAG部署提供了实用见解，提出的轻量级防御无需修改生成模型，计算成本低。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.

</details>


### [35] [MaRCA: Multi-Agent Reinforcement Learning for Dynamic Computation Allocation in Large-Scale Recommender Systems](https://arxiv.org/abs/2512.24325)
*Wan Jiang,Xinyi Zang,Yudong Zhao,Yusi Zou,Yunfei Lu,Junbo Tong,Yang Liu,Ming Li,Jiani Shi,Xin Yang*

Main category: cs.IR

TL;DR: MaRCA是一个基于多智能体强化学习的端到端计算资源分配框架，用于大规模推荐系统，通过建模推荐系统各阶段为协作智能体，在现有计算资源下实现了16.67%的收入提升。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统面临模型复杂性和流量规模增长带来的计算挑战，现有方法通常简化多阶段计算资源分配，忽略了阶段间依赖关系，限制了全局最优性。

Method: 提出MaRCA框架：1) 将推荐系统各阶段建模为协作智能体，采用集中训练分散执行(CTDE)方法；2) 引入AutoBucket TestBench进行精确计算成本估计；3) 使用基于模型预测控制(MPC)的收入-成本平衡器主动预测流量负载并调整收入-成本权衡。

Result: 自2024年11月在领先全球电商平台的广告管道中端到端部署以来，MaRCA每天持续处理数千亿广告请求，在现有计算资源下实现了16.67%的收入提升。

Conclusion: MaRCA通过多智能体强化学习框架有效解决了大规模推荐系统中的计算资源分配问题，实现了在资源约束下的收入最大化，证明了端到端优化方法的有效性。

Abstract: Modern recommender systems face significant computational challenges due to growing model complexity and traffic scale, making efficient computation allocation critical for maximizing business revenue. Existing approaches typically simplify multi-stage computation resource allocation, neglecting inter-stage dependencies, thus limiting global optimality. In this paper, we propose MaRCA, a multi-agent reinforcement learning framework for end-to-end computation resource allocation in large-scale recommender systems. MaRCA models the stages of a recommender system as cooperative agents, using Centralized Training with Decentralized Execution (CTDE) to optimize revenue under computation resource constraints. We introduce an AutoBucket TestBench for accurate computation cost estimation, and a Model Predictive Control (MPC)-based Revenue-Cost Balancer to proactively forecast traffic loads and adjust the revenue-cost trade-off accordingly. Since its end-to-end deployment in the advertising pipeline of a leading global e-commerce platform in November 2024, MaRCA has consistently handled hundreds of billions of ad requests per day and has delivered a 16.67% revenue uplift using existing computation resources.

</details>


### [36] [On the Factual Consistency of Text-based Explainable Recommendation Models](https://arxiv.org/abs/2512.24366)
*Ben Kabongo,Vincent Guigue*

Main category: cs.IR

TL;DR: 该研究提出了一个评估基于文本的可解释推荐系统事实一致性的框架，发现现有模型虽然语义相似度高但事实一致性极低


<details>
  <summary>Details</summary>
Motivation: 尽管现有基于文本的可解释推荐系统利用大语言模型生成流畅的解释，但这些解释是否与可用证据保持事实一致性仍是一个未充分探索的关键问题

Method: 设计基于提示的流水线，使用大语言模型从评论中提取原子解释语句构建事实内容的地面真值；创建增强基准用于细粒度评估；提出结合大语言模型和自然语言推理的语句级对齐指标来评估事实一致性和相关性

Result: 对六个最先进的可解释推荐模型进行广泛实验发现：虽然模型获得高语义相似度分数（BERTScore F1: 0.81-0.90），但所有事实性指标都显示极低的性能（大语言模型基于语句级精度：4.38%-32.88%）

Conclusion: 研究结果强调了可解释推荐中事实感知评估的必要性，并为开发更可信的解释系统提供了基础

Abstract: Text-based explainable recommendation aims to generate natural-language explanations that justify item recommendations, to improve user trust and system transparency. Although recent advances leverage LLMs to produce fluent outputs, a critical question remains underexplored: are these explanations factually consistent with the available evidence? We introduce a comprehensive framework for evaluating the factual consistency of text-based explainable recommenders. We design a prompting-based pipeline that uses LLMs to extract atomic explanatory statements from reviews, thereby constructing a ground truth that isolates and focuses on their factual content. Applying this pipeline to five categories from the Amazon Reviews dataset, we create augmented benchmarks for fine-grained evaluation of explanation quality. We further propose statement-level alignment metrics that combine LLM- and NLI-based approaches to assess both factual consistency and relevance of generated explanations. Across extensive experiments on six state-of-the-art explainable recommendation models, we uncover a critical gap: while models achieve high semantic similarity scores (BERTScore F1: 0.81-0.90), all our factuality metrics reveal alarmingly low performance (LLM-based statement-level precision: 4.38%-32.88%). These findings underscore the need for factuality-aware evaluation in explainable recommendation and provide a foundation for developing more trustworthy explanation systems.

</details>


### [37] [MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints](https://arxiv.org/abs/2512.24711)
*Kangyang Luo,Shuzheng Si,Yuzhuo Bai,Cheng Gao,Zhitong Wang,Cheng Huang,Yingli Shen,Yufeng Han,Wenhao Li,Cunliang Kong,Maosong Sun*

Main category: cs.IR

TL;DR: MEIC-DT是一种基于轻量级Transformer的双阈值内存高效增量聚类方法，通过双阈值约束机制控制Transformer输入规模，结合统计感知驱逐策略和内部正则化策略，在严格内存限制下实现竞争性的共指消解性能。


<details>
  <summary>Details</summary>
Motivation: 尽管监督神经方法在大型语言模型时代仍是共指消解的最先进技术，但其在增量聚类方面的潜力尚未充分探索，特别是在平衡长文本处理效率与性能方面存在关键挑战。

Method: 提出MEIC-DT方法：1) 双阈值约束机制，在预定义内存预算内精确控制Transformer输入规模；2) 统计感知驱逐策略(SAES)，利用训练和推理阶段的不同统计特征进行智能缓存管理；3) 内部正则化策略(IRP)，通过选择最具代表性的提及来战略性地压缩聚类，保持语义完整性。

Result: 在常见基准测试上的广泛实验表明，MEIC-DT在严格内存约束下实现了高度竞争性的共指消解性能。

Conclusion: MEIC-DT通过创新的双阈值机制和内存优化策略，有效解决了增量聚类中效率与性能的平衡问题，为内存受限环境下的共指消解提供了有效解决方案。

Abstract: In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget. This mechanism incorporates a Statistics-Aware Eviction Strategy (\textbf{SAES}), which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. Furthermore, we introduce an Internal Regularization Policy (\textbf{IRP}) that strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity. Extensive experiments on common benchmarks demonstrate that MEIC-DT achieves highly competitive coreference performance under stringent memory constraints.

</details>


### [38] [MDiffFR: Modality-Guided Diffusion Generation for Cold-start Items in Federated Recommendation](https://arxiv.org/abs/2512.24715)
*Kang Fu,Honglei Zhang,Xuechao Zou,Yidong Li*

Main category: cs.IR

TL;DR: 该论文提出MDiffFR方法，使用模态引导的扩散模型生成联邦推荐系统中冷启动项目的嵌入表示，解决了传统属性到嵌入映射方法在数据分布变化时的嵌入对齐问题。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统在保护用户隐私的同时，由于严格的隐私约束限制了跨客户端用户-项目交互数据和用户画像的访问，使得学习新项目（冷启动项目）的全局有效表示变得困难。现有解决方案通常通过属性到嵌入的映射范式预测新项目的嵌入，但这种一对一映射范式难以建模变化的数据分布，容易导致嵌入对齐问题。

Method: 提出MDiffFR方法：1）在服务器端使用定制的扩散模型生成新项目的嵌入表示，然后分发给客户端进行冷启动推理；2）部署预训练的模态编码器提取模态特征作为条件信号，指导反向去噪过程以对齐项目语义；3）通过理论分析验证该方法相比现有映射方法具有更强的隐私保证。

Result: 在四个真实数据集上的广泛实验表明，该方法在联邦推荐系统中始终优于所有基线方法。

Conclusion: MDiffFR通过模态引导的扩散生成方法有效解决了联邦推荐系统中的项目冷启动问题，相比传统的属性到嵌入映射方法，能够更好地处理数据分布变化并确保嵌入对齐，同时提供更强的隐私保证。

Abstract: Federated recommendations (FRs) provide personalized services while preserving user privacy by keeping user data on local clients, which has attracted significant attention in recent years. However, due to the strict privacy constraints inherent in FRs, access to user-item interaction data and user profiles across clients is highly restricted, making it difficult to learn globally effective representations for new (cold-start) items. Consequently, the item cold-start problem becomes even more challenging in FRs. Existing solutions typically predict embeddings for new items through the attribute-to-embedding mapping paradigm, which establishes a fixed one-to-one correspondence between item attributes and their embeddings. However, this one-to-one mapping paradigm often fails to model varying data distributions and tends to cause embedding misalignment, as verified by our empirical studies. To this end, we propose MDiffFR, a novel generation-based modality-guided diffusion method for cold-start items in FRs. In this framework, we employ a tailored diffusion model on the server to generate embeddings for new items, which are then distributed to clients for cold-start inference. To align item semantics, we deploy a pre-trained modality encoder to extract modality features as conditional signals to guide the reverse denoising process. Furthermore, our theoretical analysis verifies that the proposed method achieves stronger privacy guarantees compared to existing mapping-based approaches. Extensive experiments on four real datasets demonstrate that our method consistently outperforms all baselines in FRs.

</details>


### [39] [OpenOneRec Technical Report](https://arxiv.org/abs/2512.24762)
*Guorui Zhou,Honghui Bao,Jiaming Huang,Jiaxin Deng,Jinghao Zhang,Junda She,Kuo Cai,Lejian Ren,Lu Ren,Qiang Luo,Qianqian Wang,Qigen Hu,Rongzhou Zhang,Ruiming Tang,Shiyao Wang,Wuchao Li,Xiangyu Wu,Xinchen Luo,Xingmei Wang,Yifei Hu,Yunfan Wu,Zhanyu Liu,Zhiyang Zhang,Zixing Zhang,Bo Chen,Bin Wen,Chaoyi Ma,Chengru Song,Chenglong Chu,Defu Lian,Fan Yang,Feng Jiang,Hongtao Cheng,Huanjie Wang,Kun Gai,Pengfei Zheng,Qiang Wang,Rui Huang,Siyang Mao,Tingting Gao,Wei Yuan,Yan Wang,Yang Zhou,Yi Su,Zexuan Cheng,Zhixin Ling,Ziming Li*

Main category: cs.IR

TL;DR: 论文提出了RecIF-Bench基准测试和OneRec-Foundation模型系列，旨在弥合推荐系统与通用智能之间的差距，通过统一的生成框架提升推荐系统的推理能力和指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 当前OneRec系列虽然成功将碎片化的推荐流程统一为端到端生成框架，但推荐系统与通用智能之间仍存在显著差距。现有系统受限于孤立数据，仅擅长模式匹配而缺乏世界知识、推理能力和指令跟随能力，且缺乏评估这些综合能力的整体基准。

Method: 1) 提出RecIF-Bench基准测试，涵盖8个多样化任务，全面评估从基础预测到复杂推理的能力；2) 发布包含9600万交互的大规模训练数据集；3) 开源完整的训练流程，包括数据处理、协同预训练和后训练；4) 发布OneRec-Foundation模型系列（1.7B和8B参数）。

Result: OneRec-Foundation模型在RecIF-Bench所有任务中均取得新的SOTA结果。在Amazon基准测试中，模型在10个多样化数据集上的Recall@10平均提升26.8%。研究还展示了推荐能力可以可预测地扩展，同时减轻通用知识的灾难性遗忘。

Conclusion: 这项工作向构建真正智能的推荐系统迈出了一步，但实现这一愿景仍面临显著的技术和理论挑战，需要更广泛的研究参与。提出的基准、数据集和开源框架为可重复研究提供了基础。

Abstract: While the OneRec series has successfully unified the fragmented recommendation pipeline into an end-to-end generative framework, a significant gap remains between recommendation systems and general intelligence. Constrained by isolated data, they operate as domain specialists-proficient in pattern matching but lacking world knowledge, reasoning capabilities, and instruction following. This limitation is further compounded by the lack of a holistic benchmark to evaluate such integrated capabilities. To address this, our contributions are: 1) RecIF Bench & Open Data: We propose RecIF-Bench, a holistic benchmark covering 8 diverse tasks that thoroughly evaluate capabilities from fundamental prediction to complex reasoning. Concurrently, we release a massive training dataset comprising 96 million interactions from 160,000 users to facilitate reproducible research. 2) Framework & Scaling: To ensure full reproducibility, we open-source our comprehensive training pipeline, encompassing data processing, co-pretraining, and post-training. Leveraging this framework, we demonstrate that recommendation capabilities can scale predictably while mitigating catastrophic forgetting of general knowledge. 3) OneRec-Foundation: We release OneRec Foundation (1.7B and 8B), a family of models establishing new state-of-the-art (SOTA) results across all tasks in RecIF-Bench. Furthermore, when transferred to the Amazon benchmark, our models surpass the strongest baselines with an average 26.8% improvement in Recall@10 across 10 diverse datasets (Figure 1). This work marks a step towards building truly intelligent recommender systems. Nonetheless, realizing this vision presents significant technical and theoretical challenges, highlighting the need for broader research engagement in this promising direction.

</details>


### [40] [HiGR: Efficient Generative Slate Recommendation via Hierarchical Planning and Multi-Objective Preference Alignment](https://arxiv.org/abs/2512.24787)
*Yunsheng Pang,Zijian Liu,Yudong Li,Shaojie Zhu,Zijian Luo,Chenyun Yu,Sikai Wu,Shichen Shen,Cong Xu,Bin Wang,Kai Jiang,Hongyong Yu,Chengxiang Zhuo,Zang Li*

Main category: cs.IR

TL;DR: HiGR是一个高效的生成式slate推荐框架，通过分层规划和列表级偏好对齐，解决了现有自回归方法在语义纠缠和低效解码方面的问题，在商业媒体平台上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归方法在slate推荐中存在两个主要问题：1）语义纠缠的项目标记化；2）缺乏整体slate规划的低效顺序解码。这限制了生成模型在slate推荐中的实际应用效果。

Method: HiGR框架包含三个核心组件：1）使用残差量化和对比约束的自动编码器，将项目标记为语义结构化的ID；2）分层生成过程，先进行列表级规划确定全局slate意图，再进行项目级解码选择具体项目；3）列表级偏好对齐目标，直接使用隐式用户反馈优化slate质量。

Result: 在大规模商业媒体平台上的实验表明，HiGR在离线评估中比最先进方法提升超过10%的推荐质量，推理速度提升5倍；在线A/B测试中，平均观看时间提升1.22%，平均视频观看量提升1.73%。

Conclusion: HiGR通过分层规划和列表级偏好对齐，有效解决了生成式slate推荐中的语义纠缠和低效解码问题，在实际商业应用中取得了显著的效果提升，证明了该框架的实用性和有效性。

Abstract: Slate recommendation, where users are presented with a ranked list of items simultaneously, is widely adopted in online platforms. Recent advances in generative models have shown promise in slate recommendation by modeling sequences of discrete semantic IDs autoregressively. However, existing autoregressive approaches suffer from semantically entangled item tokenization and inefficient sequential decoding that lacks holistic slate planning. To address these limitations, we propose HiGR, an efficient generative slate recommendation framework that integrates hierarchical planning with listwise preference alignment. First, we propose an auto-encoder utilizing residual quantization and contrastive constraints to tokenize items into semantically structured IDs for controllable generation. Second, HiGR decouples generation into a list-level planning stage for global slate intent, followed by an item-level decoding stage for specific item selection. Third, we introduce a listwise preference alignment objective to directly optimize slate quality using implicit user feedback. Experiments on our large-scale commercial media platform demonstrate that HiGR delivers consistent improvements in both offline evaluations and online deployment. Specifically, it outperforms state-of-the-art methods by over 10% in offline recommendation quality with a 5x inference speedup, while further achieving a 1.22% and 1.73% increase in Average Watch Time and Average Video Views in online A/B tests.

</details>


### [41] [RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment](https://arxiv.org/abs/2512.24943)
*Chenji Lu,Zhuo Chen,Hui Zhao,Zhenyi Wang,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: RAIR是一个中文电子商务搜索相关性评估基准，包含通用、长尾困难和视觉显著性三个子集，为行业提供标准化评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有相关性评估基准缺乏足够的复杂性，无法全面评估模型性能，导致行业内缺乏标准化的相关性评估指标。

Method: 提出RAIR基准，包含三个子集：1）行业平衡采样的通用子集评估基本能力；2）关注挑战性案例的长尾困难子集评估性能极限；3）视觉显著性子集评估多模态理解能力。

Result: 在14个开源和闭源模型上进行的实验表明，RAIR对GPT-5也构成足够挑战，GPT-5取得了最佳性能。

Conclusion: RAIR为相关性评估提供了行业基准，同时为通用LLM和视觉语言模型评估提供了新见解。

Abstract: Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Network Traffic Analysis with Process Mining: The UPSIDE Case Study](https://arxiv.org/abs/2512.23718)
*Francesco Vitale,Paolo Palmiero,Massimiliano Rak,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 本文提出了一种基于过程挖掘的方法来分析游戏网络流量，能够无监督地识别游戏状态、通过Petri网编码这些状态，并对不同游戏进行分类。


<details>
  <summary>Details</summary>
Motivation: 在线游戏作为流行活动涉及复杂的系统和网络基础设施，产生大量市场收入。研究游戏网络设备行为对于评估带宽消耗、预测高负载和检测恶意活动具有重要意义。过程挖掘因其结合数据驱动分析和模型洞察的能力而显示出潜力。

Method: 提出基于过程挖掘的方法分析游戏网络流量：1) 无监督地从游戏网络数据中表征不同状态；2) 通过过程挖掘将这些状态编码为可解释的Petri网；3) 对游戏网络流量数据进行分类以识别正在玩的不同视频游戏。

Result: 在UPSIDE案例研究中应用该方法，涉及与两款游戏（Clash Royale和Rocket League）交互的多个设备的游戏网络数据。结果显示：游戏网络行为可以通过Petri网表示的状态有效且可解释地建模，具有足够的连贯性（设备间相似度94.02%）和特异性（状态间分离度174.99%），同时保持对两款不同游戏的良好分类准确率（AUC 73.84%）。

Conclusion: 过程挖掘方法能够有效分析游戏网络流量，实现状态识别、模型表示和游戏分类，为游戏网络行为建模提供了有前景的技术途径。

Abstract: Online gaming is a popular activity involving the adoption of complex systems and network infrastructures. The relevance of gaming, which generates large amounts of market revenue, drove research in modeling network devices' behavior to evaluate bandwidth consumption, predict and sustain high loads, and detect malicious activity. In this context, process mining appears promising due to its ability to combine data-driven analyses with model-based insights. In this paper, we propose a process mining-based method that analyzes gaming network traffic, allowing: unsupervised characterization of different states from gaming network data; encoding such states through process mining into interpretable Petri nets; and classification of gaming network traffic data to identify different video games being played. We apply the method to the UPSIDE case study, involving gaming network data of several devices interacting with two video games: Clash Royale and Rocket League. Results demonstrate that the gaming network behavior can be effectively and interpretably modeled through states represented as Petri nets with sufficient coherence (94.02% inter-device similarity) and specificity (174.99% inter-state separation) while maintaining a good classification accuracy of the two different video games (73.84% AUC).

</details>


### [43] [A Comprehensive Study of Deep Learning Model Fixing Approaches](https://arxiv.org/abs/2512.23745)
*Hanmo You,Zan Wang,Zishuo Dong,Luanqi Mo,Jianjun Zhao,Junjie Chen*

Main category: cs.LG

TL;DR: 对16种最先进的深度学习模型修复方法进行了大规模实证研究，评估了它们在模型修复效果、鲁棒性、公平性和向后兼容性等方面的表现，发现模型级方法修复效果最好，但没有单一方法能在保持所有其他属性的同时达到最佳修复性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统像传统软件一样容易出错，其故障可能给用户带来重大风险。虽然已有许多修复方法被提出，但缺乏对这些方法性能的全面评估，特别是在修复效果之外对其他关键属性（如鲁棒性、公平性、向后兼容性）的影响。

Method: 对16种最先进的深度学习模型修复方法进行了大规模实证研究，涵盖模型级、层级和神经元级三个类别。在统一的实验设置下，使用多样化的数据集、模型架构和应用领域，全面评估这些方法的修复效果及其对其他关键属性的影响。

Result: 模型级方法在修复效果上优于其他方法；没有单一方法能在提高准确性的同时保持所有其他属性（鲁棒性、公平性、向后兼容性）的最佳表现；修复方法通常会对其他关键属性产生副作用。

Conclusion: 学术界应优先研究减轻修复方法的副作用，工业界在选择修复方法时需要权衡不同属性的优先级。这些发现为该领域的未来探索指明了方向。

Abstract: Deep Learning (DL) has been widely adopted in diverse industrial domains, including autonomous driving, intelligent healthcare, and aided programming. Like traditional software, DL systems are also prone to faults, whose malfunctioning may expose users to significant risks. Consequently, numerous approaches have been proposed to address these issues. In this paper, we conduct a large-scale empirical study on 16 state-of-the-art DL model fixing approaches, spanning model-level, layer-level, and neuron-level categories, to comprehensively evaluate their performance. We assess not only their fixing effectiveness (their primary purpose) but also their impact on other critical properties, such as robustness, fairness, and backward compatibility. To ensure comprehensive and fair evaluation, we employ a diverse set of datasets, model architectures, and application domains within a uniform experimental setup for experimentation. We summarize several key findings with implications for both industry and academia. For example, model-level approaches demonstrate superior fixing effectiveness compared to others. No single approach can achieve the best fixing performance while improving accuracy and maintaining all other properties. Thus, academia should prioritize research on mitigating these side effects. These insights highlight promising directions for future exploration in this field.

</details>


### [44] [A Review of Diffusion-based Simulation-Based Inference: Foundations and Applications in Non-Ideal Data Scenarios](https://arxiv.org/abs/2512.23748)
*Haley Rosso,Talea Mayo*

Main category: cs.LG

TL;DR: 本文综述了基于扩散模型的仿真推理方法，从数学基础到实际应用，重点分析了其在非理想科学数据条件下的鲁棒性优势。


<details>
  <summary>Details</summary>
Motivation: 针对复杂仿真问题中似然函数难以处理的情况，需要无需显式似然函数的仿真推理方法。扩散模型作为一种基于分数匹配和反向时间随机动力学的生成模型，为SBI提供了灵活框架。

Method: 回顾扩散模型的数学基础（前向加噪、反向时间SDE/ODE、概率流、去噪分数匹配），分析条件分数如何实现无似然后验采样，比较扩散模型与归一化流在神经后验/似然估计中的优劣。

Result: 扩散模型在科学数据的非理想条件下表现出鲁棒性：模型误设（仿真训练数据与现实不匹配）、非结构化或无限维观测、数据缺失等。综合了基于Schrodinger桥公式、条件与序列后验采样器、非结构化数据摊销架构、推理时先验适应等方法。

Conclusion: 基于扩散的SBI为科学不确定性量化提供了有前景的框架，特别是在地球物理概率模型等应用中。文章强调准确后验所需的条件和注意事项，并讨论了开放性问题。

Abstract: For complex simulation problems, inferring parameters of scientific interest often precludes the use of classical likelihood-based techniques due to intractable likelihood functions. Simulation-based inference (SBI) methods forego the need for explicit likelihoods by directly utilizing samples from the simulator to learn posterior distributions over parameters $\mathbfθ$ given observed data $\mathbf{x}_{\text{o}}$. Recent work has brought attention to diffusion models -- a type of generative model rooted in score matching and reverse-time stochastic dynamics -- as a flexible framework SBI tasks. This article reviews diffusion-based SBI from first principles to applications in practice. We first recall the mathematical foundations of diffusion modeling (forward noising, reverse-time SDE/ODE, probability flow, and denoising score matching) and explain how conditional scores enable likelihood-free posterior sampling. We then examine where diffusion models address pain points of normalizing flows in neural posterior/likelihood estimation and where they introduce new trade-offs (e.g., iterative sampling costs). The key theme of this review is robustness of diffusion-based SBI in non-ideal conditions common to scientific data: misspecification (mismatch between simulated training data and reality), unstructured or infinite-dimensional observations, and missingness. We synthesize methods spanning foundations drawing from Schrodinger-bridge formulations, conditional and sequential posterior samplers, amortized architectures for unstructured data, and inference-time prior adaptation. Throughout, we adopt consistent notation and emphasize conditions and caveats required for accurate posteriors. The review closes with a discussion of open problems with an eye toward applications of uncertainty quantification for probabilistic geophysical models that may benefit from diffusion-based SBI.

</details>


### [45] [Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents](https://arxiv.org/abs/2512.23749)
*Amin Sadri,M Maruf Hossain*

Main category: cs.LG

TL;DR: CM²是一个坐标矩阵机，通过单样本学习实现人类级概念学习，专注于文档结构特征而非语义向量，提供绿色AI解决方案。


<details>
  <summary>Details</summary>
Motivation: 人类通常能从单个示例中学习新概念，而机器学习算法需要大量样本。现有AI方法依赖大规模预训练和GPU基础设施，能耗高。需要开发能够像人类一样识别重要结构特征、实现单样本学习的绿色AI方案。

Method: 提出坐标矩阵机（CM²），这是一种小型专用模型。它通过学习文档结构信息进行分类，专注于识别人类会考虑的结构性"重要特征"，而不是使用详尽的语义向量。模型设计为绿色AI解决方案，可在仅CPU环境中运行。

Result: CM²在单样本学习方面优于传统向量化器和需要大数据集的复杂深度学习模型。它实现了人类级概念学习，能够仅使用每个类别的一个样本来分类非常相似的文档。

Conclusion: CM²通过专注于结构坐标而非语义向量，提供了一种高效、可解释、环保的AI解决方案，实现了人类级概念学习能力，特别适合资源受限环境和单样本学习场景。

Abstract: Human-level concept learning argues that humans typically learn new concepts from a single example, whereas machine learning algorithms typically require hundreds of samples to learn a single concept. Our brain subconsciously identifies important features and learns more effectively. \vspace*{6pt}
  Contribution: In this paper, we present the Coordinate Matrix Machine (CM$^2$). This purpose-built small model augments human intelligence by learning document structures and using this information to classify documents. While modern "Red AI" trends rely on massive pre-training and energy-intensive GPU infrastructure, CM$^2$ is designed as a Green AI solution. It achieves human-level concept learning by identifying only the structural "important features" a human would consider, allowing it to classify very similar documents using only one sample per class.
  Advantage: Our algorithm outperforms traditional vectorizers and complex deep learning models that require larger datasets and significant compute. By focusing on structural coordinates rather than exhaustive semantic vectors, CM$^2$ offers: 1. High accuracy with minimal data (one-shot learning) 2. Geometric and structural intelligence 3. Green AI and environmental sustainability 4. Optimized for CPU-only environments 5. Inherent explainability (glass-box model) 6. Faster computation and low latency 7. Robustness against unbalanced classes 8. Economic viability 9. Generic, expandable, and extendable

</details>


### [46] [Geometric Scaling of Bayesian Inference in LLMs](https://arxiv.org/abs/2512.23752)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: cs.LG

TL;DR: 研究发现现代语言模型保留了支持贝叶斯推理的几何结构，其值表示沿着与预测熵相关的单一主导轴组织，这种几何结构是特权的不确定性读取机制而非单一计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明小型transformer在受控环境中可实现精确贝叶斯推理，并产生编码后验结构的几何基底。本研究旨在探究这种几何特征是否存在于生产级语言模型中。

Method: 在Pythia、Phi-2、Llama-3和Mistral等模型家族中分析最后一层值表示的组织结构；通过针对Pythia-410M模型的熵对齐轴进行干预实验，研究该几何结构的作用。

Result: 发现所有模型的值表示都沿着单一主导轴组织，该轴位置与预测熵强相关；领域限制提示将该结构压缩为与合成设置中观察到的相同的低维流形；干预熵对齐轴会破坏局部不确定性几何，但不会导致贝叶斯行为的成比例退化。

Conclusion: 现代语言模型保留了支持贝叶斯推理的几何基底，并沿着该基底组织其近似贝叶斯更新；该几何结构是特权的不确定性读取机制，而非单一计算瓶颈。

Abstract: Recent work has shown that small transformers trained in controlled "wind-tunnel'' settings can implement exact Bayesian inference, and that their training dynamics produce a geometric substrate -- low-dimensional value manifolds and progressively orthogonal keys -- that encodes posterior structure. We investigate whether this geometric signature persists in production-grade language models. Across Pythia, Phi-2, Llama-3, and Mistral families, we find that last-layer value representations organize along a single dominant axis whose position strongly correlates with predictive entropy, and that domain-restricted prompts collapse this structure into the same low-dimensional manifolds observed in synthetic settings.
  To probe the role of this geometry, we perform targeted interventions on the entropy-aligned axis of Pythia-410M during in-context learning. Removing or perturbing this axis selectively disrupts the local uncertainty geometry, whereas matched random-axis interventions leave it intact. However, these single-layer manipulations do not produce proportionally specific degradation in Bayesian-like behavior, indicating that the geometry is a privileged readout of uncertainty rather than a singular computational bottleneck. Taken together, our results show that modern language models preserve the geometric substrate that enables Bayesian inference in wind tunnels, and organize their approximate Bayesian updates along this substrate.

</details>


### [47] [Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation](https://arxiv.org/abs/2512.23753)
*Deep Shankar Pandey,Hyomin Choi,Qi Yu*

Main category: cs.LG

TL;DR: 本文分析了基于主观逻辑的证据深度学习模型中的激活函数问题，提出了新的激活函数和正则化方法来解决学习停滞问题。


<details>
  <summary>Details</summary>
Motivation: 证据深度学习模型虽然能有效量化不确定性，但由于主观逻辑框架要求证据必须非负，特定的激活函数在低证据区域会导致梯度极小，产生学习停滞现象，限制了模型的学习能力。

Method: 首先理论分析了激活函数导致的学习停滞行为，然后设计了一族通用的激活函数和相应的证据正则化器，确保在不同激活机制下证据更新的一致性。

Result: 在四个基准分类任务（MNIST、CIFAR-10、CIFAR-100、Tiny-ImageNet）、两个少样本分类任务以及盲人脸恢复问题上进行了广泛实验，验证了理论分析并证明了所提方法的有效性。

Conclusion: 提出的广义正则化证据模型解决了证据深度学习中的学习停滞问题，为构建更稳定、高效的证据神经网络提供了理论支持和实用方法。

Abstract: Evidential deep learning (EDL) models, based on Subjective Logic, introduce a principled and computationally efficient way to make deterministic neural networks uncertainty-aware. The resulting evidential models can quantify fine-grained uncertainty using learned evidence. However, the Subjective-Logic framework constrains evidence to be non-negative, requiring specific activation functions whose geometric properties can induce activation-dependent learning-freeze behavior: a regime where gradients become extremely small for samples mapped into low-evidence regions. We theoretically characterize this behavior and analyze how different evidential activations influence learning dynamics. Building on this analysis, we design a general family of activation functions and corresponding evidential regularizers that provide an alternative pathway for consistent evidence updates across activation regimes. Extensive experiments on four benchmark classification problems (MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet), two few-shot classification problems, and blind face restoration problem empirically validate the developed theory and demonstrate the effectiveness of the proposed generalized regularized evidential models.

</details>


### [48] [HINTS: Extraction of Human Insights from Time-Series Without External Sources](https://arxiv.org/abs/2512.23755)
*Sheo Yon Jhin,Noseong Park*

Main category: cs.LG

TL;DR: HINTS是一个自监督学习框架，无需外部数据，从时间序列残差中提取潜在人类因素，通过意见动力学模型建模社会影响，提升时间序列预测精度。


<details>
  <summary>Details</summary>
Motivation: 人类决策、情感和集体心理是影响金融经济系统时间动态的复杂因素。现有方法依赖外部数据（如新闻、社交媒体）来捕捉这些因素，但会产生高昂的数据依赖成本（财务、计算、实践）。

Method: 提出HINTS自监督学习框架，从时间序列残差中内生提取潜在人类因素，无需外部数据。利用Friedkin-Johnsen意见动力学模型作为结构归纳偏置，建模演化中的社会影响、记忆和偏见模式。提取的人类因素作为注意力图集成到最先进的主干模型中。

Result: 在9个真实世界和基准数据集上的实验结果表明，HINTS能持续提升预测准确性。多个案例研究和消融研究验证了HINTS的可解释性，显示提取的因素与现实世界事件有很强的语义对齐。

Conclusion: HINTS框架通过内生提取人类因素，无需外部数据依赖，有效提升了时间序列预测性能，同时具有良好的可解释性和实际应用价值。

Abstract: Human decision-making, emotions, and collective psychology are complex factors that shape the temporal dynamics observed in financial and economic systems. Many recent time series forecasting models leverage external sources (e.g., news and social media) to capture human factors, but these approaches incur high data dependency costs in terms of financial, computational, and practical implications. In this study, we propose HINTS, a self-supervised learning framework that extracts these latent factors endogenously from time series residuals without external data. HINTS leverages the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns. The extracted human factors are integrated into a state-of-the-art backbone model as an attention map. Experimental results using nine real-world and benchmark datasets demonstrate that HINTS consistently improves forecasting accuracy. Furthermore, multiple case studies and ablation studies validate the interpretability of HINTS, demonstrating strong semantic alignment between the extracted factors and real-world events, demonstrating the practical utility of HINTS.

</details>


### [49] [Learning Coupled System Dynamics under Incomplete Physical Constraints and Missing Data](https://arxiv.org/abs/2512.23761)
*Esha Saha,Hao Wang*

Main category: cs.LG

TL;DR: MUSIC框架通过稀疏诱导的多任务神经网络，将部分物理约束与数据驱动学习结合，解决耦合系统中物理约束变量与数据驱动变量互斥时的全维度解恢复问题。


<details>
  <summary>Details</summary>
Motivation: 复杂系统建模中经常存在物理约束变量与数据驱动变量不匹配的问题：通常只有一个变量的控制方程已知，而其他变量只能通过数据获取。现有物理信息机器学习方法要么假设完全已知控制方程，要么假设所有变量都有完整数据，无法处理这种部分物理知识的情况。

Method: 提出MUSIC（稀疏和不完全约束下的多任务学习）框架，采用稀疏诱导的多任务神经网络，结合部分物理约束与数据驱动学习。使用无网格（随机）采样训练数据和稀疏正则化，生成高度压缩的模型，提高训练和评估效率。

Result: MUSIC能够在数据稀缺和噪声条件下准确学习复杂耦合系统的解（冲击波解、不连续解、模式形成解），在稀疏和非稀疏公式对比中表现一致优于非稀疏方法。

Conclusion: MUSIC为具有不完全物理知识的部分观测系统建模提供了一种灵活有效的方法，能够处理物理约束变量与数据驱动变量互斥的情况，在效率和准确性方面都有显著优势。

Abstract: Advances in data acquisition and computational methods have accelerated the use of differential equation based modelling for complex systems. Such systems are often described by coupled (or more) variables, yet governing equation is typically available for one variable, while the remaining variable can be accessed only through data. This mismatch between known physics and observed data poses a fundamental challenge for existing physics-informed machine learning approaches, which generally assume either complete knowledge of the governing equations or full data availability across all variables. In this paper, we introduce MUSIC (Multitask Learning Under Sparse and Incomplete Constraints), a sparsity induced multitask neural network framework that integrates partial physical constraints with data-driven learning to recover full-dimensional solutions of coupled systems when physics-constrained and data-informed variables are mutually exclusive. MUSIC employs mesh-free (random) sampling of training data and sparsity regularization, yielding highly compressed models with improved training and evaluation efficiency. We demonstrate that MUSIC accurately learns solutions (shock wave solutions, discontinuous solutions, pattern formation solutions) to complex coupled systems under data-scarce and noisy conditions, consistently outperforming non-sparse formulations. These results highlight MUSIC as a flexible and effective approach for modeling partially observed systems with incomplete physical knowledge.

</details>


### [50] [Drift-Based Dataset Stability Benchmark](https://arxiv.org/abs/2512.23762)
*Dominik Soukup,Richard Plný,Daniel Vašata,Tomáš Čejka*

Main category: cs.LG

TL;DR: 该论文提出了一种评估网络流量分类数据集稳定性的新方法，通过概念漂移检测和特征权重提升检测性能，并在CESNET-TLS-Year22数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 网络流量分类中，机器学习模型部署后可能因数据集过时和网络快速演变而性能下降，特别是数据或概念漂移会导致模型性能骤降。通常采用完全重新训练，但缺乏对根本原因的深入分析，而数据集质量并不总是可靠的。

Method: 提出了一种新颖的数据集稳定性评估方法和基准工作流程，基于概念漂移检测方法，并利用机器学习特征权重来提升检测性能。

Result: 在CESNET-TLS-Year22数据集上验证了方法的有效性，提供了初始数据集稳定性基准，用于描述数据集稳定性和识别优化方向，并展示了优化对数据集变体的影响。

Conclusion: 该研究提出的框架能够有效评估网络流量分类数据集的稳定性，为数据集优化提供了系统化的基准方法，有助于提高机器学习模型在实际部署中的鲁棒性。

Abstract: Machine learning (ML) represents an efficient and popular approach for network traffic classification. However, network traffic classification is a challenging domain, and trained models may degrade soon after deployment due to the obsolete datasets and quick evolution of computer networks as new or updated protocols appear. Moreover, significant change in the behavior of a traffic type (and, therefore, the underlying features representing the traffic) can produce a large and sudden performance drop of the deployed model, known as a data or concept drift. In most cases, complete retraining is performed, often without further investigation of root causes, as good dataset quality is assumed. However, this is not always the case and further investigation must be performed. This paper proposes a novel methodology to evaluate the stability of datasets and a benchmark workflow that can be used to compare datasets.
  The proposed framework is based on a concept drift detection method that also uses ML feature weights to boost the detection performance. The benefits of this work are demonstrated on CESNET-TLS-Year22 dataset. We provide the initial dataset stability benchmark that is used to describe dataset stability and weak points to identify the next steps for optimization. Lastly, using the proposed benchmarking methodology, we show the optimization impact on the created dataset variants.

</details>


### [51] [Neural Optimal Design of Experiment for Inverse Problems](https://arxiv.org/abs/2512.23763)
*John E. Darges,Babak Maboudi Afkham,Matthias Chung*

Main category: cs.LG

TL;DR: 提出Neural Optimal Design of Experiments (NODE)框架，通过联合训练神经重建模型和连续设计变量来优化实验设计，避免传统双层优化和间接稀疏正则化


<details>
  <summary>Details</summary>
Motivation: 传统最优实验设计方法通常采用双层优化和间接稀疏正则化，计算复杂且需要调参。需要一种更直接、高效的方法来优化传感器位置、采样时间或测量角度等设计变量

Method: NODE框架在单一优化循环中联合训练神经重建模型和固定预算的连续设计变量。通过直接优化测量位置而非加权密集候选网格，实现设计层面的稀疏性，无需l1正则化调参

Result: 在指数增长基准测试、MNIST图像采样和稀疏视角X射线CT实验中，NODE均优于基线方法，展示了改进的重建精度和任务特定性能

Conclusion: NODE提供了一种有效的最优实验设计框架，通过设计层面的稀疏性和联合优化，在减少计算复杂度的同时提高了重建性能

Abstract: We introduce Neural Optimal Design of Experiments, a learning-based framework for optimal experimental design in inverse problems that avoids classical bilevel optimization and indirect sparsity regularization. NODE jointly trains a neural reconstruction model and a fixed-budget set of continuous design variables representing sensor locations, sampling times, or measurement angles, within a single optimization loop. By optimizing measurement locations directly rather than weighting a dense grid of candidates, the proposed approach enforces sparsity by design, eliminates the need for l1 tuning, and substantially reduces computational complexity. We validate NODE on an analytically tractable exponential growth benchmark, on MNIST image sampling, and illustrate its effectiveness on a real world sparse view X ray CT example. In all cases, NODE outperforms baseline approaches, demonstrating improved reconstruction accuracy and task-specific performance.

</details>


### [52] [Exploring Cumulative Effects in Survival Data Using Deep Learning Networks](https://arxiv.org/abs/2512.23764)
*Kang-Chung Yang,Shinsheng Yuan*

Main category: cs.LG

TL;DR: CENNSurv是一种新颖的深度学习生存分析方法，专门处理时间依赖性暴露的累积效应，在保持可解释性的同时提高了大规模数据集的扩展性。


<details>
  <summary>Details</summary>
Motivation: 流行病学研究中，时间依赖性暴露对生存结果的累积效应建模具有挑战性。传统样条方法需要重复数据转换且计算量大，而现有神经网络方法虽然准确但缺乏对累积暴露模式的可解释性。

Method: 提出了CENNSurv，一种新颖的深度学习方法，能够从时间依赖性数据中捕捉动态风险关系，解决了传统方法的扩展性问题和神经网络方法的可解释性不足。

Result: 在两个不同的真实世界数据集上评估，CENNSurv揭示了慢性环境暴露与关键生存结果之间的多年滞后关联，以及订阅流失前的关键短期行为转变，证明了其建模复杂时间模式的能力。

Conclusion: CENNSurv为研究累积效应的研究者提供了一个实用工具，能够提供可解释的洞察，同时改善了大规模数据处理的扩展性。

Abstract: In epidemiological research, modeling the cumulative effects of time-dependent exposures on survival outcomes presents a challenge due to their intricate temporal dynamics. Conventional spline-based statistical methods, though effective, require repeated data transformation for each spline parameter tuning, with survival analysis computations relying on the entire dataset, posing difficulties for large datasets. Meanwhile, existing neural network-based survival analysis methods focus on accuracy but often overlook the interpretability of cumulative exposure patterns. To bridge this gap, we introduce CENNSurv, a novel deep learning approach that captures dynamic risk relationships from time-dependent data. Evaluated on two diverse real-world datasets, CENNSurv revealed a multi-year lagged association between chronic environmental exposure and a critical survival outcome, as well as a critical short-term behavioral shift prior to subscription lapse. This demonstrates CENNSurv's ability to model complex temporal patterns with improved scalability. CENNSurv provides researchers studying cumulative effects a practical tool with interpretable insights.

</details>


### [53] [A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit](https://arxiv.org/abs/2512.23766)
*Karim Salta,Michael Kirby,Chris Peterson*

Main category: cs.LG

TL;DR: 提出SVBF-LBG算法，用可训练的舒伯特簇最佳拟合原型替代传统子空间均值，在多种数据上提升聚类纯度


<details>
  <summary>Details</summary>
Motivation: 在子空间聚类中，传统方法使用Grassmann或旗流形上的均值或中值作为几何代表，但可能无法充分捕捉子空间数据的几何结构。需要一种能更好表示子空间簇的数学结构，同时保持下游分析所需的几何特性。

Method: 提出舒伯特簇最佳拟合(SVBF)作为可训练原型，定义为在至少一个固定方向上尽可能接近与每个簇成员相交的子空间。将SVBF集成到Linde-Buzo-Grey(LBG)管道中，形成SVBF-LBG算法。

Result: SVBF-LBG在合成数据、图像数据、光谱数据和视频动作数据上均显示出比传统方法更高的聚类纯度，同时保留了数学结构以支持下游分析。

Conclusion: SVBF作为子空间聚类的可训练原型，在保持数学结构完整性的同时，显著提升了聚类性能，为子空间数据分析提供了更有效的几何表示方法。

Abstract: In many classification and clustering tasks, it is useful to compute a geometric representative for a dataset or a cluster, such as a mean or median. When datasets are represented by subspaces, these representatives become points on the Grassmann or flag manifold, with distances induced by their geometry, often via principal angles. We introduce a subspace clustering algorithm that replaces subspace means with a trainable prototype defined as a Schubert Variety of Best Fit (SVBF) - a subspace that comes as close as possible to intersecting each cluster member in at least one fixed direction. Integrated in the Linde-Buzo-Grey (LBG) pipeline, this SVBF-LBG scheme yields improved cluster purity on synthetic, image, spectral, and video action data, while retaining the mathematical structure required for downstream analysis.

</details>


### [54] [Enabling Physical AI at the Edge: Hardware-Accelerated Recovery of System Dynamics](https://arxiv.org/abs/2512.23767)
*Bin Xu,Ayan Banerjee,Sandeep Gupta*

Main category: cs.LG

TL;DR: MERINDA是一个FPGA加速的模型恢复框架，用于在资源受限的边缘设备上实现物理AI，相比GPU方案能大幅降低能耗和内存占用，同时保持模型恢复精度。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上实现物理AI需要硬件高效的学习和推理能力。模型恢复是安全可解释监控的关键技术，但现有方法依赖需要迭代求解器的神经ODE，难以在边缘硬件上高效加速。

Method: MERINDA采用硬件友好的架构，结合GRU离散化动态、密集逆ODE层、稀疏驱动dropout和轻量级ODE求解器，替换昂贵的神经ODE组件，实现流式并行计算。

Result: 在四个非线性动态系统基准测试中，MERINDA相比GPU实现：能耗降低114倍（434J vs 49,375J），内存占用减少28倍（214MB vs 6,118MB），训练速度提升1.68倍，同时保持最先进的模型恢复精度。

Conclusion: MERINDA能够将准确、可解释的模型恢复技术带到边缘设备，实现对自主系统的实时监控，使物理AI在资源受限设备上变得实用可行。

Abstract: Physical AI at the edge -- enabling autonomous systems to understand and predict real-world dynamics in real time -- requires hardware-efficient learning and inference. Model recovery (MR), which identifies governing equations from sensor data, is a key primitive for safe and explainable monitoring in mission-critical autonomous systems operating under strict latency, compute, and power constraints. However, state-of-the-art MR methods (e.g., EMILY and PINN+SR) rely on Neural ODE formulations that require iterative solvers and are difficult to accelerate efficiently on edge hardware. We present \textbf{MERINDA} (Model Recovery in Reconfigurable Dynamic Architecture), an FPGA-accelerated MR framework designed to make physical AI practical on resource-constrained devices. MERINDA replaces expensive Neural ODE components with a hardware-friendly formulation that combines (i) GRU-based discretized dynamics, (ii) dense inverse-ODE layers, (iii) sparsity-driven dropout, and (iv) lightweight ODE solvers. The resulting computation is structured for streaming parallelism, enabling critical kernels to be fully parallelized on the FPGA. Across four benchmark nonlinear dynamical systems, MERINDA delivers substantial gains over GPU implementations: \textbf{114$\times$ lower energy} (434~J vs.\ 49{,}375~J), \textbf{28$\times$ smaller memory footprint} (214~MB vs.\ 6{,}118~MB), and \textbf{1.68$\times$ faster training}, while matching state-of-the-art model-recovery accuracy. These results demonstrate that MERINDA can bring accurate, explainable MR to the edge for real-time monitoring of autonomous systems.

</details>


### [55] [Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions](https://arxiv.org/abs/2512.23770)
*Ankit Kanwar,Dominik Wagner,Luke Ong*

Main category: cs.LG

TL;DR: SB-TRPO是一种新的信任区域算法，通过自适应偏置策略更新来满足安全约束，在保证安全的同时寻求奖励改进。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域的强化学习中，现有方法（如拉格朗日法和投影法）要么无法确保接近零的安全违规，要么在面对硬约束时牺牲奖励性能。需要一种能够在严格安全约束下最大化奖励的新方法。

Method: 提出Safety-Biased Trust Region Policy Optimisation (SB-TRPO)，这是一种用于硬约束RL的信任区域算法。它使用成本和奖励的自然策略梯度的凸组合执行信任区域更新，确保每一步都有固定比例的最优成本减少。

Result: 在标准和安全挑战性的Safety Gymnasium任务上的实验表明，SB-TRPO相比最先进方法，始终实现了安全性和有意义任务完成之间的最佳平衡。

Conclusion: SB-TRPO通过自适应偏置策略更新来满足安全约束，同时寻求奖励改进，为硬约束强化学习提供了一种有效的解决方案。

Abstract: Reinforcement learning (RL) in safety-critical domains requires agents to maximise rewards while strictly adhering to safety constraints. Existing approaches, such as Lagrangian and projection-based methods, often either fail to ensure near-zero safety violations or sacrifice reward performance in the face of hard constraints. We propose Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a new trust-region algorithm for hard-constrained RL. SB-TRPO adaptively biases policy updates towards constraint satisfaction while still seeking reward improvement. Concretely, it performs trust-region updates using a convex combination of the natural policy gradients of cost and reward, ensuring a fixed fraction of optimal cost reduction at each step. We provide a theoretical guarantee of local progress towards safety, with reward improvement when gradients are suitably aligned. Experiments on standard and challenging Safety Gymnasium tasks show that SB-TRPO consistently achieves the best balance of safety and meaningful task completion compared to state-of-the-art methods.

</details>


### [56] [FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading](https://arxiv.org/abs/2512.23773)
*Molei Qin,Xinyu Cai,Yewen Li,Haochong Xia,Chuqiao Zong,Shuo Sun,Xinrun Wang,Bo An*

Main category: cs.LG

TL;DR: FineFT提出了一种三阶段集成强化学习框架，专门针对高杠杆期货交易设计，通过选择性更新、盈利能力筛选和VAE引导的风险管理，解决了高杠杆带来的训练不稳定和风险控制问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法主要针对现货市场，无法直接应用于高杠杆期货市场。高杠杆会放大奖励波动导致训练不稳定，且现有方法缺乏对能力边界的自我认知，在遇到新市场状态（如黑天鹅事件）时容易遭受重大损失。

Method: 提出三阶段集成强化学习框架：第一阶段通过集成TD误差选择性更新Q学习器以提高收敛性；第二阶段基于盈利能力筛选Q学习器，并训练VAE识别学习器的能力边界；第三阶段根据训练好的VAE指导，在筛选后的集成策略和保守策略之间选择，以维持盈利能力并降低新市场状态下的风险。

Result: 在高频交易环境、5倍杠杆的加密期货实验中，FineFT在6个金融指标上优于12个最先进的基线方法，风险降低超过40%，同时获得比第二名更优的盈利能力。可视化显示不同智能体专注于不同的市场动态，消融研究证实VAE路由有效降低最大回撤，选择性更新改善收敛和性能。

Conclusion: FineFT成功解决了高杠杆期货交易中的训练不稳定和风险管理问题，通过三阶段集成强化学习框架实现了稳定的训练和有效的风险控制，在保持盈利能力的同时显著降低风险暴露。

Abstract: Futures are contracts obligating the exchange of an asset at a predetermined date and price, notable for their high leverage and liquidity and, therefore, thrive in the Crypto market. RL has been widely applied in various quantitative tasks. However, most methods focus on the spot and could not be directly applied to the futures market with high leverage because of 2 challenges. First, high leverage amplifies reward fluctuations, making training stochastic and difficult to converge. Second, prior works lacked self-awareness of capability boundaries, exposing them to the risk of significant loss when encountering new market state (e.g.,a black swan event like COVID-19). To tackle these challenges, we propose the Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading (FineFT), a novel three-stage ensemble RL framework with stable training and proper risk management. In stage I, ensemble Q learners are selectively updated by ensemble TD errors to improve convergence. In stage II, we filter the Q-learners based on their profitabilities and train VAEs on market states to identify the capability boundaries of the learners. In stage III, we choose from the filtered ensemble and a conservative policy, guided by trained VAEs, to maintain profitability and mitigate risk with new market states. Through extensive experiments on crypto futures in a high-frequency trading environment with high fidelity and 5x leverage, we demonstrate that FineFT outperforms 12 SOTA baselines in 6 financial metrics, reducing risk by more than 40% while achieving superior profitability compared to the runner-up. Visualization of the selective update mechanism shows that different agents specialize in distinct market dynamics, and ablation studies certify routing with VAEs reduces maximum drawdown effectively, and selective update improves convergence and performance.

</details>


### [57] [A Survey on Graph Neural Networks for Fraud Detection in Ride Hailing Platforms](https://arxiv.org/abs/2512.23777)
*Kanishka Hewageegana,Janani Harischandra,Nipuna Senanayake,Gihan Danansuriya,Kavindu Hapuarachchi,Pooja Illangarathne*

Main category: cs.LG

TL;DR: 该研究通过图神经网络（GNNs）调查网约车平台的欺诈检测，比较不同模型的有效性，并关注类别不平衡和欺诈伪装问题。


<details>
  <summary>Details</summary>
Motivation: 随着网约车行业的快速发展，欺诈活动日益增多，需要有效的检测方法来保护平台和用户利益。现有研究在应对类别不平衡和欺诈伪装方面存在不足，需要更先进的图神经网络技术来提升检测效果。

Method: 研究采用图神经网络（GNNs）方法，分析并比较了多种GNN模型在欺诈检测中的应用。重点关注如何通过GNN架构处理类别不平衡问题和识别欺诈伪装行为，同时对现有的GNN异常检测方法进行了系统性的梳理和评估。

Result: 研究识别了GNN在网约车欺诈检测中的显著方法论进展和现有差距。比较了不同GNN模型的有效性，并强调了处理类别不平衡和欺诈伪装问题的重要性。研究为网约车平台提供了实用的欺诈检测参考框架。

Conclusion: 图神经网络在网约车欺诈检测中具有重要应用价值，但需要进一步探索实际应用场景和技术改进。未来研究应关注GNN模型在真实世界环境中的适用性，以及如何应对快速变化的网约车行业欺诈模式，以提升整体欺诈检测策略的有效性。

Abstract: This study investigates fraud detection in ride hailing platforms through Graph Neural Networks (GNNs),focusing on the effectiveness of various models. By analyzing prevalent fraudulent activities, the research highlights and compares the existing work related to fraud detection which can be useful when addressing fraudulent incidents within the online ride hailing platforms. Also, the paper highlights addressing class imbalance and fraudulent camouflage. It also outlines a structured overview of GNN architectures and methodologies applied to anomaly detection, identifying significant methodological progress and gaps. The paper calls for further exploration into real-world applicability and technical improvements to enhance fraud detection strategies in the rapidly evolving ride-hailing industry.

</details>


### [58] [TabMixNN: A Unified Deep Learning Framework for Structural Mixed Effects Modeling on Tabular Data](https://arxiv.org/abs/2512.23787)
*Deniz Akdemir*

Main category: cs.LG

TL;DR: TabMixNN是一个结合混合效应模型与神经网络的PyTorch框架，用于处理层次结构表格数据，支持回归、分类和多任务学习。


<details>
  <summary>Details</summary>
Motivation: 满足处理层次结构数据的需求，同时支持多种结果类型，将经典混合效应模型的可解释性与现代神经网络的灵活性相结合。

Method: 采用模块化三阶段架构：1) 具有变分随机效应和灵活协方差结构的混合效应编码器；2) 包括广义结构方程模型和时空流形网络的主干架构；3) 支持多种结果族的预测头。包含R风格公式接口、DAG约束、SPDE核等创新功能。

Result: 展示了框架在纵向数据分析、基因组预测和时空建模等应用中的灵活性，为研究人员提供了统一接口。

Conclusion: TabMixNN提供了一个统一框架，让研究人员能够在保持经典混合效应模型可解释性和理论基础的同时，利用深度学习的优势。

Abstract: We present TabMixNN, a flexible PyTorch-based deep learning framework that synthesizes classical mixed-effects modeling with modern neural network architectures for tabular data analysis. TabMixNN addresses the growing need for methods that can handle hierarchical data structures while supporting diverse outcome types including regression, classification, and multitask learning. The framework implements a modular three-stage architecture: (1) a mixed-effects encoder with variational random effects and flexible covariance structures, (2) backbone architectures including Generalized Structural Equation Models (GSEM) and spatial-temporal manifold networks, and (3) outcome-specific prediction heads supporting multiple outcome families. Key innovations include an R-style formula interface for accessibility, support for directed acyclic graph (DAG) constraints for causal structure learning, Stochastic Partial Differential Equation (SPDE) kernels for spatial modeling, and comprehensive interpretability tools including SHAP values and variance decomposition. We demonstrate the framework's flexibility through applications to longitudinal data analysis, genomic prediction, and spatial-temporal modeling. TabMixNN provides a unified interface for researchers to leverage deep learning while maintaining the interpretability and theoretical grounding of classical mixed-effects models.

</details>


### [59] [Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems](https://arxiv.org/abs/2512.23809)
*Samaresh Kumar Singh,Joyjit Roy,Martin So*

Main category: cs.LG

TL;DR: 提出ZTA-FL框架，结合TPM认证、SHAP加权聚合和隐私保护对抗训练，在工业物联网入侵检测中实现高精度、强鲁棒性和低通信开销。


<details>
  <summary>Details</summary>
Motivation: 工业物联网关键基础设施面临严重安全威胁，现有联邦学习框架存在拜占庭攻击脆弱性和代理认证不足的问题，需要更强大的安全防护方案。

Method: 提出零信任代理联邦学习框架，包含：1) TPM加密认证；2) SHAP加权聚合算法用于可解释拜占庭检测；3) 隐私保护设备端对抗训练。

Result: 在三个IDS基准测试中，ZTA-FL达到97.8%检测准确率，在30%拜占庭攻击下保持93.2%准确率（优于FLAME 3.1%），对抗鲁棒性89.3%，通信开销降低34%。

Conclusion: ZTA-FL框架为工业物联网入侵检测提供了高安全性、高准确性和高鲁棒性的解决方案，具有理论保证和实际部署价值。

Abstract: Recent attacks on critical infrastructure, including the 2021 Oldsmar water treatment breach and 2023 Danish energy sector compromises, highlight urgent security gaps in Industrial IoT (IIoT) deployments. While Federated Learning (FL) enables privacy-preserving collaborative intrusion detection, existing frameworks remain vulnerable to Byzantine poisoning attacks and lack robust agent authentication. We propose Zero-Trust Agentic Federated Learning (ZTA-FL), a defense in depth framework combining: (1) TPM-based cryptographic attestation achieving less than 0.0000001 false acceptance rate, (2) a novel SHAP-weighted aggregation algorithm providing explainable Byzantine detection under non-IID conditions with theoretical guarantees, and (3) privacy-preserving on-device adversarial training. Comprehensive experiments across three IDS benchmarks (Edge-IIoTset, CIC-IDS2017, UNSW-NB15) demonstrate that ZTA-FL achieves 97.8 percent detection accuracy, 93.2 percent accuracy under 30 percent Byzantine attacks (outperforming FLAME by 3.1 percent, p less than 0.01), and 89.3 percent adversarial robustness while reducing communication overhead by 34 percent. We provide theoretical analysis, failure mode characterization, and release code for reproducibility.

</details>


### [60] [Improved Bounds for Private and Robust Alignment](https://arxiv.org/abs/2512.23816)
*Wenqian Weng,Yi He,Xingyu Zhou*

Main category: cs.LG

TL;DR: 本文从理论角度研究语言模型的私有化和鲁棒对齐，在离线和在线设置中建立了次优性差距的上界，考虑了隐私约束和对抗性污染下的偏好标签，并分析了隐私优先和污染优先两种交互模式。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型对齐中的隐私保护和鲁棒性问题，特别是在偏好标签同时受到隐私约束和对抗性污染的情况下，需要建立理论框架来分析这两种挑战的交互影响。

Method: 使用对数损失和均方损失，在隐私和污染条件下建立统一的收敛保证；分析MLE风格算法在隐私设置下的性能；研究现有离线算法在联合隐私-污染设置下的保证；首次提出私有和鲁棒的在线对齐结果。

Result: 1) 仅隐私设置中，对数损失配合MLE风格算法达到接近最优的收敛率；2) 联合隐私-污染设置中，现有离线算法提供比已知更强的保证；3) 首次获得私有和鲁棒的在线对齐结果；4) 建立了隐私和污染条件下对数损失和均方损失的统一收敛理论。

Conclusion: 本文为语言模型的私有化和鲁棒对齐建立了系统的理论框架，揭示了隐私保护和对抗性污染之间的复杂交互关系，提出的统一收敛理论在学习和统计领域具有广泛适用性。

Abstract: In this paper, we study the private and robust alignment of language models from a theoretical perspective by establishing upper bounds on the suboptimality gap in both offline and online settings. We consider preference labels subject to privacy constraints and/or adversarial corruption, and analyze two distinct interplays between them: privacy-first and corruption-first. For the privacy-only setting, we show that log loss with an MLE-style algorithm achieves near-optimal rates, in contrast to conventional wisdom. For the joint privacy-and-corruption setting, we first demonstrate that existing offline algorithms in fact provide stronger guarantees -- simultaneously in terms of corruption level and privacy parameters -- than previously known, which further yields improved bounds in the corruption-only regime. In addition, we also present the first set of results for private and robust online alignment. Our results are enabled by new uniform convergence guarantees for log loss and square loss under privacy and corruption, which we believe have broad applicability across learning theory and statistics.

</details>


### [61] [MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling](https://arxiv.org/abs/2512.23824)
*Mahdi Karami,Ali Behrouz,Peilin Zhong,Razvan Pascanu,Vahab Mirrokni*

Main category: cs.LG

TL;DR: MS-SSM：一种多尺度状态空间模型框架，通过多分辨率处理解决传统SSM内存有限和多尺度依赖捕获不足的问题，在保持计算效率的同时提升长距离和层次任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统状态空间模型（SSMs）存在有效内存有限的问题，需要更大的状态尺寸来改善记忆能力，同时现有SSMs难以捕获多尺度依赖关系，而多尺度依赖对于时间序列、图像和自然语言中的复杂结构建模至关重要。

Method: 提出多尺度SSM框架，在多个分辨率上表示序列动态，每个分辨率使用专门的状态空间动态进行处理；引入输入依赖的尺度混合器，实现跨分辨率的动态信息融合。

Result: 在Long Range Arena、层次推理、时间序列分类和图像识别等基准测试中，MS-SSM持续优于先前的SSM模型，显著提升了序列建模性能，特别是在长距离和层次任务上。

Conclusion: 多分辨率处理在状态空间架构中具有显著优势，MS-SSM通过捕获细粒度高频模式和粗粒度全局趋势，增强了内存效率和长距离建模能力，同时保持了计算效率。

Abstract: State-space models (SSMs) have recently attention as an efficient alternative to computationally expensive attention-based models for sequence modeling. They rely on linear recurrences to integrate information over time, enabling fast inference, parallelizable training, and control over recurrence stability. However, traditional SSMs often suffer from limited effective memory, requiring larger state sizes for improved recall. Moreover, existing SSMs struggle to capture multi-scale dependencies, which are essential for modeling complex structures in time series, images, and natural language. This paper introduces a multi-scale SSM framework that addresses these limitations by representing sequence dynamics across multiple resolution and processing each resolution with specialized state-space dynamics. By capturing both fine-grained, high-frequency patterns and coarse, global trends, MS-SSM enhances memory efficiency and long-range modeling. We further introduce an input-dependent scale-mixer, enabling dynamic information fusion across resolutions. The proposed approach significantly improves sequence modeling, particularly in long-range and hierarchical tasks, while maintaining computational efficiency. Extensive experiments on benchmarks, including Long Range Arena, hierarchical reasoning, time series classification, and image recognition, demonstrate that MS-SSM consistently outperforms prior SSM-based models, highlighting the benefits of multi-resolution processing in state-space architectures.

</details>


### [62] [Exploiting the Prior of Generative Time Series Imputation](https://arxiv.org/abs/2512.23832)
*YuYang Miao,Chang Li,Zehua Chen*

Main category: cs.LG

TL;DR: Bridge-TS提出了一种基于数据到数据生成过程的时间序列插补方法，通过专家先验和组合先验设计，显著提升了生成式时间序列插补的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式时间序列插补方法（如扩散概率模型和Schrödinger桥模型）使用高斯噪声或线性插值作为先验，这些先验信息不足，导致生成过程负担增加且插补精度有限。需要设计更信息丰富的先验来提高生成式时间序列插补的性能。

Method: 提出Bridge-TS方法，构建数据到数据生成过程，包含两个核心设计：1）专家先验：使用预训练的基于Transformer的专家模块对缺失值进行确定性估计，将结果作为真实目标的先验；2）组合先验：利用多个预训练模型提供不同的估计结果，在数据到数据生成过程中组合它们，实现组合先验到目标的插补过程。

Result: 在ETT、Exchange和Weather等多个基准数据集上的实验表明，Bridge-TS在均方误差和平均绝对误差方面达到了新的插补精度记录，证明了改进先验对生成式时间序列插补的优越性。

Conclusion: 通过设计信息丰富的先验（专家先验和组合先验），Bridge-TS显著提升了生成式时间序列插补的准确性和效率，为时间序列插补任务提供了新的有效解决方案。

Abstract: Time series imputation, i.e., filling the missing values of a time recording, finds various applications in electricity, finance, and weather modelling. Previous methods have introduced generative models such as diffusion probabilistic models and Schrodinger bridge models to conditionally generate the missing values from Gaussian noise or directly from linear interpolation results. However, as their prior is not informative to the ground-truth target, their generation process inevitably suffer increased burden and limited imputation accuracy. In this work, we present Bridge-TS, building a data-to-data generation process for generative time series imputation and exploiting the design of prior with two novel designs. Firstly, we propose expert prior, leveraging a pretrained transformer-based module as an expert to fill the missing values with a deterministic estimation, and then taking the results as the prior of ground truth target. Secondly, we explore compositional priors, utilizing several pretrained models to provide different estimation results, and then combining them in the data-to-data generation process to achieve a compositional priors-to-target imputation process. Experiments conducted on several benchmark datasets such as ETT, Exchange, and Weather show that Bridge-TS reaches a new record of imputation accuracy in terms of mean square error and mean absolute error, demonstrating the superiority of improving prior for generative time series imputation.

</details>


### [63] [Trellis: Learning to Compress Key-Value Memory in Attention Models](https://arxiv.org/abs/2512.23852)
*Mahdi Karami,Ali Behrouz,Praneeth Kacham,Vahab Mirrokni*

Main category: cs.LG

TL;DR: Trellis是一种具有有限内存的新型Transformer架构，通过动态压缩KV缓存来解决传统Transformer的二次计算复杂度和不断增长的KV缓存问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer存在两个主要问题：1）二次计算复杂度；2）注意力机制中不断增长的键值（KV）缓存。这些问题限制了模型处理长序列的能力，特别是在长上下文应用中。

Method: Trellis用固定大小的内存替换标准KV缓存，并训练一个两遍循环压缩机制来存储新的键值到内存中。该方法利用带有遗忘门的在线梯度下降过程，使压缩内存能够递归更新，同时学习在测试时保留来自传入令牌的重要上下文信息。

Result: 在语言建模、常识推理、召回密集型任务和时间序列上的广泛实验表明，该架构优于强基线模型。特别值得注意的是，随着序列长度的增加，其性能提升更加明显，突显了其在长上下文应用中的潜力。

Conclusion: Trellis通过引入有限内存和动态KV压缩机制，有效解决了传统Transformer的内存和计算效率问题，为长序列处理提供了有前景的解决方案。

Abstract: Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications.

</details>


### [64] [Flow Matching Neural Processes](https://arxiv.org/abs/2512.23853)
*Hussen Abu Hamad,Dan Rosenbaum*

Main category: cs.LG

TL;DR: 提出基于流匹配（flow matching）的新型神经过程模型，通过ODE求解器实现条件分布采样，在精度和运行时间之间提供可控权衡，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 神经过程模型能够直接从数据中学习随机过程并用于推理和采样，但现有方法在实现复杂性和采样效率方面存在局限。本文旨在开发更简单、更高效的神经过程模型。

Method: 基于流匹配生成建模范式构建新型神经过程模型，采用ODE求解器进行条件分布采样，无需辅助条件方法，通过调整ODE求解步数控制精度与运行时间的权衡。

Result: 在合成1D高斯过程数据、2D图像和真实世界天气数据等多个基准测试中，该模型优于先前最先进的神经过程方法，实现了更好的性能表现。

Conclusion: 提出的基于流匹配的神经过程模型简单易实现，能够高效进行条件分布采样，在精度和计算效率之间提供灵活权衡，为神经过程建模提供了新的有效方法。

Abstract: Neural processes (NPs) are a class of models that learn stochastic processes directly from data and can be used for inference, sampling and conditional sampling. We introduce a new NP model based on flow matching, a generative modeling paradigm that has demonstrated strong performance on various data modalities. Following the NP training framework, the model provides amortized predictions of conditional distributions over any arbitrary points in the data. Compared to previous NP models, our model is simple to implement and can be used to sample from conditional distributions using an ODE solver, without requiring auxiliary conditioning methods. In addition, the model provides a controllable tradeoff between accuracy and running time via the number of steps in the ODE solver. We show that our model outperforms previous state-of-the-art neural process methods on various benchmarks including synthetic 1D Gaussian processes data, 2D images, and real-world weather data.

</details>


### [65] [Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding](https://arxiv.org/abs/2512.23858)
*Yue Guan,Changming Yu,Shihan Fang,Weiming Hu,Zaifeng Pan,Zheng Wang,Zihan Liu,Yangjie Zhou,Yufei Ding,Minyi Guo,Jingwen Leng*

Main category: cs.LG

TL;DR: Yggdrasil是一个协同设计的系统，通过上下文感知的树形草稿和编译器友好的执行，实现延迟最优的推测解码，相比现有基线最高可达3.98倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码系统由于动态推测与静态运行时假设不匹配，导致性能不理想。需要设计一个能够实现延迟最优推测解码的系统。

Method: Yggdrasil采用协同设计方法：1）引入等增长树结构以兼容静态图；2）基于延迟感知的优化目标进行草稿选择；3）采用阶段化调度减少开销；4）支持未经修改的LLM。

Result: Yggdrasil在多种硬件设置下，相比最先进的基线实现了最高3.98倍的加速，支持未经修改的LLM，并实现了延迟最优的推测解码。

Conclusion: Yggdrasil通过协同设计方法解决了现有推测解码系统的性能瓶颈，实现了延迟最优的推测解码，显著提升了LLM推理效率。

Abstract: Speculative decoding improves LLM inference by generating and verifying multiple tokens in parallel, but existing systems suffer from suboptimal performance due to a mismatch between dynamic speculation and static runtime assumptions. We present Yggdrasil, a co-designed system that enables latency-optimal speculative decoding through context-aware tree drafting and compiler-friendly execution. Yggdrasil introduces an equal-growth tree structure for static graph compatibility, a latency-aware optimization objective for draft selection, and stage-based scheduling to reduce overhead. Yggdrasil supports unmodified LLMs and achieves up to $3.98\times$ speedup over state-of-the-art baselines across multiple hardware setups.

</details>


### [66] [Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining](https://arxiv.org/abs/2512.23862)
*Ruizhe Huang,Kexuan Zhang,Yihao Fang,Baifeng Yu*

Main category: cs.LG

TL;DR: 本文研究了小型语言模型的小规模预训练，重点关注Infini-attention架构，该架构通过压缩记忆机制提升紧凑模型的长上下文外推能力，在300M参数的LLaMA模型上验证了训练稳定性，并在长上下文检索任务中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索小型语言模型的小规模预训练，以在有限数据和计算资源下实现高效使用，提高低资源环境下的可访问性并降低成本。同时，针对紧凑模型的长上下文外推能力不足的问题，研究如何通过Infini-attention架构来增强这一能力。

Method: 采用Infini-attention架构，该架构从过去的文本段构建压缩记忆，同时保留局部注意力机制。研究使用300M参数的LLaMA模型进行预训练，并进行实证研究，特别关注平衡因子对模型性能的影响，以及重复记忆压缩对长序列检索准确性的影响。

Result: Infini-attention模型表现出训练稳定性，并在长上下文检索任务中优于基线模型。研究发现平衡因子是模型性能的关键因素，同时观察到随着长序列中重复记忆压缩，检索准确性会下降。尽管如此，Infini-attention仍能有效补偿小型语言模型参数有限的不足，在16,384个token的上下文长度下，虽然性能有所下降，但仍比基线模型准确率高31%。

Conclusion: 研究结果表明，在小型语言模型中实现稳健的长上下文能力受益于像Infini-attention这样的架构化记忆机制。这种架构能够有效提升紧凑模型的长上下文处理能力，为资源受限环境下的语言模型应用提供了有价值的解决方案。

Abstract: This study investigates small-scale pretraining for Small Language Models (SLMs) to enable efficient use of limited data and compute, improve accessibility in low-resource settings and reduce costs. To enhance long-context extrapolation in compact models, we focus on Infini-attention, which builds a compressed memory from past segments while preserving local attention. In our work, we conduct an empirical study using 300M-parameter LLaMA models pretrained with Infini-attention. The model demonstrates training stability and outperforms the baseline in long-context retrieval. We identify the balance factor as a key part of the model performance, and we found that retrieval accuracy drops with repeated memory compressions over long sequences. Even so, Infini-attention still effectively compensates for the SLM's limited parameters. Particularly, despite performance degradation at a 16,384-token context, the Infini-attention model achieves up to 31% higher accuracy than the baseline. Our findings suggest that achieving robust long-context capability in SLMs benefits from architectural memory like Infini-attention.

</details>


### [67] [Max-Entropy Reinforcement Learning with Flow Matching and A Case Study on LQR](https://arxiv.org/abs/2512.23870)
*Yuyang Zhang,Yang Hu,Bo Dai,Na Li*

Main category: cs.LG

TL;DR: 提出一种基于流模型的SAC变体算法，使用重要性采样流匹配技术更新策略，在最大熵线性二次调节器问题上验证了学习最优动作分布的能力。


<details>
  <summary>Details</summary>
Motivation: 传统SAC算法中基于能量的策略通常使用简单策略类别进行近似以提高效率，但这牺牲了表达能力和鲁棒性。需要一种既能保持表达能力又能高效学习的策略参数化方法。

Method: 提出基于流模型的SAC算法变体：1) 使用流模型参数化策略，利用其丰富的表达能力；2) 采用瞬时变量变换技术评估流策略；3) 提出重要性采样流匹配在线变体更新策略，仅需从用户指定的采样分布而非未知目标分布中采样。

Result: 开发了ISFM的理论分析，阐明了不同采样分布选择对学习效率的影响。在最大熵线性二次调节器问题上进行案例研究，证明所提算法能够学习最优动作分布。

Conclusion: 基于流模型的SAC算法变体能够有效结合流模型的表达能力和重要性采样流匹配的高效更新机制，在保持策略表达能力的同时实现高效学习，特别适用于需要丰富策略表示的最大熵强化学习问题。

Abstract: Soft actor-critic (SAC) is a popular algorithm for max-entropy reinforcement learning. In practice, the energy-based policies in SAC are often approximated using simple policy classes for efficiency, sacrificing the expressiveness and robustness. In this paper, we propose a variant of the SAC algorithm that parameterizes the policy with flow-based models, leveraging their rich expressiveness. In the algorithm, we evaluate the flow-based policy utilizing the instantaneous change-of-variable technique and update the policy with an online variant of flow matching developed in this paper. This online variant, termed importance sampling flow matching (ISFM), enables policy update with only samples from a user-specified sampling distribution rather than the unknown target distribution. We develop a theoretical analysis of ISFM, characterizing how different choices of sampling distributions affect the learning efficiency. Finally, we conduct a case study of our algorithm on the max-entropy linear quadratic regulator problems, demonstrating that the proposed algorithm learns the optimal action distribution.

</details>


### [68] [Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City](https://arxiv.org/abs/2512.23898)
*Tin Hoang*

Main category: cs.LG

TL;DR: 该研究对10种深度学习架构进行全球水平辐照度(GHI)短期预测基准测试，发现Transformer表现最佳(R²=0.9696)，并通过知识蒸馏压缩模型23.5%同时降低误差，为边缘设备部署提供可行方案。


<details>
  <summary>Details</summary>
Motivation: 可靠的全球水平辐照度(GHI)预测对于缓解太阳能并网波动至关重要，需要评估不同深度学习架构在短期预测中的性能，特别是新兴架构与传统方法的对比。

Method: 使用NSRDB卫星数据(2011-2020)对10种深度学习架构进行1小时前GHI预测基准测试，包括LSTM、TCN、Transformer、Informer、iTransformer、TSMixer和Mamba等，并采用SHAP分析解释模型的时间推理模式。

Result: Transformer架构表现最佳，R²达到0.9696；SHAP分析显示Transformer具有"近期偏好"，而Mamba则利用24小时周期性依赖；知识蒸馏可将Transformer压缩23.5%同时降低MAE至23.78 W/m²。

Conclusion: Transformer在GHI短期预测中表现最优，知识蒸馏技术可有效压缩高性能模型并降低误差，为资源受限的边缘设备部署复杂预测模型提供了可行路径。

Abstract: Reliable forecasting of Global Horizontal Irradiance (GHI) is essential for mitigating the variability of solar energy in power grids. This study presents a comprehensive benchmark of ten deep learning architectures for short-term (1-hour ahead) GHI time series forecasting in Ho Chi Minh City, leveraging high-resolution NSRDB satellite data (2011-2020) to compare established baselines (e.g. LSTM, TCN) against emerging state-of-the-art architectures, including Transformer, Informer, iTransformer, TSMixer, and Mamba. Experimental results identify the Transformer as the superior architecture, achieving the highest predictive accuracy with an R^2 of 0.9696. The study further utilizes SHAP analysis to contrast the temporal reasoning of these architectures, revealing that Transformers exhibit a strong "recency bias" focused on immediate atmospheric conditions, whereas Mamba explicitly leverages 24-hour periodic dependencies to inform predictions. Furthermore, we demonstrate that Knowledge Distillation can compress the high-performance Transformer by 23.5% while surprisingly reducing error (MAE: 23.78 W/m^2), offering a proven pathway for deploying sophisticated, low-latency forecasting on resource-constrained edge devices.

</details>


### [69] [Rethinking Dense Linear Transformations: Stagewise Pairwise Mixing (SPM) for Near-Linear Training in Neural Networks](https://arxiv.org/abs/2512.23905)
*Peter Farag*

Main category: cs.LG

TL;DR: SPM是一种结构化线性算子，通过稀疏成对混合阶段替代密集矩阵，实现O(nL)时间和参数复杂度，作为密集线性层的即插即用替代方案


<details>
  <summary>Details</summary>
Motivation: 密集线性层在现代机器学习模型中占据主要计算和参数成本，但其二次复杂度与学习表示的组合结构不匹配

Method: 引入阶段式成对混合器(SPM)，用稀疏成对混合阶段的组合替代密集矩阵，提供正交保范旋转和通用2×2混合两种参数化

Result: 在结构化学习问题上显著降低计算成本并提高准确性，在真实基准测试中保持竞争力

Conclusion: SPM不仅提供计算优势，其阶段式结构还引入了显式组合归纳偏置，当与任务结构对齐时能约束模型容量并改善泛化

Abstract: Dense linear layers are a dominant source of computational and parametric cost in modern machine learning models, despite their quadratic complexity and often being misaligned with the compositional structure of learned representations. We introduce Stagewise Pairwise Mixers (SPM), a structured linear operator that replaces dense matrices with a composition of sparse pairwise-mixing stages. An SPM layer implements a global linear transformation in $O(nL)$ time with $O(nL)$ parameters, where $L$ is typically constant or $log_2n$, and admits exact closed-form forward and backward computations. SPM is designed as a drop-in replacement for dense linear layers in feedforward networks, recurrent architectures, attention mechanisms, etc. We derive complete forward and backward expressions for two parameterizations: an orthogonal norm-preserving rotation-based variant and a fully general $2 \times 2$ mixing variant. Beyond computational savings, the stagewise structure of SPM induces an explicit compositional inductive bias that constrains model capacity and improves generalization when aligned with task structure. We present proof-of-concept experiments demonstrating substantial reductions in wall-clock cost and improved accuracy on structured learning problems, while retaining competitive performance on real-world benchmarks.

</details>


### [70] [Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias](https://arxiv.org/abs/2512.23916)
*Xia Chen*

Main category: cs.LG

TL;DR: 论文提出物理约束（如代谢限制）不是限制而是时间归纳偏置，能促进泛化。通过相空间分析发现膨胀动态放大噪声，适当耗散动态压缩相空间并与网络谱偏置对齐，迫使提取不变特征。这种条件可通过外部输入编码或内部时间动态实现，需要能进行时间积分的架构。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习注重无约束优化，但生物系统在严格代谢约束下运行。作者认为这些物理约束不是限制，而是塑造动态的时间归纳偏置，能够促进泛化能力。

Method: 通过信号传播的相空间分析，揭示膨胀动态与耗散动态的基本不对称性。提出可通过外部输入编码或网络自身时间动态施加约束条件，需要具备时间积分能力的架构来解码诱导的不变特征。

Result: 在监督分类、无监督重建和零样本强化学习的综合评估中，发现"过渡"临界状态能最大化泛化能力。膨胀动态放大噪声，而适当耗散动态压缩相空间并与网络谱偏置对齐，迫使抽象不变特征。

Conclusion: 动态约束是一类独特的归纳偏置，表明稳健AI开发不仅需要扩展和移除限制，还需要计算性地掌握自然促进泛化的时间特性。

Abstract: Conventional deep learning prioritizes unconstrained optimization, yet biological systems operate under strict metabolic constraints. We propose that these physical constraints shape dynamics to function not as limitations, but as a temporal inductive bias that breeds generalization. Through a phase-space analysis of signal propagation, we reveal a fundamental asymmetry: expansive dynamics amplify noise, whereas proper dissipative dynamics compress phase space that aligns with the network's spectral bias, compelling the abstraction of invariant features. This condition can be imposed externally via input encoding, or intrinsically through the network's own temporal dynamics. Both pathways require architectures capable of temporal integration and proper constraints to decode induced invariants, whereas static architectures fail to capitalize on temporal structure. Through comprehensive evaluations across supervised classification, unsupervised reconstruction, and zero-shot reinforcement learning, we demonstrate that a critical "transition" regime maximizes generalization capability. These findings establish dynamical constraints as a distinct class of inductive bias, suggesting that robust AI development requires not only scaling and removing limitations, but computationally mastering the temporal characteristics that naturally promote generalization.

</details>


### [71] [Interactive Machine Learning: From Theory to Scale](https://arxiv.org/abs/2512.23924)
*Yinglun Zhu*

Main category: cs.LG

TL;DR: 该论文研究交互式机器学习，开发了在主动学习、序列决策和大动作空间情境赌博问题中的新算法，实现了统计最优和计算高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习依赖大量标注数据或在线交互，但在实践中获取高质量标签和通过试错做决策成本高、耗时长且风险大，特别是在大规模或高风险场景中。需要研究交互式机器学习来主动引导信息收集和决策过程。

Method: 研究交互式机器学习，开发新的算法原理和理论基础，重点关注三个维度：1) 噪声数据和丰富模型类的主动学习；2) 大动作空间的序列决策；3) 部分反馈下的模型选择。采用理论分析和算法设计相结合的方法。

Result: 取得了多项突破性成果：1) 首个无需低噪声假设就能实现指数级标签节省的计算高效主动学习算法；2) 首个保证与动作空间大小无关的高效通用情境赌博算法；3) 首次对序列决策中模型选择的基本成本进行了紧致刻画。

Conclusion: 该论文通过开发统计最优且计算高效的算法，推进了交互式机器学习的理论基础，同时为在大规模现实场景中部署交互式学习方法提供了原则性指导。

Abstract: Machine learning has achieved remarkable success across a wide range of applications, yet many of its most effective methods rely on access to large amounts of labeled data or extensive online interaction. In practice, acquiring high-quality labels and making decisions through trial-and-error can be expensive, time-consuming, or risky, particularly in large-scale or high-stakes settings. This dissertation studies interactive machine learning, in which the learner actively influences how information is collected or which actions are taken, using past observations to guide future interactions. We develop new algorithmic principles and establish fundamental limits for interactive learning along three dimensions: active learning with noisy data and rich model classes, sequential decision making with large action spaces, and model selection under partial feedback. Our results include the first computationally efficient active learning algorithms achieving exponential label savings without low-noise assumptions; the first efficient, general-purpose contextual bandit algorithms whose guarantees are independent of the size of the action space; and the first tight characterizations of the fundamental cost of model selection in sequential decision making. Overall, this dissertation advances the theoretical foundations of interactive learning by developing algorithms that are statistically optimal and computationally efficient, while also providing principled guidance for deploying interactive learning methods in large-scale, real-world settings.

</details>


### [72] [Improved Balanced Classification with Theoretically Grounded Loss Functions](https://arxiv.org/abs/2512.23947)
*Corinna Cortes,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文研究了类别不平衡下的分类问题，提出了两种先进的替代损失函数家族：广义对数调整损失（GLA）和广义类别感知加权损失（GCA），并进行了全面的理论分析，证明GCA在高度不平衡设置中具有更强的理论保证。


<details>
  <summary>Details</summary>
Motivation: 平衡损失在类别不平衡的多类别分类中被广泛采用，通过赋予所有类别同等重要性来促进公平性并确保少数类别不被忽视。然而，直接最小化平衡分类损失通常是难以处理的，因此设计有效的替代损失成为一个核心问题。

Method: 提出了两种先进的替代损失函数家族：1）广义对数调整损失（GLA），将对数调整损失推广到更广泛的通用交叉熵损失家族；2）广义类别感知加权损失（GCA），通过引入类别相关的置信度边界并将标准类别加权损失扩展到通用交叉熵家族。

Result: 理论分析表明：GLA损失是贝叶斯一致的，但仅对完整（无界）假设集是H一致的，其H一致性边界与最小类别概率成反比，至少按1/p_min缩放；而GCA损失对任何有界或完整的假设集都是H一致的，其H一致性边界按1/√p_min缩放，在高度不平衡设置中提供更强的理论保证。实验结果显示，两种损失在类别不平衡设置中都能显著优于简单的类别加权损失和对数调整损失。

Conclusion: GCA损失在高度不平衡设置中具有更有利的理论保证，而GLA损失在常见基准测试中表现略好。两种损失函数家族都为类别不平衡分类问题提供了有效的解决方案，其中GCA在理论保证方面更优，特别是在极端不平衡情况下。

Abstract: The balanced loss is a widely adopted objective for multi-class classification under class imbalance. By assigning equal importance to all classes, regardless of their frequency, it promotes fairness and ensures that minority classes are not overlooked. However, directly minimizing the balanced classification loss is typically intractable, which makes the design of effective surrogate losses a central question. This paper introduces and studies two advanced surrogate loss families: Generalized Logit-Adjusted (GLA) loss functions and Generalized Class-Aware weighted (GCA) losses. GLA losses generalize Logit-Adjusted losses, which shift logits based on class priors, to the broader general cross-entropy loss family. GCA loss functions extend the standard class-weighted losses, which scale losses inversely by class frequency, by incorporating class-dependent confidence margins and extending them to the general cross-entropy family. We present a comprehensive theoretical analysis of consistency for both loss families. We show that GLA losses are Bayes-consistent, but only $H$-consistent for complete (i.e., unbounded) hypothesis sets. Moreover, their $H$-consistency bounds depend inversely on the minimum class probability, scaling at least as $1/\mathsf p_{\min}$. In contrast, GCA losses are $H$-consistent for any hypothesis set that is bounded or complete, with $H$-consistency bounds that scale more favorably as $1/\sqrt{\mathsf p_{\min}}$, offering significantly stronger theoretical guarantees in imbalanced settings. We report the results of experiments demonstrating that, empirically, both the GCA losses with calibrated class-dependent confidence margins and GLA losses can greatly outperform straightforward class-weighted losses as well as the LA losses. GLA generally performs slightly better in common benchmarks, whereas GCA exhibits a slight edge in highly imbalanced settings.

</details>


### [73] [DivQAT: Enhancing Robustness of Quantized Convolutional Neural Networks against Model Extraction Attacks](https://arxiv.org/abs/2512.23948)
*Kacem Khaled,Felipe Gohring de Magalhães,Gabriela Nicolescu*

Main category: cs.LG

TL;DR: DivQAT是一种基于量化感知训练的新型算法，通过修改量化过程将模型提取防御集成到训练中，增强量化CNN对提取攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量化CNN容易受到提取攻击，存在知识产权盗窃风险。现有防御方法存在局限性：通常作为训练后附加措施而非集成到模型设计中，计算成本高，且对边缘设备实现不切实际，也不适用于量化模型。

Method: 提出DivQAT算法，基于量化感知训练修改量化过程，将模型提取防御直接集成到训练过程中。这是首个通过修改量化过程将防御机制融入训练的技术。

Result: 在基准视觉数据集上的实证验证表明，该技术能有效防御模型提取攻击，同时不损害模型准确性。与传统QAT相比，结合其他防御机制时能提高其有效性。

Conclusion: DivQAT为量化CNN提供了一种有效的模型提取防御方案，将防御机制集成到训练过程中，解决了现有方法的局限性，在保护模型知识产权方面具有实际应用价值。

Abstract: Convolutional Neural Networks (CNNs) and their quantized counterparts are vulnerable to extraction attacks, posing a significant threat of IP theft. Yet, the robustness of quantized models against these attacks is little studied compared to large models. Previous defenses propose to inject calculated noise into the prediction probabilities. However, these defenses are limited since they are not incorporated during the model design and are only added as an afterthought after training. Additionally, most defense techniques are computationally expensive and often have unrealistic assumptions about the victim model that are not feasible in edge device implementations and do not apply to quantized models. In this paper, we propose DivQAT, a novel algorithm to train quantized CNNs based on Quantization Aware Training (QAT) aiming to enhance their robustness against extraction attacks. To the best of our knowledge, our technique is the first to modify the quantization process to integrate a model extraction defense into the training process. Through empirical validation on benchmark vision datasets, we demonstrate the efficacy of our technique in defending against model extraction attacks without compromising model accuracy. Furthermore, combining our quantization technique with other defense mechanisms improves their effectiveness compared to traditional QAT.

</details>


### [74] [Physics-informed Graph Neural Networks for Operational Flood Modeling](https://arxiv.org/abs/2512.23964)
*Carlo Malapad Acosta,Herath Mudiyanselage Viraj Vidura Herath,Jia Yu Lim,Abhishek Saha,Sanka Rasnayaka,Lucy Marshall*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的洪水图神经网络架构DUALFloodGNN，通过在全局和局部尺度嵌入物理约束，联合预测节点水体积和边缘水流，采用多步损失和动态课程学习提升自回归推理性能，相比现有方法在保持高效计算的同时显著提升了水文变量预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理的数值洪水模型虽然准确但计算成本高，难以满足需要快速预测的操作性场景需求。图神经网络(GNNs)能够处理非结构化空间域，兼具速度和准确性，且易于与物理信息方法结合提升可解释性。

Method: 提出了DUALFloodGNN架构，通过显式损失项在全局和局部尺度嵌入物理约束，采用共享消息传递框架联合预测节点水体积和边缘水流。训练时使用多步损失增强动态课程学习，以提升自回归推理性能。

Result: 相比标准GNN架构和最先进的GNN洪水模型，DUALFloodGNN在预测多个水文变量方面取得了显著改进，同时保持了高计算效率。

Conclusion: DUALFloodGNN通过物理约束嵌入和多步训练策略，为洪水预测提供了一种既准确又高效的解决方案，模型已开源供研究使用。

Abstract: Flood models inform strategic disaster management by simulating the spatiotemporal hydrodynamics of flooding. While physics-based numerical flood models are accurate, their substantial computational cost limits their use in operational settings where rapid predictions are essential. Models designed with graph neural networks (GNNs) provide both speed and accuracy while having the ability to process unstructured spatial domains. Given its flexible input and architecture, GNNs can be leveraged alongside physics-informed techniques with ease, significantly improving interpretability. This study introduces a novel flood GNN architecture, DUALFloodGNN, which embeds physical constraints at both global and local scales through explicit loss terms. The model jointly predicts water volume at nodes and flow along edges through a shared message-passing framework. To improve performance for autoregressive inference, model training is conducted with a multi-step loss enhanced with dynamic curriculum learning. Compared with standard GNN architectures and state-of-the-art GNN flood models, DUALFloodGNN achieves substantial improvements in predicting multiple hydrologic variables while maintaining high computational efficiency. The model is open-sourced at https://github.com/acostacos/dual_flood_gnn.

</details>


### [75] [Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing](https://arxiv.org/abs/2512.23977)
*Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: DataFlow是一个用于在无限时间序列数据上构建、测试和部署高性能机器学习系统的计算框架，通过统一的执行模型解决传统批处理到流式生产转换中的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数据科学工作流假设有限数据集，从批处理原型迁移到流式生产系统时需要大量重新实现，这导致因果性违反、批处理边界伪影和实时故障难以复现等问题。

Method: 基于具有时间点幂等性的有向无环图统一执行模型：任何时间t的输出仅依赖于t之前的固定长度上下文窗口。通过自动跟踪所有转换的知识时间来确保严格因果性，支持跨时间和特征维度的灵活分块。

Result: 在批处理模式下开发的模型无需代码更改即可在流式生产中完全一致地执行，消除了未来窥探错误，支持不同频率和内存配置，并在金融交易、物联网、欺诈检测和实时分析等领域展示了有效性。

Conclusion: DataFlow通过统一的执行模型解决了批处理到流式生产转换中的关键问题，确保了因果一致性、可重现性和部署效率，为时间序列机器学习系统提供了可靠的基础框架。

Abstract: We present DataFlow, a computational framework for building, testing, and deploying high-performance machine learning systems on unbounded time-series data. Traditional data science workflows assume finite datasets and require substantial reimplementation when moving from batch prototypes to streaming production systems. This gap introduces causality violations, batch boundary artifacts, and poor reproducibility of real-time failures.
  DataFlow resolves these issues through a unified execution model based on directed acyclic graphs (DAGs) with point-in-time idempotency: outputs at any time t depend only on a fixed-length context window preceding t. This guarantee ensures that models developed in batch mode execute identically in streaming production without code changes. The framework enforces strict causality by automatically tracking knowledge time across all transformations, eliminating future-peeking bugs.
  DataFlow supports flexible tiling across temporal and feature dimensions, allowing the same model to operate at different frequencies and memory profiles via configuration alone. It integrates natively with the Python data science stack and provides fit/predict semantics for online learning, caching and incremental computation, and automatic parallelization through DAG-based scheduling. We demonstrate its effectiveness across domains including financial trading, IoT, fraud detection, and real-time analytics.

</details>


### [76] [Assured Autonomy: How Operations Research Powers and Orchestrates Generative AI Systems](https://arxiv.org/abs/2512.23978)
*Tinglong Dai,David Simchi-Levi,Michelle Xiao Wu,Yao Xie*

Main category: cs.LG

TL;DR: GenAI正从对话助手转向自主决策系统，这产生了自主性悖论：自主性越高，越需要形式化结构、明确约束和尾部风险控制。论文提出了基于运筹学的保证自主性框架，包括基于流的生成模型和对抗鲁棒性方法。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能正从对话助手转向自主决策系统，这种转变带来了自主性悖论：系统自主性越高，越需要形式化结构、明确约束和尾部风险控制。随机生成模型在操作领域可能很脆弱，需要可验证的可行性、对分布偏移的鲁棒性以及高后果场景下的压力测试机制。

Method: 提出了基于运筹学的保证自主性概念框架，包含两种互补方法：1) 基于流的生成模型，将生成视为由常微分方程描述的确定性传输，支持可审计性、约束感知生成，并与最优传输、鲁棒优化和序列决策控制连接；2) 通过对抗鲁棒性视角制定操作安全性，在不确定性或模糊集内评估决策规则对最坏情况扰动的鲁棒性。

Result: 该框架阐明了自主性增加如何改变运筹学的角色：从求解器到护栏再到系统架构师，负责控制逻辑、激励协议、监控机制和安全边界。这些要素定义了在安全关键、可靠性敏感的操作领域中保证自主性的研究议程。

Conclusion: 随着GenAI系统获得更大操作自主性，它们需要更强的形式化结构、明确约束和尾部风险控制。基于运筹学的保证自主性框架为解决这一挑战提供了概念基础，将生成式人工智能与可验证的可行性、鲁棒性和压力测试机制相结合，为安全关键领域的自主系统设计指明了方向。

Abstract: Generative artificial intelligence (GenAI) is shifting from conversational assistants toward agentic systems -- autonomous decision-making systems that sense, decide, and act within operational workflows. This shift creates an autonomy paradox: as GenAI systems are granted greater operational autonomy, they should, by design, embody more formal structure, more explicit constraints, and stronger tail-risk discipline. We argue stochastic generative models can be fragile in operational domains unless paired with mechanisms that provide verifiable feasibility, robustness to distribution shift, and stress testing under high-consequence scenarios. To address this challenge, we develop a conceptual framework for assured autonomy grounded in operations research (OR), built on two complementary approaches. First, flow-based generative models frame generation as deterministic transport characterized by an ordinary differential equation, enabling auditability, constraint-aware generation, and connections to optimal transport, robust optimization, and sequential decision control. Second, operational safety is formulated through an adversarial robustness lens: decision rules are evaluated against worst-case perturbations within uncertainty or ambiguity sets, making unmodeled risks part of the design. This framework clarifies how increasing autonomy shifts OR's role from solver to guardrail to system architect, with responsibility for control logic, incentive protocols, monitoring regimes, and safety boundaries. These elements define a research agenda for assured autonomy in safety-critical, reliability-sensitive operational domains.

</details>


### [77] [Information-Theoretic Quality Metric of Low-Dimensional Embeddings](https://arxiv.org/abs/2512.23981)
*Sebastián Gutiérrez-Bernal,Hector Medel Cobaxin,Abiel Galindo González*

Main category: cs.LG

TL;DR: 本文从信息论角度提出ERPM度量，评估低维嵌入的信息保留质量，发现距离、几何和谱度量之间存在显著差异，ERPM能识别信息损失严重的邻域。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标（如应力、基于排名的邻域准则或局部Procrustes）主要量化距离或局部几何的失真，但无法直接评估高维数据投影到低维空间时保留了多少信息。需要从信息论角度直接评估嵌入质量。

Method: 引入熵秩保持度量（ERPM），这是一种基于邻域矩阵奇异值谱的香农熵和稳定秩的局部度量，量化原始表示与其降维投影之间的不确定性变化，提供邻域级指标和全局汇总统计。

Result: 通过金融时间序列和文献中常见流形验证，发现基于距离的准则与几何和谱度量相关性很低，而ERPM与局部Procrustes显示出强平均相关性，但在局部区域存在显著差异。ERPM能识别信息损失严重的邻域。

Conclusion: ERPM通过识别信息损失严重的邻域，补充了现有度量方法，特别是在构建早期预警指标等对信息敏感的应用中，能够实现更全面的嵌入评估。

Abstract: In this work we study the quality of low-dimensional embeddings from an explicitly information-theoretic perspective. We begin by noting that classical evaluation metrics such as stress, rank-based neighborhood criteria, or Local Procrustes quantify distortions in distances or in local geometries, but do not directly assess how much information is preserved when projecting high-dimensional data onto a lower-dimensional space. To address this limitation, we introduce the Entropy Rank Preservation Measure (ERPM), a local metric based on the Shannon entropy of the singular-value spectrum of neighborhood matrices and on the stable rank, which quantifies changes in uncertainty between the original representation and its reduced projection, providing neighborhood-level indicators and a global summary statistic. To validate the results of the metric, we compare its outcomes with the Mean Relative Rank Error (MRRE), which is distance-based, and with Local Procrustes, which is based on geometric properties, using a financial time series and a manifold commonly studied in the literature. We observe that distance-based criteria exhibit very low correlation with geometric and spectral measures, while ERPM and Local Procrustes show strong average correlation but display significant discrepancies in local regimes, leading to the conclusion that ERPM complements existing metrics by identifying neighborhoods with severe information loss, thereby enabling a more comprehensive assessment of embeddings, particularly in information-sensitive applications such as the construction of early-warning indicators.

</details>


### [78] [Tracing the Heart's Pathways: ECG Representation Learning from a Cardiac Conduction Perspective](https://arxiv.org/abs/2512.24002)
*Tan Pan,Yixuan Sun,Chen Jiang,Qiong Gao,Rui Sun,Xingmeng Zhang,Zhenqi Yang,Limei Han,Yixiu Liang,Yuan Cheng,Kaiyu Guo*

Main category: cs.LG

TL;DR: CLEAR-HUG是一个两阶段ECG自监督学习框架，通过捕捉心脏传导过程中的细微变化并遵循ECG诊断指南，显著提升了心电图分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有ECG自监督学习方法存在两个主要问题：1）只关注跨导联和心跳的一致模式，忽略了心脏传导过程中固有的心跳差异，而这些细微变化携带独特的生理特征；2）ECG表示学习应与ECG诊断指南（从单个心跳到单导联再到导联组合的顺序逻辑）对齐，但预训练模型在下游任务中往往忽视这一临床工作流程。

Method: 提出两阶段框架CLEAR-HUG：第一阶段是自监督学习模型CLEAR（Conduction-LEAd Reconstructor），采用简单有效的稀疏注意力机制，将每个心跳视为独立实体进行信号重建，捕捉心跳间的特定变化和一般共性；第二阶段是用于疾病诊断的分层导联统一组头HUG（Hierarchical lead-Unified Group head），模拟临床工作流程。

Result: 在六个任务上的实验结果显示，CLEAR-HUG实现了6.84%的性能提升，验证了其有效性，表明该方法能够增强心脏传导表示并使其模式与专家诊断指南对齐。

Conclusion: CLEAR-HUG通过捕捉心脏传导的细微变化并遵循ECG诊断指南，显著提升了心电图表示学习能力，为心脏诊断提供了更符合临床实践的解决方案。

Abstract: The multi-lead electrocardiogram (ECG) stands as a cornerstone of cardiac diagnosis. Recent strides in electrocardiogram self-supervised learning (eSSL) have brightened prospects for enhancing representation learning without relying on high-quality annotations. Yet earlier eSSL methods suffer a key limitation: they focus on consistent patterns across leads and beats, overlooking the inherent differences in heartbeats rooted in cardiac conduction processes, while subtle but significant variations carry unique physiological signatures. Moreover, representation learning for ECG analysis should align with ECG diagnostic guidelines, which progress from individual heartbeats to single leads and ultimately to lead combinations. This sequential logic, however, is often neglected when applying pre-trained models to downstream tasks. To address these gaps, we propose CLEAR-HUG, a two-stage framework designed to capture subtle variations in cardiac conduction across leads while adhering to ECG diagnostic guidelines. In the first stage, we introduce an eSSL model termed Conduction-LEAd Reconstructor (CLEAR), which captures both specific variations and general commonalities across heartbeats. Treating each heartbeat as a distinct entity, CLEAR employs a simple yet effective sparse attention mechanism to reconstruct signals without interference from other heartbeats. In the second stage, we implement a Hierarchical lead-Unified Group head (HUG) for disease diagnosis, mirroring clinical workflow. Experimental results across six tasks show a 6.84% improvement, validating the effectiveness of CLEAR-HUG. This highlights its ability to enhance representations of cardiac conduction and align patterns with expert diagnostic guidelines.

</details>


### [79] [Hyperspherical Graph Representation Learning via Adaptive Neighbor-Mean Alignment and Uniformity](https://arxiv.org/abs/2512.24062)
*Rui Chen,Junjun Guo,Hongbin Wang,Yan Xiang,Yantuan Xian,Zhengtao Yu*

Main category: cs.LG

TL;DR: HyperGRL是一个在超球面上进行图表示学习的统一框架，通过自适应邻居均值对齐和无采样均匀性实现，避免了传统对比学习方法的复杂架构和负采样需求。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法依赖代理对比目标或互信息最大化，通常需要复杂架构、负采样策略和敏感的超参数调优，这些设计选择可能导致过平滑、过压缩和训练不稳定问题。

Method: 提出HyperGRL框架，将节点嵌入到单位超球面上，通过两个对抗耦合目标：邻居均值对齐和无采样均匀性。对齐目标使用每个节点局部邻域的均值表示构建语义基础稳定的目标；均匀性目标通过基于L2的超球面正则化实现分散。还引入了熵引导的自适应平衡机制来动态调节对齐和均匀性之间的相互作用。

Result: 在节点分类、节点聚类和链接预测任务上的广泛实验表明，HyperGRL在表示质量和泛化能力方面优于现有方法，分别实现了1.49%、0.86%和0.74%的平均改进。

Conclusion: 基于几何基础的无采样对比目标对于图表示学习是有效的，HyperGRL框架通过自适应邻居均值对齐和均匀性目标，在避免传统方法复杂性的同时提高了表示质量。

Abstract: Graph representation learning (GRL) aims to encode structural and semantic dependencies of graph-structured data into low-dimensional embeddings. However, existing GRL methods often rely on surrogate contrastive objectives or mutual information maximization, which typically demand complex architectures, negative sampling strategies, and sensitive hyperparameter tuning. These design choices may induce over-smoothing, over-squashing, and training instability. In this work, we propose HyperGRL, a unified framework for hyperspherical graph representation learning via adaptive neighbor-mean alignment and sampling-free uniformity. HyperGRL embeds nodes on a unit hypersphere through two adversarially coupled objectives: neighbor-mean alignment and sampling-free uniformity. The alignment objective uses the mean representation of each node's local neighborhood to construct semantically grounded, stable targets that capture shared structural and feature patterns. The uniformity objective formulates dispersion via an L2-based hyperspherical regularization, encouraging globally uniform embedding distributions while preserving discriminative information. To further stabilize training, we introduce an entropy-guided adaptive balancing mechanism that dynamically regulates the interplay between alignment and uniformity without requiring manual tuning. Extensive experiments on node classification, node clustering, and link prediction demonstrate that HyperGRL delivers superior representation quality and generalization across diverse graph structures, achieving average improvements of 1.49%, 0.86%, and 0.74% over the strongest existing methods, respectively. These findings highlight the effectiveness of geometrically grounded, sampling-free contrastive objectives for graph representation learning.

</details>


### [80] [How and Why LLMs Generalize: A Fine-Grained Analysis of LLM Reasoning from Cognitive Behaviors to Low-Level Patterns](https://arxiv.org/abs/2512.24063)
*Haoyue Bai,Yiyou Sun,Wenjie Hu,Shi Qiu,Maggie Ziyu Huan,Peiyang Song,Robert Nowak,Dawn Song*

Main category: cs.LG

TL;DR: 该研究通过分解推理为原子核心技能的新基准，揭示了SFT和RL调优在LLM泛化行为上的差异：SFT导致能力窄化，而RL能保持更稳定的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖粗粒度的准确率指标，无法解释为什么监督微调（SFT）会窄化LLM能力，而强化学习（RL）调优却能保持能力。需要更细粒度的分析框架来理解LLM中推理的本质和泛化行为差异。

Method: 1. 引入新基准，将推理分解为计算、事实检索、模拟、枚举、诊断等原子核心技能；2. 结合分布差异和参数统计等低层统计模式分析；3. 使用元探测框架追踪不同训练阶段模型行为；4. 在数学、科学推理和非推理任务上研究SFT和RL的泛化演变。

Result: 1. RL调优模型保持更稳定的行为特征，抵抗推理技能崩溃；2. SFT模型表现出更尖锐的漂移，过度拟合表面模式；3. 通过原子技能分解提供了LLM推理能力的细粒度视图；4. 揭示了不同认知能力在训练后如何出现、转移和崩溃。

Conclusion: 该工作为理解LLM中的推理本质提供了新见解，并指出了设计促进广泛、稳健泛化的训练策略的原则。RL调优在保持推理能力稳定性方面优于SFT，这为优化LLM训练方法提供了重要指导。

Abstract: Large Language Models (LLMs) display strikingly different generalization behaviors: supervised fine-tuning (SFT) often narrows capability, whereas reinforcement-learning (RL) tuning tends to preserve it. The reasons behind this divergence remain unclear, as prior studies have largely relied on coarse accuracy metrics. We address this gap by introducing a novel benchmark that decomposes reasoning into atomic core skills such as calculation, fact retrieval, simulation, enumeration, and diagnostic, providing a concrete framework for addressing the fundamental question of what constitutes reasoning in LLMs. By isolating and measuring these core skills, the benchmark offers a more granular view of how specific cognitive abilities emerge, transfer, and sometimes collapse during post-training. Combined with analyses of low-level statistical patterns such as distributional divergence and parameter statistics, it enables a fine-grained study of how generalization evolves under SFT and RL across mathematical, scientific reasoning, and non-reasoning tasks. Our meta-probing framework tracks model behavior at different training stages and reveals that RL-tuned models maintain more stable behavioral profiles and resist collapse in reasoning skills, whereas SFT models exhibit sharper drift and overfit to surface patterns. This work provides new insights into the nature of reasoning in LLMs and points toward principles for designing training strategies that foster broad, robust generalization.

</details>


### [81] [Time-varying Mixing Matrix Design for Energy-efficient Decentralized Federated Learning](https://arxiv.org/abs/2512.24069)
*Xusheng Zhang,Tuan Nguyen,Ting He*

Main category: cs.LG

TL;DR: 本文提出了一种用于无线网络中分散式联邦学习（DFL）的混合矩阵设计方法，旨在最小化最大节点能耗，同时考虑无线通信的广播特性。


<details>
  <summary>Details</summary>
Motivation: 现有DFL混合矩阵设计主要关注最小化通信时间，而忽略了能量受限设备的关键需求——最小化节点能耗。无线通信的广播特性未被充分利用，且现有方法未考虑能耗均衡问题。

Method: 基于允许任意时变混合矩阵的新收敛定理，提出多阶段设计框架：激活时变通信拓扑，在优化预算下权衡每次迭代能耗与收敛速度，同时平衡节点间能耗分布。

Result: 基于真实数据的评估验证了所提方案的有效性，成功结合了稀疏混合矩阵的低能耗和密集混合矩阵的快速收敛优势。

Conclusion: 该工作填补了DFL混合矩阵设计中能耗最小化的空白，通过时变拓扑和多阶段框架实现了能耗与收敛速度的优化平衡，为能量受限的无线设备提供了实用解决方案。

Abstract: We consider the design of mixing matrices to minimize the operation cost for decentralized federated learning (DFL) in wireless networks, with focus on minimizing the maximum per-node energy consumption. As a critical hyperparameter for DFL, the mixing matrix controls both the convergence rate and the needs of agent-to-agent communications, and has thus been studied extensively. However, existing designs mostly focused on minimizing the communication time, leaving open the minimization of per-node energy consumption that is critical for energy-constrained devices. This work addresses this gap through a theoretically-justified solution for mixing matrix design that aims at minimizing the maximum per-node energy consumption until convergence, while taking into account the broadcast nature of wireless communications. Based on a novel convergence theorem that allows arbitrarily time-varying mixing matrices, we propose a multi-phase design framework that activates time-varying communication topologies under optimized budgets to trade off the per-iteration energy consumption and the convergence rate while balancing the energy consumption across nodes. Our evaluations based on real data have validated the efficacy of the proposed solution in combining the low energy consumption of sparse mixing matrices and the fast convergence of dense mixing matrices.

</details>


### [82] [Multi-Scenario Highway Lane-Change Intention Prediction: A Temporal Physics-Informed Multi-Modal Framework](https://arxiv.org/abs/2512.24075)
*Jiazhao Shi,Ziyu Wang,Yichen Lin,Shoufeng Lu*

Main category: cs.LG

TL;DR: TPI-AI框架融合深度时序表征与物理启发交互特征，通过Bi-LSTM编码轨迹历史，结合运动学、安全性和交互感知特征，使用LightGBM分类器进行三类别换道意图识别，在高速公路和匝道丰富场景中显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 换道意图预测对自动驾驶安全至关重要，但在自然交通中面临挑战：运动学噪声、严重类别不平衡以及异构高速公路场景的泛化能力有限。

Method: 提出TPI-AI混合框架：1）两层双向LSTM编码器学习多步轨迹历史的紧凑嵌入；2）将嵌入与运动学、安全性和交互感知特征（如车头时距、碰撞时间、安全间隙指标）拼接；3）使用LightGBM分类器进行三类别换道意图识别；4）采用重采样/加权和阈值校准处理类别不平衡。

Result: 在两个大规模无人机数据集（highD和exiD）上评估，预测时域T=1,2,3秒。TPI-AI在highD上macro-F1分别为0.9562、0.9124、0.8345，在exiD上分别为0.9247、0.8197、0.7605，显著优于单独的LightGBM和Bi-LSTM基线。

Conclusion: 将物理启发的交互特征与学习的时序嵌入相结合，能够实现鲁棒的多场景换道意图预测，为自动驾驶系统提供更可靠的意图识别能力。

Abstract: Lane-change intention prediction is safety-critical for autonomous driving and ADAS, but remains difficult in naturalistic traffic due to noisy kinematics, severe class imbalance, and limited generalization across heterogeneous highway scenarios. We propose Temporal Physics-Informed AI (TPI-AI), a hybrid framework that fuses deep temporal representations with physics-inspired interaction cues. A two-layer bidirectional LSTM (Bi-LSTM) encoder learns compact embeddings from multi-step trajectory histories; we concatenate these embeddings with kinematics-, safety-, and interaction-aware features (e.g., headway, TTC, and safe-gap indicators) and train a LightGBM classifier for three-class intention recognition (No-LC, Left-LC, Right-LC). To improve minority-class reliability, we apply imbalance-aware optimization including resampling/weighting and fold-wise threshold calibration. Experiments on two large-scale drone-based datasets, highD (straight highways) and exiD (ramp-rich environments), use location-based splits and evaluate prediction horizons T = 1, 2, 3 s. TPI-AI outperforms standalone LightGBM and Bi-LSTM baselines, achieving macro-F1 of 0.9562, 0.9124, 0.8345 on highD and 0.9247, 0.8197, 0.7605 on exiD at T = 1, 2, 3 s, respectively. These results show that combining physics-informed interaction features with learned temporal embeddings yields robust multi-scenario lane-change intention prediction.

</details>


### [83] [Autoregressivity in the Latent Space of a GP-VAE Language Model: An Empirical Ablation Study](https://arxiv.org/abs/2512.24102)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 该论文对GP-VAE模型中的潜在自回归进行了消融分析，比较了三种模型：具有自回归潜在动态的完整GP-VAE、潜在变量独立的非自回归消融版本，以及标准token级自回归Transformer。研究发现，在中等规模语料库和短训练上下文的设定下，潜在自回归能产生更符合高斯过程先验且具有更好长时稳定性的潜在轨迹。


<details>
  <summary>Details</summary>
Motivation: 语言模型通常依赖于token级的自回归分解，但作者先前的工作提出将序列结构转移到潜在空间，通过因果高斯过程实现，同时使用非自回归解码器。本文旨在系统性地分析潜在自回归在GP-VAE模型中的作用，理解其如何影响潜在表示的结构和长期稳定性。

Method: 采用消融研究方法，比较三种模型：(1) 具有自回归潜在动态的完整GP-VAE模型；(2) 潜在变量独立的非自回归消融版本；(3) 标准token级自回归Transformer。在中等规模语料库和短训练上下文的设定下进行实验，分析潜在轨迹与高斯过程先验的兼容性以及长期稳定性。

Result: 实验结果表明，潜在自回归能够产生显著更符合高斯过程先验的潜在轨迹，并展现出更好的长期稳定性。相比之下，移除自回归会导致潜在结构退化，长期行为不稳定。这些发现表明潜在自回归是组织长期结构的有效机制，同时与token级自回归建模互补。

Conclusion: 潜在自回归在GP-VAE模型中起着重要作用，能够有效组织长期结构并提高潜在表示的稳定性。该研究应被理解为对表示结构的实证分析，而非提出新架构。潜在自回归与token级自回归建模是互补的，共同为序列建模提供有效的表示学习机制。

Abstract: This paper provides an ablation-based analysis of latent autoregression in GP-VAE models, building upon our previous work introducing the architecture. Language models typically rely on an autoregressive factorization over tokens. In contrast, our prior work proposed shifting sequential structure to the latent space through a causal Gaussian process, while using a non-autoregressive decoder. Here, we conduct a systematic ablation study of the role played by latent autoregression. We compare (i) a full GP-VAE model with autoregressive latent dynamics, (ii) a non-autoregressive ablation in which latent variables are independent, and (iii) a standard token-level autoregressive Transformer. Our results show that, within the considered regime (medium-scale corpora and short training contexts), latent autoregression induces latent trajectories that are significantly more compatible with the Gaussian-process prior and exhibit greater long-horizon stability. In contrast, removing autoregression leads to degraded latent structure and unstable long-range behavior. These findings highlight the role of latent autoregression as an effective mechanism for organizing long-range structure, while remaining complementary to token-level autoregressive modeling. They should be interpreted as an empirical analysis of representational structure rather than as a proposal for a new architecture.

</details>


### [84] [Enhancing LLM Planning Capabilities through Intrinsic Self-Critique](https://arxiv.org/abs/2512.24103)
*Bernd Bohnet,Pierre-Alexandre Kamienny,Hanie Sedghi,Dilan Gorur,Pranjal Awasthi,Aaron Parisi,Kevin Swersky,Rosanne Liu,Azade Nova,Noah Fiedel*

Main category: cs.LG

TL;DR: 本文提出了一种让大语言模型通过自我批评来提升规划性能的方法，在多个基准测试中取得了显著改进，超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究对LLMs自我批评方法的有效性持怀疑态度，但本文旨在证明通过内在自我批评（无需外部验证器）可以显著提升LLMs在规划任务上的性能。

Method: 采用少样本学习技术并逐步扩展到多样本方法作为基础，然后通过迭代的修正和精炼过程实现性能提升。该方法专注于内在自我改进能力，不依赖特定模型版本。

Result: 在Blocksworld、Logistics和Mini-grid数据集上均取得了显著性能提升，超越了强基线准确率，在2024年10月的LLM模型检查点上达到了新的最先进水平。

Conclusion: 自我批评可以显著提升规划性能，该方法具有内在自我改进能力，适用于不同模型版本。将其应用于更复杂的搜索技术和更强大的模型有望获得更好的性能。

Abstract: We demonstrate an approach for LLMs to critique their \emph{own} answers with the goal of enhancing their performance that leads to significant improvements over established planning benchmarks. Despite the findings of earlier research that has cast doubt on the effectiveness of LLMs leveraging self critique methods, we show significant performance gains on planning datasets in the Blocksworld domain through intrinsic self-critique, without external source such as a verifier. We also demonstrate similar improvements on Logistics and Mini-grid datasets, exceeding strong baseline accuracies. We employ a few-shot learning technique and progressively extend it to a many-shot approach as our base method and demonstrate that it is possible to gain substantial improvement on top of this already competitive approach by employing an iterative process for correction and refinement. We illustrate how self-critique can significantly boost planning performance. Our empirical results present new state-of-the-art on the class of models considered, namely LLM model checkpoints from October 2024. Our primary focus lies on the method itself, demonstrating intrinsic self-improvement capabilities that are applicable regardless of the specific model version, and we believe that applying our method to more complex search techniques and more capable models will lead to even better performance.

</details>


### [85] [OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization](https://arxiv.org/abs/2512.24124)
*Advait Gadhikar,Riccardo Grazzi,James Hensman*

Main category: cs.LG

TL;DR: 该论文提出OptRot和OptRot⁺方法，通过学习可融合的旋转来减少LLM权重和激活中的异常值，改善量化效果，主要针对GPTQ量化方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）权重和激活中存在异常值，这使得量化变得困难。现有方法使用旋转来缓解这些异常值，但需要更有效的方法。

Method: 提出两种方法：1) OptRot：通过最小化旋转权重的元素四次方来减少权重异常值；2) OptRot⁺：数据依赖方法，通过结合激活协方差信息进一步改进性能。主要针对GPTQ量化方法。

Result: OptRot在权重量化方面优于Hadamard旋转以及更昂贵的数据依赖方法（如SpinQuant和OSTQuant）。在W4A8设置下也能改善激活量化。OptRot⁺通过结合激活协方差信息进一步提升了性能。但在W4A4设置下，两种方法表现较差，突显了权重和激活量化之间的权衡。

Conclusion: 提出的OptRot方法通过简单有效的旋转策略显著改善了LLM的量化效果，特别是在权重量化方面表现优异。OptRot⁺通过数据依赖的改进进一步提升了性能，但需要权衡权重和激活量化的需求。

Abstract: The presence of outliers in Large Language Models (LLMs) weights and activations makes them difficult to quantize. Recent work has leveraged rotations to mitigate these outliers. In this work, we propose methods that learn fusible rotations by minimizing principled and cheap proxy objectives to the weight quantization error. We primarily focus on GPTQ as the quantization method. Our main method is OptRot, which reduces weight outliers simply by minimizing the element-wise fourth power of the rotated weights. We show that OptRot outperforms both Hadamard rotations and more expensive, data-dependent methods like SpinQuant and OSTQuant for weight quantization. It also improves activation quantization in the W4A8 setting. We also propose a data-dependent method, OptRot$^{+}$, that further improves performance by incorporating information on the activation covariance. In the W4A4 setting, we see that both OptRot and OptRot$^{+}$ perform worse, highlighting a trade-off between weight and activation quantization.

</details>


### [86] [GARDO: Reinforcing Diffusion Models without Reward Hacking](https://arxiv.org/abs/2512.24138)
*Haoran He,Yuxiao Ye,Jie Liu,Jiajun Liang,Zhiyong Wang,Ziyang Yuan,Xintao Wang,Hangyu Mao,Pengfei Wan,Ling Pan*

Main category: cs.LG

TL;DR: GARDO框架通过选择性惩罚高不确定性样本、自适应更新参考模型和多样性感知优化，解决了扩散模型在线强化学习中奖励黑客攻击、探索受限和模式崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型通过在线强化学习进行微调可以提升文本-图像对齐效果，但由于视觉任务的真实目标难以精确指定，模型通常使用只能部分捕捉真实目标的代理奖励进行优化。这种不匹配导致奖励黑客攻击问题——代理分数上升而真实图像质量下降、生成多样性崩溃。现有解决方案通常通过正则化参考策略来防止奖励黑客攻击，但这会牺牲样本效率并阻碍对新颖高奖励区域的探索，因为参考策略通常是次优的。

Method: 提出了GARDO（Gated and Adaptive Regularization with Diversity-aware Optimization）框架，包含三个核心机制：1）选择性正则化：只惩罚高不确定性的样本子集；2）自适应正则化：定期更新参考模型以匹配在线策略的能力，确保正则化目标的相关性；3）多样性感知优化：对高质量且高多样性的样本放大奖励，鼓励模式覆盖而不破坏优化稳定性。

Result: 在多种代理奖励和未见过的评估指标上的广泛实验表明，GARDO能够有效缓解奖励黑客攻击，增强生成多样性，同时不牺牲样本效率或探索能力，证明了其有效性和鲁棒性。

Conclusion: GARDO框架通过选择性正则化、自适应参考模型更新和多样性感知优化的协同作用，成功解决了扩散模型在线强化学习中的奖励黑客攻击、探索受限和模式崩溃问题，为视觉任务中的代理奖励优化提供了有效的解决方案。

Abstract: Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.

</details>


### [87] [Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction](https://arxiv.org/abs/2512.24139)
*Qianyi Chen,Bo Li*

Main category: cs.LG

TL;DR: 本文提出了一种改进共形预测条件覆盖性能的新方法，通过优化分位数回归组件来最小化条件覆盖的均方误差，使用密度加权的pinball损失函数，并设计了三个头的分位数网络来估计权重。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽然能提供稳健的边缘覆盖保证，但在特定输入上实现可靠的条件覆盖仍然具有挑战性。虽然有限样本下无法实现精确的无分布条件覆盖，但本文旨在直接最小化条件覆盖的均方误差，而不是采用放松的条件覆盖概念。

Method: 通过泰勒展开推导出分位数回归的替代目标函数：密度加权的pinball损失，其中权重由一致性得分在真实分位数处的条件密度给出。提出了一个三个头的分位数网络，使用辅助分位数水平(1-α±δ)通过有限差分估计这些权重，然后通过优化加权损失来微调中心分位数。

Result: 理论分析提供了精确的非渐近保证，描述了由此产生的超额风险。在高维真实世界数据集上的广泛实验表明，条件覆盖性能有显著改善。

Conclusion: 该方法通过直接优化条件覆盖的均方误差，显著提高了共形预测的条件覆盖性能，为改进分位数回归组件提供了新的理论框架和实用工具。

Abstract: While conformal prediction provides robust marginal coverage guarantees, achieving reliable conditional coverage for specific inputs remains challenging. Although exact distribution-free conditional coverage is impossible with finite samples, recent work has focused on improving the conditional coverage of standard conformal procedures. Distinct from approaches that target relaxed notions of conditional coverage, we directly minimize the mean squared error of conditional coverage by refining the quantile regression components that underpin many conformal methods. Leveraging a Taylor expansion, we derive a sharp surrogate objective for quantile regression: a density-weighted pinball loss, where the weights are given by the conditional density of the conformity score evaluated at the true quantile. We propose a three-headed quantile network that estimates these weights via finite differences using auxiliary quantile levels at \(1-α\pm δ\), subsequently fine-tuning the central quantile by optimizing the weighted loss. We provide a theoretical analysis with exact non-asymptotic guarantees characterizing the resulting excess risk. Extensive experiments on diverse high-dimensional real-world datasets demonstrate remarkable improvements in conditional coverage performance.

</details>


### [88] [Paired Seed Evaluation: Statistical Reliability for Learning-Based Simulators](https://arxiv.org/abs/2512.24145)
*Udit Sharma*

Main category: cs.LG

TL;DR: 论文提出"配对种子评估设计"，通过让竞争系统在相同的随机种子下运行来减少评估方差，提高统计效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统虽然看起来是随机的，但实际上是确定性随机的（由种子伪随机数生成器控制）。基于学习的模拟器广泛用于算法比较，但由于随机初始化和学习随机性，评估结果通常具有高方差。标准的独立评估设计未能利用不同替代方案之间共享的随机源。

Method: 提出配对种子评估设计，让竞争系统在相同的随机种子下进行评估，从而诱导随机组件的匹配实现。当结果在种子级别正相关时，这种方法能严格减少方差。该方法在存在相关性时提高统计可靠性，在没有相关性时则退化为独立评估而不损失有效性。

Result: 经验表明，种子级别的相关性通常很大且为正，能产生数量级的效率提升。配对种子评估在实践中是弱主导的，能在固定计算预算下获得更紧的置信区间、更高的统计功效和有效的样本量增益。

Conclusion: 配对种子评估设计是一种有效的统计方法，通过利用共享随机源来减少评估方差，提高机器学习系统比较的统计效率和可靠性，在实践中具有显著优势。

Abstract: Machine learning systems appear stochastic but are deterministically random, as seeded pseudorandom number generators produce identical realisations across executions. Learning-based simulators are widely used to compare algorithms, design choices, and interventions under such dynamics, yet evaluation outcomes often exhibit high variance due to random initialisation and learning stochasticity. We analyse the statistical structure of comparative evaluation in these settings and show that standard independent evaluation designs fail to exploit shared sources of randomness across alternatives. We formalise a paired seed evaluation design in which competing systems are evaluated under identical random seeds, inducing matched realisations of stochastic components and strict variance reduction whenever outcomes are positively correlated at the seed level. This yields tighter confidence intervals, higher statistical power, and effective sample size gains at fixed computational budgets. Empirically, seed-level correlations are typically large and positive, producing order-of-magnitude efficiency gains. Paired seed evaluation is weakly dominant in practice, improving statistical reliability when correlation is present and reducing to independent evaluation without loss of validity when it is not.

</details>


### [89] [Micro-Macro Tensor Neural Surrogates for Uncertainty Quantification in Collisional Plasma](https://arxiv.org/abs/2512.24205)
*Wei Chen,Giacomo Dimarco,Lorenzo Pareschi*

Main category: cs.LG

TL;DR: 提出基于方差缩减蒙特卡洛的Vlasov-Poisson-Landau系统不确定性量化框架，使用神经网络代理替代昂贵的Landau碰撞项计算，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 等离子体动力学方程对微观扰动高度敏感，传统不确定性量化方法面临计算成本高、相空间维度高、多尺度刚性等挑战，特别是在考虑碰撞时，高维非局部碰撞积分和守恒性质带来严重约束

Method: 将高保真渐近保持VPL求解器与基于VPFP和EP方程的廉价强相关代理模型耦合；引入基于各向异性微宏观分解的张量神经网络（SPINN推广），降低速度矩成本；通过校准VPFP模型和设计渐近保持SPINN增强与VPL的相关性

Result: 相比标准蒙特卡洛方法，实现显著的方差缩减，用更少的高保真样本获得准确统计量，降低计算时间，同时保持对随机维度的鲁棒性

Conclusion: 该方法为等离子体动力学方程的不确定性量化提供了高效可靠的框架，通过神经网络代理和方差缩减技术克服了传统方法的计算瓶颈

Abstract: Plasma kinetic equations exhibit pronounced sensitivity to microscopic perturbations in model parameters and data, making reliable and efficient uncertainty quantification (UQ) essential for predictive simulations. However, the cost of uncertainty sampling, the high-dimensional phase space, and multiscale stiffness pose severe challenges to both computational efficiency and error control in traditional numerical methods. These aspects are further emphasized in presence of collisions where the high-dimensional nonlocal collision integrations and conservation properties pose severe constraints. To overcome this, we present a variance-reduced Monte Carlo framework for UQ in the Vlasov--Poisson--Landau (VPL) system, in which neural network surrogates replace the multiple costly evaluations of the Landau collision term. The method couples a high-fidelity, asymptotic-preserving VPL solver with inexpensive, strongly correlated surrogates based on the Vlasov--Poisson--Fokker--Planck (VPFP) and Euler--Poisson (EP) equations. For the surrogate models, we introduce a generalization of the separable physics-informed neural network (SPINN), developing a class of tensor neural networks based on an anisotropic micro-macro decomposition, to reduce velocity-moment costs, model complexity, and the curse of dimensionality. To further increase correlation with VPL, we calibrate the VPFP model and design an asymptotic-preserving SPINN whose small- and large-Knudsen limits recover the EP and VP systems, respectively. Numerical experiments show substantial variance reduction over standard Monte Carlo, accurate statistics with far fewer high-fidelity samples, and lower wall-clock time, while maintaining robustness to stochastic dimension.

</details>


### [90] [Early Prediction of Sepsis using Heart Rate Signals and Genetic Optimized LSTM Algorithm](https://arxiv.org/abs/2512.24253)
*Alireza Rafiei,Farshid Hajati,Alireza Rezaee,Amirhossien Panahi,Shahadat Uddin*

Main category: cs.LG

TL;DR: 该研究开发了四种基于心率数据的机器学习算法，用于在可穿戴设备上预测脓毒症发作，通过遗传算法优化模型架构，并评估了其在非病房环境中的可行性。


<details>
  <summary>Details</summary>
Motivation: 脓毒症导致高死亡率、发病率和医疗成本，及时预测脓毒症进展对早期干预至关重要。尽管已有许多ICU患者预测模型，但在非病房环境中早期检测脓毒症的方法仍存在明显空白。

Method: 研究开发了四种新型机器学习算法，通过分析心率数据预测脓毒症发作。模型架构通过遗传算法进行优化，平衡性能、计算复杂度和内存需求。最初针对1小时预测窗口设计，后通过迁移学习扩展到4小时预测。

Result: 研究提取了各模型的性能指标，评估了其在能够准确监测心率的可穿戴设备上实施的可行性。结果显示这些模型在非ICU和病房环境中具有早期脓毒症检测的潜力。

Conclusion: 该研究的积极结果表明，可穿戴技术有潜力在ICU和病房环境之外促进早期脓毒症检测，为脓毒症的及时干预提供了新的技术途径。

Abstract: Sepsis, characterized by a dysregulated immune response to infection, results in significant mortality, morbidity, and healthcare costs. The timely prediction of sepsis progression is crucial for reducing adverse outcomes through early intervention. Despite the development of numerous models for Intensive Care Unit (ICU) patients, there remains a notable gap in approaches for the early detection of sepsis in non-ward settings. This research introduces and evaluates four novel machine learning algorithms designed for predicting the onset of sepsis on wearable devices by analyzing heart rate data. The architecture of these models was refined through a genetic algorithm, optimizing for performance, computational complexity, and memory requirements. Performance metrics were subsequently extracted for each model to evaluate their feasibility for implementation on wearable devices capable of accurate heart rate monitoring. The models were initially tailored for a prediction window of one hour, later extended to four hours through transfer learning. The encouraging outcomes of this study suggest the potential for wearable technology to facilitate early sepsis detection outside ICU and ward environments.

</details>


### [91] [Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction](https://arxiv.org/abs/2512.24324)
*Haojin Li,Anbang Zhang,Chen Sun,Chenyuan Feng,Kaiqian Qu,Tony Q. S. Quek,Haijun Zhang*

Main category: cs.LG

TL;DR: 提出SaM2B框架，通过可靠性感知的动态权重分配和跨模态对比学习，解决无人机通信中多模态波束预测的模态可靠性波动和模态失配问题


<details>
  <summary>Details</summary>
Motivation: 低空经济快速发展，无人机通信需要快速准确的波束预测。现有多模态方法使用固定权重，假设各模态始终同等可靠，但实际上模态重要性随无人机运动场景剧烈变化，静态权重会放大退化模态的负面影响，且模态失配和弱对齐进一步削弱跨场景泛化能力

Method: 提出SaM2B框架：1）利用环境视觉、飞行姿态和地理空间等轻量级线索，通过可靠性感知的动态权重更新方案，在不同时间点自适应分配各模态贡献；2）使用跨模态对比学习，将与特定波束信息相关的"多源表示波束语义"对齐到共享语义空间，增强判别能力和模态噪声及分布偏移下的鲁棒性

Result: 在真实世界低空无人机数据集上的实验表明，SaM2B相比基线方法取得了更令人满意的结果

Conclusion: SaM2B通过可靠性感知的动态权重分配和跨模态语义对齐，有效解决了多模态波束预测中的模态可靠性波动和泛化问题，为低空经济中的无人机通信提供了更可靠的连接保障

Abstract: The low-altitude economy (LAE) is rapidly expanding driven by urban air mobility, logistics drones, and aerial sensing, while fast and accurate beam prediction in uncrewed aerial vehicles (UAVs) communications is crucial for achieving reliable connectivity. Current research is shifting from single-signal to multi-modal collaborative approaches. However, existing multi-modal methods mostly employ fixed or empirical weights, assuming equal reliability across modalities at any given moment. Indeed, the importance of different modalities fluctuates dramatically with UAV motion scenarios, and static weighting amplifies the negative impact of degraded modalities. Furthermore, modal mismatch and weak alignment further undermine cross-scenario generalization. To this end, we propose a reliability-aware dynamic weighting scheme applied to a semantic-aware multi-modal beam prediction framework, named SaM2B. Specifically, SaM2B leverages lightweight cues such as environmental visual, flight posture, and geospatial data to adaptively allocate contributions across modalities at different time points through reliability-aware dynamic weight updates. Moreover, by utilizing cross-modal contrastive learning, we align the "multi-source representation beam semantics" associated with specific beam information to a shared semantic space, thereby enhancing discriminative power and robustness under modal noise and distribution shifts. Experiments on real-world low-altitude UAV datasets show that SaM2B achieves more satisfactory results than baseline methods.

</details>


### [92] [Tubular Riemannian Laplace Approximations for Bayesian Neural Networks](https://arxiv.org/abs/2512.24381)
*Rodrigo Pereira David*

Main category: cs.LG

TL;DR: TRL是一种新的贝叶斯近似方法，通过建模概率管状结构来适应神经网络损失曲面的高度各向异性和对称性，在保持单模型效率的同时达到集成模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统拉普拉斯近似在欧几里得空间中难以处理现代深度模型的高度各向异性、弯曲的损失曲面和大对称群，需要更适应这种结构的几何方法。

Method: 提出管状黎曼拉普拉斯近似，将后验建模为沿着由函数对称性诱导的低损失谷的概率管，使用Fisher/高斯-牛顿度量分离先验主导的切向不确定性和数据主导的横向不确定性。

Result: 在ResNet-18（CIFAR-10和CIFAR-100）上的实验表明，TRL实现了优异的校准性能，匹配或超越了深度集成的可靠性（就ECE而言），同时仅需1/5的训练成本。

Conclusion: TRL有效地弥合了单模型效率和集成级可靠性之间的差距，是一种可扩展的重新参数化高斯近似方法，利用隐式曲率估计在高维参数空间中运行。

Abstract: Laplace approximations are among the simplest and most practical methods for approximate Bayesian inference in neural networks, yet their Euclidean formulation struggles with the highly anisotropic, curved loss surfaces and large symmetry groups that characterize modern deep models. Recent work has proposed Riemannian and geometric Gaussian approximations to adapt to this structure. Building on these ideas, we introduce the Tubular Riemannian Laplace (TRL) approximation. TRL explicitly models the posterior as a probabilistic tube that follows a low-loss valley induced by functional symmetries, using a Fisher/Gauss-Newton metric to separate prior-dominated tangential uncertainty from data-dominated transverse uncertainty. We interpret TRL as a scalable reparametrised Gaussian approximation that utilizes implicit curvature estimates to operate in high-dimensional parameter spaces. Our empirical evaluation on ResNet-18 (CIFAR-10 and CIFAR-100) demonstrates that TRL achieves excellent calibration, matching or exceeding the reliability of Deep Ensembles (in terms of ECE) while requiring only a fraction (1/5) of the training cost. TRL effectively bridges the gap between single-model efficiency and ensemble-grade reliability.

</details>


### [93] [Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning](https://arxiv.org/abs/2512.24404)
*Soham Pahari,M. Srinivas*

Main category: cs.LG

TL;DR: 提出ViReLoc视觉推理定位框架，仅使用视觉表示进行规划和定位，通过强化学习优化，在空间推理任务中表现优于基于文本的推理方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能系统主要依赖文本信息进行推理，限制了在视觉导航和地理定位等空间任务中的有效性，需要开发纯视觉推理范式来解决这一问题。

Method: 提出Geo-Consistent Visual Planning范式，构建ViReLoc框架，学习空间依赖和几何关系，通过视觉域逐步推理和强化学习目标优化，结合对比学习和自适应特征交互来对齐跨视角表示。

Result: 在多样化导航和定位场景实验中，ViReLoc在空间推理准确性和跨视角检索性能方面均取得一致提升，证明了视觉推理在导航定位任务中的有效性。

Conclusion: 视觉推理可作为导航定位的强有力补充方法，无需实时GPS数据即可完成任务，为更安全的导航解决方案提供了可能。

Abstract: Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.

</details>


### [94] [Efficient Inference for Inverse Reinforcement Learning and Dynamic Discrete Choice Models](https://arxiv.org/abs/2512.24407)
*Lars van der Laan,Aurelien Bibaut,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出了一种半参数化的去偏逆强化学习框架，用于在最大熵IRL和Gumbel-shock DDC模型中实现统计有效推断，支持灵活的非参数估计同时保持√n一致性和渐近正态性。


<details>
  <summary>Details</summary>
Motivation: 现有IRL方法依赖机器学习但缺乏有效推断保证，而经典DDC方法参数限制严格且需要重复动态规划计算，需要一种既能灵活建模又能保证统计推断的方法。

Method: 开发半参数框架，将行为策略对数作为伪奖励识别策略价值差异和奖励本身，形式化目标为行为策略和转移核的光滑泛函，推导有效影响函数，构建自动去偏机器学习估计器。

Result: 实现了√n一致性、渐近正态性和半参数效率，允许灵活非参数估计辅助组件，将经典DDC推断扩展到非参数奖励和现代机器学习工具。

Conclusion: 提供了一个统一且计算可行的IRL统计推断方法，弥合了灵活IRL方法与经典DDC推断之间的差距。

Abstract: Inverse reinforcement learning (IRL) and dynamic discrete choice (DDC) models explain sequential decision-making by recovering reward functions that rationalize observed behavior. Flexible IRL methods typically rely on machine learning but provide no guarantees for valid inference, while classical DDC approaches impose restrictive parametric specifications and often require repeated dynamic programming. We develop a semiparametric framework for debiased inverse reinforcement learning that yields statistically efficient inference for a broad class of reward-dependent functionals in maximum entropy IRL and Gumbel-shock DDC models. We show that the log-behavior policy acts as a pseudo-reward that point-identifies policy value differences and, under a simple normalization, the reward itself. We then formalize these targets, including policy values under known and counterfactual softmax policies and functionals of the normalized reward, as smooth functionals of the behavior policy and transition kernel, establish pathwise differentiability, and derive their efficient influence functions. Building on this characterization, we construct automatic debiased machine-learning estimators that allow flexible nonparametric estimation of nuisance components while achieving $\sqrt{n}$-consistency, asymptotic normality, and semiparametric efficiency. Our framework extends classical inference for DDC models to nonparametric rewards and modern machine-learning tools, providing a unified and computationally tractable approach to statistical inference in IRL.

</details>


### [95] [Sparse classification with positive-confidence data in high dimensions](https://arxiv.org/abs/2512.24443)
*The Tien Mai,Mai Anh Nguyen,Trung Nghia Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种针对高维正置信分类的稀疏惩罚框架，使用凸和非凸惩罚项解决高维弱监督学习中的变量选择问题。


<details>
  <summary>Details</summary>
Motivation: 高维学习问题通常需要稀疏正则化进行有效预测和变量选择，但现有方法主要针对完全监督数据。在弱监督设置如正置信分类中，这些技术尚未充分探索，现有Pconf方法不适合高维场景。

Method: 提出新颖的稀疏惩罚框架，使用凸惩罚（Lasso）和非凸惩罚（SCAD、MCP）来解决收缩偏差并改进特征恢复。开发了高效近端梯度算法来求解复合目标函数。

Result: 为L1正则化Pconf估计器建立了估计和预测误差界限，证明其在限制强凸性条件下达到近似极小极大最优稀疏恢复率。模拟实验显示所提方法在预测性能和变量选择准确性方面与完全监督方法相当。

Conclusion: 该框架有效弥合了弱监督学习与高维统计之间的差距，为高维正置信分类提供了有效的稀疏正则化解决方案。

Abstract: High-dimensional learning problems, where the number of features exceeds the sample size, often require sparse regularization for effective prediction and variable selection. While established for fully supervised data, these techniques remain underexplored in weak-supervision settings such as Positive-Confidence (Pconf) classification. Pconf learning utilizes only positive samples equipped with confidence scores, thereby avoiding the need for negative data. However, existing Pconf methods are ill-suited for high-dimensional regimes. This paper proposes a novel sparse-penalization framework for high-dimensional Pconf classification. We introduce estimators using convex (Lasso) and non-convex (SCAD, MCP) penalties to address shrinkage bias and improve feature recovery. Theoretically, we establish estimation and prediction error bounds for the L1-regularized Pconf estimator, proving it achieves near minimax-optimal sparse recovery rates under Restricted Strong Convexity condition. To solve the resulting composite objective, we develop an efficient proximal gradient algorithm. Extensive simulations demonstrate that our proposed methods achieve predictive performance and variable selection accuracy comparable to fully supervised approaches, effectively bridging the gap between weak supervision and high-dimensional statistics.

</details>


### [96] [Adaptive Learning Guided by Bias-Noise-Alignment Diagnostics](https://arxiv.org/abs/2512.24445)
*Akash Samanta,Sheldon Williamson*

Main category: cs.LG

TL;DR: 提出基于偏差-噪声-对齐分解的诊断驱动自适应学习框架，通过建模误差演化提升动态环境中的学习稳定性和适应性


<details>
  <summary>Details</summary>
Motivation: 现有学习方法在非平稳安全关键环境中存在不稳定、收敛慢或适应性差的问题，主要原因是忽略了误差信号的时间结构

Method: 提出诊断驱动自适应学习框架，将误差演化分解为偏差（持续漂移）、噪声（随机变异）和对齐（重复方向激励导致超调）三个诊断指标，在线计算损失或时序差分误差轨迹的轻量统计量

Result: 该分解为监督优化、演员-评论家强化学习和学习优化器提供了统一控制框架，建立了稳定监督优化器、诊断调节演员-评论家方案和诊断条件学习优化器，在标准平滑假设下证明了有效更新有界和稳定性

Conclusion: 将误差演化提升为自适应学习的一等对象，为动态环境中的可靠学习提供了可解释、轻量级的基础框架

Abstract: Learning systems deployed in nonstationary and safety-critical environments often suffer from instability, slow convergence, or brittle adaptation when learning dynamics evolve over time. While modern optimization, reinforcement learning, and meta-learning methods adapt to gradient statistics, they largely ignore the temporal structure of the error signal itself. This paper proposes a diagnostic-driven adaptive learning framework that explicitly models error evolution through a principled decomposition into bias, capturing persistent drift; noise, capturing stochastic variability; and alignment, capturing repeated directional excitation leading to overshoot. These diagnostics are computed online from lightweight statistics of loss or temporal-difference error trajectories and are independent of model architecture or task domain. We show that the proposed bias-noise-alignment decomposition provides a unifying control backbone for supervised optimization, actor-critic reinforcement learning, and learned optimizers. Building on this framework, we derive diagnostic-driven instantiations including a stabilized supervised optimizer, a diagnostic-regulated actor-critic scheme, and a diagnostic-conditioned learned optimizer. Under standard smoothness assumptions, we establish bounded effective updates and stability properties for all cases. Representative diagnostic illustrations in actor-critic learning highlight how the proposed signals modulate adaptation in response to temporal-difference error structure. Overall, this work elevates error evolution to a first-class object in adaptive learning and provides an interpretable, lightweight foundation for reliable learning in dynamic environments.

</details>


### [97] [HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors](https://arxiv.org/abs/2512.24478)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: HOLOGRAPH框架通过层理论将LLM引导的因果发现形式化，用预层表示局部因果信念，全局因果结构对应全局截面，拓扑障碍表现为非零层上同调。该框架为LLM引导的因果发现提供了严格的数学基础。


<details>
  <summary>Details</summary>
Motivation: 观测数据中的因果发现受到可识别性约束的根本限制。现有方法依赖启发式集成，缺乏理论基础。需要为LLM引导的因果发现建立严格的数学框架。

Method: 引入HOLOGRAPH框架，通过层理论形式化LLM引导的因果发现：1) 将局部因果信念表示为变量子集上预层的截面；2) 提出代数潜在投影处理隐藏混杂因子；3) 在信念流形上使用自然梯度下降进行优化。

Result: 在合成和真实世界基准测试中，HOLOGRAPH在50-100个变量的因果发现任务上实现了有竞争力的性能。层理论分析显示：恒等、传递和粘合公理在数值精度内满足，但局部性公理在大图上失效，表明潜在变量投影中存在基本的非局部耦合。

Conclusion: HOLOGRAPH为LLM引导的因果发现提供了严格的数学基础，同时保持了实际性能。层理论分析揭示了因果发现中局部性公理的失效，为理解LLM引导方法的局限性提供了理论洞见。

Abstract: Causal discovery from observational data remains fundamentally limited by identifiability constraints. Recent work has explored leveraging Large Language Models (LLMs) as sources of prior causal knowledge, but existing approaches rely on heuristic integration that lacks theoretical grounding. We introduce HOLOGRAPH, a framework that formalizes LLM-guided causal discovery through sheaf theory--representing local causal beliefs as sections of a presheaf over variable subsets. Our key insight is that coherent global causal structure corresponds to the existence of a global section, while topological obstructions manifest as non-vanishing sheaf cohomology. We propose the Algebraic Latent Projection to handle hidden confounders and Natural Gradient Descent on the belief manifold for principled optimization. Experiments on synthetic and real-world benchmarks demonstrate that HOLOGRAPH provides rigorous mathematical foundations while achieving competitive performance on causal discovery tasks with 50-100 variables. Our sheaf-theoretic analysis reveals that while Identity, Transitivity, and Gluing axioms are satisfied to numerical precision (<10^{-6}), the Locality axiom fails for larger graphs, suggesting fundamental non-local coupling in latent variable projections. Code is available at [https://github.com/hyunjun1121/holograph](https://github.com/hyunjun1121/holograph).

</details>


### [98] [Generative forecasting with joint probability models](https://arxiv.org/abs/2512.24446)
*Patrick Wyrod,Ashesh Chattopadhyay,Daniele Venturi*

Main category: cs.LG

TL;DR: 将混沌系统预测重构为完全生成问题：学习滞后系统状态在短时间窗口上的联合概率分布，通过边缘化获得预测，相比传统条件预测模型有更好的短期预测能力和长期统计特性。


<details>
  <summary>Details</summary>
Motivation: 混沌动力系统对初始条件高度敏感且包含未解析的多尺度过程，使得确定性预测存在根本限制。现有生成模型大多关注下一步条件预测，而非底层动态结构。

Method: 将预测重构为完全生成问题，学习滞后系统状态在短时间窗口上的联合概率分布，通过边缘化获得预测。引入模型无关的训练和推理框架，使用三种不确定性量化指标评估预测稳健性。

Result: 在Lorenz-63系统和Kuramoto-Sivashinsky方程上评估，联合生成模型相比传统条件预测模型具有更好的短期预测能力、保持吸引子几何结构、获得更准确的长程统计行为。

Conclusion: 将混沌系统预测重构为联合生成问题能有效捕捉非线性时间依赖关系，产生与学习分布一致的预测，并通过不确定性量化提供预测可靠性的评估框架。

Abstract: Chaotic dynamical systems exhibit strong sensitivity to initial conditions and often contain unresolved multiscale processes, making deterministic forecasting fundamentally limited. Generative models offer an appealing alternative by learning distributions over plausible system evolutions; yet, most existing approaches focus on next-step conditional prediction rather than the structure of the underlying dynamics. In this work, we reframe forecasting as a fully generative problem by learning the joint probability distribution of lagged system states over short temporal windows and obtaining forecasts through marginalization. This new perspective allows the model to capture nonlinear temporal dependencies, represent multistep trajectory segments, and produce next-step predictions consistent with the learned joint distribution. We also introduce a general, model-agnostic training and inference framework for joint generative forecasting and show how it enables assessment of forecast robustness and reliability using three complementary uncertainty quantification metrics (ensemble variance, short-horizon autocorrelation, and cumulative Wasserstein drift), without access to ground truth. We evaluate the performance of the proposed method on two canonical chaotic dynamical systems, the Lorenz-63 system and the Kuramoto-Sivashinsky equation, and show that joint generative models yield improved short-term predictive skill, preserve attractor geometry, and achieve substantially more accurate long-range statistical behaviour than conventional conditional next-step models.

</details>


### [99] [Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice](https://arxiv.org/abs/2512.24503)
*Jiachen T. Wang,Tong Wu,Kaifeng Lyu,James Zou,Dawn Song,Ruoxi Jia,Prateek Mittal*

Main category: cs.LG

TL;DR: 研究发现传统数据配方评估协议存在缺陷：使用固定训练配置评估不同数据配方会导致结论不可靠，因为最优训练配置是数据依赖的。提出使用降低学习率作为简单修复方法，能显著提高小规模实验的可靠性。


<details>
  <summary>Details</summary>
Motivation: AI前沿公司的数据团队通常用小规模代理模型评估预训练数据配方，但社区对这些小规模实验结论能否可靠迁移到全规模训练缺乏理解。研究发现标准实验协议存在关键问题：为追求"公平"比较，对所有数据配方使用相同的训练配置，而实际上最优配置是数据依赖的。

Method: 提出对评估协议进行简单修复：在代理模型训练中使用降低的学习率。理论上证明对于随机特征模型，这种方法能保持数据集按其最优可达到损失的排序。实证上在23个数据配方上验证，涵盖数据整理的四个关键维度。

Result: 降低学习率的方法能产生与全规模调优LLM预训练运行强相关的相对性能。实证验证显示这种方法能显著提高小规模实验的可靠性，实验结论对数据配方的评估更加准确。

Conclusion: 数据配方评估的目标应是识别在数据特定调优下表现最佳的配方。降低学习率的简单修复方法能有效解决传统固定配置协议的问题，提高小规模实验结论向全规模训练迁移的可靠性。

Abstract: Data teams at frontier AI companies routinely train small proxy models to make critical decisions about pretraining data recipes for full-scale training runs. However, the community has a limited understanding of whether and when conclusions drawn from small-scale experiments reliably transfer to full-scale model training. In this work, we uncover a subtle yet critical issue in the standard experimental protocol for data recipe assessment: the use of identical small-scale model training configurations across all data recipes in the name of "fair" comparison. We show that the experiment conclusions about data quality can flip with even minor adjustments to training hyperparameters, as the optimal training configuration is inherently data-dependent. Moreover, this fixed-configuration protocol diverges from full-scale model development pipelines, where hyperparameter optimization is a standard step. Consequently, we posit that the objective of data recipe assessment should be to identify the recipe that yields the best performance under data-specific tuning. To mitigate the high cost of hyperparameter tuning, we introduce a simple patch to the evaluation protocol: using reduced learning rates for proxy model training. We show that this approach yields relative performance that strongly correlates with that of fully tuned large-scale LLM pretraining runs. Theoretically, we prove that for random-feature models, this approach preserves the ordering of datasets according to their optimal achievable loss. Empirically, we validate this approach across 23 data recipes covering four critical dimensions of data curation, demonstrating dramatic improvements in the reliability of small-scale experiments.

</details>


### [100] [More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization](https://arxiv.org/abs/2512.24545)
*Yuma Ichikawa,Yoshihiko Fujisawa,Yudai Fujimoto,Akira Sakai,Katsuki Fujisawa*

Main category: cs.LG

TL;DR: 提出MDBF方法改进大语言模型的极低比特量化，通过多包络设计增强幅度表达能力，在保持二进制载波的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 现有的双二进制分解（DBF）方法在极低比特量化中，其缩放参数过于受限——分解符号后所有秩分量共享相同的幅度轮廓，导致性能饱和

Method: 提出多包络DBF（MDBF），保留共享的1比特符号基，但将单一包络替换为秩-l包络；通过共享符号矩阵和优化包络分量，在保持二进制载波的同时增强幅度表达能力；引入闭式初始化和交替优化方法

Result: 在LLaMA和Qwen系列模型上，MDBF在相同比特/权重下相比之前的二进制格式显著提升了困惑度和零样本准确率，同时保持了相同的部署友好推理原语

Conclusion: MDBF通过多包络设计有效解决了DBF的幅度表达能力限制问题，在极低比特量化中实现了更好的精度-效率权衡，为大语言模型的高效部署提供了改进方案

Abstract: For extreme low-bit quantization of large language models (LLMs), Double Binary Factorization (DBF) is attractive as it enables efficient inference without sacrificing accuracy. However, the scaling parameters of DBF are too restrictive; after factoring out signs, all rank components share the same magnitude profile, resulting in performance saturation. We propose Multi-envelope DBF (MDBF), which retains a shared pair of 1-bit sign bases but replaces the single envelope with a rank-$l$ envelope. By sharing sign matrices among envelope components, MDBF effectively maintains a binary carrier and utilizes the limited memory budget for magnitude expressiveness. We also introduce a closed-form initialization and an alternating refinement method to optimize MDBF. Across the LLaMA and Qwen families, MDBF enhances perplexity and zero-shot accuracy over previous binary formats at matched bits per weight while preserving the same deployment-friendly inference primitive.

</details>


### [101] [Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space](https://arxiv.org/abs/2512.24617)
*Xingwei Qu,Shaowen Wang,Zihao Huang,Kai Hua,Fan Yin,Rui-Jie Zhu,Jundong Zhou,Qiyang Min,Zihao Wang,Yizhi Li,Tianyu Zhang,He Xing,Zheng Zhang,Yuxuan Song,Tianyu Zheng,Zhiyuan Zeng,Chenghua Lin,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: DLCM通过分层压缩将计算从token转移到概念空间，提出压缩感知缩放定律和解耦μP参数化，在相同FLOPs下实现性能提升


<details>
  <summary>Details</summary>
Motivation: 传统LLM对所有token采用统一计算，但语言信息密度不均匀，导致在可预测片段浪费算力，在关键语义转换处算力不足

Method: 提出动态大概念模型(DLCM)：1)从潜在表示学习语义边界，将计算从token转移到压缩概念空间；2)提出压缩感知缩放定律，分离token级容量、概念级推理能力和压缩比；3)开发解耦μP参数化，支持跨宽度和压缩机制的零样本超参数迁移

Result: 在R=4(平均4个token/概念)的设定下，DLCM将约三分之一的推理计算重新分配到更高容量的推理骨干，在12个零样本基准测试中平均提升2.69%(相同推理FLOPs下)

Conclusion: DLCM通过分层压缩和概念级推理，改变了语言模型的缩放行为，实现了更高效的算力分配，为LLM架构设计提供了新方向

Abstract: Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.

</details>


### [102] [Generalising E-prop to Deep Networks](https://arxiv.org/abs/2512.24506)
*Beren Millidge*

Main category: cs.LG

TL;DR: 该论文将E-prop框架扩展到深度网络，提出了一种同时处理时间和深度信用分配的在线学习算法，无需通过时间反向传播。


<details>
  <summary>Details</summary>
Motivation: 大脑中的学习涉及多层结构和时间动态，但现有的RTRL和E-prop方法主要研究单层循环网络，无法处理深度网络的信用分配问题。

Method: 扩展E-prop框架到任意深度网络，推导出跨深度的递归关系，将E-prop的资格迹扩展到更深层，实现同时跨时间和深度的在线信用分配。

Result: 开发了一种在线学习算法，能够同时跨时间和深度进行准确的信用分配，无需通过时间反向传播即可训练深度循环网络。

Conclusion: 该研究证明了在线学习算法可以同时处理时间和深度的信用分配，为训练深度循环网络提供了一种生物学上更合理的替代方案。

Abstract: Recurrent networks are typically trained with backpropagation through time (BPTT). However, BPTT requires storing the history of all states in the network and then replaying them sequentially backwards in time. This computation appears extremely implausible for the brain to implement. Real Time Recurrent Learning (RTRL) proposes an mathematically equivalent alternative where gradient information is propagated forwards in time locally alongside the regular forward pass, however it has significantly greater computational complexity than BPTT which renders it impractical for large networks. E-prop proposes an approximation of RTRL which reduces its complexity to the level of BPTT while maintaining a purely online forward update which can be implemented by an eligibility trace at each synapse. However, works on RTRL and E-prop ubiquitously investigate learning in a single layer with recurrent dynamics. However, learning in the brain spans multiple layers and consists of both hierarchal dynamics in depth as well as time. In this mathematical note, we extend the E-prop framework to handle arbitrarily deep networks, deriving a novel recursion relationship across depth which extends the eligibility traces of E-prop to deeper layers. Our results thus demonstrate an online learning algorithm can perform accurate credit assignment across both time and depth simultaneously, allowing the training of deep recurrent networks without backpropagation through time.

</details>


### [103] [AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt](https://arxiv.org/abs/2512.24625)
*Zijian Zhao,Yitong Shang,Sen Li*

Main category: cs.LG

TL;DR: AutoFed是一个用于交通预测的个性化联邦学习框架，通过引入联邦表示器和客户端对齐适配器，无需手动超参数调优即可实现跨客户端的知识共享。


<details>
  <summary>Details</summary>
Motivation: 交通预测对智能交通系统至关重要，但交通数据存在隐私问题，导致数据孤岛和知识共享受限。联邦学习虽然提供隐私保护方案，但面临非独立同分布问题，而现有的个性化联邦学习方法需要大量超参数调优，这在现实场景中难以实现。

Method: 提出AutoFed框架，受提示学习启发，引入联邦表示器，使用客户端对齐适配器将本地数据蒸馏为紧凑的全局共享提示矩阵，该提示矩阵条件化个性化预测器，使每个客户端既能受益于跨客户端知识，又能保持本地特异性。

Result: 在真实世界数据集上的广泛实验表明，AutoFed在不同场景下始终实现优越性能，无需手动超参数调优。

Conclusion: AutoFed为交通预测提供了一个实用且有效的个性化联邦学习解决方案，解决了现有方法对超参数调优的依赖问题，促进了实际部署。

Abstract: Accurate traffic prediction is essential for Intelligent Transportation Systems, including ride-hailing, urban road planning, and vehicle fleet management. However, due to significant privacy concerns surrounding traffic data, most existing methods rely on local training, resulting in data silos and limited knowledge sharing. Federated Learning (FL) offers an efficient solution through privacy-preserving collaborative training; however, standard FL struggles with the non-independent and identically distributed (non-IID) problem among clients. This challenge has led to the emergence of Personalized Federated Learning (PFL) as a promising paradigm. Nevertheless, current PFL frameworks require further adaptation for traffic prediction tasks, such as specialized graph feature engineering, data processing, and network architecture design. A notable limitation of many prior studies is their reliance on hyper-parameter optimization across datasets-information that is often unavailable in real-world scenarios-thus impeding practical deployment. To address this challenge, we propose AutoFed, a novel PFL framework for traffic prediction that eliminates the need for manual hyper-parameter tuning. Inspired by prompt learning, AutoFed introduces a federated representor that employs a client-aligned adapter to distill local data into a compact, globally shared prompt matrix. This prompt then conditions a personalized predictor, allowing each client to benefit from cross-client knowledge while maintaining local specificity. Extensive experiments on real-world datasets demonstrate that AutoFed consistently achieves superior performance across diverse scenarios. The code of this paper is provided at https://github.com/RS2002/AutoFed .

</details>


### [104] [Nested Learning: The Illusion of Deep Learning Architectures](https://arxiv.org/abs/2512.24695)
*Ali Behrouz,Meisam Razaviyayn,Peilin Zhong,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 提出了一种新的学习范式——嵌套学习（NL），将机器学习模型表示为嵌套、多层次、并行的优化问题，每个问题都有自己的上下文流。通过NL视角，现有深度学习方法通过学习压缩自身上下文流来学习数据，而上下文学习在大模型中自然涌现。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型等取得了进展，但关于模型如何持续学习/记忆、自我改进和找到有效解决方案仍存在基本挑战和未解问题。需要新的学习范式来解决这些根本性问题。

Method: 提出了嵌套学习（NL）范式，通过三个核心贡献：1）表达性优化器：将梯度优化器视为关联记忆模块，提出具有深度记忆和更强大学习规则的优化器；2）自修改学习模块：利用NL见解构建能够学习自身更新算法的序列模型；3）连续记忆系统：提出泛化传统长短时记忆的新记忆系统。

Result: 结合自修改序列模型和连续记忆系统，提出了名为Hope的持续学习模块，在语言建模、知识整合、少样本泛化任务、持续学习和长上下文推理任务中显示出有希望的结果。

Conclusion: 嵌套学习为设计更表达性的学习算法提供了哲学框架，通过增加层次实现高阶上下文学习，有望解锁有效的持续学习能力，为解决机器学习中的根本挑战提供了新视角。

Abstract: Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.

</details>


### [105] [From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme](https://arxiv.org/abs/2512.24555)
*Xueyan Li,Yingyi Xue,Mengjie Jiang,Qingzi Zhu,Yazhe Niu*

Main category: cs.LG

TL;DR: HUMOR框架通过分层推理和群体偏好对齐，提升视觉语言模型生成幽默梗图的能力


<details>
  <summary>Details</summary>
Motivation: 生成幽默梗图是一个具有挑战性的多模态任务，需要超越直接的图像-描述监督，涉及对视觉内容、上下文线索和主观幽默的细致推理。当前方法在视觉感知与幽默笑点创造之间存在差距。

Method: 提出HUMOR框架：1) 采用分层多路径思维链：从模板级意图识别开始，在不同上下文下探索多样化推理路径，最终锚定高质量、上下文特定的路径；2) 训练基于相同模板的梗图群体内的成对奖励模型，捕捉主观幽默偏好；3) 使用群体强化学习优化，在信任区域内保证单调改进。

Result: 大量实验表明，HUMOR赋予各种视觉语言模型更优越的推理多样性、更可靠的偏好对齐以及更高的整体梗图质量。

Conclusion: 该工作为开放式、人类对齐的多模态生成提供了一个通用训练范式，其中成功通过连贯输出群体内的比较判断来指导。

Abstract: Generating humorous memes is a challenging multimodal task that moves beyond direct image-to-caption supervision. It requires a nuanced reasoning over visual content, contextual cues, and subjective humor. To bridge this gap between visual perception and humorous punchline creation, we propose HUMOR}, a novel framework that guides VLMs through hierarchical reasoning and aligns them with group-wise human preferences. First, HUMOR employs a hierarchical, multi-path Chain-of-Thought (CoT): the model begins by identifying a template-level intent, then explores diverse reasoning paths under different contexts, and finally anchors onto a high-quality, context-specific path. This CoT supervision, which traces back from ground-truth captions, enhances reasoning diversity. We further analyze that this multi-path exploration with anchoring maintains a high expected humor quality, under the practical condition that high-quality paths retain significant probability mass. Second, to capture subjective humor, we train a pairwise reward model that operates within groups of memes sharing the same template. Following established theory, this approach ensures a consistent and robust proxy for human preference, even with subjective and noisy labels. The reward model then enables a group-wise reinforcement learning optimization, guaranteeing providing a theoretical guarantee for monotonic improvement within the trust region. Extensive experiments show that HUMOR empowers various VLMs with superior reasoning diversity, more reliable preference alignment, and higher overall meme quality. Beyond memes, our work presents a general training paradigm for open-ended, human-aligned multimodal generation, where success is guided by comparative judgment within coherent output group.

</details>


### [106] [BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework](https://arxiv.org/abs/2512.24708)
*András Millinghoffer,András Formanek,András Antos,Péter Antal*

Main category: cs.LG

TL;DR: BandiK是一个三阶段多任务辅助任务子集选择方法，使用多臂老虎机框架，通过估计任务间转移、构建线性候选集和集成多老虎机结构，高效选择有益辅助任务集。


<details>
  <summary>Details</summary>
Motivation: 在多任务学习中，有效选择有益辅助任务集面临计算成本高、候选集数量庞大、不同目标任务选择复杂度差异大等挑战，需要开发高效的选择方法。

Method: 提出三阶段方法：1) 估计任务间成对转移关系；2) 基于初始估计为每个目标任务构建线性数量的候选辅助任务集；3) 为每个任务使用多臂老虎机框架，并将这些任务特定老虎机集成到多老虎机结构中，利用神经网络实现多个老虎机臂的共享特性。

Result: BandiK显著减少了候选辅助任务集的指数级数量，通过多老虎机结构提高了选择效率，能够有效识别哪些任务可能从联合学习中受益。

Conclusion: BandiK通过创新的三阶段多老虎机方法，解决了多任务学习中辅助任务子集选择的关键挑战，为知识转移和避免负迁移提供了有效的解决方案。

Abstract: The challenge of effectively transferring knowledge across multiple tasks is of critical importance and is also present in downstream tasks with foundation models. However, the nature of transfer, its transitive-intransitive nature, is still an open problem, and negative transfer remains a significant obstacle. Selection of beneficial auxiliary task sets in multi-task learning is frequently hindered by the high computational cost of their evaluation, the high number of plausible candidate auxiliary sets, and the varying complexity of selection across target tasks.
  To address these constraints, we introduce BandiK, a novel three-stage multi-task auxiliary task subset selection method using multi-bandits, where each arm pull evaluates candidate auxiliary sets by training and testing a multiple output neural network on a single random train-test dataset split. Firstly, BandiK estimates the pairwise transfers between tasks, which helps in identifying which tasks are likely to benefit from joint learning. In the second stage, it constructs a linear number of candidate sets of auxiliary tasks (in the number of all tasks) for each target task based on the initial estimations, significantly reducing the exponential number of potential auxiliary task sets. Thirdly, it employs a Multi-Armed Bandit (MAB) framework for each task, where the arms correspond to the performance of candidate auxiliary sets realized as multiple output neural networks over train-test data set splits. To enhance efficiency, BandiK integrates these individual task-specific MABs into a multi-bandit structure. The proposed multi-bandit solution exploits that the same neural network realizes multiple arms of different individual bandits corresponding to a given candidate set. This semi-overlapping arm property defines a novel multi-bandit cost/reward structure utilized in BandiK.

</details>


### [107] [CPR: Causal Physiological Representation Learning for Robust ECG Analysis under Distribution Shifts](https://arxiv.org/abs/2512.24564)
*Shunbo Jia,Caizhi Liao*

Main category: cs.LG

TL;DR: 提出CPR方法解决ECG诊断深度学习模型对抗性攻击脆弱性问题，通过因果解缠框架分离病理形态与伪影，在保持单次推理效率的同时达到随机平滑的认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有ECG诊断深度学习模型对平滑对抗性扰动（SAP）脆弱，现有防御方法面临两难：对抗训练计算成本高，随机平滑等认证方法推理延迟大，不适合实时临床监测。作者认为这种脆弱性源于模型依赖非鲁棒的伪相关而非不变的病理特征。

Method: 提出因果生理表征学习（CPR），在因果解缠框架中融入生理结构先验。通过结构因果模型（SCM）建模ECG生成，强制执行结构干预，严格分离不变病理形态（P-QRS-T复合波）与非因果伪影。

Result: 在PTB-XL数据集上，CPR显著优于标准临床预处理方法。在SAP攻击下，CPR达到0.632的F1分数，比中值平滑（0.541 F1）提升9.1%。关键的是，CPR在保持单次推理效率的同时，匹配了随机平滑的认证鲁棒性。

Conclusion: CPR在鲁棒性、效率和临床可解释性之间提供了优越的权衡，为解决ECG诊断模型对抗性脆弱性问题提供了有效方案，特别适合实时临床监测应用。

Abstract: Deep learning models for Electrocardiogram (ECG) diagnosis have achieved remarkable accuracy but exhibit fragility against adversarial perturbations, particularly Smooth Adversarial Perturbations (SAP) that mimic biological morphology. Existing defenses face a critical dilemma: Adversarial Training (AT) provides robustness but incurs a prohibitive computational burden, while certified methods like Randomized Smoothing (RS) introduce significant inference latency, rendering them impractical for real-time clinical monitoring. We posit that this vulnerability stems from the models' reliance on non-robust spurious correlations rather than invariant pathological features. To address this, we propose Causal Physiological Representation Learning (CPR). Unlike standard denoising approaches that operate without semantic constraints, CPR incorporates a Physiological Structural Prior within a causal disentanglement framework. By modeling ECG generation via a Structural Causal Model (SCM), CPR enforces a structural intervention that strictly separates invariant pathological morphology (P-QRS-T complex) from non-causal artifacts. Empirical results on PTB-XL demonstrate that CPR significantly outperforms standard clinical preprocessing methods. Specifically, under SAP attacks, CPR achieves an F1 score of 0.632, surpassing Median Smoothing (0.541 F1) by 9.1%. Crucially, CPR matches the certified robustness of Randomized Smoothing while maintaining single-pass inference efficiency, offering a superior trade-off between robustness, efficiency, and clinical interpretability.

</details>


### [108] [MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control](https://arxiv.org/abs/2512.24955)
*Yongwei Zhang,Yuanzhe Xing,Quan Quan,Zhikun She*

Main category: cs.LG

TL;DR: MSACL是一个将指数稳定性理论与最大熵强化学习相结合的新框架，通过多步Lyapunov证书学习实现可验证的安全控制，在简单奖励下实现指数稳定性和快速收敛。


<details>
  <summary>Details</summary>
Motivation: 模型无关强化学习在实现可证明稳定性方面存在挑战，特别是在平衡探索与严格安全性方面。现有方法依赖复杂的奖励工程，需要一种能提供理论稳定性保证的框架。

Method: MSACL通过多步Lyapunov证书学习，利用离策略多步数据学习满足理论稳定性条件的Lyapunov证书。引入指数稳定性标签(ESL)和λ加权聚合机制平衡多步学习中的偏差-方差权衡。策略优化由稳定性感知优势函数指导，确保学习策略促进快速Lyapunov下降。

Result: 在六个基准测试（包括稳定化和非线性跟踪任务）中，MSACL优于最先进的基于Lyapunov的RL算法。在简单奖励下实现指数稳定性和快速收敛，对不确定性具有显著鲁棒性，并能泛化到未见轨迹。敏感性分析确定多步视野n=20作为跨不同系统的鲁棒默认值。

Conclusion: MSACL通过将Lyapunov理论与离策略actor-critic框架相结合，为可验证的安全学习控制提供了基础。该框架在简单奖励下实现指数稳定性和快速收敛，具有鲁棒性和泛化能力。

Abstract: Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-step data to learn Lyapunov certificates satisfying theoretical stability conditions. By introducing Exponential Stability Labels (ESL) and a $λ$-weighted aggregation mechanism, the framework effectively balances the bias-variance trade-off in multi-step learning. Policy optimization is guided by a stability-aware advantage function, ensuring the learned policy promotes rapid Lyapunov descent. We evaluate MSACL across six benchmarks, including stabilization and nonlinear tracking tasks, demonstrating its superiority over state-of-the-art Lyapunov-based RL algorithms. MSACL achieves exponential stability and rapid convergence under simple rewards, while exhibiting significant robustness to uncertainties and generalization to unseen trajectories. Sensitivity analysis establishes the multi-step horizon $n=20$ as a robust default across diverse systems. By linking Lyapunov theory with off-policy actor-critic frameworks, MSACL provides a foundation for verifiably safe learning-based control. Source code and benchmark environments will be made publicly available.

</details>


### [109] [A Scalable Framework for logP Prediction: From Terabyte-Scale Data Integration to Interpretable Ensemble Modeling](https://arxiv.org/abs/2512.24643)
*Malikussaid,Septian Caesar Floresko,Ade Romadhony,Isman Kurniawan,Warih Maharani,Hilal Hudan Nuha*

Main category: cs.LG

TL;DR: 本研究开发了一个大规模logP预测框架，使用来自PubChem、ChEMBL和eMolecules三个数据库的426,850个生物活性化合物，通过创新的计算基础设施实现了740倍的处理速度提升，发现树集成模型在logP预测中表现最佳，并提出了分层建模策略。


<details>
  <summary>Details</summary>
Motivation: logP（脂水分配系数）是药物设计中关键的物理化学性质，但现有预测方法面临数据整合挑战和模型性能限制。本研究旨在开发一个大规模、高效的logP预测框架，探索脂溶性的多变量性质，并建立基于2D描述符的稳健基准模型。

Method: 1. 数据整合：从PubChem、ChEMBL和eMolecules三个权威化学数据库交叉筛选426,850个生物活性化合物；2. 计算基础设施：采用字节偏移索引架构，将处理时间从预计100多天减少到3.2小时；3. 模型评估：系统评估线性模型和树集成模型（随机森林、XGBoost）；4. 分层建模：针对药物样分子（91%）和极端情况（9%）分别建立专门模型；5. 特征分析：使用SHAP分析确定分子权重为最重要的全局预测因子。

Result: 1. 树集成模型表现最佳：随机森林和XGBoost在测试集上达到R平方0.765和RMSE 0.731 logP单位；2. 线性模型存在异方差性问题，传统修正方法无效；3. 分层建模策略效果最优：药物样分子子集RMSE为0.838，极端分子R平方为0.767；4. 分子权重被确定为最重要的全局预测因子，尽管其双变量相关性较弱；5. 基于2D描述符的集成模型与最先进的图神经网络架构保持竞争力。

Conclusion: 本研究成功开发了一个高效的大规模logP预测框架，证明了精心策划的描述符集成模型在脂溶性预测中的竞争力。研究提供了分子设计的实用指导，建立了基于2D描述符的稳健基准，并展示了分层建模策略在提升预测性能方面的优势。这些发现对药物发现和分子设计具有重要意义。

Abstract: This study presents a large-scale predictive modeling framework for logP prediction using 426850 bioactive compounds rigorously curated from the intersection of three authoritative chemical databases: PubChem, ChEMBL, and eMolecules. We developed a novel computational infrastructure to address the data integration challenge, reducing processing time from a projected over 100 days to 3.2 hours through byte-offset indexing architecture, a 740-fold improvement. Our comprehensive analysis revealed critical insights into the multivariate nature of lipophilicity: while molecular weight exhibited weak bivariate correlation with logP, SHAP analysis on ensemble models identified it as the single most important predictor globally. We systematically evaluated multiple modeling approaches, discovering that linear models suffered from inherent heteroskedasticity that classical remediation strategies, including weighted least squares and Box-Cox transformation, failed to address. Tree-based ensemble methods, including Random Forest and XGBoost, proved inherently robust to this violation, achieving an R-squared of 0.765 and RMSE of 0.731 logP units on the test set. Furthermore, a stratified modeling strategy, employing specialized models for drug-like molecules (91 percent of dataset) and extreme cases (nine percent), achieved optimal performance: an RMSE of 0.838 for the drug-like subset and an R-squared of 0.767 for extreme molecules, the highest of all evaluated approaches. These findings provide actionable guidance for molecular design, establish robust baselines for lipophilicity prediction using only 2D descriptors, and demonstrate that well-curated, descriptor-based ensemble models remain competitive with state-of-the-art graph neural network architectures.

</details>


### [110] [Semi-overlapping Multi-bandit Best Arm Identification for Sequential Support Network Learning](https://arxiv.org/abs/2512.24959)
*András Antos,András Millinghoffer,Péter Antal*

Main category: cs.LG

TL;DR: 论文提出了序列支持网络学习（SSNL）框架，用于通过试验选择最优合作伙伴集合，并引入半重叠多臂老虎机（SOMMAB）模型来高效学习支持网络。开发了GapE算法并推导出指数误差界，展示了共享评估带来的样本复杂度优势。


<details>
  <summary>Details</summary>
Motivation: 现代AI/ML问题需要评估合作伙伴贡献，但现有方法在共享评估和选择最优候选集方面存在效率问题。需要统一框架来处理这种不对称、计算密集的序列选择过程。

Method: 提出序列支持网络学习（SSNL）框架，引入半重叠多臂老虎机（SOMMAB）模型，其中单个评估为多个老虎机提供不同反馈。开发了广义GapE算法，并推导了新的指数误差界。

Result: SOMMAB模型能高效从稀疏候选列表中学习支持网络。新误差界改进了多臂老虎机最佳臂识别的指数常数，样本复杂度随重叠度线性缩放，共享评估带来显著效率提升。

Conclusion: 该工作为多任务学习、辅助任务学习、联邦学习和多智能体系统中的序列学习工具提供了理论基础和性能保证，展示了共享评估在识别支持网络中的优势。

Abstract: Many modern AI and ML problems require evaluating partners' contributions through shared yet asymmetric, computationally intensive processes and the simultaneous selection of the most beneficial candidates. Sequential approaches to these problems can be unified under a new framework, Sequential Support Network Learning (SSNL), in which the goal is to select the most beneficial candidate set of partners for all participants using trials; that is, to learn a directed graph that represents the highest-performing contributions. We demonstrate that a new pure-exploration model, the semi-overlapping multi-(multi-armed) bandit (SOMMAB), in which a single evaluation provides distinct feedback to multiple bandits due to structural overlap among their arms, can be used to learn a support network from sparse candidate lists efficiently.
  We develop a generalized GapE algorithm for SOMMABs and derive new exponential error bounds that improve the best known constant in the exponent for multi-bandit best-arm identification. The bounds scale linearly with the degree of overlap, revealing significant sample-complexity gains arising from shared evaluations.
  From an application point of view, this work provides a theoretical foundation and improved performance guarantees for sequential learning tools for identifying support networks from sparse candidates in multiple learning problems, such as in multi-task learning (MTL), auxiliary task learning (ATL), federated learning (FL), and in multi-agent systems (MAS).

</details>


### [111] [HeteroHBA: A Generative Structure-Manipulating Backdoor Attack on Heterogeneous Graphs](https://arxiv.org/abs/2512.24665)
*Honglin Gao,Lan Zhao,Junhao Ren,Xiang Li,Gaoxi Xiao*

Main category: cs.LG

TL;DR: 提出了一种针对异构图神经网络的生成式后门攻击框架HeteroHBA，通过选择有影响力的辅助邻居节点、合成多样化的触发器特征和连接模式，并利用AdaIN和MMD损失提高隐蔽性，在保持清洁准确率的同时实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 异构图神经网络在实际应用中表现出色，但针对异构图的定向后门中毒攻击研究较少。现有攻击方法在异构图上的效果有限，需要开发更隐蔽、更有效的攻击框架来揭示异构图的潜在安全风险。

Method: 1) 通过显著性筛选选择有影响力的辅助邻居节点进行触发器附着；2) 合成多样化的触发器特征和连接模式以匹配局部异质上下文；3) 结合自适应实例归一化(AdaIN)和最大均值差异(MMD)损失对齐触发器特征分布与良性统计特征，提高隐蔽性；4) 使用双层优化目标联合促进攻击成功并保持清洁准确率。

Result: 在多个真实世界异构图数据集和代表性HGNN架构上的实验表明，HeteroHBA相比现有后门基线方法，在保持可比或更小清洁准确率影响的同时，实现了更高的攻击成功率。攻击在异构感知的结构防御CSD下仍然有效。

Conclusion: 该研究揭示了异构图学习中的实际后门风险，强调了开发更强防御机制的必要性。提出的HeteroHBA框架为评估异构图神经网络的安全性提供了有效工具。

Abstract: Heterogeneous graph neural networks (HGNNs) have achieved strong performance in many real-world applications, yet targeted backdoor poisoning on heterogeneous graphs remains less studied. We consider backdoor attacks for heterogeneous node classification, where an adversary injects a small set of trigger nodes and connections during training to force specific victim nodes to be misclassified into an attacker-chosen label at test time while preserving clean performance. We propose HeteroHBA, a generative backdoor framework that selects influential auxiliary neighbors for trigger attachment via saliency-based screening and synthesizes diverse trigger features and connection patterns to better match the local heterogeneous context. To improve stealthiness, we combine Adaptive Instance Normalization (AdaIN) with a Maximum Mean Discrepancy (MMD) loss to align the trigger feature distribution with benign statistics, thereby reducing detectability, and we optimize the attack with a bilevel objective that jointly promotes attack success and maintains clean accuracy. Experiments on multiple real-world heterogeneous graphs with representative HGNN architectures show that HeteroHBA consistently achieves higher attack success than prior backdoor baselines with comparable or smaller impact on clean accuracy; moreover, the attack remains effective under our heterogeneity-aware structural defense, CSD. These results highlight practical backdoor risks in heterogeneous graph learning and motivate the development of stronger defenses.

</details>


### [112] [Generative Classifiers Avoid Shortcut Solutions](https://arxiv.org/abs/2512.25034)
*Alexander C. Li,Ananya Kumar,Deepak Pathak*

Main category: cs.LG

TL;DR: 生成式分类器通过建模所有特征（包括核心和虚假特征）来避免判别式分类器过度依赖虚假相关性的问题，在分布偏移下表现更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 判别式分类器在分布内学习时经常依赖虚假相关性特征，这些特征在分布偏移下会失效。需要一种能避免这种问题的分类方法。

Method: 使用基于扩散模型和自回归模型的生成式分类器，通过建模类条件生成模型来学习所有特征，而不专门针对虚假相关性。

Result: 在五个标准图像和文本分布偏移基准测试中达到最先进性能，在医学和卫星数据集等实际应用中有效减少虚假相关性的影响。

Conclusion: 生成式分类器通过建模所有特征而非主要依赖虚假特征，能更鲁棒地处理分布偏移，在特定数据条件下优于判别式分类器。

Abstract: Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.

</details>


### [113] [Mobility-Assisted Decentralized Federated Learning: Convergence Analysis and A Data-Driven Approach](https://arxiv.org/abs/2512.24694)
*Reza Jahani,Md Farhamdur Reza,Richeng Jin,Huaiyu Dai*

Main category: cs.LG

TL;DR: 本文系统研究了移动性在去中心化联邦学习中的作用，提出利用移动用户增强稀疏网络中的信息传播，显著提升DFL性能


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习在稀疏网络和数据异构情况下性能显著下降，而下一代无线网络中移动性日益普遍，但移动性对DFL的影响尚未得到充分研究，尽管移动用户可作为中继或桥梁增强信息流

Method: 首先建立稀疏网络中移动用户下DFL的收敛性理论，证明随机移动即可提升性能；然后提出利用诱导移动模式的DFL框架，使移动用户能够根据数据分布知识确定轨迹以增强网络信息传播

Result: 理论分析表明即使部分用户随机移动也能显著提升性能；实验验证了理论发现，证明所提方法优于基线方法，并全面分析了各种网络参数对移动网络中DFL性能的影响

Conclusion: 移动性在提升去中心化联邦学习性能方面具有重要作用，利用移动用户的诱导移动模式可以有效增强稀疏网络中的信息传播，为下一代无线网络中的DFL应用提供了新思路

Abstract: Decentralized Federated Learning (DFL) has emerged as a privacy-preserving machine learning paradigm that enables collaborative training among users without relying on a central server. However, its performance often degrades significantly due to limited connectivity and data heterogeneity. As we move toward the next generation of wireless networks, mobility is increasingly embedded in many real-world applications. The user mobility, either natural or induced, enables clients to act as relays or bridges, thus enhancing information flow in sparse networks; however, its impact on DFL has been largely overlooked despite its potential. In this work, we systematically investigate the role of mobility in improving DFL performance. We first establish the convergence of DFL in sparse networks under user mobility and theoretically demonstrate that even random movement of a fraction of users can significantly boost performance. Building upon this insight, we propose a DFL framework that utilizes mobile users with induced mobility patterns, allowing them to exploit the knowledge of data distribution to determine their trajectories to enhance information propagation through the network. Through extensive experiments, we empirically confirm our theoretical findings, validate the superiority of our approach over baselines, and provide a comprehensive analysis of how various network parameters influence DFL performance in mobile networks.

</details>


### [114] [Causal Discovery with Mixed Latent Confounding via Precision Decomposition](https://arxiv.org/abs/2512.24696)
*Amir Asiaee,Samhita Pal,James O'quinn,James P. Long*

Main category: cs.LG

TL;DR: 提出DCL-DECOR方法，用于从受混合潜在混杂影响的线性高斯系统中进行因果发现，该方法通过精度矩阵分解分离广泛混杂效应，然后应用相关噪声DAG学习器恢复有向边


<details>
  <summary>Details</summary>
Motivation: 现实世界中观测数据常受混合潜在混杂影响，其中一些未观测因素广泛影响多个变量，而另一些只影响少数变量。现有方法面临挑战：可微分和基于分数的DAG学习器可能将全局潜在效应误解为因果边，而潜在变量图模型只能恢复无向结构

Method: 提出DCL-DECOR模块化精度导向流程：1)通过将观测精度矩阵分解为结构化分量和低秩分量来隔离广泛混杂效应；2)对去混杂表示应用相关噪声DAG学习器恢复有向边，同时建模剩余的结构化误差相关性；3)执行简单协调步骤以确保无弓形结构

Result: 提供了混合混杂下可恢复因果目标的识别性结果，展示了整体问题如何简化为具有模块化保证的已研究子问题。合成实验表明，在广泛混杂强度和维度变化的情况下，相比直接将相关噪声DAG学习应用于混杂数据，该方法在有向边恢复方面有持续改进

Conclusion: DCL-DECOR方法有效处理混合潜在混杂问题，通过精度矩阵分解分离广泛混杂效应，结合相关噪声DAG学习，在因果发现中实现了更好的有向边恢复性能

Abstract: We study causal discovery from observational data in linear Gaussian systems affected by \emph{mixed latent confounding}, where some unobserved factors act broadly across many variables while others influence only small subsets. This setting is common in practice and poses a challenge for existing methods: differentiable and score-based DAG learners can misinterpret global latent effects as causal edges, while latent-variable graphical models recover only undirected structure.
  We propose \textsc{DCL-DECOR}, a modular, precision-led pipeline that separates these roles. The method first isolates pervasive latent effects by decomposing the observed precision matrix into a structured component and a low-rank component. The structured component corresponds to the conditional distribution after accounting for pervasive confounders and retains only local dependence induced by the causal graph and localized confounding. A correlated-noise DAG learner is then applied to this deconfounded representation to recover directed edges while modeling remaining structured error correlations, followed by a simple reconciliation step to enforce bow-freeness.
  We provide identifiability results that characterize the recoverable causal target under mixed confounding and show how the overall problem reduces to well-studied subproblems with modular guarantees. Synthetic experiments that vary the strength and dimensionality of pervasive confounding demonstrate consistent improvements in directed edge recovery over applying correlated-noise DAG learning directly to the confounded data.

</details>


### [115] [FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference](https://arxiv.org/abs/2512.24713)
*Fen-Yu Hsieh,Yun-Chang Teng,Ding-Yong Hong,Jan-Jan Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种结合权重剪枝和低位量化的自动化框架，通过软硬件协同设计在FPGA平台上生成加速器，显著降低LLM的内存占用并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异，但计算和内存需求巨大，阻碍了其在资源受限环境中的部署。需要开发高效的压缩和加速技术来解决这一挑战。

Method: 采用N:M结构化剪枝和4位整数量化的统一流水线来减少内存占用，结合优化的反量化和矩阵乘法，在CPU、GPU和自定义的基于脉动阵列的FPGA加速器上实现LLM推理加速。

Result: 在4096×4096矩阵上，结合2:4稀疏性和量化，实现了权重存储减少4倍，矩阵乘法加速1.71倍，端到端延迟降低1.29倍。在LLaMA-7B模型上，结构化稀疏性使每token吞吐量提升1.36倍。

Conclusion: 细粒度N:M稀疏性和量化的协同作用能够实现高效且可部署的LLM推理，而提出的FPGA加速器为支持超出固定2:4硬件约束的更广泛稀疏模式提供了灵活的架构路径。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of language processing tasks. However, this success comes at the cost of substantial computation and memory requirements, which significantly impedes their deployment in resource-constrained environments. To address this challenge, this work introduces an automation framework that leverages weight pruning and low-bit quantization, and presents a hardware-software co-design method that generates accelerators on the Field-Programmable Gate Array (FPGA) platform. In particular, we implement a unified pipeline that applies N:M structured pruning and 4-bit integer quantization to reduce the memory footprint, followed by optimized dequantization and matrix multiplication to enhance LLM inference on several hardware platforms, including CPUs, NVIDIA GPUs with Dense and 2:4 Sparse Tensor Cores, and a custom systolic-array-based FPGA accelerator. Utilizing 2:4 sparsity combined with quantization on $4096 \times 4096$ matrices, our approach achieves a reduction of up to $4\times$ in weight storage and a $1.71\times$ speedup in matrix multiplication, yielding a $1.29\times$ end-to-end latency reduction compared to dense GPU baselines. Scaling analysis on the LLaMA-7B model further shows that structured sparsity enhances the throughput per token by $1.36\times$. These results demonstrate the synergy of fine-grained N:M sparsity and quantization for enabling efficient and deployable LLM inference, while the proposed FPGA accelerator offers a flexible architectural path for supporting a broader class of sparsity patterns beyond the fixed 2:4 hardware constraints.

</details>


### [116] [From Trial to Deployment: A SEM Analysis of Traveler Adoptions to Fully Operational Autonomous Taxis](https://arxiv.org/abs/2512.24767)
*Yutong Cai,Hua Wang*

Main category: cs.LG

TL;DR: 基于武汉百度Apollo Robotaxi实际运营数据的调查显示，成本敏感性和行为意向是自动驾驶出租车采用的最强预测因素，为政策制定和定价策略提供实证依据。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于假设场景探讨用户对自动驾驶出租车的接受度，缺乏基于实际运营服务的用户行为研究。本研究旨在填补这一空白，利用武汉百度Apollo Robotaxi实际运营数据，分析真实用户行为。

Method: 在武汉（百度Apollo Robotaxi运营城市）设计包含实际服务属性的调查问卷，收集336份有效用户响应。使用结构方程模型识别六个潜在心理构念：信任与政策支持、成本敏感性、性能、行为意向、生活方式和教育，并分析它们对自动驾驶出租车采用行为（通过10个场景中的选择频率测量）的影响。

Result: 成本敏感性和行为意向是自动驾驶出租车采用的最强正向预测因素，其他潜在构念发挥更微妙的作用。模型在多个拟合指数上表现出良好的拟合度。

Conclusion: 研究结果为现实城市环境中自动驾驶出租车规模化部署的政策制定、票价设计和公众推广策略提供了实证支持，强调了基于实际运营数据研究的重要性。

Abstract: Autonomous taxi services represent a transformative advancement in urban mobility, offering safety, efficiency, and round-the-clock operations. While existing literature has explored user acceptance of autonomous taxis through stated preference experiments and hypothetical scenarios, few studies have investigated actual user behavior based on operational AV services. This study addresses that gap by leveraging survey data from Wuhan, China, where Baidu's Apollo Robotaxi service operates at scale. We design a realistic survey incorporating actual service attributes and collect 336 valid responses from actual users. Using Structural Equation Modeling, we identify six latent psychological constructs, namely Trust \& Policy Support, Cost Sensitivity, Performance, Behavioral Intention, Lifestyle, and Education. Their influences on adoption behavior, measured by the selection frequency of autonomous taxis in ten scenarios, are examined and interpreted. Results show that Cost Sensitivity and Behavioral Intention are the strongest positive predictors of adoption, while other latent constructs play more nuanced roles. The model demonstrates strong goodness-of-fit across multiple indices. Our findings offer empirical evidence to support policymaking, fare design, and public outreach strategies for scaling autonomous taxis deployments in real-world urban settings.

</details>


### [117] [Gradient Descent as Implicit EM in Distance-Based Neural Models](https://arxiv.org/abs/2512.24780)
*Alan Oursland*

Main category: cs.LG

TL;DR: 论文证明了对数-求和-指数结构的损失函数梯度等于负后验责任，使梯度下降隐式执行期望最大化，统一了无监督混合建模、注意力机制和分类学习


<details>
  <summary>Details</summary>
Motivation: 神经网络在标准目标训练中表现出概率推断特征（软聚类、原型专业化、贝叶斯不确定性跟踪），但现有解释依赖混合模型的松散类比或事后架构解释，缺乏直接推导

Method: 对于任何具有对数-求和-指数结构（基于距离或能量）的目标函数，证明梯度相对于每个距离的偏导数等于对应分量的负后验责任：∂L/∂d_j = -r_j，这是一个代数恒等式而非近似

Result: 梯度下降在此类目标上隐式执行期望最大化，责任不是需要计算的辅助变量而是需要应用的梯度，无需显式推断算法，因为推断已嵌入优化过程

Conclusion: 该结果在单一机制下统一了三种学习范式：无监督混合建模（责任完全潜在）、注意力机制（责任以查询为条件）、交叉熵分类（监督将责任固定为目标），训练transformer中观察到的贝叶斯结构不是涌现属性而是目标几何的必要结果

Abstract: Neural networks trained with standard objectives exhibit behaviors characteristic of probabilistic inference: soft clustering, prototype specialization, and Bayesian uncertainty tracking. These phenomena appear across architectures -- in attention mechanisms, classification heads, and energy-based models -- yet existing explanations rely on loose analogies to mixture models or post-hoc architectural interpretation. We provide a direct derivation. For any objective with log-sum-exp structure over distances or energies, the gradient with respect to each distance is exactly the negative posterior responsibility of the corresponding component: $\partial L / \partial d_j = -r_j$. This is an algebraic identity, not an approximation. The immediate consequence is that gradient descent on such objectives performs expectation-maximization implicitly -- responsibilities are not auxiliary variables to be computed but gradients to be applied. No explicit inference algorithm is required because inference is embedded in optimization. This result unifies three regimes of learning under a single mechanism: unsupervised mixture modeling, where responsibilities are fully latent; attention, where responsibilities are conditioned on queries; and cross-entropy classification, where supervision clamps responsibilities to targets. The Bayesian structure recently observed in trained transformers is not an emergent property but a necessary consequence of the objective geometry. Optimization and inference are the same process.

</details>


### [118] [Self-Supervised Neural Architecture Search for Multimodal Deep Neural Networks](https://arxiv.org/abs/2512.24793)
*Shota Suzuki,Satoshi Ono*

Main category: cs.LG

TL;DR: 提出了一种基于自监督学习的多模态神经网络架构搜索方法，能够在无标注数据上完成架构设计和模型预训练


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络需要融合多个模态的特征，结构复杂，传统NAS方法需要大量标注数据，这限制了其在多模态场景中的应用

Method: 提出一种自监督学习方法，将SSL全面应用于架构搜索和模型预训练两个过程，从而在无标注数据上完成多模态神经网络的架构设计

Result: 实验结果表明，该方法能够成功地从无标注训练数据中设计出多模态神经网络的架构

Conclusion: 该方法解决了多模态NAS需要大量标注数据的问题，为无标注数据下的多模态神经网络架构设计提供了有效解决方案

Abstract: Neural architecture search (NAS), which automates the architectural design process of deep neural networks (DNN), has attracted increasing attention. Multimodal DNNs that necessitate feature fusion from multiple modalities benefit from NAS due to their structural complexity; however, constructing an architecture for multimodal DNNs through NAS requires a substantial amount of labeled training data. Thus, this paper proposes a self-supervised learning (SSL) method for architecture search of multimodal DNNs. The proposed method applies SSL comprehensively for both the architecture search and model pretraining processes. Experimental results demonstrated that the proposed method successfully designed architectures for DNNs from unlabeled training data.

</details>


### [119] [DTI-GP: Bayesian operations for drug-target interactions using deep kernel Gaussian processes](https://arxiv.org/abs/2512.24810)
*Bence Bolgár,András Millinghoffer,Péter Antal*

Main category: cs.LG

TL;DR: 本文提出DTI-GP，一种基于深度核学习的高斯过程架构，用于药物-靶点相互作用预测，提供概率信息并支持贝叶斯分类、拒绝、top-K选择和排序等操作。


<details>
  <summary>Details</summary>
Motivation: 药物-靶点相互作用预测的精确概率信息对于理解模型局限性和提升预测性能至关重要。现有方法缺乏可靠的贝叶斯推断框架来支持新颖的操作。

Method: 提出DTI-GP架构，包含化学化合物和蛋白质靶点的神经嵌入模块与高斯过程模块。通过从预测分布中采样估计贝叶斯优先矩阵，用于快速准确的筛选和排序操作。

Result: DTI-GP在性能上超越了现有最先进方法，能够构建贝叶斯准确度-置信度富集分数，实现改进富集的拒绝方案，以及估计和搜索具有高期望效用的top-K选择和排序。

Conclusion: DTI-GP为药物-靶点相互作用预测提供了一个可扩展的贝叶斯框架，支持多种新颖操作，显著提升了预测性能和实用性。

Abstract: Precise probabilistic information about drug-target interaction (DTI) predictions is vital for understanding limitations and boosting predictive performance. Gaussian processes (GP) offer a scalable framework to integrate state-of-the-art DTI representations and Bayesian inference, enabling novel operations, such as Bayesian classification with rejection, top-$K$ selection, and ranking. We propose a deep kernel learning-based GP architecture (DTI-GP), which incorporates a combined neural embedding module for chemical compounds and protein targets, and a GP module. The workflow continues with sampling from the predictive distribution to estimate a Bayesian precedence matrix, which is used in fast and accurate selection and ranking operations. DTI-GP outperforms state-of-the-art solutions, and it allows (1) the construction of a Bayesian accuracy-confidence enrichment score, (2) rejection schemes for improved enrichment, and (3) estimation and search for top-$K$ selections and ranking with high expected utility.

</details>


### [120] [Unregularized Linear Convergence in Zero-Sum Game from Preference Feedback](https://arxiv.org/abs/2512.24818)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文首次证明了乐观乘性权重更新（OMWU）在Nash学习人类反馈（NLHF）中的收敛性，表明在存在全支撑纳什均衡时，OMWU在预热阶段后能实现最后迭代线性收敛，且收敛到原始纳什均衡，无需纳什均衡唯一性假设。


<details>
  <summary>Details</summary>
Motivation: 标准偏好建模使用Bradley-Terry模型假设传递性，忽略了人类群体偏好的内在复杂性。Nash学习人类反馈（NLHF）通过将非传递性偏好建模为两人零和博弈来解决此问题，但对齐简化为寻找纳什均衡。现有算法通常依赖正则化，在计算原始博弈的对偶间隙时会产生不可避免的偏差。

Method: 采用乐观乘性权重更新（OMWU）算法，分析其在NLHF中的收敛性。该方法将非传递性偏好建模为两人零和博弈，通过OMWU寻找纳什均衡，无需正则化即可直接收敛到原始博弈的纳什均衡。

Result: 首次证明了OMWU在NLHF中的收敛保证：当存在全支撑纳什均衡时，OMWU在预热阶段后能实现最后迭代线性收敛，收敛到原始纳什均衡，且收敛率是实例依赖的线性收敛率。相比先前结果，无需纳什均衡唯一性假设。分析发现了一种新的边际收敛行为，即很少被选择的动作概率从指数小的值开始指数增长。

Conclusion: OMWU在NLHF中具有理论优势，无需正则化即可收敛到原始纳什均衡，且无需纳什均衡唯一性假设。实验验证了OMWU在表格和神经网络策略类别中的有效性，展示了其在大型语言模型应用中的潜力。

Abstract: Aligning large language models (LLMs) with human preferences has proven effective for enhancing model capabilities, yet standard preference modeling using the Bradley-Terry model assumes transitivity, overlooking the inherent complexity of human population preferences. Nash learning from human feedback (NLHF) addresses this by framing non-transitive preferences as a two-player zero-sum game, where alignment reduces to finding the Nash equilibrium (NE). However, existing algorithms typically rely on regularization, incurring unavoidable bias when computing the duality gap in the original game. In this work, we provide the first convergence guarantee for Optimistic Multiplicative Weights Update ($\mathtt{OMWU}$) in NLHF, showing that it achieves last-iterate linear convergence after a burn-in phase whenever an NE with full support exists, with an instance-dependent linear convergence rate to the original NE, measured by duality gaps. Compared to prior results in Wei et al. (2020), we do not require the assumption of NE uniqueness. Our analysis identifies a novel marginal convergence behavior, where the probability of rarely played actions grows exponentially from exponentially small values, enabling exponentially better dependence on instance-dependent constants than prior results. Experiments corroborate the theoretical strengths of $\mathtt{OMWU}$ in both tabular and neural policy classes, demonstrating its potential for LLM applications.

</details>


### [121] [Discovering Coordinated Joint Options via Inter-Agent Relative Dynamics](https://arxiv.org/abs/2512.24827)
*Raul D. Steleac,Mohan Sridharan,David Abel*

Main category: cs.LG

TL;DR: 提出一种多智能体选项发现新方法，通过联合状态抽象和费马状态近似来发现强协调行为，相比现有方法能产生更好的协调能力。


<details>
  <summary>Details</summary>
Motivation: 多智能体环境中，联合状态空间随智能体数量指数增长，使得协调行为设计特别困难。现有多智能体选项发现方法往往牺牲协调性，产生松散耦合或完全独立的行为。

Method: 提出基于联合状态抽象的方法：1) 近似费马状态（与团队最大对齐的虚构状态）；2) 定义"分散度"度量团队级不对齐；3) 使用神经图拉普拉斯估计器推导捕获智能体间状态同步模式的选项。

Result: 在多个场景和两个多智能体领域评估，结果显示该方法产生的选项相比其他选项发现方法具有更强的下游协调能力。

Conclusion: 该方法通过状态同步模式发现强协调的多智能体选项，有效解决了多智能体选项发现中的协调性挑战，在无明确目标情况下提供了自然的协调基础。

Abstract: Temporally extended actions improve the ability to explore and plan in single-agent settings. In multi-agent settings, the exponential growth of the joint state space with the number of agents makes coordinated behaviours even more valuable. Yet, this same exponential growth renders the design of multi-agent options particularly challenging. Existing multi-agent option discovery methods often sacrifice coordination by producing loosely coupled or fully independent behaviours. Toward addressing these limitations, we describe a novel approach for multi-agent option discovery. Specifically, we propose a joint-state abstraction that compresses the state space while preserving the information necessary to discover strongly coordinated behaviours. Our approach builds on the inductive bias that synchronisation over agent states provides a natural foundation for coordination in the absence of explicit objectives. We first approximate a fictitious state of maximal alignment with the team, the \textit{Fermat} state, and use it to define a measure of \textit{spreadness}, capturing team-level misalignment on each individual state dimension. Building on this representation, we then employ a neural graph Laplacian estimator to derive options that capture state synchronisation patterns between agents. We evaluate the resulting options across multiple scenarios in two multi-agent domains, showing that they yield stronger downstream coordination capabilities compared to alternative option discovery methods.

</details>


### [122] [AODDiff: Probabilistic Reconstruction of Aerosol Optical Depth via Diffusion-based Bayesian Inference](https://arxiv.org/abs/2512.24847)
*Linhao Fan,Hongqiang Fang,Jingyang Dai,Yong Jiang,Qixing Zhang*

Main category: cs.LG

TL;DR: AODDiff：基于扩散贝叶斯推断的概率重建框架，用于气溶胶光学厚度场重建，支持不确定性量化


<details>
  <summary>Details</summary>
Motivation: 当前AOD重建模型受限于完整训练数据的稀缺性和缺乏不确定性量化能力，需要更灵活、鲁棒的解决方案

Method: 提出AODDiff框架：1）使用污染感知训练策略从自然不完整数据学习时空先验；2）采用解耦退火后验采样策略整合异构观测约束

Result: 在再分析数据上的降尺度和修复任务验证了AODDiff的有效性和鲁棒性，特别是在保持高空间光谱保真度方面表现出优势

Conclusion: AODDiff作为生成模型能够灵活适应多种重建任务而无需重新训练，并通过多重采样实现不确定性量化，为下游应用提供关键置信度指标

Abstract: High-quality reconstruction of Aerosol Optical Depth (AOD) fields is critical for Atmosphere monitoring, yet current models remain constrained by the scarcity of complete training data and a lack of uncertainty quantification.To address these limitations, we propose AODDiff, a probabilistic reconstruction framework based on diffusion-based Bayesian inference. By leveraging the learned spatiotemporal probability distribution of the AOD field as a generative prior, this framework can be flexibly adapted to various reconstruction tasks without requiring task-specific retraining. We first introduce a corruption-aware training strategy to learns a spatiotemporal AOD prior solely from naturally incomplete data. Subsequently, we employ a decoupled annealing posterior sampling strategy that enables the more effective and integration of heterogeneous observations as constraints to guide the generation process. We validate the proposed framework through extensive experiments on Reanalysis data. Results across downscaling and inpainting tasks confirm the efficacy and robustness of AODDiff, specifically demonstrating its advantage in maintaining high spatial spectral fidelity. Furthermore, as a generative model, AODDiff inherently enables uncertainty quantification via multiple sampling, offering critical confidence metrics for downstream applications.

</details>


### [123] [Characterization of Transfer Using Multi-task Learning Curves](https://arxiv.org/abs/2512.24866)
*András Millinghoffer,Bence Bolgár,Péter Antal*

Main category: cs.LG

TL;DR: 本文提出使用多任务学习曲线来量化迁移效应，通过扰动数据集而非模型梯度更新来更基础地表征迁移现象，并在药物-靶点相互作用数据集上验证了该方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过梯度更新扰动模型来研究迁移效应，但作者认为通过增加样本量扰动数据集能提供更基础的表征。需要一种定量建模迁移效应的方法，特别是在多任务学习场景下。

Method: 提出使用多任务学习曲线来近似不同样本量下的归纳性能，开发了类似任务亲和性分组的高效近似方法，比较了统计和计算两种迁移方法的成本和效果。

Result: 在药物-靶点相互作用基准数据集上的评估表明，学习曲线能更好地捕捉多任务学习效应，其多任务扩展可以描述基础模型中的成对和上下文迁移效应。统计方法计算成本更高但效果更好、适用范围更广。

Conclusion: 通过扰动数据集而非模型来研究迁移效应提供了更基础的表征，多任务学习曲线是量化迁移效应的有效工具，能够揭示基础模型中复杂的迁移模式。

Abstract: Transfer effects manifest themselves both during training using a fixed data set and in inductive inference using accumulating data. We hypothesize that perturbing the data set by including more samples, instead of perturbing the model by gradient updates, provides a complementary and more fundamental characterization of transfer effects. To capture this phenomenon, we quantitatively model transfer effects using multi-task learning curves approximating the inductive performance over varying sample sizes. We describe an efficient method to approximate multi-task learning curves analogous to the Task Affinity Grouping method applied during training. We compare the statistical and computational approaches to transfer, which indicates considerably higher compute costs for the previous but better power and broader applicability. Evaluations are performed using a benchmark drug-target interaction data set. Our results show that learning curves can better capture the effects of multi-task learning and their multi-task extensions can delineate pairwise and contextual transfer effects in foundation models.

</details>


### [124] [PRISM: A hierarchical multiscale approach for time series forecasting](https://arxiv.org/abs/2512.24898)
*Zihao Chen,Alexandre Andre,Wenrui Ma,Ian Knight,Sergey Shuvaev,Eva Dyer*

Main category: cs.LG

TL;DR: PRISM是一种新的时间序列预测方法，通过可学习的树状分割结构，同时捕捉信号的全局趋势和局部细粒度特征，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列包含全局趋势、局部细粒度结构以及多尺度特征，这使得准确预测具有挑战性。现有方法难以同时有效捕捉这些不同尺度的特征。

Method: 提出PRISM方法，采用可学习的树状分割结构：根节点捕捉信号的全局趋势，递归分割揭示信号的局部视图；每层将数据投影到时频基（如小波或指数移动平均）提取尺度特定特征，并在层次结构中聚合这些特征。

Result: 在多个基准数据集上的实验表明，PRISM方法在预测性能上优于当前最先进的方法。

Conclusion: 这种分层方法为多变量时间序列预测提供了一个轻量级且灵活的框架，能够同时捕捉信号的全局结构和局部动态。

Abstract: Forecasting is critical in areas such as finance, biology, and healthcare. Despite the progress in the field, making accurate forecasts remains challenging because real-world time series contain both global trends, local fine-grained structure, and features on multiple scales in between. Here, we present a new forecasting method, PRISM (Partitioned Representation for Iterative Sequence Modeling), that addresses this challenge through a learnable tree-based partitioning of the signal. At the root of the tree, a global representation captures coarse trends in the signal, while recursive splits reveal increasingly localized views of the signal. At each level of the tree, data are projected onto a time-frequency basis (e.g., wavelets or exponential moving averages) to extract scale-specific features, which are then aggregated across the hierarchy. This design allows the model to jointly capture global structure and local dynamics of the signal, enabling accurate forecasting. Experiments across benchmark datasets show that our method outperforms state-of-the-art methods for forecasting. Overall, these results demonstrate that our hierarchical approach provides a lightweight and flexible framework for forecasting multivariate time series. The code is available at https://github.com/nerdslab/prism.

</details>


### [125] [Spectral Graph Neural Networks for Cognitive Task Classification in fMRI Connectomes](https://arxiv.org/abs/2512.24901)
*Debasis Maji,Arghya Banerjee,Debaditya Barman*

Main category: cs.LG

TL;DR: 提出SpectralBrainGNN模型，基于图傅里叶变换的谱卷积框架，用于从fMRI连接组中解码认知任务，在HCPTask数据集上达到96.25%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉大脑功能连接中的拓扑依赖性和多尺度交互，需要更先进的图神经网络方法来更好地解码认知状态。

Method: 提出SpectralBrainGNN模型，基于图傅里叶变换的谱卷积框架，通过归一化拉普拉斯矩阵特征分解计算GFT，将大脑区域建模为节点，功能连接建模为边。

Result: 在Human Connectome Project-Task数据集上的实验表明，该方法有效，达到96.25%的分类准确率，优于传统方法。

Conclusion: SpectralBrainGNN通过谱卷积框架成功捕捉了大脑网络中的复杂拓扑模式，为认知任务分类提供了有效工具，代码已开源支持可重复性研究。

Abstract: Cognitive task classification using machine learning plays a central role in decoding brain states from neuroimaging data. By integrating machine learning with brain network analysis, complex connectivity patterns can be extracted from functional magnetic resonance imaging connectomes. This process transforms raw blood-oxygen-level-dependent (BOLD) signals into interpretable representations of cognitive processes. Graph neural networks (GNNs) further advance this paradigm by modeling brain regions as nodes and functional connections as edges, capturing topological dependencies and multi-scale interactions that are often missed by conventional approaches. Our proposed SpectralBrainGNN model, a spectral convolution framework based on graph Fourier transforms (GFT) computed via normalized Laplacian eigendecomposition. Experiments on the Human Connectome Project-Task (HCPTask) dataset demonstrate the effectiveness of the proposed approach, achieving a classification accuracy of 96.25\%. The implementation is publicly available at https://github.com/gnnplayground/SpectralBrainGNN to support reproducibility and future research.

</details>


### [126] [Frequent subgraph-based persistent homology for graph classification](https://arxiv.org/abs/2512.24917)
*Xinyang Chen,Amaël Broustet,Guoting Chen*

Main category: cs.LG

TL;DR: 提出基于频繁子图的图过滤方法FSF，生成频率持久同调特征FPH，并开发FPH-ML和FPH-GNN两种图分类框架，在多个基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有图持久同调方法主要依赖度或权重等有限过滤方式，忽略了数据集中重复出现的丰富特征，限制了表达能力。需要更丰富的拓扑特征提取方法。

Method: 提出频繁子图过滤FSF，从频繁子图中提取稳定且信息丰富的频率持久同调特征FPH。开发两种应用框架：FPH-ML（基于FPH的机器学习模型）和FPH-GNN（将FPH与图神经网络集成的混合框架）。

Result: FPH-ML在准确率上与基于核和基于度的过滤方法相当或更优。FPH与图神经网络集成时，在基准测试中相对性能提升0.4-21%，相比GCN和GIN骨干网络提升最高达8.2个百分点。

Conclusion: FSF和FPH方法连接了频繁子图挖掘和拓扑数据分析，为拓扑感知特征提取提供了新视角，显著提升了图分类性能。

Abstract: Persistent homology (PH) has recently emerged as a powerful tool for extracting topological features. Integrating PH into machine learning and deep learning models enhances topology awareness and interpretability. However, most PH methods on graphs rely on a limited set of filtrations, such as degree-based or weight-based filtrations, which overlook richer features like recurring information across the dataset and thus restrict expressive power. In this work, we propose a novel graph filtration called Frequent Subgraph Filtration (FSF), which is derived from frequent subgraphs and produces stable and information-rich frequency-based persistent homology (FPH) features. We study the theoretical properties of FSF and provide both proofs and experimental validation. Beyond persistent homology itself, we introduce two approaches for graph classification: an FPH-based machine learning model (FPH-ML) and a hybrid framework that integrates FPH with graph neural networks (FPH-GNNs) to enhance topology-aware graph representation learning. Our frameworks bridge frequent subgraph mining and topological data analysis, offering a new perspective on topology-aware feature extraction. Experimental results show that FPH-ML achieves competitive or superior accuracy compared with kernel-based and degree-based filtration methods. When integrated into graph neural networks, FPH yields relative performance gains ranging from 0.4 to 21 percent, with improvements of up to 8.2 percentage points over GCN and GIN backbones across benchmarks.

</details>


### [127] [Attribution-Guided Distillation of Matryoshka Sparse Autoencoders](https://arxiv.org/abs/2512.24975)
*Cristina P. Martin-Linares,Jonathan P. Ling*

Main category: cs.LG

TL;DR: DMSAEs通过蒸馏循环提取核心特征，解决稀疏自编码器特征冗余和不一致问题，实现特征跨训练和稀疏度的可迁移性


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器学习到的特征通常冗余且在不同训练运行和稀疏度下变化，这使得解释难以迁移和重用

Method: 引入蒸馏Matryoshka稀疏自编码器：通过迭代蒸馏循环训练共享核心的Matryoshka SAE，使用梯度X激活度量特征贡献，保留解释固定比例归因的最小特征子集，仅核心编码器权重在循环间传递

Result: 在Gemma-2-2B模型第12层残差流激活上，7个蒸馏循环（500M tokens，65k宽度）得到197个重复选择的核心特征，使用该核心训练改善了多个SAEBench指标

Conclusion: DMSAEs能够提取一致的核心特征集，这些特征可以跨稀疏度迁移，提高了稀疏自编码器特征的可解释性和可重用性

Abstract: Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consistently useful features and reuses it to train new SAEs. DMSAEs run an iterative distillation cycle: train a Matryoshka SAE with a shared core, use gradient X activation to measure each feature's contribution to next-token loss in the most nested reconstruction, and keep only the smallest subset that explains a fixed fraction of the attribution. Only the core encoder weight vectors are transferred across cycles; the core decoder and all non-core latents are reinitialized each time. On Gemma-2-2B layer 12 residual stream activations, seven cycles of distillation (500M tokens, 65k width) yielded a distilled core of 197 features that were repeatedly selected. Training using this distilled core improves several SAEBench metrics and demonstrates that consistent sets of latent features can be transferred across sparsity levels

</details>


### [128] [Efficiently Estimating Data Efficiency for Language Model Fine-tuning](https://arxiv.org/abs/2512.24991)
*Gyung Hyun Je,Colin Raffel*

Main category: cs.LG

TL;DR: 本文提出了一种预测任务数据效率的方法，通过少量标注样本就能预估达到目标性能所需的微调数据量，避免昂贵的增量标注和重复训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在零样本设置下表现有限，通常需要微调来提升性能。但任务的数据效率（达到目标性能所需的微调样本数）往往未知，导致昂贵的增量标注和重复训练循环。作者发现，即使在零样本下表现不佳的任务，经过微调后也能获得显著提升，这凸显了预测数据效率的重要性。

Method: 首先定义量化任务数据效率的指标，然后提出使用低置信度样本的梯度余弦相似度来预测数据效率。该方法仅需少量标注样本就能进行预测，无需进行完整的增量标注过程。

Result: 在30个专门任务的实验表明，该方法在数据效率预测上达到8.6%的误差率，通常能为每个任务节省数百个不必要的标注。验证了该方法在具有不同数据效率的多样化任务集上的有效性。

Conclusion: 提出的方法能够准确预测任务的数据效率，显著减少标注成本，为LLM微调提供了一种高效的数据需求预估方案。实验代码已在GitHub上开源。

Abstract: While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task's data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task's data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.

</details>


### [129] [Diffusion Language Models are Provably Optimal Parallel Samplers](https://arxiv.org/abs/2512.25014)
*Haozhe Jiang,Nika Haghtalab,Lijie Chen*

Main category: cs.LG

TL;DR: 扩散语言模型通过并行采样和思维链技术，能以最优步骤数模拟任何并行采样算法，但需要修订或重掩码功能来优化空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究扩散语言模型作为自回归模型替代方案的并行采样优势，探索如何通过思维链和修订/重掩码功能实现最优的时间和空间复杂度。

Method: 形式化并行采样模型，分析扩散语言模型结合多项式长度思维链的能力，研究修订（将未掩码标记转换为其他未掩码标记）和重掩码（将未掩码标记转换为掩码）对空间复杂度的优化。

Result: 扩散语言模型结合思维链能以最优步骤数模拟任何并行采样算法；增加修订或重掩码功能后能实现最优空间复杂度；修订或重掩码的扩散语言模型在表达能力上严格优于没有这些功能的模型。

Conclusion: 扩散语言模型作为最高效并行采样器的理论依据得到证实，建议在扩散语言模型中启用修订功能以实现最优性能。

Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. We prove that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. We further justify the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Our results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.

</details>


### [130] [ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning](https://arxiv.org/abs/2512.25023)
*Timo Kaufmann,Yannick Metz,Daniel Keim,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: ResponseRank方法通过利用相对强度信号来学习偏好强度，解决了二元选择中偏好强度难以可靠测量的问题，提高了样本效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 二元选择（如RLHF中使用的）只能传达偏好的方向，无法衡量偏好的强度。偏好强度对于不确定性下的决策和偏好模型的泛化至关重要，但难以可靠测量。元数据（如响应时间和标注者一致性）可以作为强度的代理信号，但通常存在噪声和混淆。

Method: 提出ResponseRank方法，利用代理信号的相对差异来对成对比较中的响应进行偏好强度排序。为了控制系统性变异，仅在精心构建的层内比较信号。该方法对强度信号做出最小假设，能够鲁棒地学习与强度派生排名一致的效用差异。

Result: 在三个任务中展示了改进的样本效率和鲁棒性：合成偏好学习（模拟响应时间）、语言建模（标注者一致性）和RL控制任务（模拟回合回报）。同时提出了Pearson距离相关性（PDC）这一新指标，用于从序数准确性中分离基数效用学习。

Conclusion: ResponseRank能够通过利用局部有效的相对强度信号来鲁棒地学习偏好强度，为解决二元选择中偏好强度测量问题提供了有效方法，并在多个任务中验证了其有效性。

Abstract: Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.

</details>


### [131] [On the geometry and topology of representations: the manifolds of modular addition](https://arxiv.org/abs/2512.25060)
*Gabriela Moisescu-Pareja,Gavin McCracken,Harley Wiltzer,Vincent Létourneau,Colin Daniels,Doina Precup,Jonathan Love*

Main category: cs.LG

TL;DR: 研究发现，之前认为不同注意力架构（Clock和Pizza）会形成不同模加电路的看法是错误的，实际上它们通过拓扑和几何等价的表示实现了相同的算法。


<details>
  <summary>Details</summary>
Motivation: 挑战先前关于不同注意力架构（均匀注意力与可训练注意力）会产生不同模加电路的观点，证明这些架构实际上实现了相同的底层算法。

Method: 超越单个神经元和权重的解释，识别所有对应每个学习表示的神经元，将其作为整体实体研究。将学习表示视为流形，运用拓扑学工具分析，并统计分析数百个电路中的学习表示。

Result: 发现均匀注意力和可训练注意力架构通过拓扑和几何等价的表示实现了相同的算法，证明了从常见深度学习范式中自然产生的学习模加电路之间的相似性。

Conclusion: Clock和Pizza解释所关联的不同架构实际上实现了相同的模加算法，挑战了先前关于架构设计差异会导致不同电路形成的观点。

Abstract: The Clock and Pizza interpretations, associated with architectures differing in either uniform or learnable attention, were introduced to argue that different architectural designs can yield distinct circuits for modular addition. In this work, we show that this is not the case, and that both uniform attention and trainable attention architectures implement the same algorithm via topologically and geometrically equivalent representations. Our methodology goes beyond the interpretation of individual neurons and weights. Instead, we identify all of the neurons corresponding to each learned representation and then study the collective group of neurons as one entity. This method reveals that each learned representation is a manifold that we can study utilizing tools from topology. Based on this insight, we can statistically analyze the learned representations across hundreds of circuits to demonstrate the similarity between learned modular addition circuits that arise naturally from common deep learning paradigms.

</details>


### [132] [Many Minds from One Model: Bayesian Transformers for Population Intelligence](https://arxiv.org/abs/2512.25063)
*Diji Yang,Yi Zhang*

Main category: cs.LG

TL;DR: B-Trans将标准LLM转换为贝叶斯Transformer，通过在归一化层引入随机偏置偏移作为高斯变分近似，从单一预训练权重中采样多样化但连贯的模型实例，实现群体智能决策。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer通常被训练为单一确定性系统，而智能可能源于多个"心智"的集合。作者希望从单一预训练权重中生成多样化的模型实例，实现群体决策优势。

Method: 将归一化层中的偏置偏移视为随机变量，采用高斯变分近似作为后验代理。在序列级别冻结采样的噪声以确保时间一致性，从单一权重集中采样多样化模型实例。

Result: 在零样本生成、带可验证奖励的强化学习(RLVR)和无显式标签的RL任务中，B-Trans通过聚合多个个体的预测，在保持语义多样性的同时获得了更好的任务性能。

Conclusion: B-Trans成功将标准LLM转换为贝叶斯Transformer，实现了从单一权重中采样多样化模型实例，证明了群体决策在提升语义多样性和任务性能方面的有效性。

Abstract: Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.
  B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.

</details>


### [133] [Scaling Open-Ended Reasoning to Predict the Future](https://arxiv.org/abs/2512.25070)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.LG

TL;DR: 研究人员开发了一个名为OpenForesight的开放预测数据集，并训练了专门的预测模型OpenForecaster 8B，该模型在准确率、校准和一致性方面表现优异，能与更大的专有模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 高风险决策需要在不确定性下对未来进行推理。当前需要训练语言模型在开放式预测问题上做出预测，但缺乏大规模的训练数据和专门的预测系统。

Method: 1. 从每日新闻中的全球事件自动合成新的预测问题；2. 使用离线新闻语料库防止未来信息泄露；3. 训练Qwen3思维模型；4. 采用检索机制和改进的强化学习奖励函数；5. 在2025年5月至8月进行保留测试。

Result: OpenForecaster 8B模型在准确率、校准和一致性方面显著提升，能与更大的专有模型相媲美。预测训练的校准改进在多个流行基准测试中具有泛化性。

Conclusion: 通过OpenForesight数据集和专门的预测模型训练，成功开发了高性能的预测系统，并将所有模型、代码和数据开源，以促进语言模型预测研究的广泛可及性。

Abstract: High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.

</details>
