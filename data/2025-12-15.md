<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 60]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: 本文提出了CORL框架，使用强化学习端到端微调混合整数线性规划方案，以最大化其在现实世界中的操作性能，将MILP求解过程视为可微分的随机策略。


<details>
  <summary>Details</summary>
Motivation: 传统的组合序列决策问题通常建模为混合整数线性规划，但准确建模现实世界随机问题很困难，导致实际性能不佳。现有机器学习方法通常依赖监督学习，需要真实最优决策，并使用MILP梯度的替代方法。

Method: 提出CORL框架，将分支定界算法求解的MILP视为可微分的随机策略，使用强化学习在现实世界数据上端到端微调MILP方案，最大化操作性能。

Result: 在简单的组合序列决策示例中验证了CORL方法的有效性，展示了该框架的可行性。

Conclusion: CORL框架提供了一种新的方法，通过强化学习直接优化MILP在实际操作中的性能，而不是追求对现实问题的精确建模，为组合序列决策问题提供了有前景的解决方案。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [2] [Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling](https://arxiv.org/abs/2512.11187)
*Haohui Zhang,Wouter van Heeswijk,Xinyu Hu,Neil Yorke-Smith,Martijn Mes*

Main category: cs.AI

TL;DR: 论文提出了一种结合Transformer神经网络和多起点大邻域搜索的混合搜索框架，用于解决在线货运交易系统中的组合捆绑优化问题，在保证亚秒级延迟的同时实现了接近最优的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在线货运交易系统(OFEX)在实时匹配货主和承运商方面发挥着关键作用，但高效的运输任务组合捆绑仍然是瓶颈。现有方法难以在亚秒级延迟内同时处理组合捆绑选择和取送货路径规划问题。

Method: 将OFEX组合捆绑问题建模为多商品一对一取送货选择性旅行商问题(m1-PDSTSP)，提出学习加速的混合搜索管道：结合基于Transformer神经网络的构造策略与创新的多起点大邻域搜索元启发式算法，采用滚动时域方案，平台反复将当前市场冻结为静态快照并在短时间预算内求解。

Result: 在基准测试中，该方法在相同时间内优于最先进的神经组合优化和元启发式基线方法，相对于最佳可用精确基线方法，总收入的最优性差距小于2%。

Conclusion: 这是首个证明基于深度神经网络的构造器能够为(多起点)改进启发式算法可靠提供高质量种子的工作，其适用性超越了m1-PDSTSP问题，可广泛应用于一类选择性旅行商问题和取送货问题。

Abstract: Online Freight Exchange Systems (OFEX) play a crucial role in modern freight logistics by facilitating real-time matching between shippers and carrier. However, efficient combinatorial bundling of transporation jobs remains a bottleneck. We model the OFEX combinatorial bundling problem as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), which optimizes revenue-driven freight bundling under capacity, precedence, and route-length constraints. The key challenge is to couple combinatorial bundle selection with pickup-and-delivery routing under sub-second latency. We propose a learning--accelerated hybrid search pipeline that pairs a Transformer Neural Network-based constructive policy with an innovative Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon scheme in which the platform repeatedly freezes the current marketplace into a static snapshot and solves it under a short time budget. This pairing leverages the low-latency, high-quality inference of the learning-based constructor alongside the robustness of improvement search; the multi-start design and plausible seeds help LNS to explore the solution space more efficiently. Across benchmarks, our method outperforms state-of-the-art neural combinatorial optimization and metaheuristic baselines in solution quality with comparable time, achieving an optimality gap of less than 2\% in total revenue relative to the best available exact baseline method. To our knowledge, this is the first work to establish that a Deep Neural Network-based constructor can reliably provide high-quality seeds for (multi-start) improvement heuristics, with applicability beyond the \textit{m1-PDSTSP} to a broad class of selective traveling salesperson problems and pickup and delivery problems.

</details>


### [3] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver：一个在固定预算下规划和优化多智能体系统中测试时计算分配的框架，通过模块化协作和双级规划提升多智能体协作性能


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展技术（如重复采样、自我验证、自我反思）能提升大语言模型性能，但这些技术在多智能体系统中难以应用，缺乏分配计算以促进智能体协作的原则性机制，也无法在明确预算约束下跨智能体分配计算

Method: 提出FutureWeaver框架：1）引入模块化协作，将可复用的多智能体工作流封装为可调用函数；2）通过自我对弈反思从过往轨迹中抽象出重复交互模式自动推导模块；3）采用双级规划架构，在推理当前任务状态的同时推测未来步骤来优化计算分配

Result: 在复杂智能体基准测试上的实验表明，FutureWeaver在不同预算设置下始终优于基线方法，验证了其在推理时优化中促进多智能体协作的有效性

Conclusion: FutureWeaver为多智能体系统中的测试时计算分配提供了系统化框架，解决了现有技术在协作环境中的应用难题，通过模块化协作和前瞻性规划实现了在固定预算下的高效计算资源分配

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [4] [A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation](https://arxiv.org/abs/2512.11270)
*Hong Je-Gal,Chan-Bin Yi,Hyun-Suk Lee*

Main category: cs.AI

TL;DR: A-LAMP是一个基于大语言模型的自动化框架，能够将自然语言任务描述自动转化为MDP建模和训练好的策略，解决了传统RL应用中建模错误、代码脆弱和目标不对齐等问题。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于实际任务需要将非正式描述转化为正式的MDP、实现可执行环境并训练策略代理。这个过程自动化面临建模错误、脆弱代码和目标不对齐等挑战，这些因素常常阻碍策略训练。

Method: 提出基于代理大语言模型的自动化MDP建模和策略生成框架（A-LAMP），将建模、编码和训练分解为可验证的阶段，确保整个流程的语义对齐。

Result: 在经典控制和自定义RL领域中，A-LAMP始终比单个最先进的大语言模型具有更高的策略生成能力。即使是基于较小语言模型的轻量级变体，也能接近更大模型的性能。失败分析揭示了这些改进的原因。

Conclusion: 案例研究表明A-LAMP生成的环境和策略能够保持任务的最优性，证实了其正确性和可靠性。该框架为自动化RL应用提供了有效的解决方案。

Abstract: Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.

</details>


### [5] [TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning](https://arxiv.org/abs/2512.11271)
*Yuxing Chen,Basem Suleiman,Qifan Chen*

Main category: cs.AI

TL;DR: TriFlow是一个渐进式多智能体框架，通过检索、规划和治理三阶段流水线，将结构化推理与语言灵活性结合，解决现实旅行规划中的约束满足问题，显著提升可行性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实旅行规划需要将开放式用户请求转化为满足严格空间、时间和预算约束的可执行行程，同时符合用户偏好。现有基于LLM的智能体在约束满足、工具协调和效率方面存在困难，经常产生不可行或成本过高的计划。

Method: TriFlow采用渐进式多智能体框架，通过三阶段流水线：1)检索阶段缩小搜索空间；2)规划阶段通过规则-LLM协作组装约束一致的行程；3)治理阶段执行有界迭代优化以确保全局可行性和个性化。

Result: 在TravelPlanner和TripTailor基准测试中取得了最先进的结果，分别达到91.1%和97%的最终通过率，相比当前SOTA实现了超过10倍的运行时效率提升。

Conclusion: TriFlow通过结合结构化推理和语言灵活性，有效解决了现实旅行规划中的约束满足问题，在可行性和效率方面显著优于现有方法。

Abstract: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.

</details>


### [6] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 本文提出了首个专门针对大型视觉语言模型（LVLMs）的CAPTCHA基准测试CAPTURE，涵盖4种主要类型和25种子类型，来自31个供应商，用于全面评估LVLMs解决验证码的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉验证码的基准测试存在局限性，无法全面覆盖所有验证码类型，且缺乏专门针对LVLMs的基准测试。先前研究根据特定研究目标定制基准，导致评估不够全面。

Method: 构建了CAPTURE基准测试，包含4种主要验证码类型和25种子类型，收集自31个供应商。该基准具有广泛的类别多样性、大规模数据以及专门为LVLMs定制的标签。

Result: 使用该基准测试评估当前LVLMs时，发现它们在解决验证码方面表现不佳，显示出LVLMs在实际验证码识别任务中的局限性。

Conclusion: CAPTURE基准填补了先前研究在数据全面性和标签针对性方面的空白，为LVLMs的验证码解决能力提供了多维度的全面评估框架。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [7] [General-purpose AI models can generate actionable knowledge on agroecological crop protection](https://arxiv.org/abs/2512.11474)
*Kris A. G. Wyckhuys*

Main category: cs.AI

TL;DR: 研究验证了基于网络和离线两种大语言模型（DeepSeek vs ChatGPT免费版）在农业生态作物保护领域的科学知识生成能力，发现DeepSeek在文献覆盖、解决方案数量和效果评估方面表现更好，但两者都存在幻觉、命名混淆等问题，建议在严格人工监督下可作为农场决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 生成式AI有潜力将科学知识民主化并转化为清晰可操作的信息，但在农业食品科学领域的应用尚未充分探索。本研究旨在验证不同大语言模型在农业生态作物保护方面的科学知识生成能力。

Method: 针对九种全球性限制性害虫、杂草和植物病害，评估了基于网络的DeepSeek和非基于网络的ChatGPT免费版两种大语言模型。评估指标包括事实准确性、数据一致性、知识广度或数据完整性。

Result: DeepSeek筛选的文献量比ChatGPT多4.8-49.7倍，报告的生物防治剂或管理解决方案多1.6-2.4倍，效果评估高21.6%，实验室到田间数据一致性更好，害虫身份和管理策略的影响更真实。但两种模型都存在幻觉（虚构代理或参考文献）、报告不可信的生态相互作用、混淆新旧科学命名、遗漏关键代理或解决方案等问题。两者都能正确报告低分辨率效果趋势。

Conclusion: 尽管存在缺陷，但在严格的人工监督下，大语言模型可能成为支持农场层面决策和释放科学创造力的强大工具。

Abstract: Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.

</details>


### [8] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出一个基于LLM的任务完成框架，通过强化学习形式化描述环境，使智能体在明确行为指导下执行任务，确保可靠性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出强大的推理和生成能力，但在多轮任务中的行为往往缺乏可靠性和可验证性，需要一种框架来确保智能体在明确指导下执行任务。

Method: 框架包含三个组件：轻量级任务分析器选择推理和生成策略；推理模块学习可验证的观察-动作映射；生成模块通过验证或确定性合成确保约束合规输出。这些组件在与环境交互中协同演化。

Result: 随着智能体与环境交互，框架的三个组件协同演化，产生可信赖的行为表现。

Conclusion: 该框架能够使基于LLM的智能体在强化学习形式化描述的环境中，在明确行为指导下执行任务，确保行为的可靠性和可验证性。

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [9] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在明确token成本和延迟预算下构建成本效益多智能体系统的框架，采用"先骨干后拓扑"设计，相比现有方法在相同预算下性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统设计通常优先考虑通信拓扑结构，很少在明确的token成本和延迟预算约束下进行建模和优化，导致在预算约束下成本效益不佳。

Method: 采用"先骨干后拓扑"的两阶段设计：1)骨干导向的智能体生成（LLM池构建、池选择、角色-骨干匹配）；2)自适应MAS拓扑生成（智能体表示学习、门控机制、延迟感知拓扑合成）。

Result: 在14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下实现高达10%的性能提升，在延迟预算下实现高达22%的性能提升，并在性能-预算曲线上表现出强大的AUC。

Conclusion: AgentBalance框架能够在明确预算约束下构建成本效益高的多智能体系统，可作为现有MAS的插件提升性能，并能很好地泛化到未见过的LLM，实现实用的预算感知部署。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [10] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 该研究揭示了当前XAI中基于保真度评估的归因方法存在严重问题：不同基线选择会偏好不同归因方法，甚至导致线性模型得出矛盾的最优方法结论。作者提出了基线应满足的两个理想属性，发现现有基线无法同时满足，并引入了一种新的模型依赖基线来改善这一权衡。


<details>
  <summary>Details</summary>
Motivation: 当前解释性人工智能（XAI）中广泛使用的归因方法通常通过保真度指标（如插入和删除）进行评估。这些指标依赖于基线函数来修改输入图像中被归因图认为最重要的像素。研究发现，基线选择会不可避免地偏好某些归因方法，甚至导致评估结果自相矛盾，这引发了"应该使用哪个基线"的重要问题。

Method: 作者首先分析了基线应满足的两个理想属性：(1) 能够移除信息；(2) 不会产生过度分布外（OOD）的图像。通过测试现有基线发现它们无法同时满足这两个标准。然后，作者利用特征可视化的最新工作，提出了一种新的模型依赖基线，该基线能够移除信息而不会产生过度OOD的图像。

Result: 研究发现现有基线存在权衡：要么能够移除信息，要么会产生一系列OOD图像，但无法同时满足两个标准。新提出的模型依赖基线在权衡方面优于现有基线，能够更好地移除信息而不产生过度OOD的图像。

Conclusion: 该研究揭示了XAI中归因方法评估的基线选择问题，提出了基线应满足的理想属性，并开发了一种改进的模型依赖基线。这项工作强调了在评估归因方法时需要谨慎选择基线，并为未来的基线设计提供了指导方向。

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [11] [Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes](https://arxiv.org/abs/2512.11463)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Minsu Ha,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.AI

TL;DR: Motif-2-12.7B-Reasoning是一个12.7B参数的语言模型，旨在缩小开源模型与专有前沿模型在复杂推理和长上下文理解方面的差距，通过创新的训练方法在有限计算资源下实现接近更大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开源模型与专有前沿模型在复杂推理和长上下文理解方面的性能差距，同时应对推理适应过程中常见的模型崩溃和训练不稳定问题。

Method: 采用综合训练方案：1）使用混合并行和内核级优化的内存高效基础设施支持64K-token上下文；2）两阶段监督微调课程，通过验证对齐的合成数据缓解分布不匹配；3）强化学习微调管道，通过难度感知数据过滤和混合策略轨迹重用来稳定训练。

Result: Motif-2-12.7B-Reasoning在数学、编码和智能体基准测试中实现了与参数数量显著更大的模型相当的性能，在现实计算约束下提供了具有竞争力的开源模型。

Conclusion: 该研究提供了一个可复现的训练方案，成功缩小了开源模型与专有前沿模型在推理能力方面的差距，为社区提供了实用的推理能力扩展蓝图。

Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.

</details>


### [12] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 本文首次系统比较了经典优化方法（整数线性规划ILP）与AI方法（PatternBoost变换器学习和PPO强化学习）在No-Three-In-Line问题上的表现，发现ILP在19×19网格内能获得最优解，而AI方法在较小网格上具有竞争力，混合方法可能是未来扩展到大问题规模的最有前景方向。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line问题是组合几何中的著名问题，传统整数线性规划方法虽然能保证最优解，但随着网格规模增大面临指数级复杂度增长。机器学习方法为这类模式识别问题提供了有前景的替代方案，但缺乏系统性的比较研究。

Method: 1. 使用整数线性规划（ILP）作为经典优化方法的代表；2. 首次将PatternBoost变换器学习应用于该问题；3. 首次将PPO强化学习应用于该问题；4. 系统比较这些方法与传统算法的性能。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格上获得完美解，但在11×11网格上因约束违反而失败。

Conclusion: 经典优化方法对于精确解仍然至关重要，而AI方法在较小实例上具有竞争力。混合方法为扩展到更大问题规模提供了最有前景的方向，结合了经典方法的精确性和AI方法的可扩展性优势。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [13] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID是一个用于评估AI文本检测器偏见的综合框架，包含超过20万个样本，涵盖7个主要类别，发现现有检测器在检测少数群体文本时存在系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育和工作场景中广泛应用，但先前研究仅发现孤立偏见案例，缺乏对更广泛社会语言因素的系统性评估。需要全面评估这些系统在不同群体间的偏见问题。

Method: 提出BAID评估框架，包含7个主要类别（人口统计、年龄、教育水平、方言、正式程度、政治倾向、主题）的超过20万个样本。为每个样本生成合成版本，使用精心设计的提示词保留原始内容同时反映特定子群体的写作风格。使用该框架评估4个开源最先进的AI文本检测器。

Result: 发现检测器存在一致的性能差异，特别是在检测来自代表性不足群体的文本时召回率较低。检测器在不同社会语言群体间的表现存在系统性偏见。

Conclusion: BAID提供了一个可扩展、透明的AI检测器审计方法，强调了在这些工具公开部署前进行偏见感知评估的必要性。研究结果呼吁开发更公平的AI文本检测系统。

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [14] [EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection](https://arxiv.org/abs/2512.11506)
*Georgios Kaoukis,Ioannis Aris Koufopoulos,Psaroudaki Eleni,Danae Pla Karidi,Evaggelia Pitoura,George Papastefanatos,Panayiotis Tsaparas*

Main category: cs.AI

TL;DR: EmeraldMind是一个基于知识图谱和检索增强生成的事实中心框架，用于自动化检测企业绿色漂洗（虚假环保声明），相比通用大语言模型具有更高准确性、覆盖范围和解释质量。


<details>
  <summary>Details</summary>
Motivation: 随着AI和网络代理在决策中日益普及，需要设计既能支持可持续发展又能防范错误信息的智能系统。绿色漂洗（误导性企业可持续发展声明）对环境进步构成重大挑战。

Method: EmeraldMind是一个事实中心框架，整合了特定领域知识图谱与检索增强生成技术。它从多样化的企业ESG报告中构建EmeraldGraph知识图谱，提供可验证的证据支持，帮助大语言模型进行声明评估。框架提供基于证据的透明判断，在无法验证时负责任地放弃判断。

Result: 在新构建的绿色漂洗声明数据集上的实验表明，EmeraldMind相比通用大语言模型实现了竞争性的准确性、更大的覆盖范围和更优的解释质量，且无需微调或重新训练。

Conclusion: EmeraldMind框架通过整合特定领域知识图谱和检索增强生成，有效解决了绿色漂洗检测问题，为可持续发展决策提供了透明、可验证的智能支持系统。

Abstract: As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.

</details>


### [15] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 研究通过模拟真实临床场景，评估主流大语言模型从含噪声的患者主诉中提取核心医疗信息的能力，发现所有模型均存在不同程度功能缺陷，并提出"AI-MASLD"概念。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在嘈杂临床环境中提取核心医疗信息的能力，验证其是否存在类似代谢功能障碍相关脂肪肝病的功能衰退现象。

Method: 采用基于标准化医疗探针的横断面分析设计，选取GPT-4o、Gemini 2.5、DeepSeek 3.1和Qwen3-Max四种主流模型，使用包含20个医疗探针的评估系统模拟真实临床交流环境，由两位独立临床医生进行双盲反向评分。

Result: 所有测试模型均表现出不同程度功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差；极端噪声下多数模型功能崩溃；GPT-4o在深静脉血栓继发肺栓塞风险评估中做出严重误判。

Conclusion: 首次实证确认大语言模型处理临床信息时表现出类似代谢功能障碍的特征，提出"AI-MASLD"创新概念，强调当前LLMs必须在人类专家监督下作为辅助工具使用。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [16] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文提出AI基准测试需要从传统静态基准转向动态自适应基准框架，以解决模型记忆静态基准、评估与现实性能脱节的问题，并倡导建立AI基准测试工艺学教育体系。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试面临多重挑战：模型架构快速演进、规模扩大、数据集更新、部署环境多样，导致评估成为移动目标。大语言模型容易记忆静态基准，造成基准测试结果与实际性能脱节。传统基准强调顶级硬件上的峰值性能，对多样化现实场景指导有限。

Method: 提出动态自适应基准测试框架，包含不断演进的模型、更新的数据和异构平台。倡导建立AI基准测试工艺学教育体系，通过技术创新和系统化教育解决资源需求高、专业硬件访问有限、基准设计专业知识缺乏等问题。

Result: 基于MLCommons、教育项目和DOE万亿参数联盟的经验，识别出关键障碍：高资源需求、专业硬件访问限制、基准设计专业知识缺乏、结果与应用领域关联不确定性。提出社区努力可为AI基准测试工艺学提供基础。

Conclusion: 基准测试必须变得动态化，纳入演进模型、更新数据和异构平台，同时保持透明度、可重现性和可解释性。动态包容的基准测试将确保评估跟上AI发展步伐，支持负责任、可重现和可访问的AI部署。

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [17] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 本文提出了一种基于结构因果模型的能源需求预测方法，通过分析天气、日历等因素的因果关系，构建贝叶斯模型实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 能源需求预测对电网运营商、工业能源消费者和服务提供商至关重要。能源需求受多种因素影响，包括天气条件（温度、湿度、风速、太阳辐射）和日历信息（小时、月份），这些因素因果相互依赖，使得传统基于相关性的学习方法难以充分解决这一复杂问题。

Method: 1. 提出结构因果模型来解释变量间的因果关系；2. 进行完整分析验证因果假设；3. 基于学到的因果洞察作为先验知识构建贝叶斯模型；4. 在未见数据上进行训练和测试。

Result: 1. 因果模型揭示了能源需求对温度波动的响应具有季节依赖性敏感性；2. 发现冬季能源需求方差较低，因为温度变化与日常活动模式之间存在解耦效应；3. 贝叶斯模型在测试集上达到3.84%的MAPE，表现优异；4. 跨两年数据的交叉验证平均MAPE为3.88%，显示强鲁棒性。

Conclusion: 通过结构因果模型分析变量间的因果关系，并利用这些洞察作为先验知识构建贝叶斯模型，能够显著提升能源需求预测的准确性和鲁棒性，为实际应用提供了有效的解决方案。

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>


### [18] [MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition](https://arxiv.org/abs/2512.11682)
*Tim Cofala,Christian Kalfar,Jingge Xiao,Johanna Schrader,Michelle Tang,Wolfgang Nejdl*

Main category: cs.AI

TL;DR: TxAgent是一个用于临床治疗决策的AI代理系统，通过迭代式检索增强生成整合多源生物医学工具，在CURE-Bench挑战赛中获得卓越科学奖。


<details>
  <summary>Details</summary>
Motivation: 临床治疗决策是高风险领域，需要AI系统进行多步骤推理并基于可靠的生物医学知识。现有通用RAG系统难以满足医疗应用的安全性和准确性要求。

Method: 使用微调的Llama-3.1-8B模型，通过动态生成和执行函数调用来访问统一的生物医学工具套件（ToolUniverse），整合FDA Drug API、OpenTargets和Monarch资源，采用迭代式检索增强生成方法。

Result: 在CURE-Bench NeurIPS 2025挑战赛中获奖，展示了工具检索质量对整体性能的影响，并通过改进工具检索策略实现了性能提升。

Conclusion: TxAgent通过整合多源生物医学工具和迭代式RAG方法，为临床治疗决策提供了安全可靠的AI指导系统，工具检索质量是影响系统性能的关键因素。

Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [19] [Emotion-Driven Personalized Recommendation for AI-Generated Content Using Multi-Modal Sentiment and Intent Analysis](https://arxiv.org/abs/2512.10963)
*Zheqi Hu,Xuanjing Chen,Jinlin Hu*

Main category: cs.IR

TL;DR: 提出基于BERT跨模态Transformer与注意力融合的多模态情感意图识别模型(MMEI)，集成到云原生个性化AIGC推荐框架，通过视觉、听觉、文本模态融合提升情感感知推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统主要依赖用户行为数据（点击、观看、评分），忽视了用户在内容交互过程中的实时情感和意图状态。随着AIGC在各领域的快速增长，对情感感知推荐系统的需求日益重要。

Method: 提出MMEI模型，基于BERT跨模态Transformer与注意力融合，通过预训练编码器ViT、Wav2Vec2和BERT分别处理视觉（面部表情）、听觉（语音语调）和文本（评论或话语）模态，然后通过注意力融合模块学习情感意图表示，最后通过上下文匹配层驱动个性化内容推荐。

Result: 在基准情感数据集（AIGC-INT、MELD、CMU-MOSEI）和AIGC交互数据集上的实验显示，MMEI模型相比最佳融合Transformer基线，F1分数提升4.3%，交叉熵损失降低12.3%。用户级在线评估表明，情感驱动推荐使参与时间增加15.2%，满意度得分提升11.8%。

Conclusion: 该工作展示了跨模态情感智能在下一代AIGC生态系统中的潜力，能够实现自适应、共情和上下文感知的推荐体验，有效将AI生成内容与用户情感意图状态对齐。

Abstract: With the rapid growth of AI-generated content (AIGC) across domains such as music, video, and literature, the demand for emotionally aware recommendation systems has become increasingly important. Traditional recommender systems primarily rely on user behavioral data such as clicks, views, or ratings, while neglecting users' real-time emotional and intentional states during content interaction. To address this limitation, this study proposes a Multi-Modal Emotion and Intent Recognition Model (MMEI) based on a BERT-based Cross-Modal Transformer with Attention-Based Fusion, integrated into a cloud-native personalized AIGC recommendation framework. The proposed system jointly processes visual (facial expression), auditory (speech tone), and textual (comments or utterances) modalities through pretrained encoders ViT, Wav2Vec2, and BERT, followed by an attention-based fusion module to learn emotion-intent representations. These embeddings are then used to drive personalized content recommendations through a contextual matching layer. Experiments conducted on benchmark emotion datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrate that the proposed MMEI model achieves a 4.3% improvement in F1-score and a 12.3% reduction in cross-entropy loss compared to the best fusion-based transformer baseline. Furthermore, user-level online evaluations reveal that emotion-driven recommendations increase engagement time by 15.2% and enhance satisfaction scores by 11.8%, confirming the model's effectiveness in aligning AI-generated content with users' affective and intentional states. This work highlights the potential of cross-modal emotional intelligence for next-generation AIGC ecosystems, enabling adaptive, empathetic, and context-aware recommendation experiences.

</details>


### [20] [FAIR: Focused Attention Is All You Need for Generative Recommendation](https://arxiv.org/abs/2512.11254)
*Longtao Xiao,Haolin Zhang,Guohao Cai,Jieming Zhu,Yifan Wang,Heng Chang,Zhenhua Dong,Xiu Li,Ruixuan Li*

Main category: cs.IR

TL;DR: FAIR是一个基于Transformer的生成式推荐框架，通过聚焦注意力机制解决多码表示导致的序列长度增加和噪声问题，在四个公开基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的生成式推荐通常需要将物品离散化为多码表示（如4个或更多码令牌），这会显著增加原始物品序列的长度。这种扩展给Transformer模型带来了挑战，因为它们在处理带有固有噪声的用户行为序列时，往往会过度关注不相关或噪声上下文。

Method: FAIR提出了三个关键技术：1）聚焦注意力机制，集成到标准Transformer中，学习两组独立的Q和K注意力权重，计算它们的差异作为最终注意力分数，以消除注意力噪声并聚焦相关上下文；2）噪声鲁棒性目标，鼓励模型在随机扰动下保持稳定的注意力模式，防止因噪声而向不相关上下文偏移；3）互信息最大化目标，指导模型识别对下一物品预测最信息丰富的上下文。

Result: 在四个公开基准上验证了FAIR的有效性，证明了其相比现有方法的优越性能。

Conclusion: FAIR是第一个具有聚焦注意力的生成式推荐框架，通过创新的注意力机制和训练目标，有效解决了多码表示带来的序列长度扩展和噪声问题，在用户行为建模中表现出色。

Abstract: Recently, transformer-based generative recommendation has garnered significant attention for user behavior modeling. However, it often requires discretizing items into multi-code representations (e.g., typically four code tokens or more), which sharply increases the length of the original item sequence. This expansion poses challenges to transformer-based models for modeling user behavior sequences with inherent noises, since they tend to overallocate attention to irrelevant or noisy context. To mitigate this issue, we propose FAIR, the first generative recommendation framework with focused attention, which enhances attention scores to relevant context while suppressing those to irrelevant ones. Specifically, we propose (1) a focused attention mechanism integrated into the standard Transformer, which learns two separate sets of Q and K attention weights and computes their difference as the final attention scores to eliminate attention noise while focusing on relevant contexts; (2) a noise-robustness objective, which encourages the model to maintain stable attention patterns under stochastic perturbations, preventing undesirable shifts toward irrelevant context due to noise; and (3) a mutual information maximization objective, which guides the model to identify contexts that are most informative for next-item prediction. We validate the effectiveness of FAIR on four public benchmarks, demonstrating its superior performance compared to existing methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Scalable Data Synthesis for Computer Use Agents with Step-Level Filtering](https://arxiv.org/abs/2512.10962)
*Yifei He,Pranit Chawla,Yaser Souri,Subhojit Som,Xia Song*

Main category: cs.LG

TL;DR: 该论文提出了一种可扩展的数据合成流水线，通过步骤级过滤将嘈杂的计算机使用代理（CUA）轨迹转化为可靠的监督数据，无需人工标注，并构建了WebSTAR和WebSCORE数据集以及StepRM奖励模型。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理（CUA）难以训练，因为图形用户界面（GUI）交互成本高，高质量轨迹数据稀缺。现有数据集依赖人工演示，可扩展性有限。虽然可以从强CUA合成数据，但其轨迹噪声大，包含大量错误或次优动作，使简单模仿无效。

Method: 提出可扩展的数据合成流水线，核心是步骤级过滤：单独评估每个动作，只保留正确步骤；结合推理增强改进规划。使用该流水线构建WebSTAR数据集（13.3K轨迹，100K分级推理丰富的步骤）。基于步骤级分级进一步创建WebSCORE数据集，并训练StepRM（7B多模态奖励模型）。

Result: 在WebSTAR上训练Qwen-2.5-VL-Instruct模型（7B和32B）。7B模型在WebVoyager上仅通过监督微调就超越SoTA开源CUA模型UI-TARS-1.5-7B超过15%。StepRM奖励模型在分级质量上与o4-mini匹配，同时部署效率更高。

Conclusion: 步骤级过滤是可扩展CUA训练的关键原则，构建的两个新数据集（WebSTAR、WebSCORE）和轻量级奖励模型（StepRM）是推进稳健高效CUAs的实用工具。

Abstract: Computer use agents (CUAs) can operate real-world digital interfaces but remain difficult to train due to the high cost of graphical user interface (GUI) interaction and the scarcity of high-quality trajectory data. Existing datasets rely on human demonstrations, limiting scalability. A natural alternative is to synthesize data from strong CUAs, yet their rollouts are highly noisy, with incorrect or suboptimal actions consisting a large proportion of the steps, making naive imitation ineffective. To tackle this challenge, we introduce a scalable data synthesis pipeline that transforms noisy rollouts into reliable supervision without human annotation. The core idea is step-level filtering, which evaluates actions individually to retain only correct steps, complemented by reasoning augmentation for improved planning. Using this pipeline, we construct WebSTAR, a dataset of 13.3K trajectories and 100K graded, reasoning-rich steps synthesized from OpenAI's computer-use-preview model. We train Qwen-2.5-VL-Instruct models (7B and 32B) on WebSTAR. On WebVoyager, our 7B model surpasses SoTA open-source CUA model UI-TARS-1.5-7B by more than 15% with only supervised finetuning. Building on step-level grading, we further create WebSCORE, a dataset of graded step-level actions, and train StepRM, a 7B multimodal reward model distilled from o4-mini, which matches its grading quality while being far more efficient to deploy at scale. Our results establish step-level filtering as a key principle for scalable CUA training and construct two new datasets (WebSTAR, WebSCORE) and a lightweight reward model (StepRM) as practical tools to advance robust and efficient CUAs.

</details>


### [22] [Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2512.10966)
*Farica Zhuang,Dinara Aliyeva,Shu Yang,Zixuan Wen,Duy Duong-Tran,Christos Davatzikos,Tianlong Chen,Song Wang,Li Shen*

Main category: cs.LG

TL;DR: MREF-AD：一种用于阿尔茨海默病诊断的多模态区域专家融合模型，通过混合专家框架自适应融合淀粉样蛋白PET和MRI等多模态脑影像数据，提供区域特异性生物标志物解释。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的准确早期诊断需要整合多模态信息，但传统融合方法通常采用简单的特征拼接，无法自适应平衡不同脑区生物标志物（如淀粉样蛋白PET和MRI）的贡献。

Method: 提出MREF-AD模型，采用混合专家框架，将每个模态的脑区建模为独立专家，使用两级门控网络学习受试者特定的融合权重，实现自适应多模态融合。

Result: 在ADNI数据上，MREF-AD实现了最先进的诊断性能，同时提供了增强的可解释性，揭示了脑区特异性生物标志物的相关性。

Conclusion: MREF-AD作为一种通用框架，为神经影像中的自适应和可解释多模态融合提供了有效工具，不仅提升诊断性能，还能提供结构和分子成像在疾病诊断中的联合贡献洞察。

Abstract: Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.

</details>


### [23] [MoB: Mixture of Bidders](https://arxiv.org/abs/2512.10969)
*Dev Vyas*

Main category: cs.LG

TL;DR: MoB用拍卖机制替代MoE中的学习门控网络，通过VCG拍卖让专家竞标真实成本（执行成本+遗忘成本），解决了门控网络的灾难性遗忘问题，在持续学习中实现了无状态路由和真实竞标。


<details>
  <summary>Details</summary>
Motivation: 混合专家架构在持续学习中存在根本限制：学习到的门控网络本身会遭受灾难性遗忘。需要一种能够避免门控网络遗忘的机制。

Method: 提出混合竞标者框架，用VCG拍卖替代学习门控网络。专家通过竞标真实成本（执行成本+遗忘成本）来竞争处理数据批次，其中遗忘成本使用弹性权重巩固惩罚计算。

Result: 在Split-MNIST基准测试中，MoB达到88.77%平均准确率，相比门控MoE的19.54%和单体EWC的27.96%，提升了4.5倍。扩展版本还能让专家自主监测知识巩固边界。

Conclusion: MoB通过将专家路由重新概念化为去中心化经济机制，解决了MoE在持续学习中的关键限制，提供了无状态路由、真实竞标保证和无需明确任务边界的涌现专业化。

Abstract: Mixture of Experts (MoE) architectures have demonstrated remarkable success in scaling neural networks, yet their application to continual learning remains fundamentally limited by a critical vulnerability: the learned gating network itself suffers from catastrophic forgetting. We introduce Mixture of Bidders (MoB), a novel framework that reconceptualizes expert routing as a decentralized economic mechanism. MoB replaces learned gating networks with Vickrey-Clarke-Groves (VCG) auctions, where experts compete for each data batch by bidding their true cost -- a principled combination of execution cost (predicted loss) and forgetting cost (Elastic Weight Consolidation penalty). This game-theoretic approach provides three key advantages: (1) {stateless routing that is immune to catastrophic forgetting, (2) \textbf{truthful bidding} guaranteed by dominant-strategy incentive compatibility, and (3) emergent specialization without explicit task boundaries. On Split-MNIST benchmarks, MoB achieves 88.77% average accuracy compared to 19.54% for Gated MoE and 27.96% for Monolithic EWC, representing a 4.5 times improvement over the strongest baseline. We further extend MoB with autonomous self-monitoring experts that detect their own knowledge consolidation boundaries, eliminating the need for explicit task demarcation.

</details>


### [24] [TECM*: A Data-Driven Assessment to Reinforcement Learning Methods and Application to Heparin Treatment Strategy for Surgical Sepsis](https://arxiv.org/abs/2512.10973)
*Jiang Liu,Yujie Li,Chan Zhou,Yihao Xie,Qilong Sun,Xin Shu,Peiwei Li,Chunyong Yang,Yiziting Zhu,Jiaqi Zhu,Yuwen Chen,Bo An,Hao Wu,Bin Yi*

Main category: cs.LG

TL;DR: 该研究提出了一种基于强化学习的框架，通过连续cxSOFA评分和TECM评估矩阵，优化外科脓毒症患者的肝素治疗策略，降低死亡率和住院时间。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是由严重感染引起的危及生命的疾病，需要优化个性化肝素治疗。传统离散SOFA评分限制了治疗策略的精细评估，需要更连续的状态表示和评估方法。

Method: 使用MIMIC-IV和eICU数据库数据，提出强化学习框架：1)将离散SOFA转换为连续cxSOFA评分；2)基于cxSOFA定义"好/坏"治疗策略；3)提出类似混淆矩阵的TECM评估矩阵。应用Q-Learning、DQN、DDQN、BCQ和CQL等算法优化治疗。

Result: cxSOFA-CQL模型表现最佳，将死亡率从1.83%降至0.74%，平均住院时间从11.11天缩短至9.42天。TECM在不同模型中显示一致结果，证明框架稳健性。

Conclusion: 该强化学习框架为外科脓毒症肝素治疗提供了可解释且稳健的优化方法，连续cxSOFA评分和TECM评估为治疗提供了更精细的评估，有望改善临床结果和决策支持可靠性。

Abstract: Objective: Sepsis is a life-threatening condition caused by severe infection leading to acute organ dysfunction. This study proposes a data-driven metric and a continuous reward function to optimize personalized heparin therapy in surgical sepsis patients. Methods: Data from the MIMIC-IV v1.0 and eICU v2.0 databases were used for model development and evaluation. The training cohort consisted of abdominal surgery patients receiving unfractionated heparin (UFH) after postoperative sepsis onset. We introduce a new RL-based framework: converting the discrete SOFA score to a continuous cxSOFA for more nuanced state and reward functions; Second, defining "good" or "bad" strategies based on cxSOFA by a stepwise manner; Third, proposing a Treatment Effect Comparison Matrix (TECM), analogous to a confusion matrix for classification tasks, to evaluate the treatment strategies. We applied different RL algorithms, Q-Learning, DQN, DDQN, BCQ and CQL to optimize the treatment and comprehensively evaluated the framework. Results: Among the AI-derived strategies, the cxSOFA-CQL model achieved the best performance, reducing mortality from 1.83% to 0.74% with the average hospital stay from 11.11 to 9.42 days. TECM demonstrated consistent outcomes across models, highlighting robustness. Conclusion: The proposed RL framework enables interpretable and robust optimization of heparin therapy in surgical sepsis. Continuous cxSOFA scoring and TECM-based evaluation provide nuanced treatment assessment, showing promise for improving clinical outcomes and decision-support reliability.

</details>


### [25] [Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems](https://arxiv.org/abs/2512.10975)
*Matvey Nepomnyaschiy,Oleg Pereziabov,Anvar Tliamov,Stanislav Mikhailov,Ilya Afanasyev*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体框架的多模态情感识别系统，通过模块化设计实现更灵活、可扩展和可维护的HAI感知模块


<details>
  <summary>Details</summary>
Motivation: 传统多模态深度学习模型虽然情感识别准确率高，但训练和维护计算密集，对模态变化不灵活，需要更高效、模块化的解决方案

Method: 采用多智能体框架，每个模态编码器和融合分类器作为自主智能体，由中央监督器协调，支持模块化集成新模态和组件替换

Result: 通过概念验证实现支持视觉、音频和文本模态的系统，分类器作为共享决策智能体，提高了训练效率

Conclusion: 该框架不仅提升训练效率，还为HAI场景中的具身和虚拟智能体设计了更灵活、可扩展和可维护的感知模块

Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.

</details>


### [26] [MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax](https://arxiv.org/abs/2512.10991)
*Zhanpeng Chen,Weihao Gao,Shunyu Wang,Yanan Zhu,Hong Meng,Yuexian Zou*

Main category: cs.LG

TL;DR: MolSculpt是一个新颖的3D分子生成框架，通过"雕刻"方式从化学语法生成精确的3D分子几何结构，将冻结的1D分子基础模型与3D分子扩散模型结合，实现了1D化学知识与3D几何生成的深度融合。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用1D表示（如SELFIES）确保分子有效性，但未能充分利用1D模型中丰富的化学知识，导致1D语法生成与3D几何实现之间存在脱节。需要弥合这一差距以实现更精确的3D分子几何生成。

Method: 提出MolSculpt框架：1）使用冻结的1D分子基础模型和3D分子扩散模型；2）引入可学习查询从基础模型中提取固有化学知识；3）通过可训练投影器将跨模态信息注入扩散模型的条件空间，指导3D几何生成；4）通过端到端优化将1D潜在化学知识深度整合到3D生成过程中。

Result: 在GEOM-DRUGS和QM9数据集上，MolSculpt在从头3D分子生成和条件3D分子生成方面实现了最先进的性能，显示出优越的3D保真度和稳定性。

Conclusion: MolSculpt成功地将1D化学知识与3D几何生成相结合，通过跨模态信息注入和端到端优化，实现了高质量的3D分子生成，为药物发现和材料科学提供了有力工具。

Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.

</details>


### [27] [Memoryless Policy Iteration for Episodic POMDPs](https://arxiv.org/abs/2512.11082)
*Roy van Zuijlen,Duarte Antunes*

Main category: cs.LG

TL;DR: 提出一种新的策略迭代算法家族，用于求解部分可观测马尔可夫决策过程（POMDPs），通过交替执行基于输出的单阶段策略改进和策略评估，实现单调改进，并在模型无关和模型相关设置中都取得了显著计算加速。


<details>
  <summary>Details</summary>
Motivation: 无记忆和有限记忆策略为求解POMDPs提供了实用替代方案，因为它们直接在输出空间而非高维信念空间中操作。然而，将经典方法（如策略迭代）扩展到这一设置仍然困难，因为输出过程是非马尔可夫的，使得策略改进步骤在不同阶段相互依赖。

Method: 引入新的单调改进策略迭代算法家族，交替执行基于输出的单阶段策略改进和按照预定周期模式进行的策略评估。该家族包含最大化计算效率指标的最优模式，并识别出具有最小周期的最简单模式。基于此结构，进一步开发了模型无关变体，直接从数据估计值并学习无记忆策略。

Result: 在多个POMDPs示例中，该方法在模型相关和模型无关设置中都实现了相对于策略梯度基线和近期专门算法的显著计算加速。

Conclusion: 提出的策略迭代算法家族为POMDPs求解提供了有效的计算框架，通过交替改进和评估的周期模式，克服了输出过程非马尔可夫性带来的挑战，在保持单调改进的同时显著提升了计算效率。

Abstract: Memoryless and finite-memory policies offer a practical alternative for solving partially observable Markov decision processes (POMDPs), as they operate directly in the output space rather than in the high-dimensional belief space. However, extending classical methods such as policy iteration to this setting remains difficult; the output process is non-Markovian, making policy-improvement steps interdependent across stages. We introduce a new family of monotonically improving policy-iteration algorithms that alternate between single-stage output-based policy improvements and policy evaluations according to a prescribed periodic pattern. We show that this family admits optimal patterns that maximize a natural computational-efficiency index, and we identify the simplest pattern with minimal period. Building on this structure, we further develop a model-free variant that estimates values from data and learns memoryless policies directly. Across several POMDPs examples, our method achieves significant computational speedups over policy-gradient baselines and recent specialized algorithms in both model-based and model-free settings.

</details>


### [28] [Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification](https://arxiv.org/abs/2512.11087)
*Duo Zhou,Jorge Chavez,Hesun Chen,Grani A. Hanasusanto,Huan Zhang*

Main category: cs.LG

TL;DR: 本文提出线性约束驱动剪裁框架，通过有效利用线性约束来减少分支定界中的子问题数量并改进神经网络验证中的中间边界，显著提升验证效率。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络验证器在处理复杂验证属性时，分支定界（BaB）与快速边界技术结合是关键。然而，如何更高效地利用线性约束来提升验证效率仍有改进空间。

Method: 提出线性约束驱动剪裁框架，开发两种新算法：1）利用线性约束减少已验证或无关的输入空间；2）直接改进网络中的中间边界。采用专门的GPU程序高效处理线性约束，无需昂贵的外部求解器。

Result: Clip-and-Verify在多个基准测试中持续收紧边界，分支定界子问题数量最多减少96%，在多个基准测试中达到最先进的验证准确率，是VNN-COMP 2025获胜者α,β-CROWN验证器的一部分。

Conclusion: 线性约束驱动剪裁框架为神经网络验证提供了可扩展且高效的方法，通过有效利用线性约束显著提升验证效率，在多个基准测试中表现出优越性能。

Abstract: State-of-the-art neural network (NN) verifiers demonstrate that applying the branch-and-bound (BaB) procedure with fast bounding techniques plays a key role in tackling many challenging verification properties. In this work, we introduce the linear constraint-driven clipping framework, a class of scalable and efficient methods designed to enhance the efficacy of NN verifiers. Under this framework, we develop two novel algorithms that efficiently utilize linear constraints to 1) reduce portions of the input space that are either verified or irrelevant to a subproblem in the context of branch-and-bound, and 2) directly improve intermediate bounds throughout the network. The process novelly leverages linear constraints that often arise from bound propagation methods and is general enough to also incorporate constraints from other sources. It efficiently handles linear constraints using a specialized GPU procedure that can scale to large neural networks without the use of expensive external solvers. Our verification procedure, Clip-and-Verify, consistently tightens bounds across multiple benchmarks and can significantly reduce the number of subproblems handled during BaB. We show that our clipping algorithms can be integrated with BaB-based verifiers such as $α,β$-CROWN, utilizing either the split constraints in activation-space BaB or the output constraints that denote the unverified input space. We demonstrate the effectiveness of our procedure on a broad range of benchmarks where, in some instances, we witness a 96% reduction in the number of subproblems during branch-and-bound, and also achieve state-of-the-art verified accuracy across multiple benchmarks. Clip-and-Verify is part of the $α,β$-CROWN verifier (http://abcrown.org), the VNN-COMP 2025 winner. Code available at https://github.com/Verified-Intelligence/Clip_and_Verify.

</details>


### [29] [Investigating ECG Diagnosis with Ambiguous Labels using Partial Label Learning](https://arxiv.org/abs/2512.11095)
*Sana Rahmani,Javad Hashemi,Ali Etemad*

Main category: cs.LG

TL;DR: 该研究首次系统性地将部分标签学习(PLL)方法应用于心电图(ECG)诊断，评估了9种PLL算法在不同临床模糊性生成策略下的表现，发现现有方法对不同类型的标签模糊性鲁棒性差异显著。


<details>
  <summary>Details</summary>
Motivation: 心电图诊断中存在固有的标签模糊性问题，源于重叠病症和诊断分歧，但现有ECG模型都假设标注是干净无歧义的，这限制了模型在真实临床环境中的发展和评估。尽管部分标签学习(PLL)框架专门用于处理模糊标签，但其在医疗时间序列领域特别是ECG诊断中的应用尚未得到充分探索。

Method: 将9种PLL算法适配到多标签ECG诊断任务中，使用多种临床驱动的模糊性生成策略进行评估，包括非结构化模糊性(如随机模糊)和结构化模糊性(如心脏病专家定义的相似性、治疗关系、诊断分类学)。在PTB-XL和Chapman数据集上进行实验。

Result: 实验表明，不同PLL方法对各种类型和程度的模糊性表现出显著不同的鲁棒性。通过深入分析，识别出现有PLL方法在临床环境中的关键局限性。

Conclusion: 该研究为ECG诊断中稳健且临床对齐的模糊感知学习框架的发展指明了未来方向，强调了需要开发更适合临床实际需求的PLL方法。

Abstract: Label ambiguity is an inherent problem in real-world electrocardiogram (ECG) diagnosis, arising from overlapping conditions and diagnostic disagreement. However, current ECG models are trained under the assumption of clean and non-ambiguous annotations, which limits both the development and the meaningful evaluation of models under real-world conditions. Although Partial Label Learning (PLL) frameworks are designed to learn from ambiguous labels, their effectiveness in medical time-series domains, ECG in particular, remains largely unexplored. In this work, we present the first systematic study of PLL methods for ECG diagnosis. We adapt nine PLL algorithms to multi-label ECG diagnosis and evaluate them using a diverse set of clinically motivated ambiguity generation strategies, capturing both unstructured (e.g., random) and structured ambiguities (e.g., cardiologist-derived similarities, treatment relationships, and diagnostic taxonomies). Our experiments on the PTB-XL and Chapman datasets demonstrate that PLL methods vary substantially in their robustness to different types and degrees of ambiguity. Through extensive analysis, we identify key limitations of current PLL approaches in clinical settings and outline future directions for developing robust and clinically aligned ambiguity-aware learning frameworks for ECG diagnosis.

</details>


### [30] [Limits and Gains of Test-Time Scaling in Vision-Language Reasoning](https://arxiv.org/abs/2512.11109)
*Mohammadjavad Ahmadpour,Amirmahdi Meighani,Payam Taebi,Omid Ghahroodi,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 测试时扩展（TTS）在提升语言模型推理能力方面效果显著，但在视觉语言模型（VLMs）中的应用仍待探索。研究发现闭源模型能从结构化推理和迭代自我优化中持续受益，而开源模型表现不一致：外部验证最可靠，迭代优化反而可能降低性能。TTS效果高度依赖数据集和任务类型。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展（TTS）已成为提升大语言模型推理能力的有效范式，但该技术在视觉语言模型（VLMs）中的应用尚未得到充分探索。研究者希望系统评估推理时方法在不同VLM上的效果，了解TTS在多模态系统中的适用性和局限性。

Method: 对开源和闭源视觉语言模型进行系统性实证研究，在不同基准测试上应用推理时方法，包括结构化推理、迭代自我优化和外部验证等技术，分析不同方法在不同类型模型和任务上的表现差异。

Result: 闭源模型能持续从结构化推理和迭代自我优化中受益，而开源模型表现不一致：外部验证提供最可靠的性能提升，迭代优化反而经常降低性能。TTS效果高度依赖数据集，在多步推理任务上改进明显，但在感知密集型基准测试上收益有限。

Conclusion: 测试时扩展并非通用解决方案，必须根据模型能力和任务特性进行定制。研究结果强调了需要开发自适应TTS策略和多模态奖励模型，以更好地适应不同视觉语言模型和任务需求。

Abstract: Test-time scaling (TTS) has emerged as a powerful paradigm for improving the reasoning ability of Large Language Models (LLMs) by allocating additional computation at inference, yet its application to multimodal systems such as Vision-Language Models (VLMs) remains underexplored. In this work, we present a systematic empirical study of inference time reasoning methods applied across both open-source and closed-source VLMs on different benchmarks. Our results reveal that while closed-source models consistently benefit from structured reasoning and iterative Self-Refinement, open-source VLMs show inconsistent behavior: external verification provides the most reliable gains, whereas iterative refinement often degrades performance. We further find that the effectiveness of TTS is dataset-dependent, yielding clear improvements on multi-step reasoning tasks but offering only limited gains on perception-focused benchmarks. These findings demonstrate that TTS is not a universal solution and must be tailored to both model capabilities and task characteristics, motivating future work on adaptive TTS strategies and multimodal reward models.

</details>


### [31] [In-Context Multi-Objective Optimization](https://arxiv.org/abs/2512.11114)
*Xinyu Zhang,Conor Hassan,Julien Martinelli,Daolang Huang,Samuel Kaski*

Main category: cs.LG

TL;DR: TAMO：基于Transformer的完全摊销化多目标黑盒优化策略，通过预训练实现跨问题迁移，无需针对每个任务重新训练代理模型或设计采集函数


<details>
  <summary>Details</summary>
Motivation: 传统多目标贝叶斯优化需要针对每个问题定制代理模型和采集函数，难以跨问题迁移；同时存在短视性、重拟合开销等问题，特别是在并行或时间敏感的应用中

Method: 使用Transformer架构构建通用优化策略，支持可变输入和目标维度；通过强化学习预训练策略，最大化整个轨迹上的累积超体积改进；在测试时通过单次前向传播生成新设计

Result: 在合成基准和实际任务中，TAMO的提议时间比替代方法快50-1000倍，同时在有限评估预算下匹配或提高了帕累托前沿质量

Conclusion: Transformer能够在上下文中完全执行多目标优化，消除了针对每个任务的代理拟合和采集工程，为科学发现工作流程开辟了基础模型式即插即用优化器的路径

Abstract: Balancing competing objectives is omnipresent across disciplines, from drug design to autonomous systems. Multi-objective Bayesian optimization is a promising solution for such expensive, black-box problems: it fits probabilistic surrogates and selects new designs via an acquisition function that balances exploration and exploitation. In practice, it requires tailored choices of surrogate and acquisition that rarely transfer to the next problem, is myopic when multi-step planning is often required, and adds refitting overhead, particularly in parallel or time-sensitive loops. We present TAMO, a fully amortized, universal policy for multi-objective black-box optimization. TAMO uses a transformer architecture that operates across varying input and objective dimensions, enabling pretraining on diverse corpora and transfer to new problems without retraining: at test time, the pretrained model proposes the next design with a single forward pass. We pretrain the policy with reinforcement learning to maximize cumulative hypervolume improvement over full trajectories, conditioning on the entire query history to approximate the Pareto frontier. Across synthetic benchmarks and real tasks, TAMO produces fast proposals, reducing proposal time by 50-1000x versus alternatives while matching or improving Pareto quality under tight evaluation budgets. These results show that transformers can perform multi-objective optimization entirely in-context, eliminating per-task surrogate fitting and acquisition engineering, and open a path to foundation-style, plug-and-play optimizers for scientific discovery workflows.

</details>


### [32] [Refining Graphical Neural Network Predictions Using Flow Matching for Optimal Power Flow with Constraint-Satisfaction Guarantee](https://arxiv.org/abs/2512.11127)
*Kshitiz Khanal*

Main category: cs.LG

TL;DR: 提出一种结合物理信息图神经网络和连续流匹配的两阶段学习框架，用于快速求解直流最优潮流问题，在保持100%可行性的同时达到接近最优解的效果。


<details>
  <summary>Details</summary>
Motivation: 传统优化求解器计算成本高，难以满足大规模电力系统频繁重计算的需求；机器学习方法虽然速度快但难以保证约束满足和成本最优性。需要一种既能快速求解又能保证约束满足和接近最优解的方法。

Method: 两阶段学习框架：第一阶段使用物理信息图神经网络生成可行初始解，通过包含经济调度最优条件、基尔霍夫定律和KKT互补条件的物理信息损失函数训练；第二阶段使用连续流匹配技术，通过学习的向量场回归将初始解优化到最优。

Result: 在IEEE 30节点系统的5个负荷场景（70%-130%额定负荷）测试中，方法在额定负荷下成本差距低于0.1%，极端条件下低于3%，同时保持100%可行性。

Conclusion: 该框架在快速神经网络预测和缓慢但最优的数值求解器之间架起桥梁，为高可再生能源渗透率、需要频繁调度更新的现代电力系统提供了实用解决方案。

Abstract: The DC Optimal Power Flow (DC-OPF) problem is fundamental to power system operations, requiring rapid solutions for real-time grid management. While traditional optimization solvers provide optimal solutions, their computational cost becomes prohibitive for large-scale systems requiring frequent recalculations. Machine learning approaches offer promise for acceleration but often struggle with constraint satisfaction and cost optimality. We present a novel two-stage learning framework that combines physics-informed Graph Neural Networks (GNNs) with Continuous Flow Matching (CFM) for solving DC-OPF problems. Our approach embeds fundamental physical principles--including economic dispatch optimality conditions, Kirchhoff's laws, and Karush-Kuhn-Tucker (KKT) complementarity conditions--directly into the training objectives. The first stage trains a GNN to produce feasible initial solutions by learning from physics-informed losses that encode power system constraints. The second stage employs CFM, a simulation-free continuous normalizing flow technique, to refine these solutions toward optimality through learned vector field regression. Evaluated on the IEEE 30-bus system across five load scenarios ranging from 70\% to 130\% nominal load, our method achieves near-optimal solutions with cost gaps below 0.1\% for nominal loads and below 3\% for extreme conditions, while maintaining 100\% feasibility. Our framework bridges the gap between fast but approximate neural network predictions and optimal but slow numerical solvers, offering a practical solution for modern power systems with high renewable penetration requiring frequent dispatch updates.

</details>


### [33] [Fairness-Regularized Online Optimization with Switching Costs](https://arxiv.org/abs/2512.11131)
*Pengfei Li,Yuelin Han,Adam Wierman,Shaolei Ren*

Main category: cs.LG

TL;DR: 该论文提出FairOBD算法，首次同时解决在线凸优化中的公平性和动作平滑性（切换成本）问题，通过引入辅助变量将长期公平成本分解为在线成本，在动态计算资源分配场景中有效降低总成本并促进公平结果。


<details>
  <summary>Details</summary>
Motivation: 在线优化问题中，公平性和动作平滑性是两个关键考虑因素，但现有研究尚未能同时解决这两个问题。论文旨在研究带有切换成本的公平性正则化平滑在线凸优化这一新挑战性场景。

Method: 提出FairOBD（公平性正则化在线平衡下降）算法：1）将长期公平成本通过引入辅助变量分解为一系列在线成本；2）利用辅助变量正则化在线动作以实现公平结果；3）采用新方法处理切换成本。

Result: 理论证明：1）即使没有切换成本，任何在线算法也无法在问题长度T增加时获得次线性遗憾或有限竞争比；2）FairOBD针对带参数约束的最优离线算法基准，提供了最坏情况下的渐近竞争比。实验验证：在动态计算资源分配场景中，FairOBD能有效降低总公平性正则化成本，比现有基线更好地促进公平结果。

Conclusion: FairOBD算法成功调和了命中成本、切换成本和公平成本之间的紧张关系，为同时解决在线优化中的公平性和动作平滑性问题提供了有效解决方案，在负责任AI推理等应用中具有实用价值。

Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.

</details>


### [34] [The Vekua Layer: Exact Physical Priors for Implicit Neural Representations via Generalized Analytic Functions](https://arxiv.org/abs/2512.11138)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: Vekua Layer (VL) 是一种基于广义解析函数理论的微分谱方法，通过将假设空间限制在控制微分算子的核空间，将隐式神经表示的学习任务转化为严格凸的最小二乘问题，相比传统方法具有更高的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在参数化物理场方面表现出强大能力，但存在谱偏差和非凸优化的计算开销问题。作者旨在开发一种更高效、更稳定的替代方法。

Method: 提出Vekua Layer (VL)，这是一种基于广义解析函数理论的微分谱方法。通过将假设空间限制在控制微分算子的核空间（使用调和基和傅里叶-贝塞尔基），将学习任务从迭代梯度下降转化为严格凸的最小二乘问题，通过线性投影求解。

Result: 在齐次椭圆偏微分方程上，VL相比SIRENs表现出显著优势：在精确重建任务中达到机器精度（MSE≈10^-33）；在非相干传感器噪声下表现出更好的稳定性（MSE≈0.03），起到物理信息谱滤波器的作用；能够通过解析延拓实现从部分边界数据的"全息"外推。

Conclusion: Vekua Layer提供了一种基于经典数学理论的隐式神经表示新范式，通过将学习问题转化为凸优化，克服了传统INRs的谱偏差和计算效率问题，在物理场重建中展现出卓越的精度、稳定性和外推能力。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful paradigm for parameterizing physical fields, yet they often suffer from spectral bias and the computational expense of non-convex optimization. We introduce the Vekua Layer (VL), a differentiable spectral method grounded in the classical theory of Generalized Analytic Functions. By restricting the hypothesis space to the kernel of the governing differential operator -- specifically utilizing Harmonic and Fourier-Bessel bases -- the VL transforms the learning task from iterative gradient descent to a strictly convex least-squares problem solved via linear projection. We evaluate the VL against Sinusoidal Representation Networks (SIRENs) on homogeneous elliptic Partial Differential Equations (PDEs). Our results demonstrate that the VL achieves machine precision ($\text{MSE} \approx 10^{-33}$) on exact reconstruction tasks and exhibits superior stability in the presence of incoherent sensor noise ($\text{MSE} \approx 0.03$), effectively acting as a physics-informed spectral filter. Furthermore, we show that the VL enables "holographic" extrapolation of global fields from partial boundary data via analytic continuation, a capability absent in standard coordinate-based approximations.

</details>


### [35] [Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles](https://arxiv.org/abs/2512.11145)
*Lennard Manuel,Hamid Gadirov,Steffen Frey*

Main category: cs.LG

TL;DR: 该论文提出了一种增强型自编码器框架，结合聚类损失和对比损失，用于高维科学集合数据集的降维和可视化。


<details>
  <summary>Details</summary>
Motivation: 科学集合数据集具有高维度和复杂性，传统的降维技术和自编码器在处理这类数据时存在困难，需要改进方法来提高可视化和可解释性。

Method: 使用EfficientNetV2为无标签数据生成伪标签，构建增强型自编码器框架，联合优化重构损失、基于软轮廓分数的聚类损失和对比损失，最后应用UMAP进行2D投影。

Result: 在两个科学集合数据集（土壤通道结构和液滴撞击薄膜动力学）上的实验表明，结合聚类或对比损失的模型在特征提取方面略优于基线方法。

Conclusion: 提出的增强型自编码器框架能够有效处理高维科学集合数据，通过联合优化多个目标函数，提高了数据可视化和特征提取的质量。

Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.

</details>


### [36] [Harnessing Rich Multi-Modal Data for Spatial-Temporal Homophily-Embedded Graph Learning Across Domains and Localities](https://arxiv.org/abs/2512.11178)
*Takuya Kurihana,Xiaojian Zhang,Wing Yee Au,Hon Yung Wong*

Main category: cs.LG

TL;DR: 提出一个异构数据管道，用于融合时空变化的多模态城市数据，通过图学习方法整合空间变化数据中的同质性，实现跨领域、跨地区的城市问题预测，具有强泛化能力和最小化重配置需求。


<details>
  <summary>Details</summary>
Motivation: 现代城市依赖数据驱动决策，但城市级数据存在异构格式、分散收集、标准不一的问题。国家级数据集虽然广泛可用，但表现出显著的异构性和多模态性，难以有效整合用于解决复杂的跨领域城市问题。

Method: 提出异构数据管道，执行跨领域数据融合，处理时空变化的时间序列数据集。数据学习模块将空间变化数据集中的同质性整合到图学习中，将不同地区的信息嵌入模型。使用50多个数据源，通过五个真实世界观察验证框架。

Result: 使用多个城市的公开可访问数据集（如拼车、交通事故和犯罪报告）进行验证，结果显示该框架表现出强大的预测性能，同时在转移到新地区或领域时只需最小化重配置。

Conclusion: 该研究推进了以可扩展方式构建数据驱动的城市系统的目标，解决了智慧城市分析中最紧迫的挑战之一，为跨领域、跨地区的城市问题提供了灵活、泛化的解决方案。

Abstract: Modern cities are increasingly reliant on data-driven insights to support decision making in areas such as transportation, public safety and environmental impact. However, city-level data often exists in heterogeneous formats, collected independently by local agencies with diverse objectives and standards. Despite their numerous, wide-ranging, and uniformly consumable nature, national-level datasets exhibit significant heterogeneity and multi-modality. This research proposes a heterogeneous data pipeline that performs cross-domain data fusion over time-varying, spatial-varying and spatial-varying time-series datasets. We aim to address complex urban problems across multiple domains and localities by harnessing the rich information over 50 data sources. Specifically, our data-learning module integrates homophily from spatial-varying dataset into graph-learning, embedding information of various localities into models. We demonstrate the generalizability and flexibility of the framework through five real-world observations using a variety of publicly accessible datasets (e.g., ride-share, traffic crash, and crime reports) collected from multiple cities. The results show that our proposed framework demonstrates strong predictive performance while requiring minimal reconfiguration when transferred to new localities or domains. This research advances the goal of building data-informed urban systems in a scalable way, addressing one of the most pressing challenges in smart city analytics.

</details>


### [37] [Bandwidth-constrained Variational Message Encoding for Cooperative Multi-agent Reinforcement Learning](https://arxiv.org/abs/2512.11179)
*Wei Duan,Jie Lu,En Yu,Junyu Xuan*

Main category: cs.LG

TL;DR: 论文提出BVME方法解决多智能体强化学习中带宽限制下的信息编码问题，在保持性能的同时大幅减少消息维度


<details>
  <summary>Details</summary>
Motivation: 现有的图基多智能体强化学习方法虽然能学习稀疏协调图（确定谁与谁通信），但未解决在硬带宽约束下应该传输什么信息的问题。简单的维度降维方法会持续降低协调性能，而确定性投影缺乏控制压缩过程的机制。

Method: 引入带宽约束变分消息编码（BVME），这是一个轻量级模块，将消息视为从学习的后验高斯分布中采样，通过KL散度正则化到无信息先验。BVME的变分框架通过可解释的超参数提供原则性、可调节的压缩强度控制，直接约束用于决策的表征。

Result: 在SMACv1、SMACv2和MPE基准测试中，BVME在使用67-83%更少消息维度的同时，实现了相当或更优的性能。在稀疏图中增益最为明显，因为消息质量对协调影响关键。消融实验显示对带宽呈U形敏感性，BVME在极端比率下表现出色，同时增加的开销最小。

Conclusion: BVME为带宽受限的多智能体协调提供了有效的消息编码解决方案，通过变分框架实现对压缩过程的控制，在保持性能的同时显著减少通信开销。

Abstract: Graph-based multi-agent reinforcement learning (MARL) enables coordinated behavior under partial observability by modeling agents as nodes and communication links as edges. While recent methods excel at learning sparse coordination graphs-determining who communicates with whom-they do not address what information should be transmitted under hard bandwidth constraints. We study this bandwidth-limited regime and show that naive dimensionality reduction consistently degrades coordination performance. Hard bandwidth constraints force selective encoding, but deterministic projections lack mechanisms to control how compression occurs. We introduce Bandwidth-constrained Variational Message Encoding (BVME), a lightweight module that treats messages as samples from learned Gaussian posteriors regularized via KL divergence to an uninformative prior. BVME's variational framework provides principled, tunable control over compression strength through interpretable hyperparameters, directly constraining the representations used for decision-making. Across SMACv1, SMACv2, and MPE benchmarks, BVME achieves comparable or superior performance while using 67--83% fewer message dimensions, with gains most pronounced on sparse graphs where message quality critically impacts coordination. Ablations reveal U-shaped sensitivity to bandwidth, with BVME excelling at extreme ratios while adding minimal overhead.

</details>


### [38] [Progress over Points: Reframing LM Benchmarks Around Scientific Objectives](https://arxiv.org/abs/2512.11183)
*Alwin Jin,Sean M. Hendryx,Vaskar Nath*

Main category: cs.LG

TL;DR: 该论文提出"进展导向基准测试"新范式，用NanoGPT速度挑战作为实例，将基准测试从静态问题排行榜转变为可测量的开放式科学研究工具。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试主要针对静态已解决问题（如数学应用题），这种方法限制了可测量的进步类型。需要一种能直接推动科学进展的基准测试范式。

Method: 创建基于NanoGPT速度挑战的进展导向基准环境，标准化数据集切片、参考模型、训练工具和丰富遥测数据，包含运行时验证和防作弊检查。

Result: 实现了新的最先进训练时间，比之前记录提高了3秒，并定性观察到新颖算法思想的出现。基准测试成功催化了语言建模堆栈的可重用改进。

Conclusion: 进展导向基准测试将"基准测试"重构为科学进步的工具，使基准测试上的进步直接等同于科学进步，推动社区从静态问题排行榜转向可测量的开放式科学研究。

Abstract: Current benchmarks that test LLMs on static, already-solved problems (e.g., math word problems) effectively demonstrated basic capability acquisition. The natural progression has been toward larger, more comprehensive and challenging collections of static problems, an approach that inadvertently constrains the kinds of advances we can measure and incentivize. To address this limitation, we argue for progress-oriented benchmarks, problem environments whose objectives are themselves the core targets of scientific progress, so that achieving state of the art on the benchmark advances the field. As a introductory step, we instantiate an environment based on the NanoGPT speedrun. The environment standardizes a dataset slice, a reference model and training harness, and rich telemetry, with run-time verification and anti-gaming checks. Evaluation centers on the scientific delta achieved: best-attained loss and the efficiency frontier. Using this environment, we achieve a new state-of-the-art training time, improving upon the previous record by 3 seconds, and qualitatively observe the emergence of novel algorithmic ideas. Moreover, comparisons between models and agents remain possible, but they are a means, not the end; the benchmark's purpose is to catalyze reusable improvements to the language modeling stack. With this release, the overarching goal is to seed a community shift from static problem leaderboards to test-time research on open-ended yet measurable scientific problems. In this new paradigm, progress on the benchmark is progress on the science, thus reframing "benchmarking" as a vehicle for scientific advancement.

</details>


### [39] [On the failure of ReLU activation for physics-informed machine learning](https://arxiv.org/abs/2512.11184)
*Conor Rowan*

Main category: cs.LG

TL;DR: 本文分析了ReLU激活函数在物理信息机器学习中表现不佳的原因，发现自动微分无法正确处理不连续场的导数，导致梯度计算错误。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习使用控制微分方程训练神经网络表示解场，激活函数的选择影响解的特征和性能。已有研究表明ReLU在基准微分方程上表现不如sigmoid、tanh和swish等激活函数，本文旨在诊断ReLU表现不佳的根本原因。

Method: 通过分析ReLU激活函数的数学特性，特别是其分段线性形式，研究其在物理信息机器学习中的表现。重点关注自动微分过程如何计算ReLU的导数，以及这如何影响物理信息损失函数的梯度计算。

Result: 研究发现ReLU不仅在二阶微分方程中表现不佳，即使在仅涉及一阶导数的变分问题中也失败。失败的根本原因是自动微分（如PyTorch）无法正确表征不连续场的导数，导致物理信息损失的梯度计算错误。

Conclusion: ReLU在物理信息机器学习中表现不佳的根本原因是其分段线性特性导致自动微分无法正确处理导数计算，从而错误指定了损失函数的梯度。这解释了为什么ReLU在物理信息训练中表现不如其他平滑激活函数。

Abstract: Physics-informed machine learning uses governing ordinary and/or partial differential equations to train neural networks to represent the solution field. Like any machine learning problem, the choice of activation function influences the characteristics and performance of the solution obtained from physics-informed training. Several studies have compared common activation functions on benchmark differential equations, and have unanimously found that the rectified linear unit (ReLU) is outperformed by competitors such as the sigmoid, hyperbolic tangent, and swish activation functions. In this work, we diagnose the poor performance of ReLU on physics-informed machine learning problems. While it is well-known that the piecewise linear form of ReLU prevents it from being used on second-order differential equations, we show that ReLU fails even on variational problems involving only first derivatives. We identify the cause of this failure as second derivatives of the activation, which are taken not in the formulation of the loss, but in the process of training. Namely, we show that automatic differentiation in PyTorch fails to characterize derivatives of discontinuous fields, which causes the gradient of the physics-informed loss to be mis-specified, thus explaining the poor performance of ReLU.

</details>


### [40] [Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models](https://arxiv.org/abs/2512.11194)
*Divya Kothandaraman,Jaclyn Pytlarz*

Main category: cs.LG

TL;DR: 提出梯度投影框架，在扩散模型训练中通过正交投影消除敏感概念特征的影响，实现概念级别的选择性遗忘，减少记忆化风险同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型的记忆化带来安全和知识产权风险，传统去记忆化技术只能限制对特定训练样本的过拟合，无法系统性地防止禁止概念级别特征的内化，需要概念级别的选择性遗忘方法。

Method: 引入梯度投影框架，在反向传播过程中系统识别并剔除与禁止属性嵌入对齐的训练信号，将每个梯度更新投影到敏感特征嵌入空间的正交补空间上，从而消除其对模型权重的影响。

Result: 实验表明该框架显著减少记忆化，同时严格保持生成质量和语义保真度，能够有效防御特征提取攻击。

Conclusion: 通过将记忆化控制重新定义为选择性学习，该方法为知识产权安全和隐私保护的生成式AI建立了新范式。

Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.

</details>


### [41] [Fast EXP3 Algorithms](https://arxiv.org/abs/2512.11201)
*Ryoma Sato,Shinji Ito*

Main category: cs.LG

TL;DR: EXP3算法可实现每轮常数时间运行，作者提出了更实用的算法，并分析了这些算法的遗憾界与时间复杂度的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 虽然EXP3是解决对抗性多臂赌博机问题的经典算法，但其实现效率可能不够理想。作者旨在改进EXP3的时间复杂度，使其在实际应用中更加高效，同时探索算法性能与计算效率之间的平衡。

Method: 作者首先指出EXP3算法可以实现每轮常数时间运行，然后提出了更实用的算法变体。这些方法可能在采样策略、权重更新机制或数据结构方面进行了优化，以降低计算复杂度。

Result: 开发了具有不同时间复杂度的算法变体，分析了这些算法在遗憾界与计算效率之间的权衡关系。可能展示了如何在保持理论保证的同时显著减少计算开销。

Conclusion: 通过优化实现方式，EXP3算法可以在保持理论性能的同时达到常数时间每轮运行，为对抗性多臂赌博机问题提供了更实用的高效解决方案，并揭示了算法性能与计算复杂度之间的重要权衡。

Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.

</details>


### [42] [Latent Variable Causal Discovery under Selection Bias](https://arxiv.org/abs/2512.11219)
*Haoyue Dai,Yiwen Qiu,Ignavier Ng,Xinshuai Dong,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 本文研究在存在选择偏差的情况下进行潜变量因果发现，提出利用协方差子矩阵的秩约束作为条件独立性的推广来处理选择偏差问题。


<details>
  <summary>Details</summary>
Motivation: 潜变量因果发现中的选择偏差问题尚未得到充分探索，主要原因是缺乏合适的统计工具。现有的处理潜变量的工具都没有针对选择偏差进行适配。

Method: 研究秩约束作为条件独立性的推广，在线性高斯模型中利用协方差子矩阵的秩。提供秩约束的图论特征化，并证明单因子模型在选择偏差下可识别。

Result: 尽管选择偏差会显著复杂化联合分布，但偏置协方差矩阵中的秩仍然保留了关于因果结构和选择机制的有意义信息。模拟和真实世界实验证实了秩约束的有效性。

Conclusion: 秩约束为处理选择偏差下的潜变量因果发现提供了有效的统计工具，单因子模型在选择偏差下可识别的证明展示了该方法的实际应用价值。

Abstract: Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.

</details>


### [43] [Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference](https://arxiv.org/abs/2512.11221)
*Adilet Metinov,Gulida M. Kudakeeva,Bolotbek uulu Nursultan,Gulnara D. Kabaeva*

Main category: cs.LG

TL;DR: 提出ASR-KF-EGR框架，通过可逆软冻结机制在推理时动态管理KV缓存，减少55-67%的活跃KV缓存大小，同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在长上下文生成中的内存瓶颈问题，特别是KV缓存的内存占用问题，为内存受限的部署提供实用解决方案

Method: 采用自适应软滚动KV冻结与熵引导恢复机制，在滑动注意力窗口中识别低重要性token并暂时冻结其KV更新，将冻结token存储在GPU外，需要时恢复；引入亚线性冻结调度，防止过度压缩

Result: 在LLaMA-3 8B上的初步实验显示，活跃KV缓存大小减少55-67%，同时保持生成质量并通过针在干草堆检索测试；方法无需训练，架构无关

Conclusion: ASR-KF-EGR为长上下文LLM的内存受限部署提供了实用的训练免费推理时解决方案，有效平衡内存效率与生成质量

Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

</details>


### [44] [Task-Aware Multi-Expert Architecture For Lifelong Deep Learning](https://arxiv.org/abs/2512.11243)
*Jianyu Wang,Jacob Nean-Hua Sheikh,Cat P. Le,Hoda Bidkhori*

Main category: cs.LG

TL;DR: TAME是一种终身深度学习算法，通过任务感知的多专家系统、重放缓冲区和注意力机制，在连续学习任务中平衡适应性和知识保留。


<details>
  <summary>Details</summary>
Motivation: 终身深度学习需要在连续学习多个任务时，既能适应新任务又能保留先前任务的知识，避免灾难性遗忘问题。

Method: 提出Task-Aware Multi-Expert (TAME)算法：1) 维护预训练神经网络池，根据任务相似性选择最相关专家；2) 共享密集层整合选定专家的特征进行预测；3) 使用重放缓冲区存储先前任务的代表性样本和嵌入；4) 注意力机制优先处理最相关的存储信息。

Result: 在基于CIFAR-100的二分类任务实验中，TAME在提高新任务准确率的同时保持了先前任务的性能，有效平衡了适应性和知识保留。

Conclusion: TAME通过任务感知的专家选择、重放缓冲和注意力机制，在终身学习环境中实现了灵活适应和知识保留的有效平衡。

Abstract: Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.

</details>


### [45] [Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language](https://arxiv.org/abs/2512.11251)
*Yunkai Zhang,Yawen Zhang,Ming Zheng,Kezhen Chen,Chongyang Gao,Ruian Ge,Siyuan Teng,Amine Jelloul,Jinmeng Rao,Xiaoyuan Guo,Chiang-Wei Fang,Zeyu Zheng,Jie Yang*

Main category: cs.LG

TL;DR: Insight Miner是一个用于生成高质量时间序列描述的多模态大模型，通过TS-Insights数据集进行训练，在时间序列分析任务上超越了现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在多个领域都很重要，但从中挖掘洞察需要深厚的领域专业知识，这个过程耗时耗力。需要一种能够自动生成高质量时间序列描述和洞察的解决方案。

Method: 提出了Insight Miner多模态大模型，并创建了TS-Insights数据集（包含100k时间序列窗口）。使用智能体工作流：先用统计工具从原始时间序列提取特征，再用GPT-4合成连贯的趋势描述。在TS-Insights上进行指令微调。

Result: Insight Miner在生成时间序列描述和洞察方面超越了最先进的多模态模型（如LLaVA和GPT-4）。

Conclusion: 这项工作展示了利用多模态大模型进行时间序列分析的有前景方向，是让大语言模型将时间序列作为原生输入模态的基础性步骤。

Abstract: Time-series data is critical across many scientific and industrial domains, including environmental analysis, agriculture, transportation, and finance. However, mining insights from this data typically requires deep domain expertise, a process that is both time-consuming and labor-intensive. In this paper, we propose \textbf{Insight Miner}, a large-scale multimodal model (LMM) designed to generate high-quality, comprehensive time-series descriptions enriched with domain-specific knowledge. To facilitate this, we introduce \textbf{TS-Insights}\footnote{Available at \href{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}{https://huggingface.co/datasets/zhykoties/time-series-language-alignment}.}, the first general-domain dataset for time series and language alignment. TS-Insights contains 100k time-series windows sampled from 20 forecasting datasets. We construct this dataset using a novel \textbf{agentic workflow}, where we use statistical tools to extract features from raw time series before synthesizing them into coherent trend descriptions with GPT-4. Following instruction tuning on TS-Insights, Insight Miner outperforms state-of-the-art multimodal models, such as LLaVA \citep{liu2023llava} and GPT-4, in generating time-series descriptions and insights. Our findings suggest a promising direction for leveraging LMMs in time series analysis, and serve as a foundational step toward enabling LLMs to interpret time series as a native input modality.

</details>


### [46] [A Simple Generalisation of the Implicit Dynamics of In-Context Learning](https://arxiv.org/abs/2512.11255)
*Francesco Innocenti,El Mehdi Achour*

Main category: cs.LG

TL;DR: 该论文扩展了Dherin等人(2025)的理论，将transformer的上下文学习能力从仅考虑最后一个位置扩展到所有序列位置、从第一个块扩展到所有transformer块，并包含了更真实的层归一化残差块。


<details>
  <summary>Details</summary>
Motivation: 先前关于上下文学习(ICL)的理论主要基于简化模型和数据设置，而Dherin等人(2025)最近展示了transformer块可以隐式更新其前馈网络权重。本研究旨在将这一理论推广到更接近实际transformer架构的情况。

Method: 提出了Dherin等人理论的简单泛化：(1)扩展到所有序列位置而不仅仅是最后一个位置；(2)扩展到所有transformer块而不仅仅是第一个块；(3)包含更真实的层归一化残差块。在简单的上下文线性回归任务上进行了实证验证。

Result: 理论扩展成功，实证验证了在简单上下文线性回归任务上的有效性，并研究了不同token之间和不同块之间隐式更新的关系。

Conclusion: 这些结果将Dherin等人的理论进一步推向实践，为在大规模模型上进行验证提供了潜力，有助于更深入地理解transformer的上下文学习机制。

Abstract: In-context learning (ICL) refers to the ability of a model to learn new tasks from examples in its input without any parameter updates. In contrast to previous theories of ICL relying on toy models and data settings, recently it has been shown that an abstraction of a transformer block can be seen as implicitly updating the weights of its feedforward network according to the context (Dherin et al., 2025). Here, we provide a simple generalisation of this result for (i) all sequence positions beyond the last, (ii) any transformer block beyond the first, and (iii) more realistic residual blocks including layer normalisation. We empirically verify our theory on simple in-context linear regression tasks and investigate the relationship between the implicit updates related to different tokens within and between blocks. These results help to bring the theory of Dherin et al. (2025) even closer to practice, with potential for validation on large-scale models.

</details>


### [47] [Features Emerge as Discrete States: The First Application of SAEs to 3D Representations](https://arxiv.org/abs/2512.11263)
*Albert Miao,Chenliang Zhou,Jiawei Zhou,Cengiz Oztireli*

Main category: cs.LG

TL;DR: 首次将稀疏自编码器应用于3D领域，分析3D重建VAE的特征表示，发现模型编码离散而非连续特征，通过相变框架解释多个反直觉行为。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在文本领域已有成功应用，但在3D领域应用有限，限制了特征分解的理论探索。本文旨在将SAE扩展到3D领域，分析3D重建模型的特征表示机制。

Method: 将稀疏自编码器应用于3D重建VAE，分析其在Objaverse数据集中53k个3D模型上的特征表示。通过状态转换框架分析特征激活模式，研究相变现象。

Result: 发现3D重建模型编码离散而非连续特征，呈现相变式状态转换。解释了三个反直觉现象：模型偏好位置编码表示、特征消融导致的S型重建损失曲线、相变点的双峰分布。

Conclusion: 3D模型通过离散状态空间近似和相变机制学习特征，模型通过重新分配叠加干扰来优化特征显著性。该框架为理解模型特征学习动态提供了新视角。

Abstract: Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the \textbf{first application of SAEs to the 3D domain}, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: \textbf{such models approximate a discrete state space, driven by phase-like transitions from feature activations}. Through this state transition framework, we address three otherwise unintuitive behaviors -- the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model \textbf{redistributes the interference caused by superposition to prioritize the saliency of different features}. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also provides a framework to explain the model's feature learning dynamics. The code and dataset of encoded 3D objects will be available on release.

</details>


### [48] [Condensation-Concatenation Framework for Dynamic Graph Continual Learning](https://arxiv.org/abs/2512.11317)
*Tingxu Yan,Ye Yuan*

Main category: cs.LG

TL;DR: 提出CCC框架解决动态图中图神经网络灾难性遗忘问题，通过压缩历史图快照为语义表示并与当前图表示选择性拼接，在四个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动态图在现实场景中普遍存在，连续的结构变化会导致图神经网络出现灾难性遗忘。现有动态图持续学习方法忽视了拓扑变化对现有节点的影响。

Method: 提出CCC框架：1) 将历史图快照压缩为紧凑的语义表示，同时保持原始标签分布和拓扑特性；2) 将这些历史嵌入与当前图表示进行选择性拼接；3) 改进遗忘度量以适应动态图场景，量化结构更新对现有节点预测性能的影响。

Result: 在四个真实世界数据集上的大量实验表明，CCC框架在性能上优于最先进的基线方法。

Conclusion: CCC框架有效解决了动态图中图神经网络的持续学习问题，通过压缩-拼接策略处理拓扑变化对现有节点的影响，改进的遗忘度量更好地适应动态图场景。

Abstract: Dynamic graphs are prevalent in real-world scenarios, where continuous structural changes induce catastrophic forgetting in graph neural networks (GNNs). While continual learning has been extended to dynamic graphs, existing methods overlook the effects of topological changes on existing nodes. To address it, we propose a novel framework for continual learning on dynamic graphs, named Condensation-Concatenation-based Continual Learning (CCC). Specifically, CCC first condenses historical graph snapshots into compact semantic representations while aiming to preserve the original label distribution and topological properties. Then it concatenates these historical embeddings with current graph representations selectively. Moreover, we refine the forgetting measure (FM) to better adapt to dynamic graph scenarios by quantifying the predictive performance degradation of existing nodes caused by structural updates. CCC demonstrates superior performance over state-of-the-art baselines across four real-world datasets in extensive experiments.

</details>


### [49] [SRLR: Symbolic Regression based Logic Recovery to Counter Programmable Logic Controller Attacks](https://arxiv.org/abs/2512.11298)
*Hao Zhou,Suman Sourav,Binbin Chen,Ke Yu*

Main category: cs.LG

TL;DR: SRLR是一种基于符号回归的逻辑恢复解决方案，用于仅基于PLC的输入输出来识别其控制逻辑，从而生成可解释的规则来检测控制器逻辑攻击。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统中的PLC面临网络攻击威胁，现有检测方法存在局限性：基于规范的方法需要专家手动工作或访问源代码，而基于机器学习的方法缺乏决策解释性。

Method: SRLR利用ICS特定属性增强深度符号回归方法：1) 在频域而非时域表示控制逻辑；2) 处理多模式操作；3) 过滤异常输入；4) 降低公式复杂度以实现有效搜索。

Result: SRLR在各种ICS设置中始终优于现有方法，恢复准确率在某些挑战性环境中可提高39%。在包含数百个电压调节器的配电网络评估中，SRLR展示了处理大规模复杂系统的稳定性。

Conclusion: SRLR通过利用ICS特定属性增强符号回归，能够有效恢复PLC控制逻辑并生成可解释的攻击检测规则，为工业控制系统安全提供了新的解决方案。

Abstract: Programmable Logic Controllers (PLCs) are critical components in Industrial Control Systems (ICSs). Their potential exposure to external world makes them susceptible to cyber-attacks. Existing detection methods against controller logic attacks use either specification-based or learnt models. However, specification-based models require experts' manual efforts or access to PLC's source code, while machine learning-based models often fall short of providing explanation for their decisions. We design SRLR -- a it Symbolic Regression based Logic Recovery} solution to identify the logic of a PLC based only on its inputs and outputs. The recovered logic is used to generate explainable rules for detecting controller logic attacks. SRLR enhances the latest deep symbolic regression methods using the following ICS-specific properties: (1) some important ICS control logic is best represented in frequency domain rather than time domain; (2) an ICS controller can operate in multiple modes, each using different logic, where mode switches usually do not happen frequently; (3) a robust controller usually filters out outlier inputs as ICS sensor data can be noisy; and (4) with the above factors captured, the degree of complexity of the formulas is reduced, making effective search possible. Thanks to these enhancements, SRLR consistently outperforms all existing methods in a variety of ICS settings that we evaluate. In terms of the recovery accuracy, SRLR's gain can be as high as 39% in some challenging environment. We also evaluate SRLR on a distribution grid containing hundreds of voltage regulators, demonstrating its stability in handling large-scale, complex systems with varied configurations.

</details>


### [50] [NeuralOGCM: Differentiable Ocean Modeling with Learnable Physics](https://arxiv.org/abs/2512.11525)
*Hao Wu,Yuan Gao,Fan Xu,Fan Zhang,Guangliang Liu,Yuxuan Liang,Xiaomeng Huang*

Main category: cs.LG

TL;DR: NeuralOGCM是一个融合可微分编程和深度学习的海洋建模框架，通过可学习的物理求解器和神经网络校正，在保持物理一致性的同时显著提升计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 科学模拟长期面临计算效率与物理保真度之间的权衡问题，传统数值模型计算成本高，而纯AI模型缺乏物理一致性，需要一种既能保持物理合理性又能高效计算的解决方案。

Method: 提出NeuralOGCM框架：1）完全可微分的动力学求解器，将物理知识作为核心归纳偏置，将关键物理参数（如扩散系数）转化为可学习参数；2）深度神经网络校正亚网格尺度过程和离散化误差；3）两个组件协同工作，通过统一的ODE求解器集成输出。

Result: 实验表明NeuralOGCM保持长期稳定性和物理一致性，在速度上显著优于传统数值模型，在精度上优于纯AI基线方法。

Conclusion: 该工作为构建快速、稳定且物理合理的科学计算模型开辟了新路径，成功融合了物理建模和数据驱动方法的优势。

Abstract: High-precision scientific simulation faces a long-standing trade-off between computational efficiency and physical fidelity. To address this challenge, we propose NeuralOGCM, an ocean modeling framework that fuses differentiable programming with deep learning. At the core of NeuralOGCM is a fully differentiable dynamical solver, which leverages physics knowledge as its core inductive bias. The learnable physics integration captures large-scale, deterministic physical evolution, and transforms key physical parameters (e.g., diffusion coefficients) into learnable parameters, enabling the model to autonomously optimize its physical core via end-to-end training. Concurrently, a deep neural network learns to correct for subgrid-scale processes and discretization errors not captured by the physics model. Both components work in synergy, with their outputs integrated by a unified ODE solver. Experiments demonstrate that NeuralOGCM maintains long-term stability and physical consistency, significantly outperforming traditional numerical models in speed and pure AI baselines in accuracy. Our work paves a new path for building fast, stable, and physically-plausible models for scientific computing.

</details>


### [51] [QGEC : Quantum Golay Code Error Correction](https://arxiv.org/abs/2512.11307)
*Hideo Mukai,Hoshitaro Ohnishi*

Main category: cs.LG

TL;DR: 提出基于Golay码的量子纠错方法QGEC，使用Transformer解码器，在三种权重集和三种噪声模型下评估性能，发现Golay码（23个数据量子位，距离7）比toric码（50个数据量子位，距离5）解码精度更高。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在特定问题上比经典计算机计算负载小得多，但量子比特易受外部噪声影响。量子纠错（QEC）对于处理量子比特至关重要，其中通过稳定子生成器的综合征测量来预测实际错误，而不是直接测量数据量子比特。

Method: 提出量子Golay码纠错（QGEC）方法，使用经典信息论中的高效编码方法Golay码。采用Transformer进行解码计算，在由生成多项式定义的码空间中评估解码器精度，使用三种不同权重集和三种不同比特翻转错误与相位翻转错误相关性的噪声模型。

Result: 较小相关性的噪声模型给出更好的精度，而生成多项式的权重对解码器精度影响很小。Golay码（23个数据量子位，距离7）比toric码（50个数据量子位，距离5）实现了更高的解码精度。

Conclusion: 使用Transformer实现量子纠错可能使Golay码更高效地实现容错量子计算。

Abstract: Quantum computers have the possibility of a much reduced calculation load compared with classical computers in specific problems. Quantum error correction (QEC) is vital for handling qubits, which are vulnerable to external noise. In QEC, actual errors are predicted from the results of syndrome measurements by stabilizer generators, in place of making direct measurements of the data qubits. Here, we propose Quantum Golay code Error Correction (QGEC), a QEC method using Golay code, which is an efficient coding method in classical information theory. We investigated our method's ability in decoding calculations with the Transformer. We evaluated the accuracy of the decoder in a code space defined by the generative polynomials with three different weights sets and three noise models with different correlations of bit-flip error and phase-flip error. Furthermore, under a noise model following a discrete uniform distribution, we compared the decoding performance of Transformer decoders with identical architectures trained respectively on Golay and toric codes. The results showed that the noise model with the smaller correlation gave better accuracy, while the weights of the generative polynomials had little effect on the accuracy of the decoder. In addition, they showed that Golay code requiring 23 data qubits and having a code distance of 7 achieved higher decoding accuracy than toric code which requiring 50 data qubits and having a code distance of 5. This suggests that implementing quantum error correction using a Transformer may enable the Golay code to realize fault-tolerant quantum computation more efficiently.

</details>


### [52] [Contrastive Time Series Forecasting with Anomalies](https://arxiv.org/abs/2512.11526)
*Joel Ekstrand,Zahra Taghiyarrenani,Slawomir Nowaczyk*

Main category: cs.LG

TL;DR: Co-TSFA是一个对比学习框架，通过区分短期异常和持久性异常来改进时间序列预测，使模型能够忽略无关噪声同时保持对有意义分布变化的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测中，一些异常事件具有持久影响需要响应，而另一些短期异常应该被忽略。标准预测模型无法区分这两类异常，要么对噪声过度反应，要么错过持久性变化。

Method: 提出Co-TSFA对比学习正则化框架：1）生成仅输入增强和输入输出增强来分别建模预测无关和预测相关异常；2）引入潜在输出对齐损失，将表示变化与预测变化关联；3）鼓励对无关扰动的不变性，同时保持对有意义分布变化的敏感性。

Result: 在Traffic和Electricity基准数据集以及真实世界现金需求数据集上的实验表明，Co-TSFA在异常条件下提高了性能，同时在正常数据上保持了准确性。

Conclusion: Co-TSFA通过对比学习框架有效区分了预测相关和预测无关的异常，改进了时间序列预测在异常条件下的鲁棒性，同时不损害正常数据的预测性能。

Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.

</details>


### [53] [Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315)
*Pranav Guruprasad,Sudipta Chowdhury,Harsh Sikka,Mridul Sharma,Helen Lu,Sean Rivera,Aryan Khurana,Hangliang Ren,Yangyue Wang*

Main category: cs.LG

TL;DR: MultiNet v1.0是一个统一基准测试，用于评估视觉语言模型和视觉语言动作模型在六个核心能力领域的跨领域泛化能力，发现当前模型在未见领域存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体评估分散在孤立基准上，难以判断基础模型是否真正超越了训练分布实现泛化。需要统一基准来衡量模型在跨领域任务中的通用性。

Method: 开发MultiNet v1.0基准测试，涵盖视觉基础、空间推理、工具使用、物理常识、多智能体协调和连续机器人控制六个能力领域，评估GPT-5、Pi0和Magma等主流模型。

Result: 所有模型都未表现出一致的泛化能力，在未见领域、不熟悉模态或跨领域任务转换时性能显著下降，出现模态错位、输出格式不稳定和知识灾难性退化等问题。

Conclusion: 当前基础模型与通用智能目标之间存在明显差距，MultiNet v1.0为诊断这些差距和指导未来通用智能体开发提供了标准化评估基础。

Abstract: Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.

</details>


### [54] [Optimizing the Training Diet: Data Mixture Search for Robust Time Series Forecasting](https://arxiv.org/abs/2512.11546)
*Federico Pennino,Maurizio Gabbrielli*

Main category: cs.LG

TL;DR: 提出一个数据选择框架，通过优化训练数据组成而非调整模型超参数，发现从大型未标记时间序列数据集中选择最优"训练食谱"，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统深度学习训练范式假设数据越多越好，但原始传感器数据通常不平衡且包含大量冗余，并非所有数据点对模型泛化都有同等贡献。本文旨在证明在某些情况下"少即是多"，通过优化训练数据组成来提升模型性能

Method: 1. 使用大规模编码器和k-means聚类将数据集划分为行为一致的不同聚类，作为训练的基本"成分"；2. 采用Optuna优化框架在可能的数据混合高维空间中进行搜索；3. 对每个试验，Optuna为每个聚类提出特定采样比例，基于此"食谱"构建新训练集；4. 训练并评估较小的目标模型

Result: 实验表明，这种以数据为中心的搜索方法一致地发现了能产生显著更高性能模型的数据混合方案。在PMSM数据集上评估，该方法将性能从基线MSE 1.70提升到1.37，实现了19.41%的改进

Conclusion: 通过优化训练数据组成而非模型超参数，可以显著提升深度学习模型在传感器数据上的性能，证明了"少即是多"的数据选择策略的有效性，为数据为中心的机器学习提供了新思路

Abstract: The standard paradigm for training deep learning models on sensor data assumes that more data is always better. However, raw sensor streams are often imbalanced and contain significant redundancy, meaning that not all data points contribute equally to model generalization. In this paper, we show that, in some cases, "less is more" when considering datasets. We do this by reframing the data selection problem: rather than tuning model hyperparameters, we fix the model and optimize the composition of the training data itself. We introduce a framework for discovering the optimal "training diet" from a large, unlabeled time series corpus. Our framework first uses a large-scale encoder and k-means clustering to partition the dataset into distinct, behaviorally consistent clusters. These clusters represent the fundamental 'ingredients' available for training. We then employ the Optuna optimization framework to search the high-dimensional space of possible data mixtures. For each trial, Optuna proposes a specific sampling ratio for each cluster, and a new training set is constructed based on this recipe. A smaller target model is then trained and evaluated. Our experiments reveal that this data-centric search consistently discovers data mixtures that yield models with significantly higher performance compared to baselines trained on the entire dataset. Specifically - evaluated on PMSM dataset - our method improved performance from a baseline MSE of 1.70 to 1.37, a 19.41% improvement.

</details>


### [55] [Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents](https://arxiv.org/abs/2512.11584)
*Stefan Tabakov,Asen Popov,Dimitar Dimitrov,S. Ensiye Kiyamousavi,Vladimir Hristov,Boris Kraychev*

Main category: cs.LG

TL;DR: 论文提出原子动作切片（AAS）方法，将长时程演示分解为短小的类型化原子动作，以提升VLA模型的泛化能力，并在LIBERO数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作（VLA）模型在需要新技能或物体组合的任务上泛化能力较差，需要更好的动作表示方法来提升规划器和策略的学习效果。

Method: 提出原子动作切片（AAS）方法，将长时程演示分解为短小的类型化原子动作，使用更强的分割模型（Gemini 2.5 Pro）匹配规划器定义的方案，并构建了包含2,124个原子片段的验证数据集。

Result: 在LIBERO数据集上，使用AAS方法微调CLIP-RT+模型后，任务成功率从94.2%提升到95.3%（LIBERO-Goal）和从83.8%提升到88.8%（LIBERO-Long）。

Conclusion: 原子动作切片方法能有效提升VLA模型的泛化能力，特别是在需要新组合的任务上，为机器人学习提供了更好的动作表示框架。

Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)

</details>


### [56] [Pace: Physics-Aware Attentive Temporal Convolutional Network for Battery Health Estimation](https://arxiv.org/abs/2512.11332)
*Sara Sameer,Wei Zhang,Kannan Dhivya Dharshini,Xin Lou,Yulin Gao,Terence Goh,Qingyu Yan*

Main category: cs.LG

TL;DR: Pace是一种用于电池健康估计的物理感知注意力时序卷积网络，通过结合原始传感器数据和电池物理特征，在公开数据集上比现有模型平均性能提升6.5倍，并在树莓派上实现实时边缘部署。


<details>
  <summary>Details</summary>
Motivation: 电池是现代能源系统（如电动汽车和电网储能）的关键组件，有效的电池健康管理对系统安全、成本效益和可持续性至关重要。现有方法需要更准确、高效的电池健康估计解决方案。

Method: 提出Pace模型，整合原始传感器测量数据和等效电路模型导出的电池物理特征。开发三个电池专用模块：用于高效时序编码的扩张时序块、用于上下文建模的分块注意力块，以及用于融合短期和长期电池退化模式的双头输出块。

Result: 在大型公开数据集上，Pace表现显著优于现有模型，相比两个最佳基线模型平均性能提升6.5倍和2.0倍。通过树莓派上的实时边缘部署验证了其实用可行性。

Conclusion: Pace为电池健康分析提供了一个实用且高性能的解决方案，能够准确高效地预测各种电池使用条件下的电池健康状况。

Abstract: Batteries are critical components in modern energy systems such as electric vehicles and power grid energy storage. Effective battery health management is essential for battery system safety, cost-efficiency, and sustainability. In this paper, we propose Pace, a physics-aware attentive temporal convolutional network for battery health estimation. Pace integrates raw sensor measurements with battery physics features derived from the equivalent circuit model. We develop three battery-specific modules, including dilated temporal blocks for efficient temporal encoding, chunked attention blocks for context modeling, and a dual-head output block for fusing short- and long-term battery degradation patterns. Together, the modules enable Pace to predict battery health accurately and efficiently in various battery usage conditions. In a large public dataset, Pace performs much better than existing models, achieving an average performance improvement of 6.5 and 2.0x compared to two best-performing baseline models. We further demonstrate its practical viability with a real-time edge deployment on a Raspberry Pi. These results establish Pace as a practical and high-performance solution for battery health analytics.

</details>


### [57] [Spectral entropy prior-guided deep feature fusion architecture for magnetic core loss](https://arxiv.org/abs/2512.11334)
*Cong Yao,Chunye Gong,Jin Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种混合模型SEPI-TFPNet，结合经验模型和深度学习来改进磁芯损耗建模，在MagNet挑战数据集上取得了更好的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统磁芯损耗建模方法预测精度有限，而纯数据驱动模型虽然拟合性能强，但可解释性和跨分布泛化能力不足。IEEE电力电子学会发起的MagNet挑战旨在通过数据驱动范式揭示磁性元件的复杂损耗模式。

Method: 提出SEPI-TFPNet混合模型，包含物理先验子模块和数据驱动子模块。物理先验子模块采用谱熵判别机制选择最适合不同激励波形的经验模型；数据驱动子模块结合CNN、多头注意力机制和双向LSTM网络提取磁通密度时间序列特征；引入自适应特征融合模块改进多模态特征交互和整合。

Result: 在包含多种磁性材料的MagNet数据集上评估，与2023年挑战中的21个代表性模型以及2024-2025年的三种先进方法进行比较，结果表明所提方法实现了改进的建模精度和鲁棒性。

Conclusion: SEPI-TFPNet混合模型通过整合经验模型和深度学习，有效解决了纯数据驱动模型的可解释性和泛化能力限制问题，为高精度磁芯损耗建模提供了有效解决方案。

Abstract: Accurate core loss modeling is critical for the design of high-efficiency power electronic systems. Traditional core loss modeling methods have limitations in prediction accuracy. To advance this field, the IEEE Power Electronics Society launched the MagNet Challenge in 2023, the first international competition focused on data-driven power electronics design methods, aiming to uncover complex loss patterns in magnetic components through a data-driven paradigm. Although purely data-driven models demonstrate strong fitting performance, their interpretability and cross-distribution generalization capabilities remain limited. To address these issues, this paper proposes a hybrid model, SEPI-TFPNet, which integrates empirical models with deep learning. The physical-prior submodule employs a spectral entropy discrimination mechanism to select the most suitable empirical model under different excitation waveforms. The data-driven submodule incorporates convolutional neural networks, multi-head attention mechanisms, and bidirectional long short-term memory networks to extract flux-density time-series features. An adaptive feature fusion module is introduced to improve multimodal feature interaction and integration. Using the MagNet dataset containing various magnetic materials, this paper evaluates the proposed method and compares it with 21 representative models from the 2023 challenge and three advanced methods from 2024-2025. The results show that the proposed method achieves improved modeling accuracy and robustness.

</details>


### [58] [DAPO: Design Structure-Aware Pass Ordering in High-Level Synthesis with Graph Contrastive and Reinforcement Learning](https://arxiv.org/abs/2512.11342)
*Jinming Ge,Linfeng Du,Likith Anaparty,Shangkun Li,Tingyuan Liang,Afzal Ahmad,Vivek Chaturvedi,Sharad Sinha,Zhiyao Xie,Jiang Xu,Wei Zhang*

Main category: cs.LG

TL;DR: DAPO是一个设计结构感知的优化顺序框架，通过提取程序语义、对比学习生成嵌入、硬件指标估计，结合强化学习为特定设计定制优化策略，相比Vitis HLS平均获得2.36倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有HLS工具采用固定的优化策略，这些策略继承自软件编译，限制了其在FPGA加速器设计中的效果。为特定设计定制优化策略需要深度语义理解、准确的硬件指标估计和高级搜索算法，而当前方法缺乏这些能力。

Method: DAPO框架从控制流和数据流图中提取程序语义，使用对比学习生成丰富的嵌入表示，利用分析模型进行准确的硬件指标估计。这些组件共同指导强化学习代理发现设计特定的优化策略。

Result: 在经典HLS设计上的评估表明，DAPO的端到端流程相比Vitis HLS平均实现了2.36倍的加速。

Conclusion: DAPO通过结合程序语义理解、对比学习嵌入、硬件指标估计和强化学习，能够为特定FPGA加速器设计发现有效的优化策略，显著提升了HLS工具的性能。

Abstract: High-Level Synthesis (HLS) tools are widely adopted in FPGA-based domain-specific accelerator design. However, existing tools rely on fixed optimization strategies inherited from software compilations, limiting their effectiveness. Tailoring optimization strategies to specific designs requires deep semantic understanding, accurate hardware metric estimation, and advanced search algorithms -- capabilities that current approaches lack.
  We propose DAPO, a design structure-aware pass ordering framework that extracts program semantics from control and data flow graphs, employs contrastive learning to generate rich embeddings, and leverages an analytical model for accurate hardware metric estimation. These components jointly guide a reinforcement learning agent to discover design-specific optimization strategies. Evaluations on classic HLS designs demonstrate that our end-to-end flow delivers a 2.36 speedup over Vitis HLS on average.

</details>


### [59] [Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits](https://arxiv.org/abs/2512.11345)
*Minwoo Park,Junwoo Chang,Jongeun Choi,Roberto Horowitz*

Main category: cs.LG

TL;DR: 本文提出了一种基于对称性的扩散策略强化学习框架，通过理论证明和实验验证，展示了利用对称性进行策略微调能显著提高样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 虽然等变扩散策略（EDPs）结合了扩散模型的生成能力和几何对称性的泛化优势，但使用标准（非等变）强化学习进行微调时，由于忽略了EDPs设计的对称性，会导致样本效率低下和不稳定。需要开发一种能够利用对称性的强化学习框架来有效微调EDPs。

Method: 1. 理论证明EDPs的扩散过程是等变的，从而诱导出一个群不变潜在噪声MDP；2. 基于此理论，提出了一个原则性的对称感知引导框架；3. 通过在不同对称程度任务上的综合实验，比较标准、等变和近似等变强化学习策略。

Result: 1. 识别了严格等变在对称性破坏下的实际边界；2. 在引导过程中利用对称性带来了显著好处：提高样本效率、防止价值发散、即使从极有限的演示数据训练EDPs也能实现强大的策略改进。

Conclusion: 对称感知的强化学习引导框架能够有效利用EDPs的几何对称性，显著提升策略微调的样本效率和稳定性，为从有限演示数据中学习高性能策略提供了有效途径。

Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.

</details>


### [60] [CAT: Can Trust be Predicted with Context-Awareness in Dynamic Heterogeneous Networks?](https://arxiv.org/abs/2512.11352)
*Jie Wang,Zheng Yan,Jiahe Lan,Xuyan Li,Elisa Bertino*

Main category: cs.LG

TL;DR: CAT是首个支持信任动态性和真实世界异质性的上下文感知GNN信任预测模型，通过连续时间表示、双重注意力机制和元路径上下文特征提取，在三个真实数据集上优于五组基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN信任预测模型存在三个主要局限：1) 无法捕捉信任动态性，导致推理不可靠；2) 很少考虑真实网络的异质性，丢失丰富语义信息；3) 不支持上下文感知这一信任的基本属性，预测结果粗糙。

Method: CAT模型包含图构建层、嵌入层、异质注意力层和预测层。使用连续时间表示处理动态图，通过时间编码函数捕捉时序信息。采用双重注意力机制建模图异质性，识别不同节点类型和类型内节点的重要性。引入元路径概念提取上下文特征，构建上下文嵌入并集成上下文感知聚合器。

Result: 在三个真实世界数据集上的大量实验表明，CAT在信任预测方面优于五组基线方法，同时对大规模图具有良好的可扩展性，对信任导向和GNN导向的攻击表现出强鲁棒性。

Conclusion: CAT是首个支持信任动态性和真实世界异质性的上下文感知GNN信任预测模型，能够同时预测上下文感知信任和整体信任，解决了现有方法的三个主要局限。

Abstract: Trust prediction provides valuable support for decision-making, risk mitigation, and system security enhancement. Recently, Graph Neural Networks (GNNs) have emerged as a promising approach for trust prediction, owing to their ability to learn expressive node representations that capture intricate trust relationships within a network. However, current GNN-based trust prediction models face several limitations: (i) Most of them fail to capture trust dynamicity, leading to questionable inferences. (ii) They rarely consider the heterogeneous nature of real-world networks, resulting in a loss of rich semantics. (iii) None of them support context-awareness, a basic property of trust, making prediction results coarse-grained.
  To this end, we propose CAT, the first Context-Aware GNN-based Trust prediction model that supports trust dynamicity and accurately represents real-world heterogeneity. CAT consists of a graph construction layer, an embedding layer, a heterogeneous attention layer, and a prediction layer. It handles dynamic graphs using continuous-time representations and captures temporal information through a time encoding function. To model graph heterogeneity and leverage semantic information, CAT employs a dual attention mechanism that identifies the importance of different node types and nodes within each type. For context-awareness, we introduce a new notion of meta-paths to extract contextual features. By constructing context embeddings and integrating a context-aware aggregator, CAT can predict both context-aware trust and overall trust. Extensive experiments on three real-world datasets demonstrate that CAT outperforms five groups of baselines in trust prediction, while exhibiting strong scalability to large-scale graphs and robustness against both trust-oriented and GNN-oriented attacks.

</details>


### [61] [Attacking and Securing Community Detection: A Game-Theoretic Framework](https://arxiv.org/abs/2512.11359)
*Yifan Niu,Aochuan Chen,Tingyang Xu,Jia Li*

Main category: cs.LG

TL;DR: 该论文将对抗图攻击概念扩展到社区检测问题，提出了针对社区检测的攻击和防御技术，并建立了一个博弈论框架CD-GAME来模拟攻击者和防御者之间的交互行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究已证明对抗图攻击能使深度图模型在分类任务中失效，但社区检测问题更具挑战性。研究旨在开发社区检测中的攻击和防御技术，应用于保护社交网络中的个人隐私和理解交易网络中的伪装模式等实际场景。

Method: 提出了针对社区检测的新型攻击和防御技术，并建立了博弈论框架CD-GAME。该框架包含两个玩家：图攻击者和Rayleigh Quotient防御者，模拟攻击与防御的交互行为，通过动态策略更新直至达到纳什均衡。

Result: 实验表明，提出的攻击和防御方法均显著优于现有基线方法。CD-GAME框架为理解社区检测中的交互攻击防御场景提供了有价值的见解，发现传统单步攻击中攻击者倾向于使用最有效但易被检测的策略，而在纳什均衡状态下攻击者会采用更隐蔽且仍能保持满意攻击效果的策略。

Conclusion: 成功将对抗图攻击扩展到社区检测领域，提出的攻击防御技术和博弈论框架CD-GAME有效解决了社区检测中的隐私保护和模型鲁棒性问题，为理解攻击防御动态交互过程提供了新视角。

Abstract: It has been demonstrated that adversarial graphs, i.e., graphs with imperceptible perturbations, can cause deep graph models to fail on classification tasks. In this work, we extend the concept of adversarial graphs to the community detection problem, which is more challenging. We propose novel attack and defense techniques for community detection problem, with the objective of hiding targeted individuals from detection models and enhancing the robustness of community detection models, respectively. These techniques have many applications in real-world scenarios, for example, protecting personal privacy in social networks and understanding camouflage patterns in transaction networks. To simulate interactive attack and defense behaviors, we further propose a game-theoretic framework, called CD-GAME. One player is a graph attacker, while the other player is a Rayleigh Quotient defender. The CD-GAME models the mutual influence and feedback mechanisms between the attacker and the defender, revealing the dynamic evolutionary process of the game. Both players dynamically update their strategies until they reach the Nash equilibrium. Extensive experiments demonstrate the effectiveness of our proposed attack and defense methods, and both outperform existing baselines by a significant margin. Furthermore, CD-GAME provides valuable insights for understanding interactive attack and defense scenarios in community detection problems. We found that in traditional single-step attack or defense, attacker tends to employ strategies that are most effective, but are easily detected and countered by defender. When the interactive game reaches a Nash equilibrium, attacker adopts more imperceptible strategies that can still achieve satisfactory attack effectiveness even after defense.

</details>


### [62] [Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization](https://arxiv.org/abs/2512.11391)
*Yifan Niu,Han Xiao,Dongyi Liu,Nuo Chen,Jia Li*

Main category: cs.LG

TL;DR: NSPO是一种新的强化学习框架，通过将安全策略梯度投影到通用任务的零空间，在保持大语言模型核心能力的同时实现安全对齐，显著减少了对齐税。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在现实应用中的部署需要确保其行为符合人类价值观、社会规范和伦理原则。然而，基于强化学习的安全对齐通常会导致模型遗忘已学习的通用能力，即所谓的"对齐税"问题。

Method: 提出了零空间约束策略优化（NSPO），将安全策略梯度几何投影到通用任务的零空间中，从而在保持模型原始核心能力的同时实现有效的安全对齐。

Result: NSPO在实验中大幅优于现有方法，在数学、代码和指令跟随等通用任务上保持准确性的同时，实现了最先进的安全性能。该方法数据效率高，仅需PKU-SafeRLHF中40%的公共安全标注数据即可达到良好的安全性能。

Conclusion: NSPO通过零空间投影技术有效解决了大语言模型安全对齐中的对齐税问题，在保持模型核心能力的同时实现了高效的安全对齐，为实际部署提供了有前景的解决方案。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.

</details>


### [63] [Bhargava Cube--Inspired Quadratic Regularization for Structured Neural Embeddings](https://arxiv.org/abs/2512.11392)
*S Sairam,Prateek P Kulkarni*

Main category: cs.LG

TL;DR: 提出一种结合数论中Bhargava立方体代数约束的神经表示学习方法，将数据映射到满足二次关系的3维潜在空间，提高可解释性和数学一致性


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在非结构化潜在空间中学习表示，缺乏可解释性和数学一致性。需要一种能够将数学结构融入神经网络表示学习的方法

Method: 通过Bhargava立方体的组合结构推导出二次关系，构建3维约束潜在空间，使用与分类目标独立的可微分辅助损失函数，将弱代数先验通过可微分约束融入标准优化

Result: 在MNIST数据集上达到99.46%的准确率，产生可解释的3D嵌入，能够自然地按数字类别聚类并满足学习的二次约束

Conclusion: 这是数论构造首次应用于神经表示学习，为在神经网络中融入结构化数学先验奠定了基础，相比需要显式几何监督的现有流形学习方法更具优势

Abstract: We present a novel approach to neural representation learning that incorporates algebraic constraints inspired by Bhargava cubes from number theory. Traditional deep learning methods learn representations in unstructured latent spaces lacking interpretability and mathematical consistency. Our framework maps input data to constrained 3-dimensional latent spaces where embeddings are regularized to satisfy learned quadratic relationships derived from Bhargava's combinatorial structures. The architecture employs a differentiable auxiliary loss function operating independently of classification objectives, guiding models toward mathematically structured representations. We evaluate on MNIST, achieving 99.46% accuracy while producing interpretable 3D embeddings that naturally cluster by digit class and satisfy learned quadratic constraints. Unlike existing manifold learning approaches requiring explicit geometric supervision, our method imposes weak algebraic priors through differentiable constraints, ensuring compatibility with standard optimization. This represents the first application of number-theoretic constructs to neural representation learning, establishing a foundation for incorporating structured mathematical priors in neural networks.

</details>


### [64] [Sliced ReLU attention: Quasi-linear contextual expressivity via sorting](https://arxiv.org/abs/2512.11411)
*Siwan Boufadène,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 提出切片ReLU注意力机制，通过一维投影和排序实现O(n log n)复杂度，适用于长上下文，保持理论表达能力


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制（如softmax和ReLU-based）在计算复杂度或表达能力方面存在限制，需要一种既能处理长上下文又保持理论表达能力的注意力机制

Method: 使用一维投影处理键-查询差异，通过排序操作获得准线性复杂度，构建可微分的非对称核函数

Result: 切片ReLU注意力具有O(n log n)计算复杂度，保持序列到序列解缠任务的能力，满足上下文通用逼近性质，在小规模实验中显示实用潜力

Conclusion: 切片ReLU注意力是一种新颖的注意力机制，在计算效率和理论表达能力之间取得了良好平衡，为处理长上下文序列提供了有前景的解决方案

Abstract: We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.

</details>


### [65] [Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces](https://arxiv.org/abs/2512.11448)
*Arghya Pratihar,Arnab Seal,Swagatam Das,Inesh Chattopadhyay*

Main category: cs.LG

TL;DR: HypeGBMS将高斯模糊均值漂移扩展到双曲空间，用于处理具有层次结构的数据集，在非欧几里得设置中显著优于传统均值漂移方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯模糊均值漂移(GBMS)在欧几里得空间中有效，但难以处理具有层次或树状结构的数据集。需要一种能够捕捉潜在层次结构的方法。

Method: HypeGBMS将GBMS扩展到双曲空间，用双曲距离替换欧几里得计算，并使用Möbius加权均值确保所有更新与空间几何保持一致。

Result: 在11个真实世界数据集上的实验评估表明，HypeGBMS在非欧几里得设置中显著优于传统均值漂移聚类方法，证明了其鲁棒性和有效性。

Conclusion: HypeGBMS将经典均值漂移聚类与双曲表示学习相结合，为弯曲空间中的基于密度的聚类提供了原则性方法，能有效捕捉层次结构。

Abstract: Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.

</details>


### [66] [Rethinking Expert Trajectory Utilization in LLM Post-training](https://arxiv.org/abs/2512.11470)
*Bowen Ding,Yuhan Chen,Jiayang Lv,Jiyao Yuan,Qi Zhu,Shuangshuang Tian,Dantong Zhu,Futing Wang,Heyuan Deng,Fei Mi,Lifeng Shang,Tao Lin*

Main category: cs.LG

TL;DR: 本文提出了塑性-天花板框架，将后训练性能分解为基础SFT性能和后续RL塑性，确立了SFT-then-RL顺序管道为最优标准，并提供了精确的扩展指南。


<details>
  <summary>Details</summary>
Motivation: 虽然有效的后训练结合了监督微调（SFT）和强化学习（RL），但如何最佳利用专家轨迹的问题仍未解决。需要理论框架来指导这一领域，并建立实用的扩展指南。

Method: 提出塑性-天花板框架，将性能分解为基础SFT性能和后续RL塑性。通过广泛的基准测试，比较不同训练策略，并推导出精确的扩展指南，包括最佳转换时机、数据规模与轨迹难度的作用，以及验证损失作为选择指标。

Result: 1. 确立了SFT-then-RL顺序管道优于同步方法；2. 在SFT稳定或轻度过拟合阶段转换到RL能最大化最终性能；3. 数据规模决定主要后训练潜力，轨迹难度作为性能乘数；4. 最小SFT验证损失是选择专家轨迹的可靠指标。

Conclusion: 塑性-天花板框架为后训练提供了理论基础，SFT-then-RL顺序管道是最优标准，提出的扩展指南为最大化专家轨迹价值提供了可操作的指导原则。

Abstract: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.

</details>


### [67] [xGR: Efficient Generative Recommendation Serving at Scale](https://arxiv.org/abs/2512.11529)
*Qingxiao Sun,Tongxuan Liu,Shen Zhang,Siyu Wu,Peijun Yang,Haotian Liang,Menxin Li,Xiaolong Ma,Zhiwei Liang,Ziyi Ren,Minchao Zhang,Xinyu Liu,Ke Zhang,Depei Qian,Hailong Yang*

Main category: cs.LG

TL;DR: xGR是一个面向生成式推荐系统的服务系统，通过统一处理prefill和decode阶段、早期排序终止和掩码过滤、重构流水线实现多级重叠和多流并行，在严格延迟约束下实现至少3.49倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统使用LLM处理长用户-物品序列，但其工作负载与标准LLM服务不同：处理长提示但生成短固定长度输出，解码阶段计算成本高（beam宽度大），beam搜索涉及大量物品空间导致排序开销大。现有系统难以满足高并发场景下的严格低延迟要求。

Method: 1. 通过分阶段计算和分离的KV缓存统一处理prefill和decode阶段；2. 实现早期排序终止和基于掩码的物品过滤，重用数据结构；3. 重构整体流水线，利用多级重叠和多流并行。

Result: 在真实世界推荐服务数据集上的实验表明，在严格延迟约束下，xGR相比最先进的基线系统实现了至少3.49倍的吞吐量提升。

Conclusion: xGR是一个专门针对生成式推荐系统工作负载特点设计的服务系统，通过创新的架构优化，有效解决了高并发场景下的低延迟挑战，显著提升了系统性能。

Abstract: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.

</details>


### [68] [Parametric Numerical Integration with (Differential) Machine Learning](https://arxiv.org/abs/2512.11530)
*Álvaro Leitao,Jonatan Ráfales*

Main category: cs.LG

TL;DR: 该论文提出了一种基于微分学习的机器学习方法来求解参数积分，相比传统方法在精度、可扩展性和样本效率方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 参数积分在科学计算和工程应用中广泛存在，传统数值方法在处理高维、复杂参数空间时面临效率和精度挑战。需要开发更高效、可扩展的积分求解方法。

Method: 提出微分学习框架，在训练过程中融入导数信息。该方法应用于三类代表性积分问题：统计泛函（包括矩和累积分布函数）、通过切比雪夫展开的函数逼近，以及微分方程直接产生的积分。

Result: 在所有测试案例中，微分学习方法均优于标准架构，实现了更低的均方误差、更好的可扩展性和更高的样本效率。该方法从光滑闭式基准到具有挑战性的数值积分都表现优异。

Conclusion: 微分机器学习方法为参数积分求解提供了有效框架，通过利用导数信息显著提升了学习性能，在多种实际应用中具有广泛潜力。

Abstract: In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.

</details>


### [69] [A Multi-Criteria Automated MLOps Pipeline for Cost-Effective Cloud-Based Classifier Retraining in Response to Data Distribution Shifts](https://arxiv.org/abs/2512.11541)
*Emmanuel K. Katalay,David O. Dimandja,Jordan F. Masakuna*

Main category: cs.LG

TL;DR: 提出一个自动化MLOps管道，用于检测数据分布漂移并自动触发神经网络分类器重训练，提高模型在动态环境中的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在数据分布随时间变化时性能会下降，而传统的MLOps流程通常是人工触发的，需要自动化解决方案来应对动态环境中的数据分布变化。

Method: 设计了一个自动化MLOps管道，采用多标准统计技术检测数据分布漂移，仅在必要时触发模型更新，确保计算效率和资源优化。

Result: 在多个基准异常检测数据集上的实验表明，相比传统重训练策略，该框架显著提高了模型准确性和鲁棒性。

Conclusion: 该工作为在数据分布变化频繁的动态现实环境中部署更可靠、自适应的机器学习系统奠定了基础。

Abstract: The performance of machine learning (ML) models often deteriorates when the underlying data distribution changes over time, a phenomenon known as data distribution drift. When this happens, ML models need to be retrained and redeployed. ML Operations (MLOps) is often manual, i.e., humans trigger the process of model retraining and redeployment. In this work, we present an automated MLOps pipeline designed to address neural network classifier retraining in response to significant data distribution changes. Our MLOps pipeline employs multi-criteria statistical techniques to detect distribution shifts and triggers model updates only when necessary, ensuring computational efficiency and resource optimization. We demonstrate the effectiveness of our framework through experiments on several benchmark anomaly detection data sets, showing significant improvements in model accuracy and robustness compared to traditional retraining strategies. Our work provides a foundation for deploying more reliable and adaptive ML systems in dynamic real-world settings, where data distribution changes are common.

</details>


### [70] [Elastic-Net Multiple Kernel Learning: Combining Multiple Data Sources for Prediction](https://arxiv.org/abs/2512.11547)
*Janaina Mourão-Miranda,Zakria Hussain,Konstantinos Tsirlis,Christophe Phillips,John Shawe-Taylor*

Main category: cs.LG

TL;DR: 本文提出了一种新的弹性网正则化多核学习方法，通过解析更新核权重，在神经影像应用中实现了更好的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 弹性网正则化多核学习（ENMKL）在需要模型可解释性和处理相关核信息时特别有价值，如神经影像分析。但现有的ENMKL方法采用两阶段优化过程，计算复杂且效率不高。

Method: 提出了一种新的ENMKL公式，能够通过简单解析更新核权重。针对支持向量机（SVM）和核岭回归（KRR）推导了显式算法，并在开源神经影像工具箱PRoNTo中实现。

Result: 在三个神经影像应用中评估，ENMKL在所有任务中均匹配或优于l1范数MKL，仅在一个场景中略逊于标准SVM。更重要的是，ENMKL通过选择性加权相关核，产生了更稀疏、更可解释的模型。

Conclusion: 提出的ENMKL方法不仅性能优越，还能产生更可解释的模型，特别适用于神经影像等需要理解模型决策过程的应用领域。

Abstract: Multiple Kernel Learning (MKL) models combine several kernels in supervised and unsupervised settings to integrate multiple data representations or sources, each represented by a different kernel. MKL seeks an optimal linear combination of base kernels that maximizes a generalized performance measure under a regularization constraint. Various norms have been used to regularize the kernel weights, including $l1$, $l2$ and $lp$, as well as the "elastic-net" penalty, which combines $l1$- and $l2$-norm to promote both sparsity and the selection of correlated kernels. This property makes elastic-net regularized MKL (ENMKL) especially valuable when model interpretability is critical and kernels capture correlated information, such as in neuroimaging. Previous ENMKL methods have followed a two-stage procedure: fix kernel weights, train a support vector machine (SVM) with the weighted kernel, and then update the weights via gradient descent, cutting-plane methods, or surrogate functions. Here, we introduce an alternative ENMKL formulation that yields a simple analytical update for the kernel weights. We derive explicit algorithms for both SVM and kernel ridge regression (KRR) under this framework, and implement them in the open-source Pattern Recognition for Neuroimaging Toolbox (PRoNTo). We evaluate these ENMKL algorithms against $l1$-norm MKL and against SVM (or KRR) trained on the unweighted sum of kernels across three neuroimaging applications. Our results show that ENMKL matches or outperforms $l1$-norm MKL in all tasks and only underperforms standard SVM in one scenario. Crucially, ENMKL produces sparser, more interpretable models by selectively weighting correlated kernels.

</details>


### [71] [Fully Inductive Node Representation Learning via Graph View Transformation](https://arxiv.org/abs/2512.11561)
*Dooho Lee,Myeong Kong,Minho Jeong,Jaemin Yoo*

Main category: cs.LG

TL;DR: 提出视图空间作为新的表示轴，通过图视图变换实现完全归纳的节点表示学习，在27个基准测试中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 预训练模型在未见数据集上的泛化能力是基础模型的关键，但图结构数据中特征空间在维度和语义上的巨大差异使得跨数据集的完全归纳推理非常困难

Method: 引入视图空间作为统一表示任意图的新轴，提出图视图变换作为节点和特征置换等变的映射，以此构建循环图视图变换作为完全归纳的节点表示学习模型

Result: 在OGBN-Arxiv上预训练并在27个节点分类基准上评估，循环图视图变换比现有完全归纳图模型GraphAny提升8.93%，比12个单独调优的GNN至少提升3.30%

Conclusion: 视图空间为完全归纳的节点表示学习提供了原理性和有效的基础，证明了该方法在跨数据集泛化方面的优越性

Abstract: Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.

</details>


### [72] [Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model](https://arxiv.org/abs/2512.11582)
*Sam Gijsen,Marc-Andre Schulz,Kerstin Ritter*

Main category: cs.LG

TL;DR: Brain-Semantoks是一个自监督学习框架，专门用于从fMRI时间序列中学习大脑动力学的抽象表征，通过语义分词器和自蒸馏目标提升表征的鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的fMRI基础模型通常在小脑区域上使用掩码-重建目标训练，这种关注低层信息的方法导致表征对噪声和时间波动敏感，需要大量微调才能用于下游任务。

Method: 提出Brain-Semantoks框架，包含两个核心创新：1）语义分词器将噪声区域信号聚合成代表功能网络的鲁棒令牌；2）自蒸馏目标强制表征在时间上的稳定性，并通过新颖的训练课程来稳定目标。

Result: 学习到的表征在各种下游任务中表现出色，即使仅使用线性探针也能实现强大性能。扩展分析表明，更多未标记数据可靠地带来分布外性能提升，无需领域适应。

Conclusion: Brain-Semantoks能够从低信噪比时间序列中稳健地学习有意义的特征，为fMRI基础模型提供了一种有效的自监督学习方法，有望改善疾病和认知相关表型的预测。

Abstract: The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.

</details>


### [73] [Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration](https://arxiv.org/abs/2512.11587)
*Alexander Tyurin*

Main category: cs.LG

TL;DR: 论文通过将神经网络梯度下降简化为广义感知机算法，证明了非线性模型相比线性模型能实现更快的迭代复杂度，解释了神经网络中的隐式加速现象。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络训练的优化动态（包括收敛速度、迭代轨迹、函数值振荡以及隐式加速现象）是一个具有挑战性的问题。即使对于梯度下降方法，其优化动态仍不明确。

Method: 将逻辑损失下的梯度下降步骤简化为广义感知机算法（Rosenblatt, 1958），使用经典线性代数工具分析简化后的算法步骤。通过一个最小化示例，理论证明非线性两层模型相比线性模型能实现更快的迭代复杂度。

Result: 非线性两层模型能实现$\tilde{O}(\sqrt{d})$的迭代复杂度，而线性模型只能达到$Ω(d)$，其中$d$是特征数量。这解释了神经网络中观察到的优化动态和隐式加速现象。理论结果得到了大量数值实验的支持。

Conclusion: 通过将梯度下降简化为广义感知机算法，为理解神经网络优化动态提供了新视角，证明了非线性模型相比线性模型具有更快的收敛速度，有助于解释神经网络中的隐式加速现象，并有望推动神经网络优化研究的进一步发展。

Abstract: Even for the gradient descent (GD) method applied to neural network training, understanding its optimization dynamics, including convergence rate, iterate trajectories, function value oscillations, and especially its implicit acceleration, remains a challenging problem. We analyze nonlinear models with the logistic loss and show that the steps of GD reduce to those of generalized perceptron algorithms (Rosenblatt, 1958), providing a new perspective on the dynamics. This reduction yields significantly simpler algorithmic steps, which we analyze using classical linear algebra tools. Using these tools, we demonstrate on a minimalistic example that the nonlinearity in a two-layer model can provably yield a faster iteration complexity $\tilde{O}(\sqrt{d})$ compared to $Ω(d)$ achieved by linear models, where $d$ is the number of features. This helps explain the optimization dynamics and the implicit acceleration phenomenon observed in neural networks. The theoretical results are supported by extensive numerical experiments. We believe that this alternative view will further advance research on the optimization of neural networks.

</details>


### [74] [A Fast Interpretable Fuzzy Tree Learner](https://arxiv.org/abs/2512.11616)
*Javier Fumanal-Idocin,Raquel Fernandez-Peralta,Javier Andreu-Perez*

Main category: cs.LG

TL;DR: 提出一种将经典树分裂算法从清晰规则扩展到模糊树的方法，结合贪心算法的计算效率和模糊逻辑的可解释性优势，在保持竞争力的预测性能的同时，显著降低了计算成本并提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 模糊规则系统因其可解释的语义规则而被广泛用于可解释决策，但现有模糊规则挖掘算法不能同时保证合理的语义划分和小规则库规模。进化方法计算成本过高，而基于神经的方法（如ANFIS）难以保持语义解释。

Method: 将经典的基于树的分裂算法从清晰规则扩展到模糊树，结合贪心算法的计算效率和模糊逻辑的可解释性优势。该方法能够产生可解释的语义划分，并在保持竞争力的预测性能的同时显著降低运行时间。

Result: 在表格分类基准测试中，该方法达到了与最先进模糊分类器相当的准确率，同时计算成本显著降低，并产生了具有约束复杂度的更可解释的规则库。

Conclusion: 提出的模糊贪心树方法成功地将经典树分裂算法扩展到模糊领域，在计算效率、预测性能和可解释性之间取得了良好平衡，为可解释模糊规则挖掘提供了有效的解决方案。

Abstract: Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public

</details>


### [75] [Bridging Streaming Continual Learning via In-Context Large Tabular Models](https://arxiv.org/abs/2512.11668)
*Afonso Lourenço,João Gama,Eric P. Xing,Goreti Marreiros*

Main category: cs.LG

TL;DR: 论文提出将大上下文表格模型作为流式持续学习的桥梁，通过将无限数据流实时压缩为紧凑摘要，同时满足流学习的数据压缩需求和持续学习的经验回放需求。


<details>
  <summary>Details</summary>
Motivation: 当前持续学习专注于长期记忆和减轻灾难性遗忘，但缺乏实时约束；流学习强调对高频数据流的快速适应，但通常忽略遗忘问题。两者研究社区孤立，缺乏明确的算法重叠，需要一种统一框架来同时处理流式数据的实时适应和长期知识保留。

Method: 提出使用大上下文表格模型作为流式持续学习的桥梁，将无限数据流实时压缩为紧凑摘要供模型使用。基于数据选择的两大核心原则：1）分布匹配（平衡可塑性和稳定性），2）分布压缩（通过多样化和检索机制控制内存大小）。

Result: 论文提出了一个理论框架，将流学习和持续学习统一在流式持续学习范式下，通过大上下文表格模型实现数据流的实时压缩和长期知识保留的平衡。

Conclusion: 大上下文表格模型为流式持续学习提供了自然桥梁，通过将数据流压缩为紧凑摘要，同时满足流学习的数据压缩需求和持续学习的经验回放需求，为解决可塑性-稳定性权衡提供了统一框架。

Abstract: In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.

</details>


### [76] [High-Dimensional Surrogate Modeling for Closed-Loop Learning of Neural-Network-Parameterized Model Predictive Control](https://arxiv.org/abs/2512.11705)
*Sebastian Hirt,Valentinus Suwanto,Hendrik Alsmeier,Maik Pfefferkorn,Rolf Findeisen*

Main category: cs.LG

TL;DR: 贝叶斯神经网络作为代理模型在密集高维控制器参数优化中优于传统高斯过程，能有效处理数百甚至上千维参数空间


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在处理密集高维控制器参数（如模型预测控制器调参）时效果有限，因为标准代理模型难以捕捉高维空间结构

Method: 使用贝叶斯神经网络作为代理模型，比较了Matern核高斯过程、有限宽度贝叶斯神经网络和无限宽度贝叶斯神经网络在cart-pole任务上的表现

Result: 贝叶斯神经网络代理模型能更快更可靠地收敛闭环成本，成功优化数百维参数化；无限宽度贝叶斯神经网络在超过一千参数时仍保持性能，而Matern核高斯过程迅速失效

Conclusion: 贝叶斯神经网络代理模型适用于学习密集高维控制器参数化，为基于学习的控制器设计提供了实用的代理模型选择指导

Abstract: Learning controller parameters from closed-loop data has been shown to improve closed-loop performance. Bayesian optimization, a widely used black-box and sample-efficient learning method, constructs a probabilistic surrogate of the closed-loop performance from few experiments and uses it to select informative controller parameters. However, it typically struggles with dense high-dimensional controller parameterizations, as they may appear, for example, in tuning model predictive controllers, because standard surrogate models fail to capture the structure of such spaces. This work suggests that the use of Bayesian neural networks as surrogate models may help to mitigate this limitation. Through a comparison between Gaussian processes with Matern kernels, finite-width Bayesian neural networks, and infinite-width Bayesian neural networks on a cart-pole task, we find that Bayesian neural network surrogate models achieve faster and more reliable convergence of the closed-loop cost and enable successful optimization of parameterizations with hundreds of dimensions. Infinite-width Bayesian neural networks also maintain performance in settings with more than one thousand parameters, whereas Matern-kernel Gaussian processes rapidly lose effectiveness. These results indicate that Bayesian neural network surrogate models may be suitable for learning dense high-dimensional controller parameterizations and offer practical guidance for selecting surrogate models in learning-based controller design.

</details>


### [77] [SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning](https://arxiv.org/abs/2512.11760)
*Aditya Tripathi,Karan Sharma,Rahul Mishra,Tapas Kumar Maiti*

Main category: cs.LG

TL;DR: SpectralKrum：一种结合谱子空间估计和几何邻居选择的联邦学习防御方法，用于对抗拜占庭客户端攻击，在非IID数据分布下保持鲁棒性


<details>
  <summary>Details</summary>
Motivation: 联邦学习在客户端数据分布异构（非IID）且攻击者能够观察或近似防御机制时，现有鲁棒聚合方法（如Krum、Bulyan等）的有效性会显著降低，需要新的防御策略来应对拜占庭客户端的恶意更新注入

Method: SpectralKrum融合谱子空间估计和几何邻居选择：1）从历史聚合中学习低维流形；2）将传入更新投影到学习到的子空间；3）在压缩坐标中应用Krum选择；4）过滤正交残差能量超过数据驱动阈值的候选更新。该方法无需辅助数据，完全在模型更新上操作，保持联邦学习的隐私特性

Result: 在CIFAR-10数据集上（Dirichlet分布非IID分区，alpha=0.1）对8种鲁棒基线方法和7种攻击场景进行评估，超过56,000轮训练实验表明：SpectralKrum对方向性和子空间感知攻击（adaptive-steer、buffer-drift）具有竞争力，但在标签翻转和最小-最大攻击下优势有限，因为恶意更新在谱域上与良性更新难以区分

Conclusion: SpectralKrum通过谱子空间估计和几何选择相结合，为联邦学习提供了一种有效的拜占庭防御机制，特别是在非IID数据分布下对抗方向性和子空间感知攻击时表现良好，但对于谱域不可区分的攻击类型仍有改进空间

Abstract: Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.
  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.
  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.

</details>


### [78] [The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation](https://arxiv.org/abs/2512.11776)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: AVC是一种混合架构，通过解耦流形学习和函数逼近，使用深度网络学习物理域的微分同胚扭曲，将复杂时空动力学投影到潜在流形上，用广义解析函数基表示解，并用可微线性求解器替代梯度下降输出层，实现谱系数的最优解析。


<details>
  <summary>Details</summary>
Motivation: 解决基于坐标的神经网络在表示连续物理场时面临的两个基本问题：谱偏差（阻碍高频动力学学习）和维度灾难（导致离散特征网格参数爆炸）。

Method: 提出自适应Vekua级联（AVC）混合架构，将深度学习与经典逼近理论结合。使用深度网络学习物理域的微分同胚扭曲，将复杂时空动力学投影到潜在流形，用广义解析函数基表示解。用可微线性求解器替代标准梯度下降输出层，在前向传播中以闭式形式最优解析谱系数。

Result: 在五个严格的物理基准测试中（包括高频Helmholtz波传播、稀疏医学重建和非稳态3D Navier-Stokes湍流），AVC实现了最先进的精度，同时将参数数量减少数个数量级（例如，840个参数 vs. 3D网格的420万个参数），收敛速度比隐式神经表示快2-3倍。

Conclusion: 这项工作为内存高效、谱精度高的科学机器学习建立了新范式，通过解耦流形学习和函数逼近，结合深度学习和经典逼近理论，显著提高了连续物理场表示的效率和精度。

Abstract: Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua.

</details>


### [79] [Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective](https://arxiv.org/abs/2512.11784)
*Etienne Boursier,Claire Boyer*

Main category: cs.LG

TL;DR: 该论文建立了软注意力在有限和无限提示下的统一分析框架，证明了在长提示下软注意力收敛到线性算子，并分析了其训练动态和统计特性。


<details>
  <summary>Details</summary>
Motivation: 软注意力是Transformer架构的核心组件，但其非线性结构给理论分析带来重大挑战。现有研究缺乏对软注意力在有限和无限提示下行为的统一理论框架，特别是在训练动态和统计特性方面的系统性分析。

Method: 开发了基于测度的统一框架，分析单层软注意力在有限和无限提示下的行为。利用软算子在无限提示极限下收敛到线性算子的性质，建立了输出和梯度的非渐近集中界。在上下文线性回归等具体场景中，利用可处理的无限提示动态来分析有限提示长度下的训练。

Result: 证明了软注意力在长提示下快速收敛到其无限提示对应物，且这种集中在整个训练轨迹上保持稳定。在上下文线性回归中，展示了如何利用无限提示动态分析有限提示训练。结果表明，当提示足够长时，线性注意力的优化分析可以直接迁移到软注意力。

Conclusion: 软注意力在长提示下继承了其线性对应物的分析结构，这为研究软注意力层在大提示机制下的训练动态和统计行为提供了原则性且广泛适用的工具包，弥合了非线性软注意力与可分析线性模型之间的理论鸿沟。

Abstract: Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.

</details>


### [80] [A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions](https://arxiv.org/abs/2512.11793)
*Ahmad Shamail,Claire McWhite*

Main category: cs.LG

TL;DR: 提出一种基于几何模式的L形分析方法，通过随机顺序添加元素并绘制贡献图来发现特征间的交互作用、冗余和独立性。


<details>
  <summary>Details</summary>
Motivation: 许多系统存在复杂的组件交互：有些特征相互放大效应，有些提供冗余信息，有些独立贡献。需要一种统一的方法来发现和量化这些交互结构。

Method: 采用随机顺序添加元素的方法，多次试验中绘制元素贡献图，观察L形模式。提出L-score连续度量（-1到+1），通过二维点云可视化配对贡献，分析L形臂的相对比例揭示特征主导性。

Result: 方法能够区分交互作用、独立性和冗余：冗余对形成L形模式（仅先添加元素贡献），协同对形成L形模式（仅元素共同贡献），独立元素显示顺序不变分布。L-score提供连续量化，相对缩放揭示特征主导性。

Conclusion: 该方法提供了一种统一、度量无关的几何方法，适用于任何可以按非重复元素序列增量评估性能的领域，能够自然揭示高阶交互结构。

Abstract: Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.

</details>
