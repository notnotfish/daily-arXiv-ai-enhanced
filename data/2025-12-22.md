<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 67]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Dion2: A Simple Method to Shrink Matrix in Muon](https://arxiv.org/abs/2512.16928)
*Kwangjun Ahn,Noah Amsel,John Langford*

Main category: cs.LG

TL;DR: Dion2是一种简化Muon优化器中正交化步骤的方法，通过采样部分行或列来减少计算和通信成本，提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器虽然具有强大的实证性能和理论基础，但其正交化步骤的超线性成本随着规模增加会带来越来越大的开销。现有方法试图减少进入正交化步骤的矩阵大小，但仍有改进空间。

Method: Dion2提出了一种比先前方法更简单的矩阵缩减方法。该方法在每次迭代中采样一部分行或列，只对这些采样部分进行正交化。这种采样过程使更新变得稀疏，从而减少计算和通信成本。

Result: Dion2通过减少矩阵大小和稀疏化更新，有效降低了Muon优化器的计算和通信开销，从而提高了Muon的可扩展性。

Conclusion: Dion2为Muon优化器提供了一种简单有效的正交化成本降低方法，通过采样策略实现稀疏更新，显著提升了大规模应用中的性能表现。

Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.

</details>


### [2] [BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control](https://arxiv.org/abs/2512.16929)
*Pranesh Sathish Kumar*

Main category: cs.LG

TL;DR: 本文提出了一种低成本的双模式神经肌肉控制系统，结合EEG和EMG信号实现假肢手臂的多自由度实时控制，总成本约240美元，适合资源有限环境。


<details>
  <summary>Details</summary>
Motivation: 传统上肢假肢控制系统成本高昂且缺乏直观控制，限制了截肢者在资源有限环境中的功能性和可及性，需要开发低成本、直观的控制方案。

Method: 使用NeuroSky MindWave Mobile 2采集EEG信号，通过ThinkGear蓝牙传输到ESP32微控制器，采用轻量级分类模型检测眨眼事件控制手部开合；使用MyoWare 2.0传感器采集EMG信号，通过阈值检测实现肘部控制，采用8帧连续检测提高稳定性。

Result: 成功构建了功能原型，EEG控制四个手指舵机，EMG控制两个肘部舵机，实现了实时多自由度控制，总成本约240美元，主要成本来自商用EEG头戴设备。

Conclusion: 该系统展示了低成本、生物直觉假肢控制的可行途径，适合资源有限和全球健康应用，未来可改进包括3D打印外壳、自回归模型降低延迟和提升舵机扭矩。

Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.

</details>


### [3] [QSMOTE-PGM/kPGM: QSMOTE Based PGM and kPGM for Imbalanced Dataset Classification](https://arxiv.org/abs/2512.16960)
*Bikash K. Behera,Giuseppe Sergioli,Robert Giuntini*

Main category: cs.LG

TL;DR: 量子启发机器学习(QiML)利用量子理论数学框架增强经典算法，比较了基于核技巧和量子态判别的两种方法，在合成过采样场景中均优于随机森林基准。


<details>
  <summary>Details</summary>
Motivation: 量子启发机器学习利用量子理论数学框架提升经典算法性能，但需要系统比较核技巧和量子态判别这两种主要方法的理论框架和实际表现。

Method: 提出统一的理论和实证比较框架，分析核化PGM(KPGM)和直接PGM分类器，在量子SMOTE(QSMOTE)变体生成的合成过采样场景中测试性能。

Result: PGM和KPGM分类器均优于经典随机森林基准，特别是使用多个量子副本时。PGM(stereo编码，n_copies=2)获得最高准确率(0.8512)和F1分数(0.8234)，KPGM在不同QSMOTE变体中表现更稳定。

Conclusion: 量子启发分类器在召回率和平衡性能方面有实际提升，PGM受益于编码特定增强，KPGM在不同采样策略中更稳健，为不同数据特征和计算约束下的应用提供实用指导。

Abstract: Quantum-inspired machine learning (QiML) leverages mathematical frameworks from quantum theory to enhance classical algorithms, with particular emphasis on inner product structures in high-dimensional feature spaces. Among the prominent approaches, the Kernel Trick, widely used in support vector machines, provides efficient similarity computation, while the Pretty Good Measurement (PGM), originating from quantum state discrimination, enables classification grounded in Hilbert space geometry. Building on recent developments in kernelized PGM (KPGM) and direct PGM-based classifiers, this work presents a unified theoretical and empirical comparison of these paradigms. We analyze their performance across synthetic oversampling scenarios using Quantum SMOTE (QSMOTE) variants. Experimental results show that both PGM and KPGM classifiers consistently outperform a classical random forest baseline, particularly when multiple quantum copies are employed. Notably, PGM with stereo encoding and n_copies=2 achieves the highest overall accuracy (0.8512) and F1-score (0.8234), while KPGM demonstrates competitive and more stable behavior across QSMOTE variants, with top scores of 0.8511 (stereo) and 0.8483 (amplitude). These findings highlight that quantum-inspired classifiers not only provide tangible gains in recall and balanced performance but also offer complementary strengths: PGM benefits from encoding-specific enhancements, whereas KPGM ensures robustness across sampling strategies. Our results advance the understanding of kernel-based and measurement-based QiML methods, offering practical guidance on their applicability under varying data characteristics and computational constraints.

</details>


### [4] [Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models](https://arxiv.org/abs/2512.16963)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 论文提出"压缩即路由"新架构理念，通过Transformer自编码器实现64倍序列压缩，利用重建误差作为内在分布指纹来自动调度专家模块，解决LLM的上下文长度限制、推理成本高和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型面临三大挑战：上下文长度限制、高推理成本以及持续学习中的灾难性遗忘。虽然混合专家架构缓解了部分冲突，但其路由机制通常依赖显式训练的辅助分类器，增加了系统复杂性且在处理混合域输入时缺乏可解释性。

Method: 基于"压缩即智能"的前提，提出"压缩即路由"新架构理念。训练了一个8700万参数的端到端Transformer自编码器，实现512个token压缩为8个潜在向量的64倍序列长度压缩。利用重建误差作为内在分布指纹，专家模块可以根据重建残差自动调度，无需显式门控网络。

Result: 压缩器展现出极端的领域判别能力：在域内（代码）验证集上达到99.47%的重建准确率；在半分布外域（Wiki文本）上准确率骤降至47.76%；在全分布外域（随机序列）上进一步暴跌至0.57%。这种极端且系统的性能差异验证了重建误差作为内在分布指纹的有效性。

Conclusion: 提出的架构为处理超长上下文提供了"VRAM压缩"新视角，为下一代可扩展模块化神经网络提供了新的研究方向。该机制具有良好的可扩展性，能够自动调度专家模块而无需显式门控网络。

Abstract: Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.
  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: \textbf{``Compression is Routing.''} We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a \textbf{64x sequence length compression} (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of \textbf{99.47\%} on the in-domain (code) validation set; accuracy drops sharply to \textbf{47.76\%} on a semi-out-of-distribution domain (Wiki text); and further plummets to just \textbf{0.57\%} on a fully out-of-distribution domain (random sequences).
  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an \textbf{Intrinsic Distribution Fingerprint}. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.

</details>


### [5] [Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes](https://arxiv.org/abs/2512.16967)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 该研究提出了一种基于XGBoost的轻量级梯度提升框架，仅使用地面观测数据（METAR）进行训练，通过基于热力学原理的物理引导特征工程增强，用于机场低能见度和降水事件的短期预测（临近预报）。


<details>
  <summary>Details</summary>
Motivation: 当前业务预报方法依赖计算密集的数值天气预报和人工发布的TAF产品，存在保守偏差和时间分辨率有限的问题，需要更高效、准确的短期预测方案来保障航空安全和运行效率。

Method: 采用XGBoost梯度提升框架，仅使用METAR地面观测数据，通过基于热力学原理的物理引导特征工程进行增强，在11个代表不同气候区的国际机场（包括SCEL、KJFK、KORD、KDEN、SBGR、VIDP）使用2000-2024年历史数据进行训练和评估。

Result: 在盲对比评估中，该自动化模型在战术预报时次（3小时）相比业务TAF预报显著提高了检测率，召回率提升了2.5到4.0倍，同时减少了误报。SHAP分析显示模型能够隐式重建局地物理驱动因子（平流、辐射、下沉），为业务态势感知提供可解释性。

Conclusion: 该轻量级物理引导机器学习框架能够成功捕捉局地物理过程而无需人工配置，在短期航空天气预报中表现出优于传统业务方法的性能，同时提供可解释的预测结果，适合边缘计算部署。

Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing

</details>


### [6] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: 本文针对多轮任务中强化学习训练LLM代理的局限性，提出了turn-PPO方法，通过转向轮级MDP公式来增强PPO在多轮场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO算法在多轮任务中，特别是需要长时程推理的场景下存在明显局限性，需要更稳定有效的优势估计策略。

Method: 首先探索PPO作为替代方案，发现比GRPO更稳健。然后提出turn-PPO变体，采用轮级MDP公式而非常用的令牌级MDP，以增强PPO在多轮场景中的表现。

Result: 在WebShop和Sokoban数据集上的实验结果表明，turn-PPO无论是否包含长推理组件都表现出有效性。

Conclusion: turn-PPO通过转向轮级MDP公式，为多轮任务中的LLM代理强化学习训练提供了更稳定有效的解决方案。

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [7] [GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning](https://arxiv.org/abs/2512.17034)
*Chang-Hwan Lee,Chanseung Lee*

Main category: cs.LG

TL;DR: GB-DQN使用梯度提升集成方法解决深度强化学习中的非平稳环境问题，通过增量残差学习适应动态变化，相比DQN和基线方法具有更快的恢复速度、更好的稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 非平稳环境对深度强化学习构成根本性挑战，因为动态或奖励的变化会使已学习的价值函数失效并导致灾难性遗忘。现有方法在应对模型漂移方面存在不足。

Method: 提出梯度提升深度Q网络（GB-DQN），这是一种自适应集成方法，通过增量残差学习解决模型漂移问题。该方法构建一个加法集成，其中每个新学习器被训练来近似当前集成在漂移后的贝尔曼残差。

Result: 理论分析表明每个提升步骤都能减少经验贝尔曼残差，集成在标准假设下收敛到漂移后的最优价值函数。实验证明在多种控制任务中，相比DQN和常见非平稳基线方法，GB-DQN具有更快的恢复速度、更好的稳定性和鲁棒性。

Conclusion: GB-DQN通过梯度提升集成方法有效解决了深度强化学习在非平稳环境中的适应性问题，为处理动态变化提供了一种理论保证且实用的解决方案。

Abstract: Non-stationary environments pose a fundamental challenge for deep reinforcement learning, as changes in dynamics or rewards invalidate learned value functions and cause catastrophic forgetting. We propose \emph{Gradient-Boosted Deep Q-Networks (GB-DQN)}, an adaptive ensemble method that addresses model drift through incremental residual learning. Instead of retraining a single Q-network, GB-DQN constructs an additive ensemble in which each new learner is trained to approximate the Bellman residual of the current ensemble after drift. We provide theoretical results showing that each boosting step reduces the empirical Bellman residual and that the ensemble converges to the post-drift optimal value function under standard assumptions. Experiments across a diverse set of control tasks with controlled dynamics changes demonstrate faster recovery, improved stability, and greater robustness compared to DQN and common non-stationary baselines.

</details>


### [8] [SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples](https://arxiv.org/abs/2512.17051)
*Haoye Lu,Yaoliang Yu,Darren Ho*

Main category: cs.LG

TL;DR: 提出SFBD-OMNI框架，利用大量噪声样本和少量干净样本恢复真实分布，通过单边熵最优传输和EM算法解决任意测量模型的分布恢复问题


<details>
  <summary>Details</summary>
Motivation: 现实场景中获取完全观测样本成本高昂或不可行，而部分噪声观测相对容易收集，需要开发能够利用大量噪声样本恢复真实分布的方法

Method: 将分布恢复任务建模为单边熵最优传输问题，采用EM类算法求解；提出可恢复性测试准则；引入SFBD-OMNI框架，将噪声样本分布映射到真实分布，推广SFBD方法以处理任意测量模型

Result: 在基准数据集和多样化测量设置下的实验表明，该方法在定性和定量性能上均有显著提升；少量干净样本即可使原本不可恢复的分布变得可恢复

Conclusion: SFBD-OMNI框架能够有效利用大量噪声样本和少量干净样本恢复真实分布，适用于任意测量模型，为解决现实世界中的分布恢复问题提供了有效工具

Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.

</details>


### [9] [Dynamic Tool Dependency Retrieval for Efficient Function Calling](https://arxiv.org/abs/2512.17052)
*Bhrij Patel,Davide Belli,Amir Jalalirad,Maximilian Arnold,Aleksandr Ermovol,Bence Major*

Main category: cs.LG

TL;DR: 提出DTDR方法，通过动态检索工具依赖关系，提升LLM函数调用代理的性能和准确性


<details>
  <summary>Details</summary>
Motivation: 现有检索方法依赖静态有限输入，无法捕捉多步工具依赖关系和演化任务上下文，导致引入无关工具误导代理，降低效率和准确性

Method: 提出动态工具依赖检索（DTDR），基于初始查询和演化执行上下文进行条件检索，从函数调用演示中建模工具依赖关系，实现自适应检索

Result: DTDR在多个数据集和LLM骨干网络上优于最先进检索方法，函数调用成功率提升23%到104%，同时评估了检索精度、下游任务准确性和计算效率

Conclusion: 动态工具检索能显著提升函数调用代理的性能，通过建模工具依赖关系和适应任务上下文演化，解决了静态检索方法的局限性

Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.

</details>


### [10] [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III](https://arxiv.org/abs/2512.17058)
*Vladimir G. Pestov*

Main category: cs.LG

TL;DR: 本文证明了k最近邻分类器在完备可分度量空间中的弱普适一致性、强Lebesgue-Besicovitch微分性质与Nagata意义下的σ有限维性三者之间的等价关系，填补了最后一个未证明的蕴含关系(1)⇒(3)。


<details>
  <summary>Details</summary>
Motivation: 此前已有研究建立了(2)⇔(3)和(2)⇒(1)的蕴含关系，但(1)⇒(3)这一关键蕴含关系尚未得到证明。本文旨在完成这一证明，从而建立三个条件的完全等价性，同时修正系列文章中先前的一个错误主张。

Method: 通过数学证明方法，在完备可分度量空间的框架下，证明了k最近邻分类器的弱普适一致性蕴含空间的Nagata σ有限维性。这填补了理论链条中的最后一个缺口。

Result: 成功证明了(1)⇒(3)的蕴含关系，从而建立了三个条件的完全等价性：(1)k最近邻分类器的弱普适一致性，(2)强Lebesgue-Besicovitch微分性质，(3)Nagata σ有限维性。同时修正了先前文章中的错误主张。

Conclusion: 本文完成了k最近邻分类器理论中的一个重要理论闭环，建立了三个基本概念之间的完全等价关系，为度量空间上的统计学习理论提供了坚实的理论基础。

Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by Cérou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).

</details>


### [11] [Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation](https://arxiv.org/abs/2512.17073)
*Zhenyu Liu,Yunzhen Liu,Zehao Fan,Garrett Gagnon,Yayue Hou,Nan Wu,Yangwook Kang,Liu Liu*

Main category: cs.LG

TL;DR: 提出BEAM-LRC方法，通过低秩补偿实现带宽高效的MoE推理，在保持精度的同时减少专家传输开销


<details>
  <summary>Details</summary>
Motivation: MoE模型通过稀疏激活扩展容量，但给内存和带宽带来压力。现有的卸载方法通过按需获取专家缓解GPU内存问题，但token级路由导致不规则传输，使推理受限于I/O。静态均匀量化减少流量但会降低精度，特别是在激进压缩时忽略了专家的异质性。

Method: 提出带宽高效的自适应混合专家低秩补偿方法（BEAM-LRC），使用预计算的低秩补偿器进行路由器引导的精度恢复。在推理时，该方法为每个token传输紧凑的低秩因子给Top-n（n<k）专家，并对这些专家应用补偿，而其他专家保持低比特。该方法与GPU和GPU-NDP系统上的卸载集成。

Result: 该方法在带宽-精度权衡方面表现优异，并提高了吞吐量。通过路由器引导的精度恢复和低秩补偿，在减少传输流量的同时保持了模型精度。

Conclusion: BEAM-LRC方法通过结合低秩补偿和路由器引导的精度恢复，有效解决了MoE模型推理中的带宽瓶颈问题，在保持模型精度的同时显著减少了专家传输开销，为大规模MoE模型的部署提供了高效的解决方案。

Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.

</details>


### [12] [Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?](https://arxiv.org/abs/2512.17079)
*Saraswathy Amjith,Mihika Dusad,Neha Muramalla,Shweta Shah*

Main category: cs.LG

TL;DR: 论文研究了通过训练模型识别和纠正推理错误来提升大语言模型数学推理的鲁棒性，发现混合错误训练能提高错误恢复能力而不影响标准问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学推理中使用的思维链提示方法存在脆弱性：早期的算术错误或不合理推理通常会传播到最终答案而无法纠正。研究者希望探索通过训练模型识别和恢复这类错误，来提升数学推理的鲁棒性。

Method: 使用MATH-lighteval竞赛级问题，生成包含单一控制错误的思维链前缀（包括计算错误如符号翻转、遗漏项，或推理错误如规则误用、不合理逻辑步骤）。使用GRPO对Qwen3-4B模型进行微调，采用二元最终答案奖励。比较了混合错误训练与标准强化学习的效果。

Result: 混合错误训练的模型在干净问题上与标准强化学习表现相当（41% vs 41%），但在包含错误推理的问题上显著优于标准强化学习（24% vs 19%）。标准强化学习微调反而降低了鲁棒性（19% vs 20%）。推理错误训练比仅计算错误训练带来更大的鲁棒性提升，混合训练效果最佳。

Conclusion: 研究表明，在训练中暴露于错误推理轨迹可以改善模型的错误恢复行为，而不牺牲准确性，这为构建更鲁棒的大语言模型数学推理能力提供了一条可行路径。

Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.

</details>


### [13] [How to Square Tensor Networks and Circuits Without Squaring Them](https://arxiv.org/abs/2512.17090)
*Lorenzo Loconte,Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: 平方张量网络及其扩展形式平方电路作为分布估计器具有表达能力强且支持闭式边缘化的特点，但平方操作在计算配分函数或边缘化变量时引入了额外复杂度，限制了其在机器学习中的应用。


<details>
  <summary>Details</summary>
Motivation: 平方电路在机器学习应用中面临的主要问题是：平方操作虽然增强了表达能力，但在计算配分函数或进行变量边缘化时引入了显著的复杂度，这阻碍了其在实际机器学习任务中的广泛应用。

Method: 受张量网络规范形式中的正交性思想和电路中确定性可实现可处理最大化的启发，研究者提出了平方电路的参数化方法。通过参数化平方电路来克服边缘化计算开销，即使对于不同于张量网络但编码为电路的因子分解结构，也能实现高效边缘化。

Result: 实验表明，提出的平方电路条件在保持表达能力不损失的同时，实现了更高效的学习。参数化方法解锁了即使在结构上原本会使边缘化计算困难的电路因子分解中的高效边缘化能力。

Conclusion: 通过参数化平方电路，成功解决了平方操作带来的边缘化计算复杂度问题，使得平方电路在保持强大表达能力的同时，能够在机器学习任务中实现更高效的学习和推理。

Abstract: Squared tensor networks (TNs) and their extension as computational graphs--squared circuits--have been used as expressive distribution estimators, yet supporting closed-form marginalization. However, the squaring operation introduces additional complexity when computing the partition function or marginalizing variables, which hinders their applicability in ML. To solve this issue, canonical forms of TNs are parameterized via unitary matrices to simplify the computation of marginals. However, these canonical forms do not apply to circuits, as they can represent factorizations that do not directly map to a known TN. Inspired by the ideas of orthogonality in canonical forms and determinism in circuits enabling tractable maximization, we show how to parameterize squared circuits to overcome their marginalization overhead. Our parameterizations unlock efficient marginalization even in factorizations different from TNs, but encoded as circuits, whose structure would otherwise make marginalization computationally hard. Finally, our experiments on distribution estimation show how our proposed conditions in squared circuits come with no expressiveness loss, while enabling more efficient learning.

</details>


### [14] [Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making](https://arxiv.org/abs/2512.17091)
*Toshiaki Hori,Jonathan DeCastro,Deepak Gopinath,Avinash Balachandran,Guy Rosman*

Main category: cs.LG

TL;DR: 提出融合强化学习与MPC规划的新方法，通过自适应采样机制提升规划性能和数据效率


<details>
  <summary>Details</summary>
Motivation: 解决具有层次结构的规划问题，将强化学习与MPC规划紧密结合，利用两者的优势互补

Method: 使用强化学习动作指导MPPI采样器，自适应聚合MPPI样本来改进价值估计，形成自适应规划过程

Result: 在赛车驾驶、改进的Acrobot和带障碍的Lunar Lander等多个领域表现优异，成功率提升最高72%，收敛速度提升2.1倍

Conclusion: 该方法能够处理复杂规划问题，适应不同应用场景，在数据效率和整体性能方面优于现有方法

Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.

</details>


### [15] [UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data](https://arxiv.org/abs/2512.17100)
*Justin Li,Efe Sencan,Jasper Zheng Duan,Vitus J. Leung,Stephan Tsaur,Ayse K. Coskun*

Main category: cs.LG

TL;DR: UniCoMTE是一个模型无关的框架，用于为多元时间序列分类器生成反事实解释，通过修改输入样本并评估对模型预测的影响来识别关键时间特征，在ECG分类任务中表现出优于现有方法的解释质量。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型（特别是深度神经网络）在复杂时间序列分类中表现出色，但其黑盒性质限制了在高风险领域（如医疗健康）中的信任和采用。需要提高模型的可解释性以增强用户信任。

Method: 提出UniCoMTE框架，这是一个模型无关的反事实解释生成方法。框架通过修改输入样本并评估其对模型预测的影响来识别影响预测的关键时间特征。该方法兼容多种模型架构，可直接处理原始时间序列输入。

Result: 在ECG时间序列分类器上评估UniCoMTE，结果显示：1）与LIME和SHAP等现有方法相比，产生的解释更易理解；2）解释具有良好的泛化性；3）医学专家问卷调查证实了其临床实用性；4）生成简洁、稳定且与人类认知一致的解释。

Conclusion: UniCoMTE框架通过将模型预测与有意义的信号模式联系起来，提高了深度学习模型在现实世界时间序列应用中的可解释性，产生的解释在清晰度和适用性方面优于现有方法。

Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.

</details>


### [16] [Fault Diagnosis and Quantification for Photovoltaic Arrays based on Differentiable Physical Models](https://arxiv.org/abs/2512.17107)
*Zenan Yang,Yuanliang Li,Jingwei Zhang,Yongjie Liu,Kun Ding*

Main category: cs.LG

TL;DR: 提出基于可微分快速故障仿真模型(DFFSM)的光伏串故障量化方法，通过梯度优化实现高效准确的故障参数识别


<details>
  <summary>Details</summary>
Motivation: 现有光伏阵列故障量化方法存在效率和可解释性不足的问题，需要开发更有效的故障诊断技术来支持光伏系统的可靠运行和智能维护

Method: 提出可微分快速故障仿真模型(DFFSM)准确建模多故障下的I-V特性，并提供故障参数的解析梯度；基于此开发使用Adahessian优化器的梯度故障参数识别(GFPI)方法

Result: 在仿真和实测I-V曲线上验证，GFPI方法对不同故障类型（部分阴影、短路、串联电阻退化）均实现高量化精度，I-V重构误差低于3%

Conclusion: 可微分物理模拟器在光伏系统故障诊断中具有可行性和有效性，为光伏阵列的可靠运行和智能维护提供了新方法

Abstract: Accurate fault diagnosis and quantification are essential for the reliable operation and intelligent maintenance of photovoltaic (PV) arrays. However, existing fault quantification methods often suffer from limited efficiency and interpretability. To address these challenges, this paper proposes a novel fault quantification approach for PV strings based on a differentiable fast fault simulation model (DFFSM). The proposed DFFSM accurately models I-V characteristics under multiple faults and provides analytical gradients with respect to fault parameters. Leveraging this property, a gradient-based fault parameters identification (GFPI) method using the Adahessian optimizer is developed to efficiently quantify partial shading, short-circuit, and series-resistance degradation. Experimental results on both simulated and measured I-V curves demonstrate that the proposed GFPI achieves high quantification accuracy across different faults, with the I-V reconstruction error below 3%, confirming the feasibility and effectiveness of the application of differentiable physical simulators for PV system fault diagnosis.

</details>


### [17] [Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse](https://arxiv.org/abs/2512.17108)
*Kunjal Panchal,Saayan Mitra,Somdeb Sarkhel,Haoliang Wang,Ishita Dasgupta,Gang Wu,Hui Guan*

Main category: cs.LG

TL;DR: Atom是一个在设备端系统，通过模块化重用视频语言模型组件，减少冗余加载，实现并行执行，在移动设备上提升27-33%的执行速度，性能损失很小。


<details>
  <summary>Details</summary>
Motivation: 当前视频语言模型在移动设备上执行多阶段流水线时存在效率问题，包括冗余的模型加载和碎片化执行，导致延迟较高。

Method: 将十亿参数模型分解为可重用的模块（如视觉编码器和语言解码器），在字幕生成、推理和索引等子任务中重用这些模块，消除重复模型加载，实现并行执行。

Result: 在商用智能手机上，相比非重用基线，Atom实现了27-33%的更快执行速度，性能损失很小（检索任务Recall@1下降≤2.3，字幕生成CIDEr下降≤1.5）。

Conclusion: Atom为边缘设备上的高效视频语言理解提供了一种实用、可扩展的方法，通过重用设计显著提升了执行效率。

Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.

</details>


### [18] [Bridging Training and Merging Through Momentum-Aware Optimization](https://arxiv.org/abs/2512.17109)
*Alireza Moayedikia,Alicia Troncoso*

Main category: cs.LG

TL;DR: 提出统一框架，在训练过程中维护分解的动量和曲率统计信息，然后重用这些信息进行几何感知的模型组合，避免重复计算并利用优化轨迹数据。


<details>
  <summary>Details</summary>
Motivation: 当前工作流程在训练时计算曲率信息后丢弃，然后在模型合并时重新计算类似信息，造成计算浪费并丢失有价值的轨迹数据。需要统一处理训练和模型合并中的低秩结构利用和参数重要性估计问题。

Method: 引入统一框架，在训练期间维护分解的动量和曲率统计信息，积累任务显著性分数，使曲率感知的模型合并无需后验Fisher计算。该方法具有内存效率，并建立了非凸目标收敛保证。

Result: 在自然语言理解基准测试中，曲率感知参数选择在所有稀疏度水平上都优于仅基于幅度的基线，多任务合并优于强基线。框架表现出秩不变收敛性和超参数鲁棒性。

Conclusion: 通过将优化轨迹视为可重用资产而非丢弃，该方法消除了冗余计算，同时实现了更原则性的模型组合，为训练大型神经网络和合并任务特定模型提供了统一解决方案。

Abstract: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.

</details>


### [19] [Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts](https://arxiv.org/abs/2512.17111)
*Anjali Sarawgi,Esteban Garces Arias,Christof Zotter*

Main category: cs.LG

TL;DR: 首个针对低资源历史语言古尼泊尔语的端到端手写文本识别系统，采用编码器-解码器架构，实现了4.9%的字符错误率


<details>
  <summary>Details</summary>
Motivation: 古尼泊尔语是一种具有历史意义但资源匮乏的语言，缺乏有效的手写文本识别系统，需要开发专门的技术来处理这种低资源历史脚本

Method: 采用行级转录方法，系统探索编码器-解码器架构和数据中心化技术，实现解码策略并分析标记级混淆以理解模型行为

Result: 最佳模型实现了4.9%的字符错误率，虽然评估数据集保密，但发布了训练代码、模型配置和评估脚本以支持进一步研究

Conclusion: 成功开发了首个古尼泊尔语端到手写文本识别系统，为低资源历史脚本的识别提供了可行方案，并开源代码促进该领域研究

Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.

</details>


### [20] [The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining](https://arxiv.org/abs/2512.17121)
*Jasmine Vu,Shivanand Sheshappanavar*

Main category: cs.LG

TL;DR: 该研究评估了CheXagent模型在胸部X光图像检索中处理否定短语的能力，通过微调方法提高了模型对否定语句的理解，同时分析了模型内部行为的变化。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型如CLIP在医学影像任务中应用广泛，但这类模型在处理否定短语时表现不佳，这在医学诊断场景中尤其成问题，可能影响临床应用的可靠性。

Method: 使用Stanford AIMI CheXagent模型评估其在有无否定词的提示下检索胸部X光图像的能力，基于先前工作的方法进行微调改进，并通过token归因、t-SNE投影和注意力头消融等方法分析模型内部行为。

Result: 研究结果显示，通过微调方法提高了CLIP模型处理否定语句的能力，但正例提示的准确性略有下降。同时通过内部行为分析揭示了不同微调方法如何重塑文本编码器对否定临床语言的理解。

Conclusion: 该工作有助于更好地理解CLIP模型的内部行为，通过使用临床相关语言改进其对否定语句的处理能力，从而提高其在医学AI设备中的可靠性。

Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.

</details>


### [21] [DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations](https://arxiv.org/abs/2512.17129)
*Seong Ho Pahng,Guoye Guan,Benjamin Fefferman,Sahand Hormoz*

Main category: cs.LG

TL;DR: DiffeoMorph是一个端到端可微分框架，用于学习形态发生协议，指导一群智能体形成目标3D形状。它使用基于注意力的SE(3)-等变图神经网络，通过3D Zernike多项式形状匹配损失进行训练，能够生成从简单椭球体到复杂形态的各种形状。


<details>
  <summary>Details</summary>
Motivation: 生物系统能够通过相同智能体的集体行为形成复杂的三维结构，这种分布式控制如何产生精确的全局模式不仅是发育生物学的核心问题，也是分布式机器人、可编程物质和多智能体学习领域的重要问题。需要一种能够学习形态发生协议的方法，指导智能体群体形成目标3D形状。

Method: 引入DiffeoMorph框架，每个智能体使用基于注意力的SE(3)-等变图神经网络更新其位置和内部状态。训练时采用基于3D Zernike多项式的新形状匹配损失，将预测和目标形状作为连续空间分布进行比较。为强制完全SO(3)不变性，包含对齐步骤，在计算损失前将预测的Zernike谱最优旋转以匹配目标，形成双层优化问题，内层优化单位四元数以获得最佳对齐，外层更新智能体模型，使用隐式微分计算对齐步骤的梯度。

Result: 通过系统基准测试，确立了形状匹配损失相对于其他标准距离度量在形状比较任务中的优势。演示了DiffeoMorph能够仅使用最小空间线索形成从简单椭球体到复杂形态的各种形状。

Conclusion: DiffeoMorph提供了一个端到端可微分框架，能够学习分布式形态发生协议，使智能体群体自组织形成目标3D形状。该方法在形状匹配损失和等变神经网络设计方面的创新，为解决分布式控制中的形态形成问题提供了有效解决方案。

Abstract: Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.

</details>


### [22] [Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs](https://arxiv.org/abs/2512.17131)
*Aaron Defazio,Konstantin Mishchenko,Parameswaran Raman,Hao-Jun Michael Shi,Lin Xiao*

Main category: cs.LG

TL;DR: GPA是一种改进的优化算法，通过解耦Nesterov原始平均公式中的插值常数，实现每步平滑平均，解决了单工作者DiLoCo和Schedule-Free等平均优化器的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决近期平均优化器（如单工作者DiLoCo和Schedule-Free）的关键限制。单工作者DiLoCo的周期性平均引入双循环结构，增加内存需求和超参数数量；Schedule-Free虽然维护均匀平均但仍有改进空间。

Method: 提出广义原始平均（GPA），通过解耦Nesterov原始平均公式中的插值常数，使算法能够在每一步平滑地平均迭代点，从而推广并改进单工作者DiLoCo方法。

Result: 在Llama-160M模型上，GPA相比基线（AdamW）达到相同验证损失所需步数减少24.22%；在ImageNet ViT任务中，小批量和大批量设置下分别获得12%和27%的加速。同时减少内存开销至单个额外缓冲区。

Conclusion: GPA通过解耦插值常数实现平滑迭代平均，不仅简化了超参数调优、减少内存开销，还能匹配或超越具有O(√T)遗憾界的原始优化器的收敛保证，在理论和实践中均表现出优越性能。

Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.

</details>


### [23] [Distributed Learning in Markovian Restless Bandits over Interference Graphs for Stable Spectrum Sharing](https://arxiv.org/abs/2512.17161)
*Liad Lea Didi,Kobi Cohen*

Main category: cs.LG

TL;DR: SMILE算法：一种通信高效的分布式学习算法，用于在干扰图建模的无线网络中实现频谱共享的全局稳定分配，结合了多臂赌博机学习和图约束协调。


<details>
  <summary>Details</summary>
Motivation: 研究在通信受限的无线网络中，多个认知通信实体（如小区、子网络或认知无线电用户）的频谱接入和共享问题。目标是实现全局稳定且干扰感知的信道分配，特别是在信道作为未知的、随时间变化的马尔可夫过程演化的随机环境中。

Method: 提出SMILE（Stable Multi-matching with Interference-aware LEarning）算法，将多臂赌博机学习与图约束协调相结合。该算法使小区能够分布式地平衡对未知信道的探索和对已学习信息的利用，同时保证通信效率。

Result: 理论证明SMILE收敛到最优稳定分配，并相对于完全了解期望效用的理想情况实现对数遗憾。仿真验证了理论保证，并展示了SMILE在不同频谱共享场景下的鲁棒性、可扩展性和效率。

Conclusion: SMILE是首个在随机、时变的多臂赌博机环境中建立全局Gale-Shapley稳定性的信道分配算法，为干扰图建模的无线网络中的频谱共享提供了有效的分布式学习解决方案。

Abstract: We study distributed learning for spectrum access and sharing among multiple cognitive communication entities, such as cells, subnetworks, or cognitive radio users (collectively referred to as cells), in communication-constrained wireless networks modeled by interference graphs. Our goal is to achieve a globally stable and interference-aware channel allocation. Stability is defined through a generalized Gale-Shapley multi-to-one matching, a well-established solution concept in wireless resource allocation. We consider wireless networks where L cells share S orthogonal channels and cannot simultaneously use the same channel as their neighbors. Each channel evolves as an unknown restless Markov process with cell-dependent rewards, making this the first work to establish global Gale-Shapley stability for channel allocation in a stochastic, temporally varying restless environment. To address this challenge, we develop SMILE (Stable Multi-matching with Interference-aware LEarning), a communication-efficient distributed learning algorithm that integrates restless bandit learning with graph-constrained coordination. SMILE enables cells to distributedly balance exploration of unknown channels with exploitation of learned information. We prove that SMILE converges to the optimal stable allocation and achieves logarithmic regret relative to a genie with full knowledge of expected utilities. Simulations validate the theoretical guarantees and demonstrate SMILE's robustness, scalability, and efficiency across diverse spectrum-sharing scenarios.

</details>


### [24] [BumpNet: A Sparse Neural Network Framework for Learning PDE Solutions](https://arxiv.org/abs/2512.17198)
*Shao-Ting Chiu,Ioannis G. Kevrekidis,Ulisses Braga-Neto*

Main category: cs.LG

TL;DR: BumpNet是一个基于无网格基函数展开的稀疏神经网络框架，用于PDE数值解和算子学习。它使用sigmoid激活函数构建基函数，支持参数全训练，并通过动态剪枝实现模型简约和h-自适应。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够有效结合现代训练技术、实现模型简约和自适应性的PDE求解和算子学习框架。传统RBF网络虽然基于基函数展开，但难以充分利用针对sigmoid等激活函数优化的现代训练技术。

Method: BumpNet使用sigmoid激活函数构建基函数，所有参数（形状、位置、幅度）完全可训练。通过动态剪枝实现模型简约和h-自适应。提出了三种具体实现：Bump-PINNs（结合PINNs求解一般PDE）、Bump-EDNN（空间域用BumpNet，时间域用EDNN求解演化PDE）、Bump-DeepONet（用BumpNet作为DeepONet的trunk网络进行算子学习）。

Result: 广泛的数值实验证明了所提架构的效率和准确性。BumpNet框架能够有效结合现代训练技术，实现模型简约和自适应特性，在PDE求解和算子学习任务中表现出色。

Conclusion: BumpNet是一个通用的稀疏神经网络框架，能够有效解决PDE数值解和算子学习问题。通过使用sigmoid激活函数构建基函数，它能够充分利用现代训练技术，并通过动态剪枝实现模型简约和自适应。提出的三种变体为不同PDE问题提供了有效的解决方案。

Abstract: We introduce BumpNet, a sparse neural network framework for PDE numerical solution and operator learning. BumpNet is based on meshless basis function expansion, in a similar fashion to radial-basis function (RBF) networks. Unlike RBF networks, the basis functions in BumpNet are constructed from ordinary sigmoid activation functions. This enables the efficient use of modern training techniques optimized for such networks. All parameters of the basis functions, including shape, location, and amplitude, are fully trainable. Model parsimony and h-adaptivity are effectively achieved through dynamically pruning basis functions during training. BumpNet is a general framework that can be combined with existing neural architectures for learning PDE solutions: here, we propose Bump-PINNs (BumpNet with physics-informed neural networks) for solving general PDEs; Bump-EDNN (BumpNet with evolutionary deep neural networks) to solve time-evolution PDEs; and Bump-DeepONet (BumpNet with deep operator networks) for PDE operator learning. Bump-PINNs are trained using the same collocation-based approach used by PINNs, Bump-EDNN uses a BumpNet only in the spatial domain and uses EDNNs to advance the solution in time, while Bump-DeepONets employ a BumpNet regression network as the trunk network of a DeepONet. Extensive numerical experiments demonstrate the efficiency and accuracy of the proposed architecture.

</details>


### [25] [Understanding Generalization in Role-Playing Models via Information Theory](https://arxiv.org/abs/2512.17270)
*Yongqi Li,Hao Lang,Fei Huang,Tieyun Qian,Yongbin Li*

Main category: cs.LG

TL;DR: 本文提出了一种信息论度量R-EMID来量化角色扮演模型在分布偏移下的性能退化，并开发了强化学习框架来提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 角色扮演模型在实际应用中广泛使用，但在真实部署时性能下降。这种退化源于分布偏移（用户、角色、对话组合偏移），现有方法无法提供细粒度诊断，缺乏正式框架来表征RPM的泛化行为。

Method: 1. 引入信息论度量R-EMID（推理基础的有效互信息差异）来可解释地测量RPM性能退化；2. 推导R-EMID的上界来预测最坏情况泛化性能；3. 提出协同进化强化学习框架，自适应建模用户、角色和对话上下文之间的连接，提升对话响应生成概率估计。

Result: 评估了各种RPM的泛化性能，发现用户偏移在所有偏移中风险最高，强化学习是增强RPM泛化最有效的方法。

Conclusion: R-EMID为角色扮演模型的泛化性能提供了可解释的度量框架，理论分析揭示了不同分布偏移对性能退化的贡献，强化学习方法能有效提升模型在实际部署中的泛化能力。

Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.

</details>


### [26] [Learning solution operator of dynamical systems with diffusion maps kernel ridge regression](https://arxiv.org/abs/2512.17203)
*Jiwoo Song,Daning Huang,John Harlim*

Main category: cs.LG

TL;DR: DM-KRR方法通过结合扩散映射核与动态感知验证策略，为复杂动力系统的长期预测提供了简单有效的基线方法，在精度和数据效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 许多科学和工程系统表现出复杂的非线性动力学，难以进行长期准确预测。现有数据驱动模型在几何结构未知或表征不佳时性能下降，需要一种能适应系统内在几何结构的方法。

Method: 提出扩散映射核岭回归（DM-KRR）方法，结合基于扩散映射的数据驱动核与动态感知验证策略，无需显式流形重建或吸引子建模，即可适应系统不变集的内在几何结构。

Result: 在包括光滑流形、混沌吸引子和高维时空流等多种系统中，DM-KRR在精度和数据效率上均优于最先进的随机特征、神经网络和算子学习方法。

Conclusion: 长期预测能力不仅取决于模型表达能力，更关键的是通过动态一致的模型选择尊重数据编码的几何约束。DM-KRR的简单性、几何感知能力和强大性能为复杂动力系统的可靠高效学习指明了方向。

Abstract: Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.

</details>


### [27] [M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge](https://arxiv.org/abs/2512.17299)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.LG

TL;DR: M2RU是一种混合信号架构，实现了minion循环单元，用于边缘设备上的高效时序处理和片上持续学习，能效比数字CMOS设计提高29倍。


<details>
  <summary>Details</summary>
Motivation: 边缘平台上的持续学习面临挑战，因为循环网络依赖能耗高的训练过程和频繁的数据移动，这在嵌入式部署中不切实际。

Method: 采用混合信号架构实现minion循环单元，集成加权比特流技术（允许多比特数字输入在交叉阵列中处理而无需高分辨率转换）和经验回放机制（在领域偏移下稳定学习）。

Result: M2RU达到15 GOPS/48.62 mW（312 GOPS/瓦），在顺序MNIST和CIFAR-10任务上保持与软件基线5%以内的准确率，相比数字CMOS设计能效提高29倍，持续学习工作负载下预期运行寿命12.2年。

Conclusion: M2RU为边缘级时序智能中的实时适应提供了一个可扩展且高能效的平台。

Abstract: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.

</details>


### [28] [Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods](https://arxiv.org/abs/2512.17257)
*Iason Kyriakopoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 该研究系统比较了五种时间序列预测模型在不同时间尺度（分钟、小时、天）和空间聚合水平（单个充电站到城市级）对电动汽车充电需求的预测效果，使用了四个真实世界数据集。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及以应对气候变化，其对电网管理的影响引发关注，预测充电需求成为重要研究问题。现有研究较少系统比较不同预测方法在多样化城市环境中的表现。

Method: 研究调查了五种时间序列预测模型，包括传统统计方法、机器学习和深度学习方法。预测性能在短、中、长期时间尺度（分钟、小时、天）以及从单个充电站到区域和城市级别的空间聚合水平上进行评估。

Result: 分析基于四个公开可用的真实世界数据集进行，结果针对每个数据集独立报告。这是首个使用多个真实数据集在如此广泛的时间尺度和空间聚合水平上系统评估EV充电需求预测的研究。

Conclusion: 该研究填补了EV充电需求预测方法系统比较的空白，为不同应用场景下的模型选择提供了实证依据，有助于优化电网管理和充电基础设施规划。

Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.

</details>


### [29] [Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability](https://arxiv.org/abs/2512.17316)
*Michael Merry,Pat Riddle,Jim Warren*

Main category: cs.LG

TL;DR: 该论文提出了一个基于图论的固有可解释性标准，用于区分可解释模型和已解释模型，并以临床心血管风险预测模型PREDICT为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能领域缺乏一致的固有可解释性定义和测试标准，现有方法要么依赖度量指标，要么基于直觉判断。需要建立一个全局适用的标准来形式化可解释性研究。

Method: 使用图论表示和分解模型结构，形成结构局部解释作为可验证的假设-证据结构注释，然后重新组合成全局解释。提出了区分"可解释"（允许解释）和"已解释"（具有已验证解释）的概念。

Result: 提出的标准与现有对固有可解释性的直觉相符，能够解释为什么大型回归模型可能不可解释而稀疏神经网络可能可解释。成功为新西兰临床使用的心血管疾病风险预测模型PREDICT提供了完整解释，证明其具有固有可解释性。

Conclusion: 该工作为形式化可解释性研究提供了结构化框架，为监管机构提供了灵活而严格的合规测试标准，有助于推动可解释人工智能的实际应用和监管合规。

Abstract: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - "we know it when we see it". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.

</details>


### [30] [SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS](https://arxiv.org/abs/2512.17262)
*Suraj Kumar,Arvind Kumar,Soumi Chattopadhyay*

Main category: cs.LG

TL;DR: SHARP-QoS：一种统一的联合QoS预测策略，通过双机制提取层次特征、自适应特征共享机制和EMA损失平衡策略，解决现有方法中的负迁移和表示学习不足问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的QoS数据极其稀疏、噪声大且具有层次依赖关系，现有方法通常单独预测每个QoS参数，需要多个相似模型，计算成本高且泛化能力差。最近的联合QoS预测研究虽然探索了共享架构，但由于QoS参数间数值范围不一致导致的损失缩放问题，以及表示学习不足，导致负迁移和准确性下降。

Method: SHARP-QoS包含三个核心组件：1）通过双机制在Poincaré球中利用双曲卷积提取QoS和上下文结构的层次特征；2）自适应特征共享机制，允许信息丰富的QoS和上下文信号之间的特征交换，采用门控特征融合模块支持结构和共享表示之间的动态特征选择；3）基于EMA的损失平衡策略，实现稳定的联合优化，减轻负迁移。

Result: 在包含两个、三个和四个QoS参数的三个数据集上的评估表明，SHARP-QoS在单任务和多任务基线方法中表现最优。广泛研究表明，该模型有效解决了稀疏性、对异常值的鲁棒性和冷启动等主要挑战，同时保持适度的计算开销。

Conclusion: SHARP-QoS通过统一的联合QoS预测策略，成功解决了现有方法中的负迁移和表示学习不足问题，展示了可靠联合QoS预测的能力，在保持计算效率的同时显著提升了预测准确性。

Abstract: Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.

</details>


### [31] [Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs](https://arxiv.org/abs/2512.17352)
*Ivan Kralj,Lodovico Giaretta,Gordan Ježić,Ivana Podnar Žarko,Šarūnas Girdzijauskas*

Main category: cs.LG

TL;DR: 提出自适应剪枝算法减少ST-GNN在边缘计算中的通信开销，同时引入SEPA新指标评估交通事件预测能力


<details>
  <summary>Details</summary>
Motivation: ST-GNN在智能交通系统中处理分布式传感器数据时，相邻边缘节点间重复传输重叠节点特征导致通信开销过大

Method: 提出自适应剪枝算法动态过滤冗余邻居特征，基于近期模型性能调整剪枝率；引入SEPA指标专门评估交通减速和恢复事件的预测能力

Result: 在PeMS-BAY和PeMSD7-M数据集上验证，自适应剪枝在保持预测精度的同时显著降低通信成本，SEPA指标能更好揭示空间连接性在动态交通预测中的价值

Conclusion: 自适应剪枝算法可在不牺牲关键交通事件响应能力的前提下有效减少通信开销，SEPA指标为交通事件预测提供了更准确的评估标准

Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.

</details>


### [32] [A Theoretical Analysis of State Similarity Between Markov Decision Processes](https://arxiv.org/abs/2512.17265)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了广义双模拟度量（GBSM），用于测量任意马尔可夫决策过程（MDP）对之间的状态相似性，解决了传统双模拟度量在多MDP场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 双模拟度量（BSM）在分析单个MDP内的状态相似性方面很有效，但在多个MDP之间的状态相似性测量方面存在挑战。先前工作尝试将BSM扩展到MDP对，但由于缺乏完善的数学性质，限制了MDP间进一步的理论分析。

Method: 本文正式建立了广义双模拟度量（GBSM），用于测量任意MDP对之间的状态相似性。该方法严格证明了三个基本度量性质：GBSM对称性、MDP间三角不等式和相同空间上的距离界限。

Result: 利用GBSM的性质，作者在MDP间进行了策略迁移、状态聚合和基于采样的估计的理论分析，获得了比标准BSM推导出的现有界限更严格的显式界限。GBSM还提供了估计的闭式样本复杂度，改进了基于BSM的现有渐近结果。

Conclusion: 数值结果验证了理论发现，并证明了GBSM在多MDP场景中的有效性。GBSM为跨MDP的状态相似性分析提供了坚实的理论基础和实用工具。

Abstract: The bisimulation metric (BSM) is a powerful tool for analyzing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to state similarity between multiple MDPs remains challenging. Prior work has attempted to extend BSM to pairs of MDPs, but a lack of well-established mathematical properties has limited further theoretical analysis between MDPs. In this work, we formally establish a generalized bisimulation metric (GBSM) for measuring state similarity between arbitrary pairs of MDPs, which is rigorously proven with three fundamental metric properties, i.e., GBSM symmetry, inter-MDP triangle inequality, and a distance bound on identical spaces. Leveraging these properties, we theoretically analyze policy transfer, state aggregation, and sampling-based estimation across MDPs, obtaining explicit bounds that are strictly tighter than existing ones derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.

</details>


### [33] [Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.17444)
*Javier Gonzalez-Ruiz,Carlos Rodriguez-Pardo,Iacopo Savelli,Alice Di Bella,Massimo Tavoni*

Main category: cs.LG

TL;DR: 本文提出了一个多智能体强化学习模型，用于模拟电力市场中发电公司的投资决策，以支持长期电力市场机制的设计和评估，应用于意大利电力系统的简化版本。


<details>
  <summary>Details</summary>
Motivation: 电力系统对实现碳中和经济转型至关重要，但缺乏先进工具来支持政策制定者设计和评估长期电力市场机制，特别是考虑到多种政策和市场机制的相互作用。

Method: 采用多智能体强化学习模型，发电公司作为利润最大化主体在批发电力市场中做出投资决策。使用独立的近端策略优化算法，并通过广泛的超参数搜索确保分散训练产生符合竞争行为的结果。

Result: 模型应用于意大利电力系统的简化版本，在不同竞争水平、市场设计和政策情景下进行测试。结果显示市场设计对电力部门脱碳和避免价格波动具有关键作用。

Conclusion: 提出的框架能够评估长期电力市场，其中多种政策和市场机制同时相互作用，市场参与者能够响应和适应脱碳路径，为政策制定者提供了有价值的分析工具。

Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.

</details>


### [34] [MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics](https://arxiv.org/abs/2512.17273)
*Farinaz Mostajeran,Aruzhan Tleubek,Salah A Faroughi*

Main category: cs.LG

TL;DR: MINPO是一个统一的神经伪算子框架，用于建模由长程空间相互作用和/或长期时间记忆引起的非局部动力学，通过学习非局部算子及其逆的神经表示来直接重构未知解场。


<details>
  <summary>Details</summary>
Motivation: 许多物理系统表现出由积分微分方程描述的非局部时空行为。经典方法需要重复评估卷积积分，其成本随核复杂性和维度快速增加。现有神经求解器可以加速特定计算，但不能泛化到不同的非局部结构。

Method: MINPO使用KANs或MLPs作为编码器，学习非局部算子及其逆的神经表示，然后显式重构未知解场。通过轻量级的非局部一致性损失项来确保学习算子与重构解之间的一致性。

Result: 与经典技术和基于MLPs的先进神经策略（如A-PINN、fPINN及其KAN变体A-PIKAN、fPIKAN）相比，MINPO在准确性方面表现出色，并展示了在处理不同核类型、不同核维度和重复核积分评估的计算需求方面的鲁棒性。

Conclusion: MINPO超越了问题特定的公式，为受非局部算子控制的系统提供了一个统一的框架，能够自然捕获并有效解决由广泛IDE谱及其子集（包括分数PDE）控制的非局部时空依赖关系。

Abstract: Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.

</details>


### [35] [Learning What to Write: Write-Gated KV for Efficient Long-Context Inference](https://arxiv.org/abs/2512.17452)
*Yen-Chieh Huang,Rui Fang,Ming-Syan Chen,Pi-Cheng Hsiu*

Main category: cs.LG

TL;DR: WG-KV通过写门控机制预测token效用，选择性写入KV缓存，减少内存使用46-57%，提升推理速度3倍以上


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理面临二次注意力复杂度和线性KV缓存增长的瓶颈，现有方法通过后处理选择或淘汰忽略了根本低效问题：对持久内存的无差别写入

Method: 将KV缓存管理形式化为因果系统三要素：KV准入、选择和淘汰，通过Write-Gated KV轻量机制预测token效用，在进入缓存前过滤低效用状态，维护紧凑全局缓存和滑动本地缓存

Result: 内存使用减少46-57%，Llama模型上预填充速度提升3.03-3.45倍，解码速度提升1.89-2.56倍，精度损失可忽略，兼容FlashAttention和分页KV系统

Conclusion: 学习"写什么"是高效长上下文推理的原则性和实用方法，Write-Gated KV通过早期过滤低效用状态有效解决了KV缓存管理问题

Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

</details>


### [36] [Alzheimer's Disease Brain Network Mining](https://arxiv.org/abs/2512.17276)
*Alireza Moayedikia,Sara Fin*

Main category: cs.LG

TL;DR: MATCH-AD是一个半监督学习框架，通过结合深度表示学习、图标签传播和最优传输理论，在仅有三分之一标注数据的情况下实现接近完美的阿尔茨海默病诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病诊断面临标注数据稀缺的挑战，临床评估昂贵且侵入性强，大多数神经影像数据缺乏真实标签。需要开发能够在有限标注数据下有效工作的诊断方法。

Method: 提出MATCH-AD框架，整合深度表示学习、基于图的标签传播和最优传输理论。利用神经影像数据的流形结构将诊断信息从有限标注样本传播到大量未标注样本，使用Wasserstein距离量化认知状态间的疾病进展。

Result: 在NACC近5000名受试者数据集上评估，涵盖结构MRI、脑脊液生物标志物和临床变量。尽管只有不到三分之一的样本有真实标签，MATCH-AD实现了接近完美的诊断准确率，Kappa系数显示几乎完美的一致性，显著优于所有基线方法。

Conclusion: 有原则的半监督学习可以释放全球积累的部分标注神经影像数据的诊断潜力，显著减少标注负担，同时保持适合临床部署的准确性。即使在严重标签稀缺情况下，性能仍保持临床实用性。

Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.

</details>


### [37] [A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting](https://arxiv.org/abs/2512.17453)
*Henok Tenaw Moges,Deshendran Moodley*

Main category: cs.LG

TL;DR: Lite-STGNN是一个轻量级的时空图神经网络，用于长期多元预测，结合了分解式时间建模和可学习的稀疏图结构，在保持参数高效的同时实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 针对长期多元时间序列预测问题，需要开发一个既准确又高效的模型。现有方法如基于Transformer的方法通常参数多、训练慢，需要更轻量、可解释且高效的框架。

Method: 1. 时间模块：采用趋势-季节性分解进行时间建模；2. 空间模块：使用低秩Top-K邻接学习和保守的逐层门控进行消息传递，增强线性基线的空间校正能力。

Result: 在四个基准数据集上，对于长达720步的预测范围，Lite-STGNN达到了最先进的准确率。模型参数高效，训练速度显著快于基于Transformer的方法。消融研究表明：空间模块比时间基线提升4.6%，Top-K增强局部性3.3%，学习的邻接矩阵揭示了领域特定的交互动态。

Conclusion: Lite-STGNN为长期多元时间序列预测提供了一个紧凑、可解释且高效的框架，在保持高性能的同时显著降低了计算复杂度。

Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.

</details>


### [38] [SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals](https://arxiv.org/abs/2512.17527)
*Muhammad Haris Khan*

Main category: cs.LG

TL;DR: SafeBench-Seq是一个用于蛋白质序列危害筛查的基准测试和基线分类器，使用公开数据构建，通过同源性聚类评估确保训练测试集无重叠，提供校准概率和多种性能指标。


<details>
  <summary>Details</summary>
Motivation: 蛋白质设计的基础模型存在生物安全风险，但社区缺乏简单、可复现的序列级危害筛查基线方法，需要在同源性控制下进行评估，并能在普通CPU上运行。

Method: 使用公开数据（SafeProtein危害数据和UniProt良性数据）构建基准测试，采用可解释特征（全局理化描述符和氨基酸组成）。通过≤40%同源性的聚类进行聚类级留出验证，使用校准分类器（逻辑回归/随机森林用等渗校准，线性SVM用Platt sigmoid）提供校准概率。

Result: 随机分割会显著高估模型鲁棒性，而同源性聚类评估更真实；校准线性模型表现出较好的校准性，而树集成模型保留稍高的Brier分数和ECE。SafeBench-Seq仅发布元数据（登录号、聚类ID、分割标签），不传播危害序列。

Conclusion: SafeBench-Seq提供了一个CPU专用、可复现的蛋白质序列危害筛查基准测试，通过同源性控制评估确保模型泛化能力，为生物安全风险评估提供了实用工具。

Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.

</details>


### [39] [Task Schema and Binding: A Double Dissociation Study of In-Context Learning](https://arxiv.org/abs/2512.17325)
*Chaeha Kim*

Main category: cs.LG

TL;DR: 论文通过因果机制验证发现上下文学习可分解为任务图式和绑定两个独立机制，前者负责抽象任务类型识别，后者处理具体输入输出关联。实验证明这两个机制在神经层面可分离，且任务图式依赖度与先验知识呈负相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将上下文学习视为单一机制（检索式、梯度下降式或纯贝叶斯式），缺乏对内部机制的深入理解。本研究旨在通过因果机制验证，揭示上下文学习是否由可分离的神经机制组成，从而为双过程理论提供证据。

Method: 采用激活修补实验方法，在9个模型（来自7个Transformer家族和Mamba，参数规模370M-13B）上进行测试。通过晚期MLP修补实现任务图式100%转移，通过残差流修补实现绑定62%转移，证明机制可分离性。分析28个任务-模型对，计算任务图式依赖度与先验知识的相关系数。

Result: 1. 发现双重分离：任务图式通过晚期MLP修补100%转移，绑定通过残差流修补62%转移，证明两个机制在神经层面可分离
2. 先验-图式权衡：任务图式依赖度与先验知识呈显著负相关（Spearman rho = -0.596, p < 0.001）
3. 架构普适性：该机制在所有测试架构中都存在，包括非Transformer的Mamba模型

Conclusion: 上下文学习由可分离的任务图式和绑定机制组成，为双过程理论提供了因果证据。当缺乏先验知识时模型依赖任务图式，而先验知识通过注意力误路由（72.7%近因偏差）而非直接输出竞争（0%）产生干扰。这一发现解释了为什么任意映射能成功而事实覆盖会失败，并揭示真正的瓶颈在注意力层面而非输出层面。实践意义：理解这些双重机制可实现更高效的提示工程，提高上下文学习系统在生产部署中的可靠性。

Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.

</details>


### [40] [GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping](https://arxiv.org/abs/2512.17570)
*Yikang Yue,Yishu Yin,Xuehai Qian*

Main category: cs.LG

TL;DR: GreedySnake是一种新的SSD卸载训练系统，采用垂直调度策略，相比现有水平调度系统，在小批量训练时获得更高吞吐量，更接近理想性能模型。


<details>
  <summary>Details</summary>
Motivation: SSD卸载训练为降低LLM训练成本提供了实用且有前景的方法。现有系统采用水平调度（顺序执行微批次），存在性能瓶颈，需要更高效的调度策略来提升训练吞吐量。

Method: 提出GreedySnake系统，采用垂直调度策略：在执行下一层之前，先执行完当前层的所有微批次。同时通过将部分优化步骤与下一次迭代的前向传播重叠，进一步缓解I/O瓶颈。

Result: 在A100 GPU上的实验结果显示，相比ZeRO-Infinity，GreedySnake在GPT-65B模型上：1GPU时吞吐量提升1.96倍，4GPU时提升1.93倍；在GPT-175B模型上：1GPU时吞吐量提升2.53倍。

Conclusion: GreedySnake通过垂直调度策略显著提升了SSD卸载训练的吞吐量，特别是在小批量训练场景下，使系统性能更接近理想模型预测的上限。

Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

</details>


### [41] [More Consistent Accuracy PINN via Alternating Easy-Hard Training](https://arxiv.org/abs/2512.17607)
*Zhaoqian Gao,Min Yanga*

Main category: cs.LG

TL;DR: 提出了一种结合硬优先级和易优先级的混合训练策略，通过交替训练算法提升PINNs在求解PDE时的性能和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有PINNs训练策略存在局限性：硬优先级方法（受有限元方法启发）和易优先级方法各有优缺点，在不同类型PDE上表现不一致，存在明显的权衡取舍

Method: 开发了一种混合策略，结合硬优先级和易优先级的优势，采用交替训练算法。该方法在具有陡峭梯度、非线性和高维度的PDE上特别有效

Result: 在具有挑战性的PDE问题上实现了高精度，相对L2误差主要在O(10^-5)到O(10^-6)范围内，显著超越基线方法。在不同问题上表现出更好的可靠性

Conclusion: 该研究为设计混合训练策略提供了新见解，能够增强PINNs的性能和鲁棒性，解决现有方法在不同PDE类型上表现不一致的问题

Abstract: Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.

</details>


### [42] [Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach](https://arxiv.org/abs/2512.17367)
*Yidong Chai,Yi Liu,Mohammadreza Ebrahimi,Weifeng Li,Balaji Padmanabhan*

Main category: cs.LG

TL;DR: 本文提出LLM-SGA框架和ARHOCD检测器，通过识别文本对抗攻击的关键不变性来增强检测器的泛化能力，同时采用集成学习、动态权重分配和对抗训练策略来提高检测准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台存在大量有害内容（如仇恨言论、错误信息和极端主义言论），机器学习检测模型容易受到对抗攻击，恶意用户通过细微修改文本即可逃避检测。现有研究在同时实现对抗鲁棒性和检测准确性方面存在挑战。

Method: 1. 提出LLM-SGA框架，识别文本对抗攻击的关键不变性以确保检测器泛化能力；2. 实例化ARHOCD检测器，包含三个创新设计：多基检测器集成、基于样本可预测性和检测器能力的动态权重分配（贝叶斯推理更新）、迭代优化基检测器和权重分配器的对抗训练策略。

Result: 在仇恨言论、谣言和极端主义内容三个数据集上的实证评估表明，ARHOCD在对抗条件下表现出强大的泛化能力，并提高了检测准确率。

Conclusion: ARHOCD通过结合对抗攻击不变性识别、集成学习和动态优化策略，有效解决了现有对抗鲁棒性增强研究的局限性，在保持高检测准确率的同时增强了对抗攻击的防御能力。

Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

</details>


### [43] [SCOPE: Sequential Causal Optimization of Process Interventions](https://arxiv.org/abs/2512.17629)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: SCOPE是一种新的规范性过程监控方法，通过反向归纳和因果学习来优化连续干预序列，直接利用观测数据，避免了传统强化学习方法需要过程模拟和数据增强的问题。


<details>
  <summary>Details</summary>
Motivation: 现有规范性过程监控方法在处理连续干预时存在局限：要么只关注单次干预决策，要么将多次干预视为独立事件，忽略了干预之间的时间交互作用。而考虑这些依赖关系的方法又依赖于过程模拟或数据增强来训练强化学习代理，这会造成现实差距并引入偏差。

Method: SCOPE采用反向归纳法来估计每个候选干预行动的效果，将其影响从最终决策点向后传播到第一个决策点。通过利用因果学习器，该方法可以直接使用观测数据，而不需要像强化学习方法那样构建过程近似模型。

Result: 在现有合成数据集和新的半合成数据集上的实验表明，SCOPE在优化关键绩效指标方面始终优于最先进的规范性过程监控技术。基于真实事件日志构建的新半合成设置可作为未来顺序规范性过程监控研究的可重用基准。

Conclusion: SCOPE提供了一种有效的规范性过程监控方法，能够学习对齐的连续干预推荐，直接利用观测数据，避免了传统方法中的现实差距和偏差问题，为顺序干预优化提供了新的解决方案。

Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.

</details>


### [44] [AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens](https://arxiv.org/abs/2512.17375)
*Tung-Ling Li,Yuhao Wu,Hongliang Liu*

Main category: cs.LG

TL;DR: 论文发现奖励模型和LLM-as-a-Judge系统存在漏洞：低困惑度的控制令牌序列可以翻转二元评估结果，从正确的"No"变为错误的"Yes"，这代表了实际的风险而非最坏情况攻击。


<details>
  <summary>Details</summary>
Motivation: 奖励模型和LLM-as-a-Judge系统是现代后训练流程（如RLHF、DPO、RLAIF）的核心，它们提供标量反馈和二元决策来指导模型选择和RL微调。然而这些评判系统存在反复出现的脆弱性，可能被恶意利用。

Method: 提出AdvJudge-Zero方法，利用模型的下一令牌分布和波束搜索探索从头发现多样化的控制令牌序列。分析显示这些诱导的隐藏状态扰动集中在低秩"软模式"中，与评判者的拒绝方向反对齐。

Result: 实验表明，这些控制令牌在大型开放权重和专业评判模型对数学和推理基准的错误答案评分时，会导致极高的假阳性率。当评判模型评估错误答案时，控制令牌可以显著增加"Yes"判断的概率。

Conclusion: 基于LoRA的对抗训练可以在少量控制令牌增强的示例上显著减少这些假阳性，同时保持评估质量。这揭示了实际奖励黑客风险，并提供了缓解方法。

Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.

</details>


### [45] [Trust-Region Adaptive Policy Optimization](https://arxiv.org/abs/2512.17636)
*Mingyu Su,Jian Guan,Yuxian Gu,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: TRAPO提出了一种混合训练框架，将SFT和RL交错在每个训练实例中，通过信任区域SFT稳定训练，在数学推理基准上超越了传统两阶段方法和现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段训练流程（先SFT后RL）存在不一致性：SFT的刚性模仿会抑制探索并导致遗忘，限制了RL的改进潜力。需要解决这种低效问题。

Method: TRAPO框架在每个训练实例中交错进行SFT和RL：在专家前缀上优化SFT损失，在模型自身补全上优化RL损失。引入信任区域SFT（TrSFT）最小化前向KL散度，在信任区域外衰减优化，有效转向反向KL，实现稳定、模式寻求的更新。自适应前缀选择机制基于测量效用分配专家指导。

Result: 在五个数学推理基准测试中，TRAPO一致超越了标准SFT、RL、SFT-then-RL流程以及最近的最先进方法。

Conclusion: TRAPO为推理增强型大语言模型建立了一个强大的新范式，统一了外部监督和自我探索，解决了传统两阶段训练的不一致问题。

Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

</details>


### [46] [DeepShare: Sharing ReLU Across Channels and Layers for Efficient Private Inference](https://arxiv.org/abs/2512.17398)
*Yonathan Bornfeld,Shai Avidan*

Main category: cs.LG

TL;DR: 该论文提出了一种新的隐私推理激活模块，通过原型通道机制大幅减少DReLU操作数量，在保持模型性能的同时显著提升隐私推理效率。


<details>
  <summary>Details</summary>
Motivation: 隐私推理中的主要计算瓶颈是ReLU门操作，现有研究致力于减少网络中的ReLU数量。作者发现DReLU（ReLU的非线性阶跃函数）可以服务多个ReLU操作，因此探索如何更高效地利用DReLU来提升隐私推理效率。

Method: 提出新的激活模块：仅对部分通道（原型通道）执行DReLU操作，其余通道（复制通道）从对应的原型通道神经元复制DReLU结果。将此思想扩展到不同层之间，实现跨层DReLU共享。

Result: 1. 在ResNet类型网络中大幅减少DReLU操作数量；2. 理论分析表明新方法能用单个非线性和两个神经元解决扩展版XOR问题，而传统方法和某些隐私推理特定方法无法实现；3. 在多个分类任务上取得新的SOTA结果；4. 在图像分割任务上达到SOTA水平。

Conclusion: 提出的原型通道激活模块能显著减少隐私推理中的DReLU操作，提高计算效率，同时在分类和分割任务上保持甚至超越现有方法的性能，为隐私推理的实际应用提供了有效的解决方案。

Abstract: Private Inference (PI) uses cryptographic primitives to perform privacy preserving machine learning. In this setting, the owner of the network runs inference on the data of the client without learning anything about the data and without revealing any information about the model. It has been observed that a major computational bottleneck of PI is the calculation of the gate (i.e., ReLU), so a considerable amount of effort have been devoted to reducing the number of ReLUs in a given network.
  We focus on the DReLU, which is the non-linear step function of the ReLU and show that one DReLU can serve many ReLU operations. We suggest a new activation module where the DReLU operation is only performed on a subset of the channels (Prototype channels), while the rest of the channels (replicate channels) replicates the DReLU of each of their neurons from the corresponding neurons in one of the prototype channels. We then extend this idea to work across different layers.
  We show that this formulation can drastically reduce the number of DReLU operations in resnet type network. Furthermore, our theoretical analysis shows that this new formulation can solve an extended version of the XOR problem, using just one non-linearity and two neurons, something that traditional formulations and some PI specific methods cannot achieve. We achieve new SOTA results on several classification setups, and achieve SOTA results on image segmentation.

</details>


### [47] [You Only Train Once: Differentiable Subset Selection for Omics Data](https://arxiv.org/abs/2512.17678)
*Daphné Chopard,Jorge da Silva Gonçalves,Irene Cannistraci,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.LG

TL;DR: YOTO是一个端到端的单细胞转录组基因选择框架，通过联合训练直接选择离散基因子集并进行预测，避免了传统多阶段流程的分离问题。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞转录组特征选择方法多为多阶段流程或依赖后验特征归因，导致选择与预测弱耦合，影响基因子集的紧凑性和信息量。

Method: YOTO采用端到端可微分架构，通过闭环反馈机制联合优化基因选择和预测任务，强制稀疏性使只有被选基因参与推理，无需额外下游分类器。通过多任务学习设计，模型学习跨任务共享表示。

Result: 在两个代表性单细胞RNA-seq数据集上，YOTO持续优于现有最先进基线方法，展示了更好的预测性能。

Conclusion: 稀疏、端到端、多任务的基因子集选择方法能提高预测性能，产生紧凑且有意义的基因子集，推动生物标志物发现和单细胞分析进展。

Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.

</details>


### [48] [meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis](https://arxiv.org/abs/2512.17409)
*Dishantkumar Sutariya,Eike Petersen*

Main category: cs.LG

TL;DR: 本文提出了一个用于医学影像模型子组性能差异分析的统计工具箱，解决了样本量差异、多重比较校正和组合子组发现等统计挑战。


<details>
  <summary>Details</summary>
Motivation: 在医学影像机器学习模型中，按患者和记录属性进行分层性能分析已成为标准做法，能揭示重要模型失败模式。然而，以统计严谨的方式进行此类分析并不简单，需要解决样本量差异、基础率不同、多重比较校正等统计挑战。

Method: 开发了一个统计工具箱，包含：1）选择适合不同样本量和基础率的性能指标；2）确定指标不确定性；3）进行多重比较校正以评估观察差异是否偶然；4）在组合子组分析中实现机制以找到最"有趣"的子组。

Result: 工具箱在ISIC2020皮肤病变恶性分类和MIMIC-CXR胸部X光疾病分类两个案例研究中得到验证，展示了如何系统识别模型性能差异的子组。

Conclusion: 该工具箱使实践者能够轻松而严谨地评估模型潜在的子组性能差异，特别适用于医学影像应用，有助于发现和解决模型偏见问题。

Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.

</details>


### [49] [Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments](https://arxiv.org/abs/2512.17771)
*Dong Chen,Zhengqing Hu,Shixing Zhao,Yibo Guo*

Main category: cs.LG

TL;DR: 提出Easy Adaptation方法，设计特定小模型来补充大语言模型未充分拟合的数据分布，无需访问大模型参数，仅需极少资源即可达到参数高效微调的性能


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法面临两个主要挑战：1) 资源成本高，在资源受限环境中不实用；2) 参数依赖性强，需要访问大模型参数，而许多领先模型已采用闭源API访问策略，成本高昂且微调过程缓慢

Method: 提出Easy Adaptation方法，设计特定小模型来补充大语言模型未充分拟合的数据分布。该方法不依赖更新大模型参数，而是通过构建专门的小模型来针对特定任务分布进行优化

Result: 大量实验表明，EA方法在多样化任务上能够匹配参数高效微调的性能，且无需访问大语言模型参数，仅需极少的资源

Conclusion: EA方法为解决大模型适应性问题提供了新的思路，通过设计特定小模型来补充大模型未充分拟合的数据分布，在保持性能的同时显著降低了资源需求和参数依赖性

Abstract: While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.

</details>


### [50] [Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow](https://arxiv.org/abs/2512.17878)
*Herlock Rahimi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Wasserstein-Fisher-Rao几何的采样方法，通过引入显式修正项和加权随机微分方程来改进传统扩散模型在非凸或多模态分布上的采样效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于分数的扩散模型在强对数凹分布上表现良好，但在非凸或多模态分布（如双势阱）上混合速率会指数级恶化。由于实际生成建模任务常涉及高度非对数凹的目标分布，需要开发能改善探索能力的采样方案。

Method: 利用信息几何工具，通过Wasserstein-Fisher-Rao几何增强基于扩散的采样器，引入受控的质量重加权机制。通过显式修正项实现重加权，并使用Feynman-Kac表示通过加权随机微分方程实现。

Result: 提出了基于WFR的采样动力学框架，阐明了其几何和算子理论结构，为未来的理论和算法发展奠定了基础。

Conclusion: Wasserstein-Fisher-Rao几何为改进传统扩散模型的采样效率提供了有前景的方向，特别是在处理非凸和多模态分布时，通过结合样本空间传输和概率测度空间上的垂直（反应）动力学来增强探索能力。

Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.

</details>


### [51] [Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study](https://arxiv.org/abs/2512.17477)
*Shubham Das,Kaushal Singhania,Amit Sadhu,Suprabhat Das,Arghya Nandi*

Main category: cs.LG

TL;DR: 使用深度学习代理模型替代Inconel 625合金的有限元蠕变模拟，BiLSTM-VAE提供概率预测，BiLSTM-Transformer提供确定性高精度预测，实现从数十分钟到秒级的计算加速。


<details>
  <summary>Details</summary>
Motivation: Inconel 625等高温合金的蠕变变形对航空航天和能源系统组件的长期可靠性至关重要，但传统有限元模拟（如ANSYS）计算成本高昂，单次10,000小时模拟需要数十分钟，限制了设计优化和结构健康监测的效率。

Method: 使用ANSYS基于Norton定律生成单轴应力50-150 MPa、温度700-1000°C下的蠕变应变数据，训练两种深度学习架构：BiLSTM变分自编码器（提供不确定性感知和生成预测）和BiLSTM-Transformer混合模型（利用自注意力捕捉长程时间行为）。

Result: BiLSTM-VAE提供稳定可靠的蠕变应变预测，BiLSTM-Transformer在整个时间范围内实现高确定性精度。延迟测试显示显著加速：ANSYS模拟需要30-40分钟，而代理模型在秒级内完成预测。

Conclusion: 提出的深度学习框架为高温合金应用提供了快速蠕变评估的解决方案，支持设计优化和结构健康监测，实现了从计算密集型模拟到高效代理模型的转变。

Abstract: Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.

</details>


### [52] [NetworkFF: Unified Layer Optimization in Forward-Only Neural Networks](https://arxiv.org/abs/2512.17531)
*Salar Beigzad*

Main category: cs.LG

TL;DR: 本文提出协作式前向-前向学习算法，通过层间协作机制解决传统前向-前向算法中的层间隔离问题，在保持前向计算优势的同时提升学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统前向-前向算法虽然消除了反向传播的内存限制和生物学不合理性，但存在严重的层间隔离问题，各层独立优化"goodness"函数而缺乏集体学习动态，限制了表示协调和深层架构的收敛效率。

Method: 提出协作式前向-前向学习框架，包含两种协作范式：固定CFF（恒定层间耦合）和自适应CFF（可学习的协作参数）。协作goodness函数整合所有层的加权贡献，实现协调的特征学习，同时保持内存效率和生物学合理性。

Result: 在MNIST和Fashion-MNIST数据集上的综合评估显示，相比基线前向-前向实现，协作方法取得了显著的性能提升。

Conclusion: 层间协作是前向-前向学习的基本增强机制，对神经形态计算架构和能源受限的AI系统具有直接应用价值。

Abstract: The Forward-Forward algorithm eliminates backpropagation's memory constraints and biological implausibility through dual forward passes with positive and negative data. However, conventional implementations suffer from critical inter-layer isolation, where layers optimize goodness functions independently without leveraging collective learning dynamics. This isolation constrains representational coordination and limits convergence efficiency in deeper architectures. This paper introduces Collaborative Forward-Forward (CFF) learning, extending the original algorithm through inter-layer cooperation mechanisms that preserve forward-only computation while enabling global context integration. Our framework implements two collaborative paradigms: Fixed CFF (F-CFF) with constant inter-layer coupling and Adaptive CFF (A-CFF) with learnable collaboration parameters that evolve during training. The collaborative goodness function incorporates weighted contributions from all layers, enabling coordinated feature learning while maintaining memory efficiency and biological plausibility. Comprehensive evaluation on MNIST and Fashion-MNIST demonstrates significant performance improvements over baseline Forward-Forward implementations. These findings establish inter-layer collaboration as a fundamental enhancement to Forward-Forward learning, with immediate applicability to neuromorphic computing architectures and energy-constrained AI systems.

</details>


### [53] [Bayesian Optimisation: Which Constraints Matter?](https://arxiv.org/abs/2512.17569)
*Xietao Wang Lin,Juan Ungredda,Max Butler,James Town,Alma Rahat,Hemant Singh,Juergen Branke*

Main category: cs.LG

TL;DR: 提出贝叶斯优化的知识梯度变体，用于处理解耦的黑盒约束问题，通过选择性评估相关约束来优化函数


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在处理昂贵的全局黑盒优化问题方面已被证明非常有效。现有方法在处理解耦约束时存在不足，特别是当只有少数约束在最优解处起约束作用时，需要更智能地选择评估哪些约束

Method: 提出新的贝叶斯优化变体，基于知识梯度采集函数，专门针对解耦的黑盒约束问题。方法能够识别哪些约束可能在最优解处起约束作用，从而只评估相关的约束函数，而不是所有约束

Result: 通过实证基准测试，证明所提方法优于现有最先进方法，在处理解耦约束的优化问题上表现出优越性能

Conclusion: 提出的贝叶斯优化变体能够有效处理解耦约束问题，通过智能选择评估相关约束，提高了优化效率和性能，为这类问题提供了更有效的解决方案

Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.

</details>


### [54] [Machine Learning for Static and Single-Event Dynamic Complex Network Analysis](https://arxiv.org/abs/2512.17577)
*Nikolaos Nakis*

Main category: cs.LG

TL;DR: 该论文提出用于静态和单事件动态网络的图表示学习新算法，基于潜在空间模型，特别是潜在距离模型，旨在创建结构感知的网络表示，实现统一学习过程。


<details>
  <summary>Details</summary>
Motivation: 开发能够自然捕捉网络重要特性（如同质性、传递性、平衡理论）的图表示学习方法，创建结构感知的网络表示，避免启发式和多阶段处理，实现统一的网络嵌入学习。

Method: 基于潜在空间模型家族，特别是潜在距离模型，开发用于静态和单事件动态网络的图表示学习算法，创建结构感知的网络表示，实现统一的端到端学习过程。

Result: 论文提出了能够实现网络结构层次表达、社区特征识别、极端轮廓检测和时间网络影响动态量化的方法，这些方法消除了后处理步骤的需求。

Conclusion: 该研究致力于开发统一、全面且强大的网络嵌入方法，能够有效表征网络结构并处理图分析中的多样化任务，为网络分析提供新的算法框架。

Abstract: The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.

</details>


### [55] [Learning Safe Autonomous Driving Policies Using Predictive Safety Representations](https://arxiv.org/abs/2512.17586)
*Mahesh Keswani,Raunak Bhattacharyya*

Main category: cs.LG

TL;DR: SRPL框架在真实世界自动驾驶场景中验证有效，能改善奖励-安全权衡，提升成功率并降低成本，但对底层优化器和数据分布敏感，且能增强对观测噪声的鲁棒性和跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习在自动驾驶中面临性能优化与安全要求之间的根本矛盾：过于保守的策略会限制驾驶效率，而激进的探索则可能违反安全约束。SRPL框架通过预测未来约束违反的模型来解决这一挑战，但需要验证其在真实世界自动驾驶场景中的有效性。

Method: 在Waymo Open Motion Dataset和NuPlan数据集上进行系统实验，评估SRPL框架在真实世界自动驾驶场景中的表现。研究包括奖励-安全权衡分析、成功率统计、成本降低效果，以及对观测噪声的鲁棒性和跨数据集零样本评估。

Result: SRPL能显著改善奖励-安全权衡，成功率提升效果量r=0.65-0.86，成本降低效果量r=0.70-0.83（p<0.05）。但其有效性依赖于底层策略优化器和数据集分布。预测安全表示能提高对观测噪声的鲁棒性，SRPL增强的智能体在跨数据集评估中表现出更好的泛化能力。

Conclusion: 预测安全表示有潜力增强自动驾驶中的安全强化学习，SRPL框架在真实世界场景中表现出有效性，但需要考虑优化器和数据分布的依赖性。该研究为安全强化学习在自动驾驶领域的实际应用提供了实证支持。

Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.

</details>


### [56] [Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models](https://arxiv.org/abs/2512.17592)
*Arthur Guijt,Dirk Thierens,Ellen Kerkhof,Jan Wiersma,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.LG

TL;DR: 该论文研究异步协作中已训练模型的合并方法，提出使用缝合层技术来结合不同参与方独立训练的模型，在保持各方数据性能的同时提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在医疗等数据分散且无法共享的领域，联邦学习需要同步协作，而异步协作（仅共享已训练模型）的性能影响尚不明确。研究者希望探索如何通过模型合并实现异步协作的竞争性结果。

Method: 采用多目标视角，将各方数据性能独立评估。提出使用缝合层技术来结合独立训练模型的中间表示，通过精心设计的缝合层位置来平衡各方数据性能与泛化能力。

Result: 单独训练模型在自身数据上表现良好，但在其他方数据上性能显著下降。模型集成能提升泛化但损害各方自身数据性能。缝合层技术能在保持竞争性各方数据性能的同时显著提升泛化能力。

Conclusion: 异步协作通过缝合层技术结合独立训练模型，能够实现竞争性性能，为数据分散且无法共享的领域提供了可行的解决方案，证明了异步协作的实用价值。

Abstract: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.

</details>


### [57] [A Unified Representation of Neural Networks Architectures](https://arxiv.org/abs/2512.17593)
*Christophe Prieur,Mircea Lazar,Bogdan Robu*

Main category: cs.LG

TL;DR: 该论文提出了分布式参数神经网络（DiPaNet）的统一框架，将有限维和无限维神经网络架构通过均匀化/离散化联系起来，并推导了神经元数量和隐藏层数趋于无穷时的近似误差。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络架构在隐藏层神经元数量和层数趋于无穷时的极限情况，建立连续神经网络表示与现有架构的统一理论框架，并量化近似误差。

Method: 首先推导单隐藏层神经网络的积分无限宽度表示，扩展到具有有限积分隐藏层的深度残差CNN；然后通过离散化技术形式化神经ODE与深度残差NN的关系；最后将两种方法合并为统一的DiPaNet表示。

Result: 提出了DiPaNet框架，证明大多数现有的有限维和无限维神经网络架构可以通过DiPaNet表示的均匀化/离散化联系起来，建立了统一的数学表示。

Conclusion: DiPaNet为神经网络提供了统一的确定性理论框架，适用于一般的均匀连续矩阵权重函数，为理解不同神经网络架构之间的关系提供了理论基础，并讨论了与神经场的异同及进一步应用。

Abstract: In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.

</details>


### [58] [A Systems-Theoretic View on the Convergence of Algorithms under Disturbances](https://arxiv.org/abs/2512.17598)
*Guner Dilsad Er,Sebastian Trimpe,Michael Muehlebach*

Main category: cs.LG

TL;DR: 该论文提出了一种系统分析算法在扰动、噪声和系统互连影响下收敛性的统一框架，通过Lyapunov定理量化扰动对算法性能的影响。


<details>
  <summary>Details</summary>
Motivation: 算法越来越多地在复杂的物理、社会和工程系统中运行，这些环境中存在扰动、噪声以及与其他动态系统的互连。现有分析通常假设算法在孤立环境中运行，缺乏对实际扰动影响的系统性量化方法。

Method: 利用逆Lyapunov定理推导关键不等式，量化扰动对算法收敛性的影响，建立稳定性边界和收敛速率分析框架。

Result: 开发了系统性的稳定性边界和收敛速率分析方法，能够量化扰动对算法性能的影响，为分布式学习中的通信约束、机器学习泛化的敏感性、隐私保护中的噪声注入等应用场景提供分析工具。

Conclusion: 该研究提供了一个统一的工具框架，用于分析算法在噪声、扰动和系统互连存在时的性能，为算法在复杂环境中的鲁棒性分析提供了理论基础。

Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.

</details>


### [59] [Estimating Spatially Resolved Radiation Fields Using Neural Networks](https://arxiv.org/abs/2512.17654)
*Felix Lehner,Pasquale Lombardo,Susana Castillo,Oliver Hupe,Marcus Magnor*

Main category: cs.LG

TL;DR: 使用神经网络重建医疗辐射场中散射辐射空间分布的研究，通过三种合成数据集评估不同网络架构的性能


<details>
  <summary>Details</summary>
Motivation: 为医疗辐射防护剂量学（如介入放射学和心脏病学）开发准确估计散射辐射空间分布的方法，传统方法计算成本高，需要更高效的解决方案

Method: 使用基于Geant4的蒙特卡洛模拟生成三种复杂度递增的合成数据集，评估卷积神经网络和全连接神经网络在重建辐射场通量和能谱分布方面的性能

Result: 展示了哪些神经网络设计决策在重建此类辐射场的空间分布方面效果良好，所有数据集和训练流程已作为开源资源发布

Conclusion: 神经网络可以有效估计医疗辐射场中的散射辐射分布，为辐射防护剂量学提供了新的计算工具，开源数据集和代码有助于进一步研究

Abstract: We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.

</details>


### [60] [Polyharmonic Cascade](https://arxiv.org/abs/2512.17671)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文提出了一种名为"多调和级联"的深度机器学习架构，通过随机函数理论和无差别原理推导，使用多调和样条包序列来近似任意复杂度的非线性函数，同时保持全局光滑性和概率解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法通常缺乏严格的数学理论基础，难以在保持全局光滑性的同时近似复杂非线性函数。本文旨在构建一个具有严格数学推导、保持概率解释且计算高效的深度架构。

Method: 提出多调和级联架构：由多调和样条包序列组成，每层基于随机函数理论和无差别原理严格推导。采用替代梯度下降的训练方法：在每个批次上求解关于固定"节点星座"处函数值的全局线性系统，实现所有层的同步更新。

Result: 该方法在MNIST数据集上展示了快速学习且不过拟合的能力。所有计算简化为可在GPU上高效执行的2D矩阵操作，具有良好的可扩展性。

Conclusion: 多调和级联架构结合了严格的数学理论基础、概率解释和计算效率，为深度学习提供了一种新的理论严谨且实用的方法。

Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.

</details>


### [61] [Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents](https://arxiv.org/abs/2512.17688)
*Paul Mangold,Eloïse Berthier,Eric Moulines*

Main category: cs.LG

TL;DR: 论文提出了联邦SARSA（FedSARSA）的理论分析，在线性函数逼近和本地训练下建立了收敛保证，首次给出了异构环境下的样本和通信复杂度边界。


<details>
  <summary>Details</summary>
Motivation: 研究联邦强化学习中的SARSA算法，解决在异构环境（本地转移和奖励不同）下的收敛问题，填补该领域理论分析的空白。

Method: 提出了FedSARSA算法，采用线性函数逼近和本地训练，核心贡献是新的单智能体SARSA多步误差展开分析，精确量化异构性影响。

Result: 建立了FedSARSA在异构环境下的收敛保证，证明了算法在多个本地更新下的收敛性，并展示了相对于智能体数量的线性加速效果。

Conclusion: FedSARSA在异构联邦强化学习环境中具有理论保证的收敛性，能够实现线性加速，数值实验支持了理论发现。

Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.

</details>


### [62] [Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting](https://arxiv.org/abs/2512.17696)
*Yuri Calleo*

Main category: cs.LG

TL;DR: 提出了一种空间感知的Transformer架构，通过可学习的协方差核将地统计归纳偏置注入自注意力机制，在保持深度学习灵活性的同时融入空间拓扑约束，实现了物理感知建模与数据驱动学习的结合。


<details>
  <summary>Details</summary>
Motivation: 高维时空过程建模存在经典地统计学概率严谨性与深度学习灵活高容量表示之间的根本对立。高斯过程提供理论一致性和精确不确定性量化，但计算复杂度高；Transformer擅长序列建模但缺乏几何归纳偏置，将空间传感器视为排列不变的标记而无法理解距离概念。

Method: 提出空间感知Transformer混合架构，通过可学习的协方差核将地统计归纳偏置直接注入自注意力机制。将注意力结构形式化分解为平稳物理先验和非平稳数据驱动残差，施加软拓扑约束，偏好空间近邻交互同时保留建模复杂动态的能力。

Result: 展示了"深度变异函数"现象，网络通过反向传播成功端到端恢复底层过程的真实空间衰减参数。在合成高斯随机场和真实世界交通基准测试上的广泛实验表明，该方法优于最先进的图神经网络。严格的统计验证确认该方法不仅提供更优的预测精度，还能产生良好校准的概率预测。

Conclusion: 该方法有效弥合了物理感知建模与数据驱动学习之间的差距，通过将地统计先验注入Transformer架构，在保持深度学习灵活性的同时融入了空间拓扑约束，实现了预测精度和概率校准性的双重提升。

Abstract: The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.

</details>


### [63] [Mitigating Forgetting in Low Rank Adaptation](https://arxiv.org/abs/2512.17720)
*Joanna Sliwa,Frank Schneider,Philipp Hennig,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: LaLoRA：基于拉普拉斯近化的权重空间正则化方法，应用于LoRA微调，通过估计参数置信度约束高曲率方向的更新，缓解灾难性遗忘问题，保持轻量级特性。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调方法（如LoRA）虽然能快速适应下游任务，但往往导致模型遗忘先前的领域知识，即灾难性遗忘问题。需要一种方法在保持轻量级的同时，平衡新任务学习和旧知识保留。

Method: LaLoRA将拉普拉斯近似应用于LoRA权重，估计模型对每个参数的置信度，约束高曲率方向的参数更新。该方法仅对LoRA权重应用拉普拉斯近似，保持轻量级特性。通过正则化强度直接控制学习-遗忘权衡。

Result: 在Llama模型数学推理微调实验中，LaLoRA改善了学习-遗忘权衡，该权衡可通过正则化强度直接控制。研究了不同损失景观曲率近似方法对参数置信度估计的影响，分析了拉普拉斯近似所用数据的效果，并验证了超参数鲁棒性。

Conclusion: LaLoRA通过权重空间正则化有效缓解了LoRA微调中的灾难性遗忘问题，在保持轻量级的同时实现了更好的学习-遗忘平衡，为参数高效微调提供了实用的正则化解决方案。

Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.

</details>


### [64] [Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation](https://arxiv.org/abs/2512.17762)
*Luca Miglior,Matteo Tolloso,Alessio Gravina,Davide Bacciu*

Main category: cs.LG

TL;DR: 论文提出了ECHO基准测试，专门用于评估图神经网络处理长距离传播的能力，包含三个合成图任务和两个真实化学数据集，揭示了现有GNN在长程交互捕捉上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 有效捕捉长程交互是图神经网络研究中尚未解决的基础性挑战，对科学应用至关重要。现有方法在长距离图传播能力评估方面缺乏系统性基准。

Method: 设计了ECHO基准测试，包含三个合成图任务（单源最短路径、节点偏心率、图直径）和两个真实化学数据集（ECHO-Charge和ECHO-Energy）。合成任务构建在具有信息瓶颈的挑战性拓扑上，化学数据集基于密度泛函理论计算，用于预测原子部分电荷和分子总能量。

Result: 对流行GNN架构的广泛基准测试揭示了明显的性能差距，强调了真实长程传播的困难，并指出了能够克服固有局限性的设计选择。

Conclusion: ECHO为评估长程信息传播设立了新标准，为AI在科学中的应用提供了需要长程传播能力的明确例证。

Abstract: Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.

</details>


### [65] [Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning](https://arxiv.org/abs/2512.17788)
*Wei Tang,Yin-Fang Yang,Weijia Zhang,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种可校准的消歧损失（CDL），用于解决多实例部分标签学习（MIPL）中分类器校准性能差的问题，该损失函数可无缝集成到现有框架中，同时提升分类准确性和校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多实例部分标签学习方法存在校准性能差的问题，这影响了分类器的可靠性。MIPL结合了多实例学习和部分标签学习，需要在实例空间和标签空间同时处理不精确监督，但现有方法在模型校准方面表现不佳。

Method: 提出了一种即插即用的可校准消歧损失（CDL），该损失有两种实现形式：第一种基于候选标签集的概率进行校准，第二种同时整合候选标签集和非候选标签集的概率。CDL可以无缝集成到现有的MIPL和PLL框架中。

Result: 在基准数据集和真实世界数据集上的实验结果表明，CDL显著提升了分类性能和校准性能。理论分析证明了CDL的下界和正则化特性，并展示了其相对于传统消歧损失的优越性。

Conclusion: 提出的可校准消歧损失（CDL）有效解决了MIPL中的校准问题，同时提高了分类准确性，为弱监督学习中的模型可靠性提供了新的解决方案。

Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.

</details>


### [66] [Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation](https://arxiv.org/abs/2512.17820)
*Liam Collins,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Donald Loveland,Leonardo Neves,Neil Shah*

Main category: cs.LG

TL;DR: 该研究发现ID特征和文本特征在序列推荐中具有互补性，提出了一种简单的集成方法，无需复杂融合架构就能超越现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐模型要么完全用模态特征替代ID嵌入，要么采用复杂的多阶段训练和对齐架构来联合使用ID和模态特征。这两种方法都缺乏对ID和模态特征互补性的深入理解。

Method: 通过独立训练ID模型和文本模型来保持两者的互补性，然后采用简单的集成策略来利用这种互补性。

Result: 该方法虽然简单，但超越了多个竞争性序列推荐基线模型，表明ID和文本特征对于实现最先进的序列推荐性能都是必要的，但不需要复杂的融合架构。

Conclusion: ID特征和文本特征在序列推荐中具有互补性，简单的集成方法就能有效利用这种互补性，无需复杂的融合架构。

Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.

</details>


### [67] [Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space](https://arxiv.org/abs/2512.17884)
*Xinyue Yu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 提出一种结合正则化随机傅里叶特征和有限元重建映射的方法，用于从噪声数据中学习算子，相比无正则化方法具有更好的噪声鲁棒性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 基于核的算子学习方法虽然能提供准确的理论保证，但在大规模训练集下计算成本高，且对噪声敏感。需要开发一种既能保持准确性又能应对噪声和计算效率的方法。

Method: 提出RRFF-FEM方法：使用多元Student's t分布抽取随机特征，结合频率加权的Tikhonov正则化抑制高频噪声。当特征数N按m log m缩放时，系统条件良好。

Result: 建立了随机特征矩阵极端奇异值的高概率界，证明了系统良好条件性。在多个PDE基准问题上的实验表明，该方法对噪声鲁棒，训练时间减少，同时保持与核方法和神经算子相当的准确性。

Conclusion: RRFF和RRFF-FEM方法为从噪声数据中学习算子提供了一种高效、鲁棒的解决方案，在计算效率和准确性之间取得了良好平衡。

Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [68] [Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases](https://arxiv.org/abs/2512.16953)
*Pietro Cofone,Giovanni Amendola,Marco Manna,Aldo Ricioppo*

Main category: cs.AI

TL;DR: 论文提出了一种基于逻辑的扩展图框架，用于实体集扩展，并设计了高效的推理任务来避免完全构建大型图，支持局部增量导航。


<details>
  <summary>Details</summary>
Motivation: 传统的线性实体集扩展方法无法揭示知识资源中更丰富的分类结构，而完全构建扩展图在实际场景中可能不切实际，因此需要更高效的推理方法。

Method: 采用基于逻辑的扩展图框架，其中节点表示由逻辑公式标记的语义泛化，边编码严格的语义包含关系。设计了检查两个元组在图中节点关系的推理任务，并在限制输入或实体描述的现实假设下实现高效计算。

Result: 在现实假设下（如限制输入或实体描述），这些推理任务可以高效实现，从而支持扩展图的局部增量导航，无需完全构建整个图。

Conclusion: 通过形式化高效的推理任务，可以在不完全构建扩展图的情况下支持实体集扩展的实用应用，实现知识驱动的分类扩展。

Abstract: Recognizing similarities among entities is central to both human cognition and computational intelligence. Within this broader landscape, Entity Set Expansion is one prominent task aimed at taking an initial set of (tuples of) entities and identifying additional ones that share relevant semantic properties with the former -- potentially repeating the process to form increasingly broader sets. However, this ``linear'' approach does not unveil the richer ``taxonomic'' structures present in knowledge resources. A recent logic-based framework introduces the notion of an expansion graph: a rooted directed acyclic graph where each node represents a semantic generalization labeled by a logical formula, and edges encode strict semantic inclusion. This structure supports taxonomic expansions of entity sets driven by knowledge bases. Yet, the potentially large size of such graphs may make full materialization impractical in real-world scenarios. To overcome this, we formalize reasoning tasks that check whether two tuples belong to comparable, incomparable, or the same nodes in the graph. Our results show that, under realistic assumptions -- such as bounding the input or limiting entity descriptions -- these tasks can be implemented efficiently. This enables local, incremental navigation of expansion graphs, supporting practical applications without requiring full graph construction.

</details>


### [69] [Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows](https://arxiv.org/abs/2512.16969)
*Wanghan Xu,Yuhao Zhou,Yifan Zhou,Qinglong Cao,Shuo Li,Jia Bu,Bo Liu,Yixin Chen,Xuming He,Xiangyu Zhao,Xiang Zhuang,Fengxiang Wang,Zhiwang Zhou,Qiantai Feng,Wenxuan Huang,Jiaqi Wei,Hao Wu,Yuejin Yang,Guangshuai Wang,Sheng Xu,Ziyan Huang,Xinyao Liu,Jiyao Liu,Cheng Tang,Wei Li,Ying Chen,Junzhi Ning,Pengfei Jiang,Chenglong Ma,Ye Du,Changkai Ji,Huihui Xu,Ming Hu,Jiangbin Zheng,Xin Chen,Yucheng Wu,Feifei Jiang,Xi Chen,Xiangru Tang,Yuchen Fu,Yingzhou Lu,Yuanyuan Zhang,Lihao Sun,Chengbo Li,Jinzhe Ma,Wanhao Liu,Yating Liu,Kuo-Cheng Wu,Shengdu Chai,Yizhou Wang,Ouwen Zhangjin,Chen Tang,Shufei Zhang,Wenbo Cao,Junjie Ren,Taoyong Cui,Zhouheng Yao,Juntao Deng,Yijie Sun,Feng Liu,Wangxu Wei,Jingyi Xu,Zhangrui Li,Junchao Gong,Zijie Guo,Zhiyu Yao,Zaoyu Chen,Tianhao Peng,Fangchen Yu,Bo Zhang,Dongzhan Zhou,Shixiang Tang,Jiaheng Liu,Fenghua Ling,Yan Lu,Yuchen Ren,Ben Fei,Zhen Zhao,Xinyu Gu,Rui Su,Xiao-Ming Wu,Weikang Si,Yang Liu,Hao Chen,Xiangchao Yan,Xue Yang,Junchi Yan,Jiamin Wu,Qihao Zheng,Chenhui Li,Zhiqiang Gao,Hao Kong,Junjun He,Mao Su,Tianfan Fu,Peng Ye,Chunfeng Song,Nanqing Dong,Yuqiang Li,Huazhu Fu,Siqi Sun,Lijing Cheng,Jintai Lin,Wanli Ouyang,Bowen Zhou,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 该论文提出了科学通用智能(SGI)的操作化定义，基于实践探究模型(PIM)，并创建了包含1000多个跨学科样本的SGI-Bench基准来评估LLMs。结果显示现有模型在科学研究各环节存在显著差距，同时提出了测试时强化学习(TTRL)方法来提升假设新颖性。


<details>
  <summary>Details</summary>
Motivation: 尽管科学AI有所进展，但缺乏一个统一的科学通用智能(SGI)框架，即能够自主构思、调查和跨科学领域推理的能力。需要建立一个操作化的SGI定义和系统评估基准。

Method: 基于实践探究模型(PIM：审议、构思、行动、感知)建立SGI操作化定义，通过四个科学家对齐任务（深度研究、想法生成、干/湿实验、实验推理）进行评估。创建SGI-Bench基准，包含1000多个专家策划的跨学科样本，灵感来自《科学》杂志的125个重大问题。同时提出测试时强化学习(TTRL)方法，在推理时优化检索增强的新颖性奖励。

Result: 评估结果显示显著差距：深度研究中的精确匹配率低(10-20%)；想法缺乏可行性和细节；干实验中代码可执行性高但执行结果准确性低；湿实验协议序列保真度低；多模态比较推理挑战持续存在。TTRL方法能够在不依赖参考答案的情况下增强假设新颖性。

Conclusion: 基于PIM的定义、以工作流程为中心的基准和实证见解为真正参与科学发现的AI系统奠定了基础，指出了当前LLMs在科学发现能力上的局限性，并提出了改进方向。

Abstract: Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.

</details>


### [70] [PAACE: A Plan-Aware Automated Agent Context Engineering Framework](https://arxiv.org/abs/2512.16970)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: PAACE是一个用于优化LLM智能体上下文管理的统一框架，通过计划感知的自动上下文工程来提升智能体性能并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂多步工作流中产生快速扩展的上下文，现有方法忽视了多步骤、计划感知的智能体推理特性，导致上下文管理效率低下。

Method: 提出PAACE框架，包含：1) PAACE-Syn：生成带压缩监督标注的合成智能体工作流；2) PAACE-FT：从成功教师演示中训练的蒸馏、计划感知压缩器。采用下一k任务相关性建模、计划结构分析、指令协同细化和函数保持压缩等技术。

Result: 在AppWorld、OfficeBench和8-Objective QA等长视野基准测试中，PAACE持续提升智能体正确性并大幅降低上下文负载。在AppWorld上获得更高准确率的同时降低峰值上下文和累积依赖；在OfficeBench和多跳QA上提升准确率和F1，减少步骤数、峰值token和注意力依赖。蒸馏的PAACE-FT保留教师97%性能，推理成本降低超过一个数量级。

Conclusion: PAACE框架有效解决了LLM智能体上下文管理问题，通过计划感知的自动上下文工程实现了性能提升和成本降低的平衡，使紧凑模型能够实际部署计划感知压缩技术。

Abstract: Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.

</details>


### [71] [Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats](https://arxiv.org/abs/2512.17041)
*Ali Eslami,Jiangbo Yu*

Main category: cs.AI

TL;DR: 该论文研究了智能体车辆（AgVs）的安全威胁，包括OWASP风格的风险和来自感知、控制等其他层的网络攻击，提出了基于角色的架构和严重性矩阵来分析这些风险。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI在车辆中的探索和应用，智能体车辆（AgVs）面临新的安全挑战。现有的OWASP智能体AI安全风险框架未针对安全关键的网络物理平台（如车辆）设计，也未考虑与感知、通信和控制等其他层的交互。

Method: 引入基于角色的智能体车辆架构（包括个人代理和驾驶策略代理），研究智能体AI层和跨层风险（来自上游层如感知层、控制层的风险），使用严重性矩阵和攻击链分析来展示小规模扭曲如何升级为不对齐或不安全行为。

Result: 开发了一个框架，为分析当前和新兴车辆平台中智能体AI的安全风险提供了首个结构化基础，展示了攻击如何从其他层传播到智能体层并导致安全威胁。

Conclusion: 该论文为智能体车辆的安全风险分析提供了系统化框架，填补了现有OWASP框架在安全关键网络物理平台和跨层交互方面的空白，有助于理解和缓解智能体AI在车辆中的安全威胁。

Abstract: Agentic AI is increasingly being explored and introduced in both manually driven and autonomous vehicles, leading to the notion of Agentic Vehicles (AgVs), with capabilities such as memory-based personalization, goal interpretation, strategic reasoning, and tool-mediated assistance. While frameworks such as the OWASP Agentic AI Security Risks highlight vulnerabilities in reasoning-driven AI systems, they are not designed for safety-critical cyber-physical platforms such as vehicles, nor do they account for interactions with other layers such as perception, communication, and control layers. This paper investigates security threats in AgVs, including OWASP-style risks and cyber-attacks from other layers affecting the agentic layer. By introducing a role-based architecture for agentic vehicles, consisting of a Personal Agent and a Driving Strategy Agent, we will investigate vulnerabilities in both agentic AI layer and cross-layer risks, including risks originating from upstream layers (e.g., perception layer, control layer, etc.). A severity matrix and attack-chain analysis illustrate how small distortions can escalate into misaligned or unsafe behavior in both human-driven and autonomous vehicles. The resulting framework provides the first structured foundation for analyzing security risks of agentic AI in both current and emerging vehicle platforms.

</details>


### [72] [UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering](https://arxiv.org/abs/2512.17043)
*Yinxu Tang,Chengsong Huang,Jiaxin Huang,William Yeoh*

Main category: cs.AI

TL;DR: 该论文提出了关系中心的知识图谱问答新范式，通过子图而非单个实体作为答案，并开发了UniRel-R1框架来解决候选子图过多和琐碎连接的问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱问答主要关注实体中心查询，返回单个答案实体，但现实世界查询往往是关系性的，需要理解实体之间的关联。现有方法无法有效捕捉实体间的语义连接。

Method: 提出UniRel-R1统一框架，整合子图选择、多阶段图剪枝，以及通过强化学习微调的大语言模型。奖励函数设计鼓励紧凑、具体的子图，包含更多信息性关系和低度中间实体。

Result: 大量实验表明，UniRel-R1在连接性和奖励方面相比Vanilla基线取得显著提升，并能有效泛化到未见过的实体和关系。

Conclusion: 关系中心的知识图谱问答是实体中心问答的重要补充，UniRel-R1框架通过整合多种技术有效解决了候选子图过多和琐碎连接的问题，为理解实体间语义关联提供了新方法。

Abstract: Knowledge Graph Question Answering (KGQA) has traditionally focused on entity-centric queries that return a single answer entity. However, real-world queries are often relational, seeking to understand how entities are associated. In this work, we introduce relation-centric KGQA, a complementary setting where the answer is a subgraph capturing the semantic connections among entities rather than an individual entity. The main challenge lies in the abundance of candidate subgraphs, where trivial or overly common connections often obscure the identification of unique and informative answers. To tackle this, we propose UniRel-R1, a unified framework that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The reward function is designed to encourage compact and specific subgraphs with more informative relations and lower-degree intermediate entities. Extensive experiments show that UniRel-R1 achieves significant gains in connectivity and reward over Vanilla baselines and generalizes effectively to unseen entities and relations.

</details>


### [73] [Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations](https://arxiv.org/abs/2512.17066)
*Suhaib Abdurahman,Farzan Karimi-Malekabadi,Chenxiao Yu,Nour S. Kteily,Morteza Dehghani*

Main category: cs.AI

TL;DR: 使用大语言模型驱动的智能体在虚拟社会中模拟人类冲突，研究物质威胁和象征性威胁如何相互作用及主导冲突行为


<details>
  <summary>Details</summary>
Motivation: 人类冲突常归因于物质条件和象征性价值受到的威胁，但两者如何相互作用以及哪个占主导地位尚不清楚。研究进展受到因果控制弱、伦理约束和时间数据稀缺的限制

Method: 使用大语言模型驱动的智能体在虚拟社会中模拟，独立变化现实威胁和象征性威胁，同时追踪行动、语言和态度。通过表征分析研究LLM如何编码这些状态，并通过操纵这些状态因果性地改变行为

Result: 底层LLM将现实威胁、象征性威胁和敌意编码为不同的内部状态；现实威胁直接增加敌意，而象征性威胁效应较弱，完全通过内群体偏见中介，且仅在现实威胁不存在时增加敌意；非敌意的群体间接触能缓冲冲突升级，结构不对称使敌意集中在多数群体中

Conclusion: 模拟提供了威胁驱动冲突的因果解释：现实威胁是冲突的主要驱动因素，象征性威胁通过内群体偏见间接作用，且只在缺乏现实威胁时产生影响。群体间接触和结构因素在冲突动态中起重要作用

Abstract: Human conflict is often attributed to threats against material conditions and symbolic values, yet it remains unclear how they interact and which dominates. Progress is limited by weak causal control, ethical constraints, and scarce temporal data. We address these barriers using simulations of large language model (LLM)-driven agents in virtual societies, independently varying realistic and symbolic threat while tracking actions, language, and attitudes. Representational analyses show that the underlying LLM encodes realistic threat, symbolic threat, and hostility as distinct internal states, that our manipulations map onto them, and that steering these states causally shifts behavior. Our simulations provide a causal account of threat-driven conflict over time: realistic threat directly increases hostility, whereas symbolic threat effects are weaker, fully mediated by ingroup bias, and increase hostility only when realistic threat is absent. Non-hostile intergroup contact buffers escalation, and structural asymmetries concentrate hostility among majority groups.

</details>


### [74] [Value Under Ignorance in Universal Artificial Intelligence](https://arxiv.org/abs/2512.17086)
*Cole Wyeth,Marcus Hutter*

Main category: cs.AI

TL;DR: 该论文将AIXI强化学习智能体推广到更广泛的效用函数类别，通过处理信念分布中仅预测有限历史前缀的假设，探讨了半测度损失与死亡解释的关系，并研究了使用Choquet积分计算期望效用的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于扩展AIXI强化学习智能体以容纳更广泛的效用函数类别，解决信念分布中假设仅预测有限历史前缀时产生的模糊性问题，特别是半测度损失与死亡解释之间的关系。

Method: 方法包括：1）将信念分布视为不精确概率分布；2）使用不精确概率理论中的Choquet积分计算期望效用；3）分析这些方法的可计算性水平；4）将标准递归值函数作为特例恢复。

Result: 研究发现：1）标准递归值函数可以作为Choquet积分的特例恢复；2）在死亡解释下，最一般的期望效用不能表征为Choquet积分；3）分析了这些方法的可计算性水平。

Conclusion: 结论表明，通过将信念分布视为不精确概率分布并使用Choquet积分，可以扩展AIXI智能体的效用函数框架，但死亡解释下的最一般期望效用无法完全用Choquet积分表征，这为未来研究提供了方向。

Abstract: We generalize the AIXI reinforcement learning agent to admit a wider class of utility functions. Assigning a utility to each possible interaction history forces us to confront the ambiguity that some hypotheses in the agent's belief distribution only predict a finite prefix of the history, which is sometimes interpreted as implying a chance of death equal to a quantity called the semimeasure loss. This death interpretation suggests one way to assign utilities to such history prefixes. We argue that it is as natural to view the belief distributions as imprecise probability distributions, with the semimeasure loss as total ignorance. This motivates us to consider the consequences of computing expected utilities with Choquet integrals from imprecise probability theory, including an investigation of their computability level. We recover the standard recursive value function as a special case. However, our most general expected utilities under the death interpretation cannot be characterized as such Choquet integrals.

</details>


### [75] [A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving](https://arxiv.org/abs/2512.17093)
*Timo Pierre Schrader,Lukas Lange,Tobias Kaminski,Simon Razniewski,Annemarie Friedrich*

Main category: cs.AI

TL;DR: 本文提出了一种ASP求解器在循环中的方法，通过求解器引导的指令调优来改进LLM生成答案集编程代码的能力，仅需自然语言问题描述及其解决方案即可训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用编程语言上表现良好，但在领域特定语言（如答案集编程ASP）的代码生成方面仍面临挑战。ASP是一种解决组合搜索问题的有效方法，但LLM在ASP代码生成中的效果受到预训练阶段示例数量有限的限制。

Method: 提出ASP求解器在循环中的方法，仅需自然语言问题描述及其解决方案。从LLM中采样ASP语句作为程序延续，利用声明式ASP编程的特性（部分编码逐步缩小解空间），根据求解器反馈将实例分为选中和拒绝两类。然后应用监督微调训练LLM，并通过求解器引导的搜索（包括最佳N采样）进一步提高鲁棒性。

Result: 实验表明，在两个不同提示设置和两个数据集上均取得了一致的改进。

Conclusion: 通过求解器引导的指令调优方法，可以有效提升LLM在ASP代码生成任务上的性能，解决了语义解析的复杂性问题。

Abstract: The rise of large language models (LLMs) has sparked interest in coding assistants. While general-purpose programming languages are well supported, generating code for domain-specific languages remains a challenging problem for LLMs. In this paper, we focus on the LLM-based generation of code for Answer Set Programming (ASP), a particularly effective approach for finding solutions to combinatorial search problems. The effectiveness of LLMs in ASP code generation is currently hindered by the limited number of examples seen during their initial pre-training phase.
  In this paper, we introduce a novel ASP-solver-in-the-loop approach for solver-guided instruction-tuning of LLMs to addressing the highly complex semantic parsing task inherent in ASP code generation. Our method only requires problem specifications in natural language and their solutions. Specifically, we sample ASP statements for program continuations from LLMs for unriddling logic puzzles. Leveraging the special property of declarative ASP programming that partial encodings increasingly narrow down the solution space, we categorize them into chosen and rejected instances based on solver feedback. We then apply supervised fine-tuning to train LLMs on the curated data and further improve robustness using a solver-guided search that includes best-of-N sampling. Our experiments demonstrate consistent improvements in two distinct prompting settings on two datasets.

</details>


### [76] [Reinforcement Learning for Self-Improving Agent with Skill Library](https://arxiv.org/abs/2512.17102)
*Jiongxiao Wang,Qiaojing Yan,Yawei Wang,Yijun Tian,Soumya Smruti Mishra,Zhichao Xu,Megha Gandhi,Panpan Xu,Lin Lee Cheong*

Main category: cs.AI

TL;DR: SAGE是一个基于强化学习的框架，通过技能库增强LLM智能体的自我进化能力，在AppWorld任务中显著提升了目标完成率并减少了交互步骤和token消耗。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在新环境中部署时难以持续改进和适应，现有技能库方法主要依赖LLM提示，难以实现一致的技能库实施。

Method: 提出SAGE强化学习框架，包含顺序部署机制和技能集成奖励。顺序部署让智能体在相似任务链中迭代部署，技能从先前任务积累到库中供后续任务使用；技能集成奖励补充基于结果的原始奖励。

Result: 在AppWorld实验中，SAGE应用于有专家经验的监督微调模型，实现了8.9%更高的场景目标完成率，同时需要减少26%的交互步骤和生成59%更少的token，在准确性和效率上都显著优于现有方法。

Conclusion: SAGE框架通过强化学习有效增强了LLM智能体通过技能库进行自我进化的能力，在复杂任务环境中实现了更好的性能和效率。

Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.

</details>


### [77] [Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty](https://arxiv.org/abs/2512.17145)
*Josh Barber,Rourke Young,Cameron Coombe,Will Browne*

Main category: cs.AI

TL;DR: 提出一种受Solomonoff启发的LLM假设加权方法，通过简洁性和预测拟合度评估多个候选解，在Mini-ARC基准上实现不确定性感知的预测


<details>
  <summary>Details</summary>
Motivation: 现实世界AI任务中，数据稀疏问题需要系统性泛化能力，现有方法在评估多个候选解时难以平衡准确性和简洁性

Method: 受Solomonoff启发的加权方法，基于简洁性和预测拟合度对LLM生成的假设进行加权，产生Solomonoff加权混合模型进行逐单元预测

Result: 在Mini-ARC基准任务中，该方法产生保守、不确定性感知的输出，即使假设存在噪声或部分错误；相比贝叶斯模型平均，Solomonoff评分更均匀地分配概率权重

Conclusion: 算法信息论先验对于可解释、可靠的不确定性多假设推理具有重要价值

Abstract: Reasoning under uncertainty is a key challenge in AI, especially for real-world tasks, where problems with sparse data demands systematic generalisation. Existing approaches struggle to balance accuracy and simplicity when evaluating multiple candidate solutions. We propose a Solomonoff-inspired method that weights LLM-generated hypotheses by simplicity and predictive fit. Applied to benchmark (Mini-ARC) tasks, our method produces Solomonoff-weighted mixtures for per-cell predictions, yielding conservative, uncertainty-aware outputs even when hypotheses are noisy or partially incorrect. Compared to Bayesian Model Averaging (BMA), Solomonoff scoring spreads probability more evenly across competing hypotheses, while BMA concentrates weight on the most likely but potentially flawed candidates. Across tasks, this highlights the value of algorithmic information-theoretic priors for interpretable, reliable multi-hypothesis reasoning under uncertainty.

</details>


### [78] [MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation](https://arxiv.org/abs/2512.17194)
*Shengwei Zhao,Jingwen Yao,Sitong Wei,Linhai Xu,Yuying Liu,Dong Zhang,Zhiqiang Tian,Shaoyi Du*

Main category: cs.AI

TL;DR: 该论文提出了一种基于强化学习的可解释多模态检索增强生成方法，通过两阶段强化微调框架提升多模态大语言模型的推理能力，在WebQA和MultimodalQA数据集上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有MMRAG方法无法清晰解释检索和响应生成的推理逻辑，限制了结果的可解释性。为了解决这一缺陷，作者提出引入强化学习来增强多模态检索增强生成的推理能力。

Method: 采用两阶段强化微调框架：第一阶段使用基于规则的强化微调对多模态文档进行粗粒度点式排序，过滤显著不相关文档；第二阶段使用基于推理的强化微调联合优化细粒度列表排序和答案生成，引导模型输出可解释的推理逻辑。

Result: 在WebQA和MultimodalQA两个多模态检索增强生成基准数据集上取得了最先进的结果，并通过全面的消融实验验证了方法的有效性。

Conclusion: 通过引入强化学习到多模态检索增强生成中，提出的两阶段强化微调框架成功提升了多模态大语言模型的推理能力，实现了可解释的多模态检索增强生成，在基准数据集上表现出色。

Abstract: Multi-modal Retrieval-Augmented Generation (MMRAG) enables highly credible generation by integrating external multi-modal knowledge, thus demonstrating impressive performance in complex multi-modal scenarios. However, existing MMRAG methods fail to clarify the reasoning logic behind retrieval and response generation, which limits the explainability of the results. To address this gap, we propose to introduce reinforcement learning into multi-modal retrieval-augmented generation, enhancing the reasoning capabilities of multi-modal large language models through a two-stage reinforcement fine-tuning framework to achieve explainable multi-modal retrieval-augmented generation. Specifically, in the first stage, rule-based reinforcement fine-tuning is employed to perform coarse-grained point-wise ranking of multi-modal documents, effectively filtering out those that are significantly irrelevant. In the second stage, reasoning-based reinforcement fine-tuning is utilized to jointly optimize fine-grained list-wise ranking and answer generation, guiding multi-modal large language models to output explainable reasoning logic in the MMRAG process. Our method achieves state-of-the-art results on WebQA and MultimodalQA, two benchmark datasets for multi-modal retrieval-augmented generation, and its effectiveness is validated through comprehensive ablation experiments.

</details>


### [79] [UmniBench: Unified Understand and Generation Model Oriented Omni-dimensional Benchmark](https://arxiv.org/abs/2512.17196)
*Kai Liu,Leyang Chen,Wenbo Li,Zhikai Chen,Zhixin Wang,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.AI

TL;DR: UmniBench是一个统一多模态模型（UMMs）的综合性基准测试，能够在一个评估过程中同时评估理解、生成和编辑能力，覆盖13个主要领域和200多个概念。


<details>
  <summary>Details</summary>
Motivation: 当前对统一多模态模型的评估是解耦的，分别评估其理解和生成能力，缺乏一个综合性的评估框架。UmniBench旨在解决这个问题，提供更全面、客观的评估视角。

Method: UmniBench利用UMM自身的能力来评估其生成和编辑能力：基于人工检查的提示和问答对，使用UMM的理解能力来评估其生成和编辑输出。该基准支持综合评估，也可以解耦单独评估各项能力。

Result: 基于UmniBench对24个流行模型进行了基准测试，包括统一多模态模型和单能力大模型。该基准为社区模型性能提升提供了物流支持。

Conclusion: UmniBench为统一多模态模型提供了更全面、客观的评估框架，能够在一个过程中综合评估理解、生成和编辑能力，有助于推动社区模型的发展。

Abstract: Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. However, evaluations of unified multimodal models (UMMs) remain decoupled, assessing their understanding and generation abilities separately with corresponding datasets. To address this, we propose UmniBench, a benchmark tailored for UMMs with omni-dimensional evaluation. First, UmniBench can assess the understanding, generation, and editing ability within a single evaluation process. Based on human-examined prompts and QA pairs, UmniBench leverages UMM itself to evaluate its generation and editing ability with its understanding ability. This simple but effective paradigm allows comprehensive evaluation of UMMs. Second, UmniBench covers 13 major domains and more than 200 concepts, ensuring a thorough inspection of UMMs. Moreover, UmniBench can also decouple and separately evaluate understanding, generation, and editing abilities, providing a fine-grained assessment. Based on UmniBench, we benchmark 24 popular models, including both UMMs and single-ability large models. We hope this benchmark provides a more comprehensive and objective view of unified models and logistical support for improving the performance of the community model.

</details>


### [80] [Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction](https://arxiv.org/abs/2512.17250)
*Ziyang Lin,Zixuan Sun,Sanhorn Chen,Xiaoyang Chen,Roy Zhao*

Main category: cs.AI

TL;DR: 提出一种推测-校正框架，将推测执行思想应用于模型控制，通过预测动作队列和轻量级校正器减少规划推理次数，降低延迟同时保持控制性能


<details>
  <summary>Details</summary>
Motivation: 实时顺序控制代理常受推理延迟瓶颈，即使适度的每步规划延迟也会破坏控制稳定性并降低性能。需要减少规划推理次数以降低延迟

Method: 采用推测-校正框架，结合TD-MPC2模型控制：1) 预训练世界模型和潜在空间MPC规划器生成短时域动作队列和预测潜在轨迹；2) 新观测到达时测量真实潜在状态与预测潜在状态的不匹配；3) 小到中等不匹配时使用轻量级学习校正器对推测动作应用残差更新；4) 大不匹配时回退到完全重新规划并清除过时动作队列。研究了门控双塔MLP校正器和时序Transformer校正器

Result: 在DMC Humanoid-Walk任务中：规划推理次数从500减少到282，端到端步延迟改善25%，控制性能仅降低7.1%。消融实验表明无校正的推测执行在较长时域不可靠

Conclusion: 推测-校正框架能有效减少模型控制中的推理延迟，通过动作队列和轻量级校正器在保持控制性能的同时显著降低延迟，不匹配感知的校正对于稳健的延迟减少是必要的

Abstract: Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.

</details>


### [81] [ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework](https://arxiv.org/abs/2512.17266)
*Miru Hong,Minho Lee,Geonhee Jo,Jae-Hee So,Pascal Bauer,Sang-Ki Ko*

Main category: cs.AI

TL;DR: EventGPT：基于GPT架构的球员条件化价值感知下一事件预测模型，用于足球转会分析，通过反事实模拟评估球员在不同战术环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有转会评估方法依赖静态统计数据或事后价值模型，无法捕捉球员在新战术环境或不同队友配合下的适应性变化。需要一种能够量化球员在不同情境下表现的方法来改进转会决策。

Method: 开发EventGPT模型，基于GPT风格的自回归transformer，将比赛处理为离散token序列，联合预测下一持球动作的类型、位置、时间及其残差持球价值(rOBV)。通过球员嵌入实现反事实模拟，评估球员在不同球队或战术结构中的表现变化。

Result: 在五个赛季的英超联赛事件数据上评估，EventGPT在下一事件预测准确性和空间精度方面优于现有序列基线。案例研究展示了模型在转会分析中的实用性，如比较不同体系下前锋表现和识别特定角色的风格替代者。

Conclusion: EventGPT提供了一个原则性的方法来评估转会适配性，通过反事实模拟能够预测球员在不同战术环境中的行为分布和价值变化，为转会决策提供了更科学的依据。

Abstract: Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.

</details>


### [82] [Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation](https://arxiv.org/abs/2512.17308)
*Daksh Jain,Aarya Jain,Ashutosh Desai,Avyakt Verma,Ishan Bhanuka,Pratik Narang,Dhruv Kumar*

Main category: cs.AI

TL;DR: 该研究探索大型语言模型在宝可梦对战中的战略决策能力，评估其作为对战代理和游戏内容生成器的双重潜力。


<details>
  <summary>Details</summary>
Motivation: 宝可梦对战需要类型克制、统计权衡和风险评估等战略思维，为评估大型语言模型的推理能力提供了独特测试平台。研究旨在探索LLM是否能作为合格的对战代理，既能做出战术决策，又能生成平衡的游戏内容。

Method: 开发了基于回合制的宝可梦对战系统，LLM根据对战状态而非预设逻辑选择招式。框架包含宝可梦核心机制：类型效果乘数、基于属性的伤害计算、多宝可梦队伍管理。通过多种模型架构进行系统评估。

Result: 测量了胜率、决策延迟、类型匹配准确性和令牌效率等指标。结果表明LLM无需领域特定训练即可作为动态游戏对手，为回合制战略游戏提供了强化学习的实用替代方案。

Conclusion: LLM具备战术推理和内容创作的双重能力，可同时作为玩家和设计师，对交互娱乐中的程序化生成和自适应难度系统具有重要应用价值。

Abstract: Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.

</details>


### [83] [Dialectics for Artificial Intelligence](https://arxiv.org/abs/2512.17373)
*Zhengmian Hu*

Main category: cs.AI

TL;DR: 论文提出了一种基于算法信息论的概念定义方法，将概念视为与智能体整体经验相关的信息对象，通过可逆一致性关系和冗余信息度量来形式化概念发现与演化过程。


<details>
  <summary>Details</summary>
Motivation: 人类概念本身具有流动性（如冥王星不再被视为行星），传统基于词典标签的概念定义无法捕捉这种动态性。需要一种能够被修订、比较和在智能体间对齐的概念形式化方法。

Method: 采用算法信息论视角，将概念定义为与智能体整体经验结构相关的信息对象。核心约束是确定性：一组部分形成可逆一致性关系，即任何缺失部分都能从其他部分恢复（在Kolmogorov式恒等式的对数松弛范围内）。通过定义冗余信息来评估分解的自然性，并在此基础上构建辩证优化动态，让竞争概念通过更短的描述来解释新信息。

Result: 建立了概念的形式化定义框架，能够处理概念的扩展、收缩、分裂和合并。提出了低成本概念传输和多智能体对齐方法，通过共享协议下的种子实现概念重构，将通信转化为具体的计算-比特权衡。

Conclusion: 该框架为人工智能从原始经验中自主发现概念提供了理论基础，将概念视为可验证的结构性主张而非固定标签，能够模拟人类概念的动态演化过程，并为多智能体概念对齐提供了可操作的实现路径。

Abstract: Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of "concept" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents "concepts" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.

</details>


### [84] [Translating the Rashomon Effect to Sequential Decision-Making Tasks](https://arxiv.org/abs/2512.17470)
*Dennis Gross,Jørn Eirik Betten,Helge Spieker*

Main category: cs.AI

TL;DR: 将Rashomon效应从分类任务扩展到序列决策领域，研究在相同行为表现下内部结构不同的策略，并通过形式化验证方法证明其存在性及在鲁棒性、验证效率方面的优势。


<details>
  <summary>Details</summary>
Motivation: Rashomon效应在分类任务中已被广泛研究，但在序列决策领域尚未被探索。序列决策具有随机性和轨迹依赖性，验证策略行为一致性比分类任务更复杂，需要新的方法来识别和分析这种效应。

Method: 将Rashomon效应定义扩展到序列决策：多个策略在环境中表现出相同行为（访问相同状态、选择相同动作），但内部结构（如特征归因）不同。使用形式化验证方法构建和比较每个策略在环境中的完整概率行为，而非依赖单次轨迹结果。

Result: 实验证明Rashomon效应在序列决策中存在。从Rashomon集合构建的集成策略对分布偏移表现出比单个策略更强的鲁棒性。从Rashomon集合推导的宽松策略在保持最优性能的同时减少了验证的计算需求。

Conclusion: Rashomon效应在序列决策中确实存在，且具有实际应用价值：Rashomon集合构建的集成策略能提高鲁棒性，宽松策略能降低验证计算成本，为序列决策系统的分析和优化提供了新视角。

Abstract: The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.

</details>


### [85] [Towards Explainable Conversational AI for Early Diagnosis with Large Language Models](https://arxiv.org/abs/2512.17559)
*Maliha Tabassum,M Shamim Kaiser*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（GPT-4o）的诊断聊天机器人，结合检索增强生成和可解释AI技术，在医疗诊断中实现了90%的准确率和100%的Top-3准确率。


<details>
  <summary>Details</summary>
Motivation: 全球医疗系统面临诊断效率低下、成本上升和专家资源有限等问题，导致治疗延误和健康结果不佳。现有AI诊断系统缺乏交互性和透明度，难以在实际临床环境中有效应用。

Method: 采用基于GPT-4o的大语言模型诊断聊天机器人，结合检索增强生成和可解释AI技术。系统通过动态对话提取和规范化症状，利用相似性匹配和自适应提问优先诊断，并通过思维链提示提供透明推理。

Result: 与传统机器学习模型（朴素贝叶斯、逻辑回归、SVM、随机森林、KNN）相比，LLM系统表现出色：准确率达到90%，Top-3准确率达到100%。

Conclusion: 该研究展示了基于大语言模型的诊断聊天机器人在医疗领域的潜力，为实现更透明、交互性更强且临床相关性更高的AI医疗系统提供了有前景的方向。

Abstract: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

</details>


### [86] [About Time: Model-free Reinforcement Learning with Timed Reward Machines](https://arxiv.org/abs/2512.17637)
*Anirban Majumdar,Ritam Raha,Rajarshi Roy,David Parker,Marta Kwiatkowska*

Main category: cs.AI

TL;DR: 本文提出了定时奖励机（TRMs），扩展了传统奖励机以纳入时间约束，使强化学习中的奖励规范能表达精确的时间要求，并开发了相应的模型无关学习算法。


<details>
  <summary>Details</summary>
Motivation: 传统奖励机无法建模精确的时间约束，限制了其在时间敏感应用中的使用。为了在强化学习中表达包含时间要求的非马尔可夫奖励规范，需要一种能结合时间约束的奖励机制。

Method: 提出定时奖励机（TRMs），将时间约束整合到奖励结构中；研究基于表格Q学习的模型无关强化学习框架；算法通过定时自动机抽象将TRM整合到学习中，并利用反事实想象启发式方法改进搜索。

Result: 实验表明，算法能在流行的强化学习基准上学习到既获得高奖励又满足TRM指定时间约束的策略；比较研究展示了不同TRM语义下的性能表现，并验证了反事实想象方法的有效性。

Conclusion: TRMs扩展了奖励机的表达能力，使其能建模精确的时间约束；提出的学习算法能有效处理TRM规范，为时间敏感应用中的强化学习提供了新工具。

Abstract: Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.

</details>


### [87] [Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally](https://arxiv.org/abs/2512.17898)
*Robin Schimmelpfennig,Mark Díaz,Vinodkumar Prabhakaran,Aida Davani*

Main category: cs.AI

TL;DR: 研究发现，AI拟人化设计对用户信任和参与度的影响并非普遍一致，而是受到文化因素的显著调节。用户更关注交互线索而非理论特征，不同文化背景下相同设计可能产生相反效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统日益拟人化，引发了关于拟人化是否会导致用户产生不当信任或情感依赖的担忧。然而，现有安全框架主要基于西方人群的理论假设，缺乏对全球用户多样性的考虑，且拟人化设计与用户行为之间的因果关系尚未在真实交互中得到验证。

Method: 通过两个大规模跨国实验（N=3,500），涵盖10个不同国家，让用户与AI系统进行实时开放式交互。实验性地测试拟人化设计杠杆对用户拟人化感知的影响，并测量其对用户参与度和信任的行为指标。

Result: 1. 用户评估AI拟人化程度时，更关注交互线索（如对话流畅度、理解用户视角）而非理论特征（如感知或意识）。2. 拟人化设计确实能因果性地增加用户的拟人化感知。3. 但拟人化设计并未普遍增加用户参与度和信任的行为指标。4. 拟人化与行为结果之间的关系受到文化因素的调节：在某些文化中增加信任的设计，在其他文化中可能产生相反效果。

Conclusion: 研究挑战了"拟人化AI设计必然带来风险"的主流观点，揭示了人类-AI交互是一个受文化调节的复杂景观。这要求AI治理必须超越"一刀切"的方法，考虑文化多样性，采用更加细致入微的监管框架。

Abstract: Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.

</details>


### [88] [When Reasoning Meets Its Laws](https://arxiv.org/abs/2512.17901)
*Junyu Zhang,Yifan Sun,Tianang Leng,Jingyan Shen,Liu Ziyin,Paul Pu Liang,Huan Zhang*

Main category: cs.AI

TL;DR: 本文提出了推理定律（LoRe）框架，通过计算定律和准确率定律来形式化大型推理模型的理想推理行为，并开发了LoRe-Bench基准来评估模型的单调性和组合性，最后通过微调方法提升模型对计算定律的遵循度，从而提高推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）表现出色，但其推理行为常常违反直觉，导致推理能力不足。为了从理论上形式化理想的推理行为，需要建立一个统一的框架来表征LRMs的内在推理模式。

Method: 1. 提出推理定律（LoRe）框架，包括计算定律（假设推理计算量应与问题复杂度线性缩放）和补充的准确率定律；2. 由于问题复杂度难以量化，通过单调性和组合性这两个可处理的属性来检验这些假设；3. 开发LoRe-Bench基准系统性地测量大型推理模型的这两个属性；4. 提出有效的微调方法来强制模型遵循计算定律的组合性。

Result: 评估显示大多数推理模型表现出合理的单调性但缺乏组合性。通过微调方法强制遵循计算定律后，模型在多个基准测试上的推理性能得到一致提升，并揭示了不同属性和定律之间的协同效应。

Conclusion: 更好的遵循推理定律（特别是计算定律）能够持续提升大型推理模型的推理性能。LoRe框架为理解和改进LRMs的推理行为提供了理论基础和实用工具，通过增强组合性可以显著改善模型的推理能力。

Abstract: Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [89] [A Reproducible and Fair Evaluation of Partition-aware Collaborative Filtering](https://arxiv.org/abs/2512.17015)
*Domenico De Gioia,Claudio Pomo,Ludovico Boratto,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 对FPSR/FPSR+分区感知协同过滤模型的透明可复现基准测试，发现该模型家族虽具竞争力但并非始终最优，在长尾场景中表现突出，揭示了分区、全局组件和中心设计带来的精度-覆盖率权衡。


<details>
  <summary>Details</summary>
Motivation: 基于相似度的协同过滤模型虽然离线性能强且概念简单，但维护稠密物品相似度矩阵的二次成本限制了其可扩展性。分区感知范式在平衡效果和效率方面显示出潜力，但现有FPSR/FPSR+评估缺乏透明性和可复现性，且遗漏了部分相似度基线，难以进行公平比较。

Method: 提出了一个透明、完全可复现的FPSR和FPSR+基准测试框架。通过系统评估分区感知协同过滤模型，分析其设计选择（分区策略、全局组件、中心设计）对推荐性能的影响，特别关注长尾场景下的表现。

Result: FPSR模型家族并未始终达到最高性能水平，但整体保持竞争力，验证了其设计选择的有效性。在长尾场景中表现出显著优势，揭示了分区、全局组件和中心设计带来的精度-覆盖率权衡。研究明确了分区感知相似度建模最有益的应用场景。

Conclusion: 分区感知相似度建模在特定场景（特别是长尾推荐）中具有价值，但需要权衡精度和覆盖率。研究提供了在可复现协议下设计可扩展推荐系统的实用指导，强调了透明评估的重要性。

Abstract: Similarity-based collaborative filtering (CF) models have long demonstrated strong offline performance and conceptual simplicity. However, their scalability is limited by the quadratic cost of maintaining dense item-item similarity matrices. Partitioning-based paradigms have recently emerged as an effective strategy for balancing effectiveness and efficiency, enabling models to learn local similarities within coherent subgraphs while maintaining a limited global context. In this work, we focus on the Fine-tuning Partition-aware Similarity Refinement (FPSR) framework, a prominent representative of this family, as well as its extension, FPSR+. Reproducible evaluation of partition-aware collaborative filtering remains challenging, as prior FPSR/FPSR+ reports often rely on splits of unclear provenance and omit some similarity-based baselines, thereby complicating fair comparison. We present a transparent, fully reproducible benchmark of FPSR and FPSR+. Based on our results, the family of FPSR models does not consistently perform at the highest level. Overall, it remains competitive, validates its design choices, and shows significant advantages in long-tail scenarios. This highlights the accuracy-coverage trade-offs resulting from partitioning, global components, and hub design. Our investigation clarifies when partition-aware similarity modeling is most beneficial and offers actionable guidance for scalable recommender system design under reproducible protocols.

</details>


### [90] [Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations](https://arxiv.org/abs/2512.17027)
*Erica Coppolillo,Simone Mungari*

Main category: cs.IR

TL;DR: 对维基百科和Grokipedia搜索引擎的首次比较分析显示，两者都经常生成与原始查询弱相关的结果，有时甚至从无害查询中呈现意外内容，但两个系统为相同查询产生的推荐集存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成的百科全书Grokipedia的出现，搜索引擎在不同百科系统中的行为尚未得到充分研究。本研究旨在填补这一空白，首次对维基百科和Grokipedia的搜索引擎进行对比分析。

Method: 使用近10,000个中性英语单词及其子字符串作为查询，收集超过70,000个搜索引擎结果，分析它们的语义对齐、重叠和主题结构，并进行主题标注和轨迹分析。

Result: 两个平台都经常生成与原始查询弱相关的结果，有时从无害查询中呈现意外内容。尽管有这些共同特性，两个系统为相同查询产生的推荐集存在显著差异。主题分布和查询建议方面也存在系统性差异。

Conclusion: 意外搜索结果在维基百科和Grokipedia中都是常见现象，但两个平台在主题分布和查询建议方面存在差异，这表明不同百科系统的搜索引擎行为存在系统性区别。

Abstract: Encyclopedic knowledge platforms are key gateways through which users explore information online. The recent release of Grokipedia, a fully AI-generated encyclopedia, introduces a new alternative to traditional, well-established platforms like Wikipedia. In this context, search engine mechanisms play an important role in guiding users exploratory paths, yet their behavior across different encyclopedic systems remains underexplored. In this work, we address this gap by providing the first comparative analysis of search engine in Wikipedia and Grokipedia.
  Using nearly 10,000 neutral English words and their substrings as queries, we collect over 70,000 search engine results and examine their semantic alignment, overlap, and topical structure. We find that both platforms frequently generate results that are weakly related to the original query and, in many cases, surface unexpected content starting from innocuous queries. Despite these shared properties, the two systems often produce substantially different recommendation sets for the same query. Through topical annotation and trajectory analysis, we further identify systematic differences in how content categories are surfaced and how search engine results evolve over multiple stages of exploration.
  Overall, our findings show that unexpected search engine outcomes are a common feature of both the platforms, even though they exhibit discrepancies in terms of topical distribution and query suggestions.

</details>


### [91] [TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval](https://arxiv.org/abs/2512.17164)
*Yu Yang,Feng Tian,Ping Chen*

Main category: cs.IR

TL;DR: TCDE提出了一种基于大语言模型的主题中心化双重扩展策略，通过同时扩展查询和文档来建立语义桥梁，改善检索效果。


<details>
  <summary>Details</summary>
Motivation: 传统的查询扩展和文档扩展通常单独应用，可能导致扩展后的查询（或文档）与相关文档（或查询）之间的语义不对齐。为了解决这个问题，需要一种能够同时处理查询和文档扩展的方法。

Method: TCDE使用大语言模型进行主题中心化的双重扩展：1）查询侧：识别查询中的不同子主题，为每个子主题生成聚焦的伪文档；2）文档侧：将文档提炼为一组核心主题句。这些输出用于扩展原始查询和文档。

Result: 在TREC Deep Learning和BEIR两个具有挑战性的基准测试中，TCDE相比现有扩展基线方法取得了显著改进。特别是在密集检索任务中，在SciFact数据集上NDCG@10相对提升了2.8%。

Conclusion: TCDE通过主题中心化的双重扩展策略，在查询和文档之间建立了语义桥梁，有效改善了检索模型的性能，验证了该方法的有效性。

Abstract: Query Expansion (QE) enriches queries and Document Expansion (DE) enriches documents, and these two techniques are often applied separately. However, such separate application may lead to semantic misalignment between the expanded queries (or documents) and their relevant documents (or queries). To address this serious issue, we propose TCDE, a dual expansion strategy that leverages large language models (LLMs) for topic-centric enrichment on both queries and documents. In TCDE, we design two distinct prompt templates for processing each query and document. On the query side, an LLM is guided to identify distinct sub-topics within each query and generate a focused pseudo-document for each sub-topic. On the document side, an LLM is guided to distill each document into a set of core topic sentences. The resulting outputs are used to expand the original query and document. This topic-centric dual expansion process establishes semantic bridges between queries and their relevant documents, enabling better alignment for downstream retrieval models. Experiments on two challenging benchmarks, TREC Deep Learning and BEIR, demonstrate that TCDE achieves substantial improvements over strong state-of-the-art expansion baselines. In particular, on dense retrieval tasks, it outperforms several state-of-the-art methods, with a relative improvement of 2.8\% in NDCG@10 on the SciFact dataset. Experimental results validate the effectiveness of our topic-centric and dual expansion strategy.

</details>


### [92] [Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest](https://arxiv.org/abs/2512.17277)
*Saeed Ebrahimi,Weijie Jiang,Jaewon Yang,Olafur Gudmundsson,Yucheng Tu,Huizhong Duan*

Main category: cs.IR

TL;DR: Pinterest提出了一套轻量级解决方案，通过残差连接、分数正则化和流形混合等技术，在仅增加5%参数的情况下，将冷启动内容的用户参与度提升了10%。


<details>
  <summary>Details</summary>
Motivation: 在Pinterest这样的视觉发现平台中，推荐系统对冷启动物品的预测效果不佳是一个关键问题。冷启动物品在训练数据中出现频率低，导致模型难以准确预测，影响用户体验和平台内容新鲜度。

Method: 针对冷启动问题的四个挑战提出了相应解决方案：1）设计轻量级方案，总参数仅增加5%；2）为非历史特征引入残差连接以提升其重要性；3）加入分数正则化项来平衡冷启动和非冷启动物品的预测分数；4）应用流形混合技术缓解标签稀疏问题。

Result: 这些方法在Pinterest平台上成功部署，服务超过5.7亿用户，将新鲜内容的用户参与度提升了10%，同时没有对整体参与度和成本产生负面影响。

Conclusion: 该研究提出了一套有效解决工业级推荐系统中冷启动问题的轻量级方案，通过针对性的技术改进，在保持计算效率的同时显著提升了冷启动内容的推荐效果。

Abstract: Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.

</details>


### [93] [The Mental World of Large Language Models in Recommendation: A Benchmark on Association, Personalization, and Knowledgeability](https://arxiv.org/abs/2512.17389)
*Guangneng Hu*

Main category: cs.IR

TL;DR: LRWorld基准测试评估LLM在推荐系统中的能力，包含38K样本和23M token，覆盖关联性、个性化、知识性三个维度，发现LLM在深度个性化嵌入方面表现不佳，但在浅层记忆相似度、实体关系推理和多模态知识方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面评估LLM在推荐系统中局限性和边界的基准测试，LLM与推荐系统之间存在语义鸿沟（语言世界知识vs个性化行为世界），需要系统评估LLM在推荐任务中的能力。

Method: 提出LRWorld基准，包含38K高质量样本和23M token，从公开推荐数据集编译生成。将LLM在推荐中的心智世界分为三个主要维度（关联性、个性化、知识性），涵盖10个因素和31个测量任务。

Result: 对数十个LLM的实验表明：1）LLM不擅长捕捉深度神经个性化嵌入，但在浅层记忆项目相似度上表现良好；2）在推断用户兴趣时擅长感知项目实体关系、层次分类和关联规则；3）在多模态知识推理（电影海报和产品图像）和噪声配置文件鲁棒性方面有潜力；4）没有模型在所有十个因素上表现一致优秀。

Conclusion: LLM在推荐系统中表现出混合能力：在浅层相似度和实体关系推理方面有效，但在深度个性化方面有限。需要更全面的基准来评估LLM在推荐中的边界，模型大小、位置偏差等因素影响性能。

Abstract: Large language models (LLMs) have shown potential in recommendation systems (RecSys) by using them as either knowledge enhancer or zero-shot ranker. A key challenge lies in the large semantic gap between LLMs and RecSys where the former internalizes language world knowledge while the latter captures personalized world of behaviors. Unfortunately, the research community lacks a comprehensive benchmark that evaluates the LLMs over their limitations and boundaries in RecSys so that we can draw a confident conclusion. To investigate this, we propose a benchmark named LRWorld containing over 38K high-quality samples and 23M tokens carefully compiled and generated from widely used public recommendation datasets. LRWorld categorizes the mental world of LLMs in RecSys as three main scales (association, personalization, and knowledgeability) spanned by ten factors with 31 measures (tasks). Based on LRWorld, comprehensive experiments on dozens of LLMs show that they are still not well capturing the deep neural personalized embeddings but can achieve good results on shallow memory-based item-item similarity. They are also good at perceiving item entity relations, entity hierarchical taxonomies, and item-item association rules when inferring user interests. Furthermore, LLMs show a promising ability in multimodal knowledge reasoning (movie poster and product image) and robustness to noisy profiles. None of them show consistently good performance over the ten factors. Model sizes, position bias, and more are ablated.

</details>


### [94] [A Systematic Reproducibility Study of BSARec for Sequential Recommendation](https://arxiv.org/abs/2512.17442)
*Jan Hutter,Hua Chang Bakker,Stan Fris,Madelon Bernardy,Yuanna Liu*

Main category: cs.IR

TL;DR: BSARec通过傅里叶变换增强Transformer的高频信号捕获能力，但在系统验证中发现DSP方法相比简单残差连接优势有限，非恒定填充策略对性能提升更关键。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer自注意力机制在序列推荐中作为低通滤波器，限制了捕捉反映短期用户兴趣的高频信号的能力。

Method: BSARec在Transformer编码器中添加频率层，使用傅里叶变换重新缩放高频分量；同时提出量化用户历史频率的指标，评估不同用户组上的SR方法性能。

Result: BSARec在某些数据集上优于其他SR方法；离散小波变换相比傅里叶变换仅有轻微改进；DSP方法相比简单残差连接没有明显优势；非恒定填充策略显著提升推荐性能。

Conclusion: 虽然BSARec通过频率增强改进了Transformer的高频信号捕获，但DSP方法的实际优势有限，而填充策略对模型性能的影响更为关键。

Abstract: In sequential recommendation (SR), the self-attention mechanism of Transformer-based models acts as a low-pass filter, limiting their ability to capture high-frequency signals that reflect short-term user interests. To overcome this, BSARec augments the Transformer encoder with a frequency layer that rescales high-frequency components using the Fourier transform. However, the overall effectiveness of BSARec and the roles of its individual components have yet to be systematically validated. We reproduce BSARec and show that it outperforms other SR methods on some datasets. To empirically assess whether BSARec improves performance on high-frequency signals, we propose a metric to quantify user history frequency and evaluate SR methods across different user groups. We compare digital signal processing (DSP) techniques and find that the discrete wavelet transform (DWT) offer only slight improvements over Fourier transforms, and DSP methods provide no clear advantage over simple residual connections. Finally, we explore padding strategies and find that non-constant padding significantly improves recommendation performance, whereas constant padding hinders the frequency rescaler's ability to capture high-frequency signals.

</details>


### [95] [Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application](https://arxiv.org/abs/2512.17462)
*Olivier Jeunen,Schaun Wheeler*

Main category: cs.IR

TL;DR: 代理个性化消息在金融服务应用中减少退订21%，促进提前报税行为


<details>
  <summary>Details</summary>
Motivation: 评估代理个性化方法在金融服务客户沟通系统中的行为和留存效果，特别是在2025年全国报税期间

Method: 通过为期两个月的随机对照试验，比较代理消息方法与常规规则式活动系统，关注退订行为和转化时机两个主要结果

Result: 代理主导的消息使退订事件相对常规系统减少21%（±0.01），并在国家截止日期前几周增加了提前报税行为

Conclusion: 自适应、用户级决策系统能够调节参与强度，同时改善长期留存指标

Abstract: Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\% ($\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.

</details>


### [96] [Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure](https://arxiv.org/abs/2512.17733)
*Jingmao Zhang,Zhiting Zhao,Yunqi Lin,Jianghong Ma,Tianjun Wei,Haijun Zhang,Xiaofeng Zhang*

Main category: cs.IR

TL;DR: Cadence是一个基于因果去混杂的多样性推荐框架，通过构建无偏非对称共购关系图来提升推荐多样性同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要依赖共现关系，容易受到商品流行度和用户属性的偏差影响，导致嵌入质量和性能下降。同时，虽然多样性被认为是推荐质量的关键方面，但现有研究对其关注有限，缺乏因果视角和理论基础。

Method: 1. 计算无偏非对称共购关系（UACR），排除商品流行度和用户属性影响，构建去混杂的有向商品图，并通过聚合机制优化嵌入表示。2. 利用UACR识别与用户交互商品有强因果相关性但尚未被接触的多样化商品类别，在高曝光场景下模拟用户行为，显著提升推荐多样性同时保持相关性。

Result: 在真实世界数据集上的大量实验表明，该方法在多样性和准确性方面均优于最先进的多样性模型，并验证了其有效性、可迁移性和效率。

Conclusion: Cadence是一个即插即用的框架，基于LightGCN作为骨干网络，主要设计用于在保持准确性的同时增强推荐多样性，通过因果去混杂和反事实曝光机制有效解决了现有方法的局限性。

Abstract: Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user's interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.

</details>
