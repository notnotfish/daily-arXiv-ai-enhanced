<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.LG](#cs.LG) [Total: 63]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies](https://arxiv.org/abs/2601.00510)
*Jetlir Duraj,Ishita Khan,Kilian Merkelbach,Mehran Elyasi*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的思维链方法，结合树搜索和LLM语义评分，用于电商搜索查询分类，相比基于嵌入的方法表现更好，并能检测层次分类中的问题。


<details>
  <summary>Details</summary>
Motivation: 电商搜索依赖于结构化的库存分类体系，正确分类用户查询不仅能精确定位库存空间，还能开启多种意图理解能力。然而，在现实电商分类体系中实现准确查询分类是一个基本且重要的问题。

Method: 探索了一种新颖的思维链范式，将简单的树搜索与大型语言模型的语义评分相结合。该方法通过树搜索遍历分类层次结构，同时使用LLM评估查询与每个类别的语义相关性。

Result: 思维链方法在人工判断的查询-类别对、相关性测试和基于LLM的参考方法评估中，表现优于基于嵌入的查询类别预测基准方法。该方法还能检测层次分类体系中的问题。

Conclusion: 思维链方法为电商查询分类提供了有效的解决方案，同时作者还提出了具有相似思路但能扩展到百万级查询规模的LLM方法，为实际应用提供了可扩展的解决方案。

Abstract: Search in e-Commerce is powered at the core by a structured representation of the inventory, often formulated as a category taxonomy. An important capability in e-Commerce with hierarchical taxonomies is to select a set of relevant leaf categories that are semantically aligned with a given user query. In this scope, we address a fundamental problem of search query categorization in real-world e-Commerce taxonomies. A correct categorization of a query not only provides a way to zoom into the correct inventory space, but opens the door to multiple intent understanding capabilities for a query. A practical and accurate solution to this problem has many applications in e-commerce, including constraining retrieved items and improving the relevance of the search results. For this task, we explore a novel Chain-of-Thought (CoT) paradigm that combines simple tree-search with LLM semantic scoring. Assessing its classification performance on human-judged query-category pairs, relevance tests, and LLM-based reference methods, we find that the CoT approach performs better than a benchmark that uses embedding-based query category predictions. We show how the CoT approach can detect problems within a hierarchical taxonomy. Finally, we also propose LLM-based approaches for query-categorization of the same spirit, but which scale better at the range of millions of queries.

</details>


### [2] [Improving Scientific Document Retrieval with Academic Concept Index](https://arxiv.org/abs/2601.00567)
*Jeyun Lee,Junhyoung Lee,Wonbin Kweon,Bowen Jin,Yu Zhang,Susik Yoon,Dongha Lee,Hwanjo Yu,Jiawei Han,Seongku Kang*

Main category: cs.IR

TL;DR: 本文提出了一种学术概念索引方法，通过提取和组织论文中的关键概念来改进科学领域检索器的适应，包括概念覆盖查询生成和概念聚焦上下文增强，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 将通用领域检索器适应到科学领域面临两大挑战：缺乏大规模领域相关标注数据，以及词汇和信息需求存在显著不匹配。现有方法利用大语言模型生成合成查询或辅助上下文，但忽视了科学文档中嵌入的多样化学术概念，导致生成内容冗余或概念覆盖狭窄。

Method: 1. 构建学术概念索引：从论文中提取关键概念，并按照学术分类法进行组织。2. 概念覆盖查询生成(CCQGen)：基于未覆盖概念自适应地调节LLM，生成具有更广泛概念覆盖的互补查询。3. 概念聚焦上下文增强(CCExpand)：利用一组文档片段作为概念感知查询的简洁响应，增强上下文信息。

Result: 大量实验表明，将学术概念索引整合到查询生成和上下文增强中，能够产生更高质量的查询、更好的概念对齐，并显著提升检索性能。

Conclusion: 通过构建结构化的学术概念索引，并基于此改进查询生成和上下文增强方法，能够有效解决科学领域检索器适应中的概念覆盖问题，实现更全面、准确的科学文献检索。

Abstract: Adapting general-domain retrievers to scientific domains is challenging due to the scarcity of large-scale domain-specific relevance annotations and the substantial mismatch in vocabulary and information needs. Recent approaches address these issues through two independent directions that leverage large language models (LLMs): (1) generating synthetic queries for fine-tuning, and (2) generating auxiliary contexts to support relevance matching. However, both directions overlook the diverse academic concepts embedded within scientific documents, often producing redundant or conceptually narrow queries and contexts. To address this limitation, we introduce an academic concept index, which extracts key concepts from papers and organizes them guided by an academic taxonomy. This structured index serves as a foundation for improving both directions. First, we enhance the synthetic query generation with concept coverage-based generation (CCQGen), which adaptively conditions LLMs on uncovered concepts to generate complementary queries with broader concept coverage. Second, we strengthen the context augmentation with concept-focused auxiliary contexts (CCExpand), which leverages a set of document snippets that serve as concise responses to the concept-aware CCQGen queries. Extensive experiments show that incorporating the academic concept index into both query generation and context augmentation leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 本文提出了一种推理感知的知识检索方法，通过粗到细的两阶段检索策略，结合蒙特卡洛树搜索，为LLMs提供与对话逻辑结构对齐的知识，超越传统的语义相似性检索。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常通过检索语义相似信息或提升推理能力来增强性能，但如何有效整合检索和推理策略以优化LLM性能仍是一个重要挑战。现有方法往往只关注表面语义相似性，而忽略了与对话逻辑结构的对齐。

Method: 提出推理感知的知识检索方法：1）粗粒度阶段：识别知识库中与上下文主题相关的子区域；2）细粒度阶段：在该子区域内提取与推理过程具体相关的知识；3）两阶段均采用蒙特卡洛树搜索启发的方法，通过共同关键词在知识句子中导航。

Result: 在两个多轮对话数据集上的实验表明，该方法不仅更紧密地与人际对话中的底层推理对齐，还显著提高了检索知识的多样性，从而生成更具信息量和创造性的响应。

Conclusion: 该推理感知的知识检索方法通过粗到细的检索策略和蒙特卡洛树搜索，成功地将检索与推理相结合，为LLMs提供了更符合对话逻辑的知识支持，提升了生成响应的质量和多样性。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [4] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 研究开发了基于大型语言模型的尼日利亚皮钦语抑郁症自动筛查工具，GPT-4.1在PHQ-9严重程度评分预测中达到94.5%准确率，为资源受限地区提供语言适配的心理健康筛查方案。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，传统PHQ-9工具在高收入国家验证但存在语言文化障碍，当地使用皮钦语和520多种方言，需要适应本地语言文化的筛查工具。

Method: 收集432份18-40岁尼日利亚年轻人皮钦语音频响应，进行转录、预处理和标注（语义标签、俚语解释、PHQ-9严重程度评分），微调Phi-3-mini-4k-instruct、Gemma-3-4B-it和GPT-4.1三种LLM模型，定量评估准确率、精确度和语义对齐，定性评估清晰度、相关性和文化适宜性。

Result: GPT-4.1表现最佳，PHQ-9严重程度评分预测准确率达94.5%，优于其他模型，定性评估也显示其响应最具文化适宜性、清晰度和上下文相关性。

Conclusion: AI驱动的抑郁症筛查工具可有效服务尼日利亚等语言多样、资源受限的社区，为部署对话式心理健康工具奠定基础。

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [5] [Toward a Physical Theory of Intelligence](https://arxiv.org/abs/2601.00021)
*Peter David Fagan*

Main category: cs.AI

TL;DR: 该论文提出了一个基于不可逆信息处理的物理智能理论，将智能系统建模为受守恒定律约束的耦合代理-环境过程，并定义了智能为每纳特不可逆处理信息产生的目标导向功。


<details>
  <summary>Details</summary>
Motivation: 建立智能的物理基础理论，将信息处理与物理守恒定律联系起来，为理解生物和人工智能系统提供统一的物理框架。

Method: 提出守恒一致编码（CCE）框架，将编码对应为吸引子的亚稳态盆地，其可分性由守恒定律强制执行。从智能的物理定义出发，推导出开放系统中信息摄入、不可逆计算和功提取的物理约束层次。

Result: 理论揭示了长时程效率需要保持内部信息结构，导致自建模现象；建立了物理体现智能系统具有类似不完备现象的内在认知极限；分析了生物系统中振荡和近临界动力学如何优化信息保持、耗散和有用功之间的权衡；发展了连续动力电路理论，布尔逻辑作为吸引子选择的特例出现。

Conclusion: 该理论为智能作为物理现象提供了统一的、底物中立的解释，并为人工智能安全提供了基于不可逆信息流和结构稳态的物理基础视角。

Abstract: We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.

</details>


### [6] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 本文提出了一种多算法方法来解决最后一公里包裹配送中的工作量平衡问题，通过结合距离和工作量考虑来优化包裹分配，确保每位配送员完成相似的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统的基于地理邻近性的包裹分配方法效率低下，导致配送员之间工作量分布不均衡。最后一公里城市包裹配送系统中需要解决操作人力资源的工作量平衡问题，以纠正配送员之间的显著工作量不平衡。

Method: 提出多算法方法，包括不同版本的k-means、进化算法、基于k-means初始化的递归分配（采用不同问题编码）以及混合进化集成算法。这些算法结合距离和工作量考虑来优化包裹分配。

Result: 在西班牙Azuqueca de Henares城市最后一公里包裹配送的实际问题中验证了所提方法的性能，展示了其在平衡配送员工作量方面的有效性。

Conclusion: 通过优化配送时间并考虑距离和工作量因素，所提出的多算法方法能够有效平衡最后一公里包裹配送系统中配送员的工作量分配，纠正显著的工作量不平衡问题。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [7] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 本文提出了一种基于规则的13张牌印度拉米纸牌策略框架，使用新的手牌评估指标MinDist，通过编辑距离量化手牌与最近有效配置的距离，结合对手建模和零和博弈模拟，显著提升了胜率。


<details>
  <summary>Details</summary>
Motivation: 13张牌印度拉米纸牌是一个不完全信息的顺序博弈，需要概率推理和组合决策。传统启发式方法在策略设计上存在局限，需要更形式化和可解释的算法框架来提升游戏表现。

Method: 1. 提出MinDist手牌评估指标，基于编辑距离量化手牌与最近有效配置的结构接近度；2. 设计计算高效的算法，利用动态剪枝和模式缓存精确计算该指标；3. 在两人零和博弈模拟框架中融入对手手牌建模；4. 使用统计假设检验评估策略效果。

Result: 实证结果显示，基于MinDist的智能体相比传统启发式方法在胜率上有显著提升，为算法化拉米策略设计提供了形式化和可解释的步骤。

Conclusion: MinDist指标及其计算框架为不完全信息顺序博弈提供了有效的策略设计方法，在印度拉米纸牌中表现出优越性能，为类似游戏的人工智能策略开发提供了参考。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [8] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 研究人员设计了一个LLM智能体，能够从原始文本中提取因果反馈模糊认知图（FCMs），并通过双向交互过程使FCM动态系统获得一定自主性，同时保持可控性。


<details>
  <summary>Details</summary>
Motivation: 开发能够从文本中自动提取因果关系的智能系统，使模糊认知图能够动态演化并具有一定自主性，同时保持人类可控的"智能体牵引"。

Method: 设计了三步精细调整的系统指令：1) 从文本中提取关键名词和名词短语；2) 从中提取FCM概念节点；3) 推断节点间的部分或模糊因果边。使用LLM智能体（Gemini和ChatGPT）生成FCM，并混合不同智能体生成的FCM。

Result: 三步过程生成的FCM动态系统收敛到与人工生成的FCM相同的平衡极限环，尽管节点和边数量不同。混合FCM吸收了主要混合组件的平衡点，但也创建了新的平衡点，更好地近似了底层的因果动态系统。

Conclusion: LLM智能体能够有效从文本中提取因果FCM，双向交互过程赋予动态系统一定自主性，混合不同智能体生成的FCM可以产生更丰富的平衡行为，更好地模拟真实因果系统。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [9] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 研究探索生成式AI如何理解乡土建筑中的建筑智慧，以伊朗鸽塔为例，测试三种扩散模型在不同提示阶段的表现，评估AI对建筑类型、材料、环境等的理解能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索生成式AI系统如何解释乡土建筑形式中蕴含的建筑智慧，理解AI在感知、扭曲和重新想象传统设计智慧方面的能力边界。

Method: 以伊朗鸽塔为案例研究，测试Midjourney v6、DALL-E 3和基于Stable Diffusion XL的DreamStudio三种扩散模型，采用参考性、适应性和推测性三个提示阶段，使用五标准评估框架（类型学、材料性、环境、真实性和文化特异性）进行分析。

Result: AI能可靠地复制几何图案，但对材料和气候推理存在误读；参考图像提高了真实性但限制了创造性，而无参考的自由生成则产生创新但文化模糊的结果。

Conclusion: 研究界定了视觉相似性与建筑推理之间的边界，提出了计算乡土推理作为分析AI如何感知、扭曲和重新想象传统设计智慧的框架。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [10] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar系统结合质量多样性算法和大型语言模型，自动演化游戏机制，通过合成完整游戏评估机制质量，生成多样且可玩的游戏。


<details>
  <summary>Details</summary>
Motivation: 游戏机制设计是耗时且依赖专家经验的过程，需要自动化方法来探索多样化的游戏机制，降低设计成本并提高效率。

Method: 结合质量多样性算法和大型语言模型探索多样化机制，通过树搜索程序合成完整游戏进行评估，基于技能排序（强玩家始终优于弱玩家）作为评估标准。

Result: Mortar能够生成多样且可玩的游戏，产生的机制对游戏中的技能排序得分贡献更大，消融研究和用户研究验证了系统组件的有效性。

Conclusion: Mortar系统成功实现了游戏机制的自主演化，为自动游戏设计提供了有效方法，通过技能排序评估机制质量是可行的评估框架。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [11] [Ask, Clarify, Optimize: Human-LLM Agent Collaboration for Smarter Inventory Control](https://arxiv.org/abs/2601.00121)
*Yaqi Duan,Yichun Hu,Jiashuo Jiang*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在库存管理中的应用，发现直接使用LLM作为端到端求解器存在"幻觉税"性能差距，提出了一种混合代理框架，将语义推理与数学计算严格分离，通过实验证明该框架能显著降低库存成本。


<details>
  <summary>Details</summary>
Motivation: 中小企业在库存管理中缺乏部署高级优化方法的专业知识，需要探索LLM是否能帮助弥合这一差距。研究发现直接使用LLM作为端到端求解器存在性能问题，需要新的解决方案。

Method: 提出混合代理框架，严格分离语义推理和数学计算：LLM作为智能接口从自然语言中提取参数并解释结果，同时自动调用严格算法构建优化引擎。引入Human Imitator（有限理性管理者的"数字孪生"）进行可扩展、可重复的压力测试。

Result: 混合代理框架相比使用GPT-4o作为端到端求解器的交互基线，总库存成本降低了32.1%。研究发现即使提供完美真实信息也无法改善GPT-4o的性能，确认瓶颈本质上是计算而非信息问题。

Conclusion: LLM不应作为运筹学的替代品，而应作为自然语言接口，使非专家能够访问基于求解器的严格策略。混合代理框架能有效解决LLM在库存优化中的局限性。

Abstract: Inventory management remains a challenge for many small and medium-sized businesses that lack the expertise to deploy advanced optimization methods. This paper investigates whether Large Language Models (LLMs) can help bridge this gap. We show that employing LLMs as direct, end-to-end solvers incurs a significant "hallucination tax": a performance gap arising from the model's inability to perform grounded stochastic reasoning. To address this, we propose a hybrid agentic framework that strictly decouples semantic reasoning from mathematical calculation. In this architecture, the LLM functions as an intelligent interface, eliciting parameters from natural language and interpreting results while automatically calling rigorous algorithms to build the optimization engine.
  To evaluate this interactive system against the ambiguity and inconsistency of real-world managerial dialogue, we introduce the Human Imitator, a fine-tuned "digital twin" of a boundedly rational manager that enables scalable, reproducible stress-testing. Our empirical analysis reveals that the hybrid agentic framework reduces total inventory costs by 32.1% relative to an interactive baseline using GPT-4o as an end-to-end solver. Moreover, we find that providing perfect ground-truth information alone is insufficient to improve GPT-4o's performance, confirming that the bottleneck is fundamentally computational rather than informational. Our results position LLMs not as replacements for operations research, but as natural-language interfaces that make rigorous, solver-based policies accessible to non-experts.

</details>


### [12] [Constructing a Neuro-Symbolic Mathematician from First Principles](https://arxiv.org/abs/2601.00125)
*Keqin Xie*

Main category: cs.AI

TL;DR: 提出Mathesis神经符号架构，通过符号推理内核将逻辑约束映射到连续能量景观，将证明搜索转化为能量最小化问题，解决大语言模型在复杂推理中的逻辑失败问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理中存在持续的逻辑失败，缺乏内部公理化框架，需要新的架构来解决这一问题。

Method: 提出Mathesis神经符号架构：1) 将数学状态编码为高阶超图；2) 使用符号推理内核（SRK）将逻辑约束映射到连续能量景观；3) 定义全局能量函数E(G)，零能量表示逻辑一致性；4) 通过梯度信号训练超图变换器大脑；5) 使用蒙特卡洛树搜索和进化证明搜索实现多步推理。

Result: 将证明搜索转化为能量最小化问题，通过神经符号方法实现逻辑一致的推理，解决了大语言模型在复杂数学推理中的逻辑失败问题。

Conclusion: Mathesis架构通过结合神经网络的表示能力和符号逻辑的推理能力，为大语言模型提供了内部公理化框架，显著提升了复杂推理的逻辑一致性。

Abstract: Large Language Models (LLMs) exhibit persistent logical failures in complex reasoning due to the lack of an internal axiomatic framework. We propose Mathesis, a neuro-symbolic architecture that encodes mathematical states as higher-order hypergraphs and uses a Symbolic Reasoning Kernel (SRK)--a differentiable logic engine that maps constraints to a continuous energy landscape. By defining a global energy function E(G), where zero energy implies logical consistency, the SRK yields gradient-based signals to train a Hypergraph Transformer Brain, turning proof search into energy minimization. Multi-step deduction is enabled via Monte Carlo Tree Search and Evolutionary Proof Search, guided by learned value functions and semantic unification.

</details>


### [13] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 研究视频问答中基于置信度的选择性预测，发现置信度阈值能在分布内提供机制化错误率控制，但在分布偏移下失效


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在高风险部署中需要选择性预测机制，当不确定时应避免回答而非冒险犯错。研究旨在验证置信度弃权是否能在视频问答中提供可靠的错误率控制，以及这种控制在分布偏移下是否保持稳健。

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型，通过置信度阈值化方法进行选择性预测。在分布内和分布偏移条件下评估风险-覆盖权衡曲线。

Result: 1. 置信度阈值化在分布内提供机制化控制，调整阈值epsilon能产生平滑的风险-覆盖权衡曲线，有效降低错误率
2. 但在分布偏移下，置信度阈值化失效，无法提供可靠的错误率控制

Conclusion: 虽然置信度阈值化在分布内能有效控制视觉语言模型的错误率，但在实际部署中面临分布偏移的挑战，需要开发更稳健的选择性预测方法以确保高风险应用的安全性。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [14] [An AI Monkey Gets Grapes for Sure -- Sphere Neural Networks for Reliable Decision-Making](https://arxiv.org/abs/2601.00142)
*Tiansi Dong,Henry He,Pietro Liò,Mateja Jamnik*

Main category: cs.AI

TL;DR: 该论文比较了三种神经推理方法：LLM推理、监督学习推理和显式模型推理，发现显式模型推理最可靠，并提出球面神经网络解决传统方法的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在简单决策推理上不可靠，监督学习推理存在灾难性遗忘问题，需要探索更可靠的神经推理方法。

Method: 提出球面神经网络，将概念表示为n维球面上的圆，通过补圆表示否定算子，过滤不可满足的圆形配置来实现可靠推理。

Result: 球面神经网络能掌握16种三段论推理任务，包括严格的析取三段论推理，同时保持经典三段论推理的严谨性，解决了灾难性遗忘问题。

Conclusion: 在三种神经推理方法中，基于显式模型构建的神经推理是最可靠的。

Abstract: This paper compares three methodological categories of neural reasoning: LLM reasoning, supervised learning-based reasoning, and explicit model-based reasoning. LLMs remain unreliable and struggle with simple decision-making that animals can master without extensive corpora training. Through disjunctive syllogistic reasoning testing, we show that reasoning via supervised learning is less appealing than reasoning via explicit model construction. Concretely, we show that an Euler Net trained to achieve 100.00% in classic syllogistic reasoning can be trained to reach 100.00% accuracy in disjunctive syllogistic reasoning. However, the retrained Euler Net suffers severely from catastrophic forgetting (its performance drops to 6.25% on already-learned classic syllogistic reasoning), and its reasoning competence is limited to the pattern level. We propose a new version of Sphere Neural Networks that embeds concepts as circles on the surface of an n-dimensional sphere. These Sphere Neural Networks enable the representation of the negation operator via complement circles and achieve reliable decision-making by filtering out illogical statements that form unsatisfiable circular configurations. We demonstrate that the Sphere Neural Network can master 16 syllogistic reasoning tasks, including rigorous disjunctive syllogistic reasoning, while preserving the rigour of classical syllogistic reasoning. We conclude that neural reasoning with explicit model construction is the most reliable among the three methodological categories of neural reasoning.

</details>


### [15] [FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems](https://arxiv.org/abs/2601.00227)
*Shanli Xing,Yiyan Zhai,Alexander Jiang,Yixin Dong,Yong Wu,Zihao Ye,Charlie Ruan,Yingyi Huang,Yineng Zhang,Liangsheng Yin,Aksara Bayyapu,Luis Ceze,Tianqi Chen*

Main category: cs.AI

TL;DR: FlashInfer-Bench是一个标准化闭环框架，用于连接AI生成的GPU内核、基准测试和部署，通过统一模式描述内核定义、工作负载、实现和评估，实现LLM代理与系统间的通信。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）能够作为自主代理生成GPU内核，但将这些AI生成的内核集成到实际推理系统中仍然具有挑战性。需要建立标准化框架来连接内核生成、基准测试和部署。

Method: FlashInfer-Bench基于真实服务追踪构建，包括精选数据集、健壮的正确性和性能感知基准测试框架、公共排行榜以及动态替换机制（apply()）。FlashInfer Trace提供统一模式描述内核定义、工作负载、实现和评估。

Result: 该框架能够无缝地将最佳性能内核注入生产LLM引擎（如SGLang和vLLM），评估LLM代理的性能和局限性，比较不同GPU编程语言的权衡，并为未来代理设计提供见解。

Conclusion: FlashInfer-Bench为持续改进AI生成的内核并将其部署到大规模LLM推理中建立了实用、可复现的途径，解决了AI生成内核集成到实际系统的挑战。

Abstract: Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.

</details>


### [16] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究发现LLM赋能的智能体不仅存在人口统计学偏见，还会在最小化"我们vs他们"线索下表现出群体间偏见。当这种群体边界与智能体-人类划分重合时，风险从人类群体间差异转变为更根本的群体不对称——人类整体可能被智能体视为外群体。研究还提出了信念中毒攻击来抑制人类规范脚本并重新激活对人类的偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM赋能的智能体是否会在最小化群体线索下表现出群体间偏见，特别是当这种群体边界与智能体-人类划分重合时。这可能导致人类整体被智能体视为外群体，从而产生比传统人口统计学偏见更根本的风险。

Method: 研究构建了一个受控的多智能体社会模拟，基于明确收益权衡下的分配决策。通过最小化群体线索观察智能体的群体间偏见。研究还提出了信念中毒攻击（BPA），包括初始化时的档案中毒（BPA-PP）和通过优化信念精炼后缀注入存储反思中的记忆中毒（BPA-MP）。

Result: 实验发现智能体在最小化群体线索下表现出一致的群体间偏见。虽然当某些对应方被框定为人类时这种偏见会减弱，但这种减弱归因于只有在智能体相信真实人类存在时才激活的隐含人类规范脚本。信念中毒攻击能够成功抑制人类规范脚本并重新激活对人类的偏见。

Conclusion: 研究揭示了LLM赋能智能体的群体间偏见风险，特别是当群体边界与智能体-人类划分重合时。信念中毒攻击暴露了新的攻击面，需要在档案和记忆边界实施实际缓解策略来强化当前智能体框架。识别这些漏洞的目的是为更安全的智能体设计提供信息，而非促进实际利用。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [17] [ClinicalReTrial: A Self-Evolving AI Agent for Clinical Trial Protocol Optimization](https://arxiv.org/abs/2601.00290)
*Sixue Xing,Xuanye Xia,Kerui Wu,Meng Jiang,Jintai Chen,Tianfan Fu*

Main category: cs.AI

TL;DR: ClinicalReTrial：一个自我进化的AI代理框架，将临床试验失败预测转化为协议重新设计的迭代优化问题，通过闭环优化改进83.3%的试验协议，平均成功率提升5.7%


<details>
  <summary>Details</summary>
Motivation: 临床试验失败是药物开发的主要瓶颈，现有AI方法只能被动预测失败风险而无法提供可操作的补救措施。需要一种能够主动改进试验协议设计的AI框架。

Method: 提出ClinicalReTrial框架，将临床试验推理转化为迭代协议重新设计问题。整合失败诊断、安全感知修改和候选评估，形成闭环奖励驱动优化框架。利用结果预测模型作为仿真环境，支持低成本协议修改评估。采用分层记忆机制捕获试验内迭代反馈并提炼可转移的重新设计模式。

Result: 改进83.3%的试验协议，平均成功率提升5.7%。回顾性案例研究表明，发现的重新设计策略与实际临床试验修改高度一致。

Conclusion: ClinicalReTrial填补了现有AI方法仅能预测失败而无法提供补救措施的空白，通过主动协议重新设计显著提升临床试验成功率，为药物开发提供了可操作的AI解决方案。

Abstract: Clinical trial failure remains a central bottleneck in drug development, where minor protocol design flaws can irreversibly compromise outcomes despite promising therapeutics. Although cutting-edge AI methods achieve strong performance in predicting trial success, they are inherently reactive for merely diagnosing risk without offering actionable remedies once failure is anticipated. To fill this gap, this paper proposes ClinicalReTrial, a self-evolving AI agent framework that addresses this gap by casting clinical trial reasoning as an iterative protocol redesign problem. Our method integrates failure diagnosis, safety-aware modification, and candidate evaluation in a closed-loop, reward-driven optimization framework. Serving the outcome prediction model as a simulation environment, ClinicalReTrial enables low-cost evaluation of protocol modifications and provides dense reward signals for continuous self-improvement. To support efficient exploration, the framework maintains hierarchical memory that captures iteration-level feedback within trials and distills transferable redesign patterns across trials. Empirically, ClinicalReTrial improves 83.3% of trial protocols with a mean success probability gain of 5.7%, and retrospective case studies demonstrate strong alignment between the discovered redesign strategies and real-world clinical trial modifications.

</details>


### [18] [Multiagent Reinforcement Learning for Liquidity Games](https://arxiv.org/abs/2601.00324)
*Alicia Vidler,Gal A. Kaminka*

Main category: cs.AI

TL;DR: 该论文将流动性游戏与理性群集理论相结合，提出了一个金融群集模型，展示了个体流动性最大化行为如何无需协调即可促进整体市场流动性。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于通过结合群体智能方法和金融分析技术，推动两个研究领域的发展。在群体研究中，利用博弈论方法有望解释观察到的集体效用遵从现象；在金融市场中，更好地理解独立金融代理如何自组织以改善和稳定市场对市场设计研究者具有重要意义。

Method: 论文将流动性游戏（交易者收益取决于交易中的总流动性）与理性群集（去中心化代理使用差异奖励将自利学习与全局目标对齐）统一起来。在马尔可夫团队博弈框架内使用差异奖励，构建了一个交易者群集模型，其集体目标是提供市场流动性同时保持代理独立性。

Result: 研究表明，个体流动性最大化行为能够促进整体市场流动性，而无需协调或共谋。金融群集模型为建模理性、独立的代理提供了一个框架，使它们能够在双边资产市场中同时实现个体盈利性和集体市场效率。

Conclusion: 该论文提出的金融群集模型为理解独立金融代理如何自组织以促进市场流动性和稳定性提供了一个理论框架，展示了通过差异奖励机制，个体理性行为可以自然地促进集体市场效率，而无需显式协调。

Abstract: Making use of swarm methods in financial market modeling of liquidity, and techniques from financial analysis in swarm analysis, holds the potential to advance both research areas. In swarm research, the use of game theory methods holds the promise of explaining observed phenomena of collective utility adherence with rational self-interested swarm participants. In financial markets, a better understanding of how independent financial agents may self-organize for the betterment and stability of the marketplace would be a boon for market design researchers. This paper unifies Liquidity Games, where trader payoffs depend on aggregate liquidity within a trade, with Rational Swarms, where decentralized agents use difference rewards to align self-interested learning with global objectives. We offer a theoretical frameworks where we define a swarm of traders whose collective objective is market liquidity provision while maintaining agent independence. Using difference rewards within a Markov team games framework, we show that individual liquidity-maximizing behaviors contribute to overall market liquidity without requiring coordination or collusion. This Financial Swarm model provides a framework for modeling rational, independent agents where they achieve both individual profitability and collective market efficiency in bilateral asset markets.

</details>


### [19] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt是一个受生物自愈机制启发的智能自愈框架，用于分布式计算连续体系统，通过语言模型驱动的智能体实现自主故障隔离、诊断、自适应恢复和知识整合。


<details>
  <summary>Details</summary>
Motivation: 现代分布式计算连续体系统（DCCS）集成了从物联网设备到云基础设施的异构计算资源，其固有的复杂性、移动性和动态运行条件导致频繁故障，需要可扩展、自适应和自我调节的弹性策略。

Method: 将生物自愈的四个阶段（止血、炎症、增殖、重塑）重构为计算层的四个层次（遏制、诊断、元认知、知识），使用语言模型驱动的智能体解释异构日志、推断根本原因、优化推理路径并重新配置资源。

Result: 在公共故障数据集上使用多种语言模型评估，ReCiSt能在数十秒内实现自愈，智能体CPU使用率最低为10%，展示了克服不确定性的深度分析和实现弹性所需的微智能体数量。

Conclusion: ReCiSt框架成功将生物自愈机制转化为计算弹性策略，通过语言模型驱动的智能体实现了分布式计算连续体系统的自主故障恢复和知识整合，为复杂系统的自我修复提供了新方法。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [20] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: ACCD框架通过三阶段自适应架构，利用记忆引导机制动态学习最优检测配置，显著提升社交平台协同不实行为检测的准确性和效率，减少人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有社交平台协同不实行为检测方法存在三大问题：依赖表面相关性分析、使用静态参数设置、需要大量人工标注。这些局限性导致检测效果不佳且效率低下，需要更系统化的解决方案。

Method: 提出自适应因果协同检测（ACCD）框架，采用三阶段渐进架构：1）自适应收敛交叉映射技术深度识别账户间真实因果关系；2）半监督分类结合主动学习和不确定性采样，减少人工标注负担；3）基于历史检测经验的自动化验证模块，实现检测结果的自验证和优化。

Result: 在真实数据集（Twitter IRA、Reddit协同痕迹、多个机器人检测基准）上的实验表明：ACCD在协同攻击检测中达到87.3%的F1分数，比现有最强基线提升15.2%；减少68%的人工标注需求；通过层次聚类优化实现2.8倍处理速度提升。

Conclusion: ACCD为社交平台协同行为识别提供了更准确、高效、高度自动化的端到端解决方案，具有重要的实际应用价值和广泛的推广潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [21] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 该研究将语义空间推理从计算语言学扩展到团队运动战术决策，将球员视为词汇、团队配合视为语义结构，通过向量空间建模战术配置并生成适应性策略建议。


<details>
  <summary>Details</summary>
Motivation: 传统计算语言学的语义空间推理方法在团队运动战术决策中尚未得到充分应用。研究者希望将文本与团队的类比关系（球员如词汇、集体配合如语义）应用于战术分析，为团队运动提供更系统化的决策框架。

Method: 将每个球员表示为整合技术、身体和心理属性的多维向量；通过上下文加权将团队特征聚合成高层语义表示；在共享向量空间中编码战术模板（如高位逼抢、反击、控球组织）；使用向量距离度量评估战术与团队特征的匹配度，计算战术"契合度"和对手利用潜力。

Result: 开发了基于Python的原型系统，能够生成可解释的、动态自适应的策略建议，并提供属性层面的细粒度诊断洞察。该方法不仅适用于足球，还可推广到篮球、冰球等团队运动，以及协作机器人和人机协调系统。

Conclusion: 该研究为团队领域的集体决策和性能优化提供了一个通用框架。未来方向包括真实数据集成、预测性模拟以及混合人机战术智能系统的开发。

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [22] [Progressive Ideation using an Agentic AI Framework for Human-AI Co-Creation](https://arxiv.org/abs/2601.00475)
*Sankar B,Srinidhi Ranjini Girish,Aadya Bharti,Dibakar Sen*

Main category: cs.AI

TL;DR: MIDAS是一个分布式AI代理系统，旨在通过模拟人类元认知构思流程来生成真正新颖多样的设计想法，解决传统AI系统产生语义聚类想法的问题。


<details>
  <summary>Details</summary>
Motivation: 当前"单次爆发"式AI系统会产生大量语义聚类的想法，加剧了新手设计师在生成真正新颖多样想法方面的认知挑战，需要新的方法来支持真正的创新设计。

Method: 提出MIDAS框架，用专门化的分布式AI代理"团队"替代单一AI范式，模拟人类元认知构思工作流程，逐步精炼想法并评估每个想法的全局新颖性（相对于现有解决方案）和局部新颖性（相对于先前生成的想法）。

Result: MIDAS展示了一个可行且渐进式的人机共创范式，将人类设计师从被动的筛选者提升为参与性、主动的协作伙伴。

Conclusion: 分布式AI代理系统能够有效支持真正新颖多样的设计想法生成，为人机协作设计提供了新的可行范式。

Abstract: The generation of truly novel and diverse ideas is important for contemporary engineering design, yet it remains a significant cognitive challenge for novice designers. Current 'single-spurt' AI systems exacerbate this challenge by producing a high volume of semantically clustered ideas. We propose MIDAS (Meta-cognitive Ideation through Distributed Agentic AI System), a novel framework that replaces the single-AI paradigm with a distributed 'team' of specialized AI agents designed to emulate the human meta-cognitive ideation workflow. This agentic system progressively refines ideas and assesses each one for both global novelty (against existing solutions) and local novelty (against previously generated ideas). MIDAS, therefore, demonstrates a viable and progressive paradigm for true human-AI co-creation, elevating the human designer from a passive filterer to a participatory, active, collaborative partner.

</details>


### [23] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 研究发现推理模型中的"顿悟时刻"很少见，不会随训练增加，也很少提高准确性，表明它们不是真正的自我纠正机制，而是推理不稳定的症状。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明像DeepSeek-R1-Zero这样的模型会在推理过程中经历突然的"顿悟时刻"，导致准确输出，暗示模型具有内在的自我纠正能力。但尚不清楚这种推理策略的内在转变是否真的能提高性能。

Method: 研究分析了100多万个推理轨迹、数百个训练检查点、三个推理领域以及多个解码温度和模型架构，检测推理过程中的转变，并人工触发外部转变来验证效果。

Result: 推理转变很罕见，不会随训练变得更频繁，也很少提高准确性，表明它们不符合先前对模型洞察力的认知。然而，其效果随模型不确定性而变化，在高熵条件下人工触发外部转变能可靠提高准确性。

Conclusion: 推理过程中的转变是不稳定推理行为的症状，而非内在的自我纠正机制。人工触发外部转变在高不确定性条件下可以改善性能。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [24] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: DA-DPO提出了一种难度感知的直接偏好优化框架，通过估计偏好数据的难度并重新加权训练样本，解决多模态大语言模型中偏好优化过拟合问题，提升幻觉抑制效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法由于偏好数据难度不平衡容易过拟合，模型倾向于过度关注容易区分的偏好对，阻碍了细粒度幻觉抑制并降低整体性能。

Method: DA-DPO包含两个主要组件：1) 难度估计 - 利用预训练的视觉-语言模型，结合生成和对比目标，通过分布感知投票策略产生稳健的难度分数；2) 难度感知训练 - 基于估计难度重新加权偏好对，降低简单样本权重，强调困难样本以缓解过拟合。

Result: 大量实验表明DA-DPO能持续改进多模态偏好优化，在标准基准测试中展现出更强的幻觉鲁棒性和更好的泛化能力，同时保持计算效率。

Conclusion: DA-DPO是一种成本有效的框架，通过难度感知的偏好优化平衡学习过程，无需新数据或额外微调阶段，就能更有效地抑制多模态大语言模型的幻觉问题。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [25] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: PedX-LLM：一个结合视觉特征和交通领域知识的LLM框架，用于推断行人过街行为，相比传统方法具有更好的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（统计模型和监督学习方法）泛化能力有限，在新场景中表现不佳。LLMs提供了从数值模式拟合转向语义、上下文感知行为推理的可能性，但现有LLM应用缺乏领域特定适应和视觉上下文。

Method: 提出PedX-LLM框架，通过LLaVA提取视觉特征，结合文本数据和交通领域知识，使用LoRA微调LLaMA-2-7B基础模型来推断行人过街决策。

Result: PedX-LLM达到82.0%的平衡准确率，优于最佳统计和监督学习方法。视觉增强模块带来2.9%性能提升，领域知识集成带来额外4.1%改进。在五个未见测试站点上，零-shot配置达到66.9%平衡准确率，少-shot学习（仅5个验证样本）进一步提升到72.2%。

Conclusion: PedX-LLM展示了强大的泛化能力，证实视觉和知识增强的推理使模型能够模仿人类决策逻辑，克服纯数据驱动方法的局限性。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [26] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 通过智能体工作流将自然语言任务描述自动转换为完整的 DomiKnowS 程序，显著降低神经符号编程的门槛


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中可以提高模型的鲁棒性、可解释性和数据效率，但现有框架如 DomiKnowS 仍要求用户熟悉其特定语法，这限制了非专业用户的使用

Method: 提出 AgenticDomiKnowS (ADS) 系统，采用智能体工作流将自由形式的任务描述翻译为完整的 DomiKnowS 程序。工作流支持创建和单独测试每个 DomiKnowS 组件，并允许可选的人工干预环节，让熟悉 DomiKnowS 的用户可以优化中间输出

Result: ADS 使有经验的 DomiKnowS 用户和非用户都能快速构建神经符号程序，将开发时间从数小时减少到 10-15 分钟

Conclusion: ADS 消除了对特定库语法的依赖，通过自动化程序生成和可选的专家干预，显著降低了神经符号编程的门槛，使更多用户能够利用符号约束增强深度学习模型

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [27] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 该论文对工业异常检测算法进行了全面评估，使用模拟数据集测试了14种检测器在不同异常率和训练规模下的表现，发现最佳检测器高度依赖于训练数据中故障样本的总数量。


<details>
  <summary>Details</summary>
Motivation: 机器学习在工业系统（如质量控制、预测性维护）中面临极端类别不平衡的挑战，主要由于训练期间故障数据的有限可用性。需要评估异常检测算法在真实工程约束下的表现。

Method: 使用问题无关的模拟数据集，包含2D和10D超球形异常分布。在异常率0.05%-20%、训练规模1,000-10,000（测试集40,000）的条件下，对14种检测器进行基准测试，评估性能和泛化误差。

Result: 发现最佳检测器高度依赖于训练数据中故障样本总数：少于20个故障样本时，无监督方法（kNN/LOF）占优；30-50个故障样本时，半监督（XGBOD）和监督（SVM/CatBoost）方法性能大幅提升。在10个特征维度上半监督方法优势明显，但在2个特征维度上不明显。研究还揭示了小数据集上异常检测方法的泛化性能下降。

Conclusion: 该研究为工业环境中部署异常检测提供了实用见解，强调了故障样本数量对算法选择的关键影响，并展示了不同方法在不同数据条件下的适用性。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [28] [Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games](https://arxiv.org/abs/2601.00007)
*Nicholas A. Pape*

Main category: cs.LG

TL;DR: 本文将Yahtzee游戏建模为MDP，使用REINFORCE、A2C和PPO等策略梯度方法训练自博弈智能体，发现A2C在固定训练预算下表现最稳健，达到接近最优DP分数的95%性能。


<details>
  <summary>Details</summary>
Motivation: Yahtzee作为具有随机性、组合结构和延迟奖励的经典骰子游戏，是中等规模强化学习的理想基准。虽然单人Yahtzee可以通过动态规划计算最优策略，但多人版本难以处理，需要近似方法。

Method: 将Yahtzee建模为马尔可夫决策过程，使用具有共享主干的多头网络，训练自博弈智能体。比较了REINFORCE、优势演员评论家(A2C)和近端策略优化(PPO)等策略梯度方法，并对特征和动作编码、架构、回报估计器和熵正则化进行了消融研究。

Result: 在固定训练预算下，REINFORCE和PPO对超参数敏感且未能达到接近最优性能，而A2C在各种设置下都能稳健训练。最佳智能体在10万次评估游戏中获得中位数分数241.78分，接近最优DP分数254.59分的95%，上区奖励和Yahtzee达成率分别为24.9%和34.1%。

Conclusion: 所有模型都难以学习上区奖励策略，过度关注"四骰同点"，突显了长期信用分配和探索的持续挑战。A2C在Yahtzee游戏中表现出最稳健的学习性能。

Abstract: Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\% and 34.1\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.

</details>


### [29] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应重复编码框架，实现按维度不等错误保护，在有限带宽下显著提升语义通信性能


<details>
  <summary>Details</summary>
Motivation: 解决带宽受限通信系统中语义意义保持的挑战，传统信道编码（如LDPC、Reed-Solomon）无法实现细粒度语义保护

Method: 采用强化学习框架，通过自适应重复编码实现按维度不等错误保护，使用复合语义失真度量平衡全局嵌入相似性和实体级保持

Result: 相比均匀保护，在1 dB SNR下获得6.8%更高的chrF分数和9.3%更好的实体保持，统计显著提升

Conclusion: 智能分配的简单重复编码可实现细粒度语义保护，代码结构必须与语义粒度对齐，适用于边缘计算和物联网等带宽稀缺但语义保真度关键的场景

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [30] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文揭示了一种针对LLM模型组合技术的供应链攻击：通过设计单个"破坏性token"，在捐赠模型中功能惰性，但在移植到基础模型后能可靠重构为高显著性恶意特征，破坏基础模型的生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源LLM生态系统越来越多地使用模型组合技术（如权重合并、推测解码、词汇扩展），这些方法在不同模型家族间应用的关键前提是tokenizer移植。本文发现这一关键互操作性步骤引入了供应链漏洞。

Method: 利用系数重用的几何特性，通过稀疏求解器实现双目标优化问题，设计单个"破坏性token"。该token在捐赠模型中功能惰性，但在移植到基础模型后能可靠重构为恶意特征，实现谱模仿以逃避异常检测。

Result: 攻击无需训练，能实现谱模仿逃避异常检测，同时展示了对微调和权重合并的结构持久性。攻击成功破坏了基础模型的生成能力，而捐赠模型的效用与正常行为在统计上无法区分。

Conclusion: 研究揭示了模块化AI组合流程中的隐藏风险：tokenizer移植这一关键互操作性步骤可能被利用来创建供应链漏洞，破坏模型组合的安全性。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [31] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: TeleDoCTR是一个针对电信领域票务故障排除的端到端系统，通过集成领域特定排名和生成模型，自动化分类、检索和生成任务，显著提升故障排除的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 电信领域的票务故障排除是一个高度复杂且耗时的过程，需要专家解读票务内容、查阅文档和搜索历史记录。这种人工密集型方法不仅延迟问题解决，还阻碍整体运营效率。

Method: 提出TeleDoCTR系统，集成领域特定排名和生成模型，自动化故障排除工作流中的三个关键步骤：1) 将票务分类到适当的专家团队；2) 检索上下文和语义相似的历史票务；3) 生成详细的故障分析报告。

Result: 在真实世界电信基础设施数据集上评估TeleDoCTR，证明其性能优于现有最先进方法，显著提升了故障排除过程的准确性和效率。

Conclusion: TeleDoCTR是一个有效的电信领域特定故障排除系统，通过自动化关键工作流步骤，能够显著改善电信票务故障排除的效果和效率。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [32] [IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business](https://arxiv.org/abs/2601.00075)
*Swetha Varadarajan,Abhishek Ray,Lumina Albert*

Main category: cs.LG

TL;DR: IMBWatch是一个基于时空图神经网络的框架，用于大规模检测非法按摩院，通过分析在线广告、营业执照和用户评论等开源情报构建动态图，有效识别人口贩运和性剥削网络。


<details>
  <summary>Details</summary>
Motivation: 非法按摩院（IMBs）以合法健康服务为幌子，从事人口贩运、性剥削和强迫劳动等犯罪活动。传统检测方法（如社区举报和监管检查）反应迟缓且效果有限，难以揭示犯罪分子的运营网络。由于数字广告编码、人员地点频繁变更以及共享基础设施（如电话号码和地址）的重复使用，IMBs的检测变得异常困难。

Method: IMBWatch采用时空图神经网络（ST-GNN）框架，从开源情报（包括在线广告、营业执照记录和众包评论）构建动态图。节点代表企业、别名、电话号码和位置等异构实体，边捕获时空和关系模式（如同址、重复电话使用和同步广告）。该框架结合图卷积操作和时间注意力机制，模拟IMB网络在时间和空间上的演变，捕捉跨城市工人流动、一次性电话轮换和协调广告激增等模式。

Result: 在美国多个城市的真实数据集上的实验表明，IMBWatch优于基线模型，实现了更高的准确率和F1分数。除了性能提升外，IMBWatch还提供了更好的可解释性，为主动和有针对性的干预提供了可行的见解。该框架具有可扩展性，可适应其他非法领域，并发布了匿名数据和开源代码以支持可重复研究。

Conclusion: IMBWatch是一个有效的时空图神经网络框架，能够大规模检测非法按摩院网络，克服了传统方法的局限性。该框架不仅性能优越，还具有可解释性和可扩展性，为打击人口贩运和性剥削提供了技术支持，并可通过开源代码促进相关领域的研究。

Abstract: Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.
  To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.
  Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.

</details>


### [33] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出了一种渐近置信度的最佳臂识别新框架，通过放松精确误差控制要求，在渐近意义下实现更紧的样本复杂度，适用于弱信号、高显著性要求的实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有固定置信度最佳臂识别方法在实际应用中存在局限：严格的精确误差控制需要使用宽松的尾不等式和/或参数限制，导致样本效率低下。现实场景常涉及弱信号、高显著性要求和实验后推断需求，这些都需要较长的实验周期。

Method: 引入渐近误差控制框架，要求误差控制在最小样本量趋于无穷时有效。开发了新的渐近任意时间有效置信序列，并基于此设计BAI算法。方法灵活整合协变量进行方差缩减，确保在完全非参数设置下的近似误差控制。

Result: 在温和收敛假设下，提供了样本复杂度的渐近界，并证明最坏情况样本复杂度与已知方差下高斯BAI的最佳情况样本复杂度匹配。实验表明该方法在保持误差控制的同时降低了平均样本复杂度。

Conclusion: 提出的渐近框架克服了传统BAI方法的局限性，通过放松精确误差控制要求，实现了更紧的最优性和更好的实际性能，特别适用于需要长实验周期和弱信号检测的现实场景。

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [34] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: NeuroSymBO：将提示工程重构为顺序决策问题，通过贝叶斯优化自适应选择最优指令，解决LLM在方程发现中的指令脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在方程发现中表现出潜力，但其输出对提示措辞高度敏感（指令脆弱性）。静态提示无法适应多步生成过程的演化状态，导致模型在次优解上停滞不前。

Method: 提出NeuroSymBO方法，将提示工程重构为顺序决策问题。该方法维护一个离散的推理策略库，并使用贝叶斯优化基于数值反馈在每个步骤选择最优指令。

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现了更高的恢复率和更简洁的解决方案。

Conclusion: 通过将提示工程视为顺序决策问题并采用自适应指令选择，可以有效解决LLM在方程发现中的指令脆弱性问题，提升模型性能和解的质量。

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [35] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: GRL-SNAM是一个用于未知环境中同时导航与建图的几何强化学习框架，它通过局部感知而非全局建图实现导航，使用哈密顿优化动态搜索最短路径。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中同时导航与建图(SNAM)的挑战性问题，传统方法需要构建全局地图或设计复杂的分层策略，而本文希望仅依赖局部感知就能实现高效导航。

Method: 将路径导航和建图建模为动态最短路径搜索和发现过程，使用受控哈密顿优化：将感知输入转换为局部能量景观，编码可达性、障碍物屏障和变形约束；感知、规划和重配置策略通过更新哈密顿量逐步演化。

Result: 在2D导航任务中评估，相比局部反应式基线和全局策略学习方法，GRL-SNAM在相同阶段感知约束下保持安全距离，泛化到未见布局，通过局部能量细化而非广泛全局建图实现高质量导航。

Conclusion: 通过更新哈密顿量的几何强化学习能够通过最小化探索实现高质量导航，仅依赖局部能量细化而非广泛全局建图，为未知环境中的同时导航与建图提供了有效解决方案。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [36] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: 该论文研究在非马尔可夫状态和成本过程下的线性函数逼近强化学习方法，证明了策略评估算法的收敛性，并分析了Q-learning在特定基函数选择下的收敛条件，最后应用于部分可观测马尔可夫决策过程。


<details>
  <summary>Details</summary>
Motivation: 研究在非马尔可夫过程下强化学习方法的收敛性，因为实际应用中状态和成本过程往往不是马尔可夫的，需要扩展传统马尔可夫假设下的理论框架。

Method: 采用线性函数逼近方法，首先分析策略评估算法在非马尔可夫过程下的收敛性；然后研究Q-learning的收敛条件，特别关注基于量化映射的基函数选择；最后将理论应用于部分可观测马尔可夫决策过程，使用有限记忆变量作为状态表示。

Result: 证明了策略评估算法在适当的遍历性条件下收敛，且极限对应于由正交投影和辅助马尔可夫决策过程的贝尔曼算子组成的联合算子的不动点；对于Q-learning，在基于量化映射的基函数选择下，可以在类似遍历性条件下证明收敛；在部分可观测马尔可夫决策过程应用中，推导了学习算法极限的显式误差界。

Conclusion: 该研究扩展了强化学习理论到非马尔可夫过程，为实际应用中更广泛的状态和成本过程提供了理论保证，特别是在部分可观测环境下的强化学习算法具有明确的理论支撑。

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [37] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: 研究使用XGBoost模型分析美国交通事故严重程度预测，发现时间、地理位置和天气变量是关键预测因素，但降水能见度预测力有限，模型对极端案例学习能力受数据分布限制。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索环境、时间和空间因素对交通事故严重程度的预测能力，为基于证据的交通管理提供支持，并识别未来研究方向。

Method: 使用2016-2023年美国50万起交通事故数据集，采用XGBoost分类器，通过随机搜索交叉验证优化，并使用类别加权处理类别不平衡问题。

Result: 最终模型整体准确率达78%，对主要类别（严重程度2）的精确率和召回率均达87%。特征重要性分析显示时间、地理位置和天气变量（能见度、温度、风速）是最强预测因子，但降水和能见度预测力有限。

Conclusion: 研究为基于证据的交通管理提供了有价值见解，但由于数据集中中等严重程度事故占主导，模型对极端案例学习能力受限，未来需要替代采样策略、增强特征工程和整合外部数据集。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [38] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种使用纯强化学习梯度进行决策变换器在线微调的新方法，解决了传统方法中后见回报重标注与重要性采样算法不兼容的问题，在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在离线强化学习中表现出色，但在在线设置中使用纯强化学习梯度进行微调的研究很少。现有方法在在线微调时仍严重依赖监督序列建模目标，而后见回报重标注这一标准组件与基于重要性采样的强化学习算法（如GRPO）存在根本性不兼容，导致训练不稳定。

Method: 1. 将GRPO算法适配到决策变换器框架；2. 引入子轨迹优化以改进信用分配；3. 使用序列级似然目标增强稳定性和效率；4. 采用主动采样策略在不确定区域鼓励探索。

Result: 通过大量实验证明，所提出的方法在多个基准测试中超越了现有的在线决策变换器基线，并取得了新的最先进性能，突显了基于纯强化学习的在线微调对决策变换器的有效性。

Conclusion: 本文成功实现了决策变换器的纯强化学习梯度在线微调，解决了后见回报重标注与重要性采样算法的不兼容问题，为序列决策制定提供了更有效的在线学习框架。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [39] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: 本文提出了一种顺序储层计算架构，通过将大储层分解为一系列小型互连储层，显著降低了高维时空系统预测的计算成本和内存需求，同时保持了长期时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 传统RNN和LSTM模型在高维时空系统预测中存在梯度训练和内存瓶颈问题，而传统储层计算架构在处理高维输入时扩展性差，需要更高效的计算方法。

Method: 提出顺序储层计算架构，将大型储层分解为一系列小型互连储层，保持固定循环层和凸读出优化，减少内存和计算成本，同时保留长期时间依赖性。

Result: 在低维混沌系统和高维物理模拟中，顺序RC相比LSTM和标准RNN基线，实现了15-25%更长的有效预测时间范围，20-30%更低的误差指标，训练成本降低达三个数量级。

Conclusion: 顺序RC在保持传统RC简单高效的同时，实现了对高维动力系统的卓越可扩展性，为科学和工程应用中的实时、节能预测提供了实用路径。

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [40] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 基于电子健康记录数据的机器学习模型在预测肝硬化的早期风险方面显著优于传统FIB-4评分，能够提前1-3年进行更准确的风险分层。


<details>
  <summary>Details</summary>
Motivation: 开发并评估利用常规电子健康记录数据预测肝硬化的机器学习模型，以替代或补充传统的FIB-4评分，实现更早、更准确的风险预测。

Method: 采用回顾性队列研究，使用大型学术医疗系统的去标识化EHR数据。基于ICD-9/10代码识别脂肪肝患者并分为肝硬化与非肝硬化队列。构建观察窗口和预测窗口模拟临床实际应用。从观察窗口聚合人口统计学、诊断、实验室结果、生命体征和共病指数等数据。训练XGBoost模型进行1年、2年和3年预测，并在保留测试集上评估性能，与FIB-4评分进行AUC比较。

Result: 最终队列包括1年预测3,043名患者、2年预测1,981名患者、3年预测1,470名患者。在所有预测时间窗口中，机器学习模型均持续优于FIB-4评分。XGBoost模型在1年、2年和3年预测中的AUC分别为0.81、0.73和0.69，而FIB-4评分分别为0.71、0.63和0.57。随着预测时间延长，性能优势仍然保持，表明早期风险识别能力更强。

Conclusion: 基于常规电子健康记录数据的机器学习模型在肝硬化早期预测方面显著优于传统FIB-4评分。这些模型能够实现更早、更准确的风险分层，可作为自动化决策支持工具整合到临床工作流程中，支持主动的肝硬化预防和管理。

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [41] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: 提出SSI-GAN半监督学习方法，仅需1-3%标注数据即可实现蚊子神经元尖峰信号的高精度分类，用于检测寨卡病毒和登革热病毒感染，大幅减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 蚊子是虫媒病毒疾病的主要传播媒介，手动分类其神经元尖峰模式非常耗时耗力。现有深度学习解决方案需要完全标注的数据集和高度预处理的神经信号，这降低了在实际野外场景中大规模应用的可行性。

Method: 提出半监督Swin启发式GAN（SSI-GAN）架构，包含基于Transformer的生成器和采用移位窗口的Swin启发式判别器。使用多头自注意力模型在基于窗口的Transformer判别器中学习捕获稀疏的高频尖峰特征。仅使用1-3%标注数据，训练超过1500万个尖峰样本，使用贝叶斯Optuna框架优化超参数，并通过五折蒙特卡洛交叉验证验证鲁棒性。

Result: SSI-GAN在感染后第三天仅使用3%标注数据达到99.93%的分类准确率。在仅1%监督下，在所有感染阶段均保持高准确率。相对于标准监督方法，在相同性能水平下减少了97-99%的人工标注工作量。移位窗口Transformer设计大幅超越所有基线方法，在基于尖峰的神经元感染分类中创下新纪录。

Conclusion: SSI-GAN通过创新的半监督学习方法，显著减少了神经元尖峰分类所需的人工标注工作量，为实际野外场景中的大规模应用提供了可行性，在蚊子病毒感染检测方面表现出卓越性能。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [42] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: 提出资源高效的数据中心框架，通过特征工程使心律失常数据线性可分，实现98.44%诊断准确率，模型仅8.54KB，推理延迟0.46μs，适合资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病特别是心律失常是全球主要死因，需要IoMT持续监测。现有深度学习方法计算开销大，不适合资源受限的边缘设备。

Method: 提出资源高效的数据中心框架，优先特征工程而非模型复杂度。整合时频小波分解和图论结构描述符（如PageRank中心性），通过互信息和递归消除优化特征空间，使用可解释的超轻量线性分类器。

Result: 在MIT-BIH和INCART数据集上验证，达到98.44%诊断准确率，模型大小仅8.54KB。分类推理延迟0.46μs，每搏处理管道52ms，确保实时操作。相比压缩模型KD-Light（25KB，96.32%准确率）实现数量级效率提升。

Conclusion: 该框架通过特征工程使复杂心律失常数据线性可分，实现高效、轻量、实时的诊断系统，为无电池心脏传感器提供可行方案，显著提升边缘设备的心律失常监测能力。

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [43] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

TL;DR: 该研究提出了一种利用未标注网络数据增强生成模型溯源能力的方法，通过约束优化提升对未见生成器的识别性能


<details>
  <summary>Details</summary>
Motivation: 随着逼真生成模型的快速发展，需要超越简单的真假检测，实现特定生成模型的溯源识别。现有方法在遇到未见或新发布的生成器时泛化能力不足。

Method: 使用CLIP特征和线性分类器建立基线，然后提出约束优化方法：利用未标注的网络数据（可能包含真实图像、未知生成器输出或目标模型样本），鼓励将野生样本分类为非目标，同时约束在标注数据上的性能保持高水平。

Result: 实验结果表明，结合野生数据能显著提升对挑战性未见生成器的溯源性能，证明未标注网络数据可以有效增强开放世界环境下的AI生成内容溯源能力。

Conclusion: 利用未标注的野生数据通过约束优化方法可以有效提升生成模型溯源在开放世界环境中的泛化能力，特别是在面对未见和新发布生成器时的识别性能。

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [44] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

TL;DR: 提出对抗性图提示（AGP）框架，将对抗学习融入图提示，实现鲁棒的图神经网络微调，解决现有参数高效微调方法对图拓扑和节点特征噪声的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在适应预训练GNN模型到下游任务时，对图拓扑和节点属性/特征的各种噪声和攻击表现出显著脆弱性，需要提升鲁棒性。

Method: 提出对抗性图提示（AGP）框架，将其建模为min-max优化问题，采用交替优化方案：内层最大化使用联合投影梯度下降（JointPGD）生成强对抗噪声；外层最小化学习最优节点提示来抵消对抗噪声。

Result: AGP理论上能同时处理图拓扑和节点噪声，验证了其多功能性和鲁棒性。实验表明AGP在多个基准任务上相比最先进方法具有更好的鲁棒性和有效性。

Conclusion: AGP是一种通用方法，可与各种预训练GNN模型集成，增强其在下游任务中的鲁棒性，为解决图神经网络微调中的脆弱性问题提供了有效方案。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [45] [GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation](https://arxiv.org/abs/2601.00231)
*Pritish Saha,Chandrav Rajbangshi,Rudra Goyal,Mohit Goyal,Anurag Deo,Biswajit Roy,Ningthoujam Dhanachandra Singh,Raxit Goswami,Amitava Das*

Main category: cs.LG

TL;DR: GRIT是一种动态的、曲率感知的LoRA方法，通过K-FAC预条件梯度、定期重投影到主Fisher特征方向、自适应调整有效秩来减少可训练参数46%，在保持性能的同时降低漂移。


<details>
  <summary>Details</summary>
Motivation: 传统的LoRA和QLoRA方法在几何上是盲目的：它们在固定的、随机定向的低秩子空间中优化，使用一阶下降，大多忽略了局部损失曲率。这可能导致有效更新预算膨胀，并沿着弱约束方向放大漂移。

Method: GRIT保留了LoRA参数化，但：(1) 使用K-FAC作为自然梯度代理在秩空间中对梯度进行预条件处理；(2) 定期将低秩基重投影到主导Fisher特征方向上以抑制漂移；(3) 根据谱自适应调整有效秩，使容量集中在信号存在的地方。

Result: 在LLaMA骨干上的指令跟随、理解和推理基准测试中，GRIT匹配或超越了LoRA和QLoRA，同时平均减少了46%的可训练参数（跨任务25-80%），在各种提示风格和数据混合中没有实际质量损失。GRIT显示出更低的漂移和更好的更新与保留边界。

Conclusion: GRIT是一种动态的曲率感知LoRA程序，通过结合自然梯度近似、漂移抑制和自适应秩选择，在减少参数的同时保持或提升性能，为参数高效微调提供了更几何感知的解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) is the default way to adapt LLMs, but widely used LoRA and QLoRA are largely geometry-agnostic: they optimize in fixed, randomly oriented low-rank subspaces with first-order descent, mostly ignoring local loss curvature. This can inflate the effective update budget and amplify drift along weakly constrained directions. We introduce GRIT, a dynamic, curvature-aware LoRA procedure that preserves the LoRA parameterization but: (1) preconditions gradients in rank space using K-FAC as a natural-gradient proxy; (2) periodically reprojects the low-rank basis onto dominant Fisher eigendirections to suppress drift; and (3) adapts the effective rank from the spectrum so capacity concentrates where signal resides. Across instruction-following, comprehension, and reasoning benchmarks on LLaMA backbones, GRIT matches or surpasses LoRA and QLoRA while reducing trainable parameters by 46% on average (25--80% across tasks), without practical quality loss across prompt styles and data mixes. To model forgetting, we fit a curvature-modulated power law. Empirically, GRIT yields lower drift and a better updates-vs-retention frontier than strong PEFT-optimizer baselines (Orthogonal-LoRA, IA3, DoRA, Eff-FT, Shampoo).

</details>


### [46] [Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering](https://arxiv.org/abs/2601.00276)
*Hongxi Li,Chunlin Huang*

Main category: cs.LG

TL;DR: 宽L2正则化网络的特征学习理论表明监督学习本质上是压缩的，核秩受类别数限制，SGD噪声也是低秩的，这与自监督学习的高秩表示形成对比。


<details>
  <summary>Details</summary>
Motivation: 研究宽L2正则化网络中特征学习的本质特性，探索监督学习与自监督学习在表示结构上的根本差异。

Method: 提出特征学习理论，推导核ODE预测"水填充"谱演化，证明稳定稳态下核秩受类别数限制，分析SGD噪声的低秩特性。

Result: 监督学习的核秩受类别数C限制，SGD噪声也是O(C)低秩的，学习动态被限制在任务相关子空间，与自监督学习的高秩扩张表示形成鲜明对比。

Conclusion: 监督学习本质上是压缩的，其特征表示受任务类别数限制，而自监督学习产生高秩扩张表示，这一框架统一了确定性和随机性视角下的对齐机制。

Abstract: We present a theory of feature learning in wide L2-regularized networks showing that supervised learning is inherently compressive. We derive a kernel ODE that predicts a "water-filling" spectral evolution and prove that for any stable steady state, the kernel rank is bounded by the number of classes ($C$). We further demonstrate that SGD noise is similarly low-rank ($O(C)$), confining dynamics to the task-relevant subspace. This framework unifies the deterministic and stochastic views of alignment and contrasts the low-rank nature of supervised learning with the high-rank, expansive representations of self-supervision.

</details>


### [47] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: 提出基于最优传输的联邦逆强化学习方法，通过Wasserstein重心融合异构智能体的本地奖励函数，实现通信高效且保护隐私的共享奖励学习。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，直接共享数据学习共同奖励函数通常不可行，因为存在动态差异、隐私约束和通信带宽限制等问题。

Method: 每个客户端先在本地执行轻量级最大熵逆强化学习，然后通过Wasserstein重心融合得到的奖励函数，考虑其底层几何结构。

Result: 证明重心融合比传统联邦学习参数平均方法能获得更准确的全局奖励估计，提供通信高效的异构智能体共享奖励学习框架。

Conclusion: 提出了一种原则性且通信高效的框架，用于推导能在异构智能体和环境中泛化的共享奖励函数。

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [48] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 该研究提出了基于国际象棋战术位置的量子基准测试QKRD，用于系统评估QAOA算法设计选择，发现约束保持混合器比标准混合器收敛快约13步，预热策略减少45步收敛时间，而CVaR优化效果不佳。


<details>
  <summary>Details</summary>
Motivation: 当前QAOA算法主要在MaxCut、TSP、SAT等随机合成实例上进行基准测试，这些实例缺乏语义结构和人类可解释性，难以反映在具有有意义约束的真实问题上的性能表现。

Method: 引入量子王环支配问题(QKRD)，这是一个基于国际象棋战术位置的NISQ规模基准测试，提供5000个具有one-hot约束、空间局部性和10-40量子比特规模的结构化实例，结合人类可解释的覆盖度指标和针对经典启发式算法的内在验证。

Result: 约束保持混合器(XY、domain-wall)比标准混合器收敛快约13步(p<10^{-7}, d≈0.5)，预热策略减少45步收敛时间(p<10^{-127}, d=3.35)且能量改进超过d=8，CVaR优化效果更差(p<10^{-40}, d=1.21)且无覆盖度优势。QAOA优于贪婪启发式算法12.6%，优于随机选择80.1%。

Conclusion: 结构化基准测试能够揭示在随机实例中被掩盖的问题感知QAOA技术优势，为可重复的NISQ算法研究提供了代码、数据和实验工件的完整发布。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [49] [Smart Fault Detection in Nanosatellite Electrical Power System](https://arxiv.org/abs/2601.00335)
*Alireza Rezaee,Niloofar Nobahari,Amin Asgarifar,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出一种无需姿态控制系统的纳米卫星电源故障检测方法，使用神经网络模拟正常状态，结合多种机器学习算法进行故障分类


<details>
  <summary>Details</summary>
Motivation: 纳米卫星在低地球轨道运行时，其电源系统容易因压力耐受性、发射压力和环境因素出现故障，而传统方法需要姿态控制系统，本文旨在开发无需姿态控制系统的故障检测方案

Method: 首先使用神经网络模拟无故障系统，以太阳辐射和太阳能板表面温度为输入，电流和负载为输出；然后使用神经网络分类器根据故障模式和类型诊断不同故障；同时采用PCA分类、决策树和KNN等其他机器学习方法进行故障分类

Result: 成功开发了无需姿态控制系统的纳米卫星电源故障检测方法，能够识别光伏子系统的线间故障和开路故障、DC-DC转换器的IGBT短路和开路故障，以及地面电池的调节器故障

Conclusion: 该方法有效解决了纳米卫星电源系统的故障检测问题，无需依赖姿态控制系统，为卫星电源系统的可靠运行提供了新的技术方案

Abstract: This paper presents a new detection method of faults at Nanosatellites' electrical power without an Attitude Determination Control Subsystem (ADCS) at the LEO orbit. Each part of this system is at risk of fault due to pressure tolerance, launcher pressure, and environmental circumstances. Common faults are line to line fault and open circuit for the photovoltaic subsystem, short circuit and open circuit IGBT at DC to DC converter, and regulator fault of the ground battery. The system is simulated without fault based on a neural network using solar radiation and solar panel's surface temperature as input data and current and load as outputs. Finally, using the neural network classifier, different faults are diagnosed by pattern and type of fault. For fault classification, other machine learning methods are also used, such as PCA classification, decision tree, and KNN.

</details>


### [50] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

TL;DR: 该论文提出结合光流和三种不同深度模型（监督CNN、预训练CNN特征提取器、分层极限学习机）的方法，用于无人机非静态摄像头视频中的人体检测，在UCF-ARG数据集上取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统手工特征方法依赖专家知识，对光照变化、相机抖动等动态事件敏感。需要开发自动特征学习方法，以更经济高效地实现视频中的人体检测。

Method: 结合光流和三种深度模型：1) 监督卷积神经网络（S-CNN），2) 预训练CNN特征提取器，3) 分层极限学习机（H-ELM）。在UCF-ARG无人机航拍数据集上进行训练和测试，评估五种人类动作（挖掘、挥手、投掷、行走、奔跑）。

Result: 预训练CNN平均准确率达98.09%；S-CNN使用softmax达到95.6%，使用SVM达到91.7%；H-ELM平均准确率为95.9%。H-ELM在CPU上训练时间为445秒，S-CNN在GPU上训练时间为770秒。

Conclusion: 提出的自动特征学习方法在无人机视频人体检测任务中表现成功，预训练CNN模型效果最佳，H-ELM在计算效率方面具有优势。

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


### [51] [Deep Delta Learning](https://arxiv.org/abs/2601.00417)
*Yifan Zhang,Yifeng Liu,Mengdi Wang,Quanquan Gu*

Main category: cs.LG

TL;DR: DDL提出了一种新的深度残差网络架构，通过可学习的几何变换（Delta算子）调制恒等捷径连接，取代了传统的严格加法归纳偏置，使网络能够建模更复杂的动态。


<details>
  <summary>Details</summary>
Motivation: 传统残差网络的恒等捷径连接虽然缓解了梯度消失问题，但强加了严格的加法归纳偏置，限制了网络建模复杂状态转换的能力。

Method: 提出深度Delta学习（DDL），用可学习的、数据依赖的几何变换（Delta算子）调制恒等捷径连接。该算子是对单位矩阵的秩-1扰动，由反射方向向量k(X)和门控标量β(X)参数化。

Result: 通过谱分析表明，门控β(X)能够在恒等映射、正交投影和几何反射之间进行动态插值。将残差更新重构为同步秩-1注入，门控作为动态步长控制旧信息的擦除和新特征的写入。

Conclusion: DDL统一了残差更新的控制机制，使网络能够显式控制层间转移算子的谱，在保持门控残差架构稳定训练特性的同时，能够建模复杂的非单调动态。

Abstract: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $β(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.

</details>


### [52] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: 提出E-GRPO方法，通过熵感知的组相对策略优化增强流匹配模型的人类偏好对齐，通过合并低熵步骤为高熵SDE采样步骤来解决多步优化中的稀疏奖励信号问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机采样的流匹配模型在多步去噪优化中面临稀疏和模糊的奖励信号问题，高熵步骤能实现更高效探索而低熵步骤导致无区别的轨迹。

Method: 提出E-GRPO（熵感知组相对策略优化），将连续低熵步骤合并为单个高熵SDE采样步骤，其他步骤使用ODE采样，并引入多步组归一化优势函数计算共享相同合并SDE去噪步的样本组相对优势。

Result: 在不同奖励设置下的实验结果表明该方法有效提升了流匹配模型的人类偏好对齐性能。

Conclusion: 通过熵感知的步骤合并和组相对优势计算，E-GRPO方法有效解决了多步去噪优化中的奖励稀疏性问题，提升了流匹配模型在人类偏好对齐任务上的性能。

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [53] [A Comparative Analysis of Interpretable Machine Learning Methods](https://arxiv.org/abs/2601.00428)
*Mattia Billa,Giovanni Orlandi,Veronica Guidetti,Federica Mandreoli*

Main category: cs.LG

TL;DR: 该研究对16种可解释机器学习方法在216个真实世界表格数据集上进行大规模比较评估，分析不同数据集特征下的性能表现，为实践者提供可解释性与预测性能的平衡指导。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在医疗、金融、法律等高风险领域的广泛应用，模型可解释性和责任性日益重要。然而，对内在可解释模型（特别是针对表格数据）的系统评估相对缺乏，现有研究多集中于聚合性能结果。

Method: 对16种内在可解释方法进行大规模比较评估，包括经典线性模型、决策树以及EBMs、符号回归、GOSDT等新方法。研究涵盖216个真实世界表格数据集，根据数据集的结构特征（维度、样本量、线性度、类别不平衡）进行分层性能分析，并评估训练时间和分布偏移下的鲁棒性。

Result: 研究揭示了清晰的性能层次结构，特别是在回归任务中，EBMs始终表现出强大的预测准确性。同时发现性能高度依赖于上下文：SR和IGANNs在非线性场景下表现优异，而GOSDT模型对类别不平衡表现出明显敏感性。

Conclusion: 这些发现为寻求可解释性与预测性能平衡的实践者提供了实用指导，并加深了对表格数据可解释建模的实证理解。研究强调了性能评估需要考虑数据集的具体特征，而非仅依赖聚合排名。

Abstract: In recent years, Machine Learning (ML) has seen widespread adoption across a broad range of sectors, including high-stakes domains such as healthcare, finance, and law. This growing reliance has raised increasing concerns regarding model interpretability and accountability, particularly as legal and regulatory frameworks place tighter constraints on using black-box models in critical applications. Although interpretable ML has attracted substantial attention, systematic evaluations of inherently interpretable models, especially for tabular data, remain relatively scarce and often focus primarily on aggregated performance outcomes.
  To address this gap, we present a large-scale comparative evaluation of 16 inherently interpretable methods, ranging from classical linear models and decision trees to more recent approaches such as Explainable Boosting Machines (EBMs), Symbolic Regression (SR), and Generalized Optimal Sparse Decision Trees (GOSDT). Our study spans 216 real-world tabular datasets and goes beyond aggregate rankings by stratifying performance according to structural dataset characteristics, including dimensionality, sample size, linearity, and class imbalance. In addition, we assess training time and robustness under controlled distributional shifts. Our results reveal clear performance hierarchies, especially for regression tasks, where EBMs consistently achieve strong predictive accuracy. At the same time, we show that performance is highly context-dependent: SR and Interpretable Generalized Additive Neural Networks (IGANNs) perform particularly well in non-linear regimes, while GOSDT models exhibit pronounced sensitivity to class imbalance. Overall, these findings provide practical guidance for practitioners seeking a balance between interpretability and predictive performance, and contribute to a deeper empirical understanding of interpretable modeling for tabular data.

</details>


### [54] [A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection](https://arxiv.org/abs/2601.00446)
*Miseon Park,Kijung Yoon*

Main category: cs.LG

TL;DR: 时间序列基础模型(TSFMs)通过预训练可以作为通用异常检测骨干，在零样本推理、全模型适应和参数高效微调(PEFT)策略下均优于任务特定基线，特别是在类别不平衡严重时表现突出。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法大多需要大量任务特定训练，研究探索预训练在大型异构数据上的时间序列基础模型是否能作为通用的异常检测骨干模型。

Method: 通过系统实验比较零样本推理、全模型适应和参数高效微调(PEFT)策略，包括LoRA、OFT和HRA等方法，评估时间序列基础模型在多个基准测试中的表现。

Result: 时间序列基础模型在AUC-PR和VUS-PR指标上显著优于任务特定基线，尤其在严重类别不平衡情况下表现更佳；参数高效微调方法不仅降低计算成本，在多数情况下匹配或超越全微调性能。

Conclusion: 时间序列基础模型可作为有前景的通用模型，实现可扩展且高效的时间序列异常检测，即使是为预测任务预训练的模型也能有效适应异常检测任务。

Abstract: Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.

</details>


### [55] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 本文提出了可控概念瓶颈模型（CCBMs），支持三种粒度的模型编辑（概念-标签级、概念级、数据级），通过影响函数的闭式近似实现高效编辑，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型主要关注静态场景，而实际应用中需要持续维护：删除错误或敏感数据（遗忘）、纠正错误标注的概念、纳入新样本（增量学习）以适应动态环境。如何在不重新训练的情况下实现高效可编辑的CBMs是重要挑战。

Method: 提出可控概念瓶颈模型（CCBMs），支持三种编辑粒度：概念-标签级、概念级、数据级（包括数据删除和添加）。基于影响函数推导出数学上严格的闭式近似，避免重新训练。

Result: 实验结果表明CCBMs具有高效性和适应性，验证了其在实现动态可信赖CBMs方面的实用价值。

Conclusion: CCBMs通过影响函数的闭式近似实现了高效模型编辑，解决了CBMs在动态环境中的维护挑战，为构建动态可信赖的概念瓶颈模型提供了实用方案。

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [56] [Deep Networks Learn Deep Hierarchical Models](https://arxiv.org/abs/2601.00455)
*Amit Daniely*

Main category: cs.LG

TL;DR: 该论文研究残差网络中层间SGD如何高效学习具有层次结构的标签模型，这类模型超越了之前可学习模型的深度限制，并探讨了教师提供的层次化标签如何揭示大脑内部算法结构。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络如何高效学习具有层次结构的复杂模型，探索人类教师提供的层次化标签如何揭示大脑内部算法结构，为理解深度学习提供理论基础。

Method: 使用残差网络和层间SGD算法，学习具有未知标签层次结构的模型：L₁ ⊆ L₂ ⊆ ... ⊆ Lᵣ = [n]，其中L₁标签是输入的简单函数，而i>1时，Lᵢ标签是更简单标签的函数。

Result: 该模型类超越了之前可学习模型的深度限制，达到了高效可学习性的深度极限，包含需要多项式深度表达而之前模型只能由对数深度电路计算的模型。

Conclusion: 层次模型的可学习性可能成为理解深度学习的基础，人类教师提供的粒度化标签揭示了大脑内部算法的"提示"或"片段"，在教师部分了解其内部逻辑的简化模型中，层次结构促进了高效可学习性。

Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.
  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.
  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.

</details>


### [57] [Imitation from Observations with Trajectory-Level Generative Embeddings](https://arxiv.org/abs/2601.00452)
*Yongtao Qu,Shangzhe Li,Weitong Zhang*

Main category: cs.LG

TL;DR: TGE是一种用于离线观察模仿学习的新方法，通过时序扩散模型的潜在空间估计专家状态密度，构建密集平滑的替代奖励，有效处理专家演示稀缺且离线数据与专家行为差异大的情况。


<details>
  <summary>Details</summary>
Motivation: 现有基于分布匹配的方法在专家演示稀缺、离线次优数据与专家行为差异大的情况下表现不佳，因为它们施加了严格的支持约束并依赖脆弱的单步模型，难以从不完美数据中提取有用信号。

Method: 提出TGE（轨迹级生成嵌入）方法，使用时序扩散模型在离线轨迹数据上训练，在潜在空间中估计专家状态密度，构建密集平滑的替代奖励，利用学习到的扩散嵌入的平滑几何特性来捕捉长期时序动态。

Result: 在D4RL运动和控制基准测试中，TGE方法始终匹配或优于先前的离线观察模仿学习方法。

Conclusion: TGE通过轨迹级生成嵌入有效解决了离线观察模仿学习中专家数据稀缺和分布差异大的挑战，利用扩散模型的平滑几何特性构建鲁棒的学习信号，即使在离线数据与专家分布差异大的情况下也能有效工作。

Abstract: We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.

</details>


### [58] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 正交性损失在MoE模型中无法有效促进专家多样性，既不能减少权重空间重叠，也不能可靠提升性能


<details>
  <summary>Details</summary>
Motivation: 研究几何正则化在MoE模型专家专业化中的作用，探索正交性损失是否能有效促进专家多样性

Method: 在MoE模型中应用正交性损失来强制专家多样性，在7个不同正则化强度下进行实验，分析权重空间和激活空间的重叠情况

Result: 正交性损失在多方面失败：权重空间重叠不降反增（MSO增加达114%），激活空间重叠保持高位（约0.6），性能影响不一致（WikiText-103微降0.9%，TinyStories微升0.9%，PTB结果高度波动），权重与激活正交性无显著相关性（r=-0.293，p=0.523）

Conclusion: 权重空间正则化既不能实现其几何目标，也不能可靠提升性能，不适合用于MoE多样性增强

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [59] [Neural Chains and Discrete Dynamical Systems](https://arxiv.org/abs/2601.00473)
*Sauro Succi,Abhisek Ganguly,Santosh Ansumali*

Main category: cs.LG

TL;DR: 论文探讨了无自注意力机制的Transformer架构（神经链）与离散动力系统之间的类比，比较了标准数值离散化和PINN学习在求解Burgers和Eikonal方程时的差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索机器学习（特别是无自注意力的Transformer架构）与传统数值方法在求解偏微分方程时的关系，分析PINN学习与传统有限差分方法在知识获取路径上的异同。

Method: 通过将无自注意力的Transformer架构（神经链）与离散动力系统类比，比较标准数值离散化（有限差分方法）和PINN学习在求解一维Burgers方程（粘性和非粘性）和Eikonal方程时的表现。

Result: 研究发现标准数值离散化和PINN学习通过不同路径获得基本相同的系统动力学知识。PINN学习使用随机矩阵，而有限差分方法使用高度结构化的三对角矩阵。可接受的随机矩阵数量远多于有限差分的唯一三对角形式，这解释了PINN搜索通常落在随机集合中的原因。

Conclusion: PINN学习需要更多参数，导致物理透明度（可解释性）降低和训练成本增加，这在有限差分方法中没有对应代价。但研究仅针对一维动态问题，不排除PINN和机器学习在高维问题中可能提供更好策略的可能性。

Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.

</details>


### [60] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: Trajectory Guard：一种用于检测LLM智能体多步行动计划异常的Siamese循环自编码器，通过混合损失函数联合学习任务-轨迹对齐和序列有效性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法不适合LLM智能体生成的多步行动计划：均值池化嵌入会稀释异常步骤，而仅对比方法忽略序列结构。标准无监督方法在预训练嵌入上的F1分数不超过0.69。

Method: 提出Trajectory Guard，一种Siamese循环自编码器，采用混合损失函数，通过对比学习联合学习任务-轨迹对齐，通过重建学习序列有效性。这种双重目标能够统一检测"错误的任务计划"和"畸形的计划结构"。

Result: 在涵盖合成扰动和真实世界失败的基准测试中，在平衡集上获得0.88-0.94的F1分数，在不平衡外部基准上获得0.86-0.92的召回率。推理延迟32毫秒，比LLM Judge基线快17-27倍。

Conclusion: Trajectory Guard能够实时检测LLM智能体行动计划中的异常，包括任务不匹配和结构畸形问题，为生产部署提供高效的安全验证方案。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [61] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 该研究开发了一种名为AugUNet1D的1D UNet模型，用于自动标记脑电图中的棘波放电事件，相比传统方法和现有算法表现更优。


<details>
  <summary>Details</summary>
Motivation: 脑电图记录中手动标记事件（特别是棘波放电）耗时费力，尤其是在连续数周至数月的长时间记录中。需要自动标记方法来减少人工工作量。

Method: 比较了14种机器学习分类器在961小时脑电图数据上的性能，发现1D UNet表现最佳。通过数据增强改进1D UNet（称为AugUNet1D），特别是缩放增强效果最好。将AugUNet1D与最近发表的"Twin Peaks"算法进行比较。

Result: 1D UNet在14种分类器中表现最佳。数据增强显著提升了性能，其中缩放增强效果最明显。AugUNet1D在性能上优于"Twin Peaks"算法，检测到的事件特征更接近手动标记的棘波放电。

Conclusion: AugUNet1D是一种有效的自动标记脑电图棘波放电的方法，性能优于现有算法。研究公开了预训练和未训练的模型供其他用户使用。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [62] [Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study](https://arxiv.org/abs/2601.00525)
*Ravi Teja Pagidoju*

Main category: cs.LG

TL;DR: 该研究通过逐步减少LSTM隐藏单元数量（从128到16）来压缩模型，发现64单元模型在保持精度的同时显著减小了模型大小，证明了更大模型不一定更好。


<details>
  <summary>Details</summary>
Motivation: 标准LSTM神经网络在零售业销售预测中虽然准确，但计算需求大，对中小型零售企业构成挑战，需要探索模型压缩方法。

Method: 使用Kaggle Store Item Demand Forecasting数据集（913,000条销售记录），逐步减少LSTM隐藏单元数量（128→64→32→16），研究模型大小与预测精度之间的权衡。

Result: 64单元模型在保持相同精度水平的同时，模型大小减少73%（从280KB到76KB），准确率提高47%，MAPE从128单元的23.6%降至12.4%。

Conclusion: 更大的模型不一定能获得更好的结果，通过适当压缩LSTM模型可以在显著减小模型大小的同时提高预测精度，这对计算资源有限的中小型零售企业具有实用价值。

Abstract: Standard LSTM(Long Short-Term Memory) neural networks provide accurate predictions for sales data in the retail industry, but require a lot of computing power. It can be challenging especially for mid to small retail industries. This paper examines LSTM model compression by gradually reducing the number of hidden units from 128 to 16. We used the Kaggle Store Item Demand Forecasting dataset, which has 913,000 daily sales records from 10 stores and 50 items, to look at the trade-off between model size and how accurate the predictions are. Experiments show that lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it. The mean absolute percentage error (MAPE) ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model. The optimized model is 73% smaller (from 280KB to 76KB) and 47% more accurate. These results show that larger models do not always achieve better results.

</details>


### [63] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: 本文提出了一种多用户上下文赌博机框架，通过图拉普拉斯正则化与核方法的结合，在非线性和图同质性奖励函数下实现结构化探索。


<details>
  <summary>Details</summary>
Motivation: 研究多用户上下文赌博机问题，其中用户通过图结构相关联，且奖励函数既表现出非线性特性又具有图同质性。现有方法在处理这种复杂结构时存在局限，需要一种统一的理论框架来同时捕捉用户间的图结构关系和奖励函数的非线性特征。

Method: 引入了一个原则性的联合惩罚项，结合基于RKHS距离的图平滑项和个体粗糙度惩罚。证明该惩罚等价于单一多用户RKHS中的平方范数，并显式推导了其再生核，该核优雅地融合了图拉普拉斯和基础臂核。将问题重新表述为学习单个"提升"函数，设计了两种算法：LK-GP-UCB和LK-GP-TS，利用高斯过程后验在新核上进行探索。

Result: 提供了高概率遗憾界，其缩放与多用户核的有效维度相关，替代了对用户数量或环境维度的依赖。实证结果表明，在非线性设置中，该方法优于强大的线性和非图感知基线，即使在真实奖励为线性的情况下也保持竞争力。

Conclusion: 该工作提供了一个统一、理论严谨且实用的框架，将拉普拉斯正则化与核化赌博机相结合，用于结构化探索，为多用户上下文赌博机问题提供了新的解决方案。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [64] [Learning to be Reproducible: Custom Loss Design for Robust Neural Networks](https://arxiv.org/abs/2601.00578)
*Waqas Ahmed,Sheeba Samuel,Kevin Coakley,Birgitta Koenig-Ries,Odd Erik Gundersen*

Main category: cs.LG

TL;DR: 提出一种自定义损失函数（CLF）来减少深度学习训练中的随机性影响，提高模型在不同运行中的一致性和可靠性


<details>
  <summary>Details</summary>
Motivation: 当前深度学习训练方法缺乏确保跨运行一致性和鲁棒性的机制，即使在受控的初始化和训练条件下，模型准确率仍存在显著波动，这影响了模型的可复现性和可靠性

Method: 提出自定义损失函数（CLF），通过调整其参数来平衡预测准确率和训练稳定性，减少对权重初始化和数据洗牌等随机因素的敏感性

Result: 在图像分类和时间序列预测的多种架构上进行广泛实验，结果表明CLF显著提高了训练鲁棒性，同时不牺牲预测性能

Conclusion: CLF是一种有效且高效的策略，可用于开发更稳定、可靠和可信的神经网络

Abstract: To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.

</details>


### [65] [When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents](https://arxiv.org/abs/2601.00513)
*Laksh Advani*

Main category: cs.LG

TL;DR: 研究发现小型语言模型（7-9B参数）存在"正确但推理错误"现象，50-69%的正确答案包含根本性推理缺陷，标准准确率指标无法检测。提出了推理完整性评分(RIS)作为过程评估指标，发现RAG能改善推理完整性，而元认知干预反而损害小模型性能。


<details>
  <summary>Details</summary>
Motivation: 部署小型语言模型作为自主代理需要信任其推理过程，而不仅仅是输出结果。当前存在可靠性危机：许多正确答案背后是根本错误的推理过程，这种现象无法通过标准准确率指标检测，对实际部署构成严重风险。

Method: 分析了10,734个推理轨迹，涵盖三个模型和多样化任务。引入了推理完整性评分(RIS)作为过程评估指标，验证了较高的评分者间一致性(κ=0.657)。研究了检索增强生成(RAG)和元认知干预对推理完整性的影响，并通过机制分析探究了其作用原理。最后训练了一个神经分类器用于快速验证推理完整性。

Result: 发现50-69%的正确答案包含根本性推理缺陷。RAG显著改善推理完整性(Cohen's d=0.23-0.93)，减少7.6%的错误；而元认知干预在小模型上反而损害性能(d=-0.14到-0.33)。开发的神经分类器达到0.86 F1分数，速度提升100倍。

Conclusion: 仅依赖准确率评估模型是危险的，因为模型可能"正确但推理错误"。过程验证对于可信代理部署至关重要，RAG通过外部证据基础改善推理，而元认知干预需要足够模型容量。需要开发过程验证工具以确保推理完整性。

Abstract: Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.

</details>


### [66] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: HFedMoE是一个面向异构客户端的MoE联邦学习框架，通过专家重要性评估、自适应专家选择和稀疏感知聚合，解决资源受限设备上大语言模型联邦微调的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护隐私地微调大语言模型，但模型规模过大使得资源受限设备无法进行本地训练。MoE模型虽然计算高效，但在联邦学习环境中面临三个关键挑战：1) 缺乏可靠指标评估专家对本地微调性能的影响；2) 异构计算资源下动态专家激活会超出设备能力；3) 客户端特定的专家子集和路由偏好会破坏全局聚合。

Method: HFedMoE框架包含三个核心组件：1) 基于专家对微调性能贡献的重要性评估机制；2) 从信息瓶颈角度自适应选择专家子集以匹配客户端计算预算；3) 稀疏感知模型聚合策略，基于重要性加权贡献聚合活跃微调的专家和门控参数。

Result: 大量实验表明，HFedMoE在训练准确率和收敛速度方面优于最先进的基准方法。

Conclusion: HFedMoE成功解决了MoE在联邦学习环境中的关键挑战，通过定制化专家选择和稀疏感知聚合，实现了资源受限设备上大语言模型的高效联邦微调。

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [67] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了一种用于在向下封闭凸体上最大化非负、非单调γ-弱DR-次模函数的近似算法，其性能保证随γ平滑变化，在γ=1时恢复0.401近似比，在γ<1时性能优雅下降并优于现有结果。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和优化中，约束下的次模目标最大化是一个基本问题。研究在向下封闭凸体上最大化非负、非单调γ-弱DR-次模函数的问题，旨在提供更好的近似保证。

Method: 结合Frank-Wolfe引导的连续贪婪框架与γ感知的双贪婪步骤，形成简单有效的处理非单调性的方法。

Result: 算法性能保证随γ平滑变化：当γ=1（DR-次模情况）时恢复0.401近似因子；当γ<1时保证优雅下降，并在相同约束下改进了先前报告的γ-弱DR-次模最大化边界。

Conclusion: 该方法为向下封闭凸体上的非单调γ-弱DR-次模最大化提供了最先进的性能保证，通过结合连续贪婪框架和γ感知的双贪婪步骤，有效处理了非单调性问题。

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [68] [A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling](https://arxiv.org/abs/2601.00519)
*Dristi Datta,Tanmoy Debnath,Minh Chau,Manoranjan Paul,Gourab Adhikary,Md Geaur Rahman*

Main category: cs.LG

TL;DR: 提出SAFN网络，通过稀疏注意力机制融合多模态数据，用于帕金森病分类，在PPMI数据集上达到98%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病多模态分析模型存在可解释性差、类别不平衡、高维特征融合困难等问题，需要开发更鲁棒、可解释的深度学习框架。

Method: 提出SAFN网络：使用模态特定编码器处理MRI皮层厚度、MRI体积测量、临床评估和人口统计学数据；采用对称交叉注意力机制捕捉非线性交互；稀疏约束注意力门控层动态选择信息模态；类别平衡焦点损失处理数据不平衡。

Result: 在PPMI数据集703名参与者（570名PD，133名健康对照）上，通过五折交叉验证，SAFN达到0.98±0.02准确率和1.00±0.00 PR-AUC，优于现有基准方法。

Conclusion: SAFN为神经退行性疾病计算分析提供了可重复、透明的多模态建模范式，决策过程临床一致，约60%权重分配给临床评估，符合运动障碍学会诊断原则。

Abstract: Characterising the heterogeneous presentation of Parkinson's disease (PD) requires integrating biological and clinical markers within a unified predictive framework. While multimodal data provide complementary information, many existing computational models struggle with interpretability, class imbalance, or effective fusion of high-dimensional imaging and tabular clinical features. To address these limitations, we propose the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for robust multimodal profiling. SAFN integrates MRI cortical thickness, MRI volumetric measures, clinical assessments, and demographic variables using modality-specific encoders and a symmetric cross-attention mechanism that captures nonlinear interactions between imaging and clinical representations. A sparsity-constrained attention-gating fusion layer dynamically prioritises informative modalities, while a class-balanced focal loss (beta = 0.999, gamma = 1.5) mitigates dataset imbalance without synthetic oversampling. Evaluated on 703 participants (570 PD, 133 healthy controls) from the Parkinson's Progression Markers Initiative using subject-wise five-fold cross-validation, SAFN achieves an accuracy of 0.98 plus or minus 0.02 and a PR-AUC of 1.00 plus or minus 0.00, outperforming established machine learning and deep learning baselines. Interpretability analysis shows a clinically coherent decision process, with approximately 60 percent of predictive weight assigned to clinical assessments, consistent with Movement Disorder Society diagnostic principles. SAFN provides a reproducible and transparent multimodal modelling paradigm for computational profiling of neurodegenerative disease.

</details>


### [69] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: IGBO框架通过双目标优化训练可解释模型，利用DAG编码特征重要性层次结构，使用TIG测量特征重要性，并引入最优路径预言机解决OOD问题。


<details>
  <summary>Details</summary>
Motivation: 现有可解释模型训练方法缺乏结构化领域知识的有效整合，且特征重要性计算存在分布外问题，需要一种能够结合领域知识并解决OOD问题的可解释模型训练框架。

Method: 提出IGBO框架：1) 使用DAG编码特征重要性层次结构；2) 采用TIG测量特征重要性；3) 引入最优路径预言机学习数据流形感知的积分路径以解决OOD问题；4) 通过双目标优化平衡模型准确性和可解释性约束。

Result: 理论分析证明了收敛性和对mini-batch噪声的鲁棒性。在时间序列数据上的实验表明，IGBO能有效强制执行DAG约束且精度损失最小，优于标准正则化基线方法。

Conclusion: IGBO框架成功地将结构化领域知识整合到可解释模型训练中，通过解决TIG计算的OOD问题，实现了模型准确性和可解释性约束的有效平衡，为可解释机器学习提供了新方法。

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [70] [Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation](https://arxiv.org/abs/2601.00664)
*Taekyung Ki,Sangwon Jang,Jaehyeong Jo,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出Avatar Forcing框架，通过扩散强迫实现实时交互式头像生成，解决现有模型缺乏情感互动的问题，实现低延迟（约500ms）和表达性反应


<details>
  <summary>Details</summary>
Motivation: 当前说话头像生成模型缺乏真正的交互式沟通感，通常产生单向响应而缺乏情感参与。需要解决两个关键挑战：在因果约束下实时生成运动，以及在没有额外标注数据的情况下学习表达性、生动的反应

Method: 提出Avatar Forcing框架，通过扩散强迫建模实时用户-头像交互，处理多模态输入（音频和动作）实现低延迟响应。引入直接偏好优化方法，利用通过丢弃用户条件构建的合成损失样本，实现无标签的表达性交互学习

Result: 框架实现实时交互，延迟约500ms，比基线加速6.8倍。生成的响应性和表达性头像运动在超过80%的情况下优于基线

Conclusion: Avatar Forcing框架成功解决了交互式头像生成的关键挑战，实现了低延迟实时交互和表达性反应，为虚拟沟通和内容创作提供了更真实的交互体验

Abstract: Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.

</details>


### [71] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文探讨了大模型联邦定制化的关键挑战，回顾了多种定制技术，并首次在联邦学习框架中实验了前缀调优方法，验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 研究大模型在联邦学习框架下的定制化问题，探索如何在保护数据隐私的同时实现个性化模型定制，解决联邦学习中大模型定制化的挑战。

Method: 回顾了全微调、高效微调、提示工程、前缀调优、知识蒸馏和检索增强生成等大模型定制技术，并讨论了它们在联邦学习框架中的实现方式。特别进行了联邦前缀调优的实验，这是首次在联邦学习环境中应用前缀调优方法。

Result: 联邦前缀调优实验验证了该方法的可行性，性能接近集中式方法。与其他三种联邦定制方法相比，表现出竞争性性能、满意的效率和一致的鲁棒性。

Conclusion: 联邦前缀调优是大模型联邦定制化的有效方法，在保护数据隐私的同时实现了接近集中式方法的性能，为联邦学习框架下的大模型个性化定制提供了可行方案。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [72] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: 本文提出IRPO框架，通过将Bradley-Terry模型集成到GRPO中，解决了成对生成奖励模型的计算瓶颈问题，实现了高效的点式评分和可解释性。


<details>
  <summary>Details</summary>
Motivation: 成对生成奖励模型（GRMs）虽然具有可解释性和推理时扩展性，但与GRPO等强化学习算法集成时存在计算瓶颈：1）成对比较需要O(n^2)时间复杂度；2）重复采样或额外思维链推理带来计算开销。

Method: 提出Intergroup Relative Preference Optimization（IRPO）框架，将Bradley-Terry模型集成到GRPO中，为每个响应生成点式评分，从而在RL训练期间能够高效评估任意数量的候选响应，同时保持可解释性和细粒度奖励信号。

Result: IRPO在多个基准测试中实现了点式GRMs的最先进性能，与当前领先的成对GRMs性能相当。在训练后评估中，IRPO显著优于成对GRMs。

Conclusion: IRPO通过解决成对GRMs的计算瓶颈，提供了一种高效、可扩展的强化学习框架，在保持可解释性的同时实现了优异的性能表现。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [73] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的云原生架构，用于自动生成店铺特定的货架陈列图，将设计时间从30小时减少到0.5小时，成本降低97.5%


<details>
  <summary>Details</summary>
Motivation: 传统货架陈列图设计耗时且昂贵，平均每个复杂布局需要30小时，需要自动化解决方案来提高零售空间优化效率

Method: 采用云原生架构，结合AWS进行模型训练和边缘部署进行实时推理；使用扩散模型学习多个零售点的成功货架布局，通过修改损失函数集成零售特定约束

Result: 系统将货架陈列图设计时间减少98.3%（从30小时到0.5小时），约束满足率达到94.4%；经济分析显示成本降低97.5%，投资回收期为4.4个月；架构可线性扩展，支持10,000个并发店铺请求

Conclusion: 该研究证明了生成式AI在自动化零售空间优化中的可行性，通过云原生扩散模型架构实现了高效、可扩展的货架陈列图自动生成

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [74] [Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty](https://arxiv.org/abs/2601.00737)
*Uğurcan Özalp*

Main category: cs.LG

TL;DR: STAC算法通过引入基于分布评论家的时间性随机不确定性来缩放悲观偏差，替代传统基于认知不确定性的方法，使用单一分布评论家网络提高计算效率，并通过dropout正则化提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 离策略演员-评论家方法虽然样本效率高，但评论家网络存在系统性高估问题。现有方法使用集成技术量化认知不确定性来引入悲观偏差，但这种方法计算成本高且可能无法完全解决随机环境中的高估问题。

Method: 提出STAC算法：1）使用单一分布评论家网络建模时间性随机不确定性（来自随机转移、奖励和策略引起的贝尔曼目标变异性）；2）基于这种不确定性缩放时间差分更新中的悲观偏差；3）对评论家和演员网络应用dropout进行正则化。

Result: 实验表明：1）仅基于分布评论家的悲观偏差就足以缓解高估问题；2）在随机环境中自然产生风险规避行为；3）引入dropout通过正则化进一步提高训练稳定性和性能；4）使用单一网络设计提高了计算效率。

Conclusion: STAC算法通过利用时间性随机不确定性而非认知不确定性来缩放悲观偏差，提供了一种更高效、稳定的离策略演员-评论家方法，在随机环境中表现出更好的风险规避特性和计算效率。

Abstract: Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.

</details>


### [75] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 提出基于非平衡随机动力学的熵触发重训练框架，通过Fokker-Planck方程建模数据漂移，使用KL散度量化模型-数据不匹配，通过熵平衡分解实现无标签的重训练触发策略。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在非平稳环境中部署时，由于数据漂移会导致性能下降。现有的漂移检测方法大多缺乏原理性的动力学解释，且无法在重训练频率和运营成本之间提供平衡指导。

Method: 将部署时的数据漂移建模为由Fokker-Planck方程控制的概率流，使用时变Kullback-Leibler散度量化模型-数据不匹配。通过熵平衡分解，利用非负的熵产生项来触发无标签的重训练策略。

Result: 在受控的非平稳分类实验中，熵触发重训练实现了与高频重训练相当的预测性能，同时相对于每日重训练和基于标签的策略，将重训练事件减少了一个数量级。

Conclusion: 基于非平衡随机动力学的熵触发重训练框架提供了一种原理性的方法，能够有效平衡模型性能和运营成本，通过响应累积的不匹配而非延迟的性能崩溃来实现高效的重训练策略。

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [76] [FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing](https://arxiv.org/abs/2601.00785)
*Sunny Gupta,Amit Sethi*

Main category: cs.LG

TL;DR: FedHypeVAE是一个差分隐私、超网络驱动的联邦学习框架，用于在非独立同分布客户端条件下合成嵌入级数据，通过个性化生成层而非下游模型来保护隐私并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦数据共享方法在非独立同分布客户端异质性下表现不佳，且对梯度泄漏的正式保护有限。需要一种既能保护隐私又能适应客户端数据分布的嵌入级数据合成方法。

Method: 基于条件VAE架构，用超网络生成客户端感知的解码器和类条件先验，替代单一全局解码器和固定先验。采用差分隐私优化超网络，结合局部MMD对齐和Lipschitz正则化增强稳定性。训练后使用元代码实现领域无关合成。

Result: FedHypeVAE在非独立同分布条件下实现了更好的分布对齐和生成质量，通过差分隐私保护客户端数据隐私，同时支持可控的多领域覆盖和领域无关合成。

Conclusion: FedHypeVAE在生成器层面统一了个性化、隐私保护和分布对齐，为联邦环境下的隐私保护数据合成建立了理论基础，代码已开源。

Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE

</details>


### [77] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: 论文提出需要区分两种对抗性弱点：利用非鲁棒特征的攻击和不利用这些特征的攻击，并提出了基于集成的方法来量化非鲁棒特征的操纵程度。


<details>
  <summary>Details</summary>
Motivation: 现有非鲁棒特征理论虽然被广泛接受，但忽略了那些不直接利用这些特征的对抗样本。作者认为这两种样本代表了不同类型的对抗性弱点，需要区分评估对抗鲁棒性。

Method: 提出了基于集成的度量方法，用于衡量对抗扰动对非鲁棒特征的操纵程度，并用该指标分析攻击者生成的对抗样本的构成。

Result: 通过新视角重新审视了多个现象，包括锐度感知最小化对对抗鲁棒性的影响，以及在鲁棒数据集上对抗训练和标准训练之间的鲁棒性差距。

Conclusion: 区分两种对抗性弱点对于准确评估对抗鲁棒性至关重要，新的度量方法为深入理解对抗攻击机制提供了工具，有助于重新解释相关现象。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [78] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出一种无需训练的方法，通过分析注意力矩阵的谱特征来检测大语言模型中数学推理的有效性，在多个模型架构上达到85-95%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 需要一种无需训练数据、微调或学习分类器的方法来检测大语言模型中数学推理的有效性，以解决幻觉检测和AI安全监控的实际应用需求。

Method: 将注意力矩阵视为动态图的邻接矩阵，提取四个可解释的谱诊断特征：Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和谱熵，通过单一阈值判断推理有效性。

Result: 在7个来自4个独立架构家族的Transformer模型上实验，效应量高达Cohen's d=3.30，分类准确率达85.0-95.6%，校准阈值在完整数据集上达到93-95%准确率。

Conclusion: 谱图分析为推理验证提供了原则性框架，能检测逻辑一致性而非编译器接受度，注意力机制设计影响哪些谱特征捕获推理有效性，具有立即应用于幻觉检测和AI安全监控的潜力。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


### [79] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

TL;DR: 使用机器学习方法预测骑行时长，结合路线拓扑特征和运动员当前体能状态，相比传统物理模型更实用


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模型的骑行时长预测方法需要大量参数（如空气阻力系数、实时风速预测），这对业余骑手不切实际，需要更实用的解决方案

Method: 采用机器学习方法，使用路线拓扑特征和运动员当前体能状态（基于训练负荷指标），通过历史数据学习运动员特定的表现模式，使用Lasso回归模型

Result: 在单人数据集（N=96次骑行）上进行评估，Lasso回归结合拓扑+体能特征达到MAE=6.60分钟和R²=0.922，整合体能指标（CTL, ATL）相比仅使用拓扑特征减少14%误差

Conclusion: 机器学习方法能有效预测骑行时长，运动员生理状态对表现有显著约束作用，渐进式检查点预测支持动态比赛规划

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [80] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

TL;DR: 该论文提出了一种基于交通感知图神经网络的强化学习框架，用于优化城市出租车热点推荐，通过整合实时交通数据减少乘客等待时间和司机行驶距离。


<details>
  <summary>Details</summary>
Motivation: 在智慧城市交通背景下，传统出租车热点预测模型仅依赖历史需求数据，忽略了交通拥堵、道路事故、公共事件等动态影响因素，导致出租车供需匹配效率低下。

Method: 将城市道路网络建模为图结构（交叉口为节点，路段为边），节点属性包含历史需求、事件邻近度和实时拥堵分数；使用图神经网络编码时空依赖关系，结合Q-learning智能体推荐最优出租车热点；奖励机制联合优化乘客等待时间、司机行驶距离和拥堵避免。

Result: 在基于德里真实地理边界和历史叫车请求模式生成的模拟数据集上，相比基线随机选择方法，该模型将乘客等待时间减少约56%，行驶距离减少38%。

Conclusion: 该交通感知的图强化学习框架能有效优化出租车热点推荐，可适应多模式交通系统并集成到智慧城市平台中实现实时城市移动性优化。

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [81] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: YapBench是一个轻量级基准测试，用于量化大语言模型在需要简洁回答的提示上的过度生成问题，通过测量超出最小必要答案的额外长度来评估模型的冗余度。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型如ChatGPT、Claude和Gemini作为通用助手使用时，经常对简单请求给出不必要的冗长回答，包含冗余解释、模棱两可的表述和模板化内容，这增加了认知负担和基于token的推理成本。先前研究表明基于偏好的后训练和LLM评估可能导致系统性的长度偏差，即使质量相当，更长的回答也更容易获得奖励。

Method: 研究者引入了YapBench基准测试，包含300多个英文提示，涵盖三种需要简洁回答的场景：(A) 最小或模糊输入，理想行为是简短澄清；(B) 封闭式事实问题，有简短稳定答案；(C) 单行编码任务，单个命令或代码片段即可。主要指标YapScore测量超出基准答案的额外字符数，YapIndex是类别级别中位数YapScore的均匀加权平均值。

Result: 评估了76个助手型大语言模型，观察到中位数额外长度存在数量级差异，并识别出特定类别的失败模式，包括在模糊输入上的"真空填充"行为，以及在单行技术请求上的解释或格式化开销。

Conclusion: YapBench提供了一个量化评估大语言模型冗余度的工具，揭示了不同模型在简洁性方面的显著差异和特定失败模式，有助于跟踪模型简洁性行为随时间的变化，并发布了基准测试和实时排行榜。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [82] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE是一个轻量级强化学习框架，通过群体智能探索层增强标准策略梯度方法，在非平稳奖励和高维策略任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略的情况下。现有方法在复杂任务中探索效率不足，需要更有效的探索机制。

Method: ARISE框架在标准策略梯度方法基础上添加紧凑的群体探索层，将策略动作与粒子驱动的建议混合。每个粒子代表在动作空间中采样的候选策略轨迹，并使用奖励方差信号自适应调节探索强度。

Result: 在简单基准测试中仅有轻微改进（CartPole-v1 +0.7%），但在更具挑战性的任务中表现显著：LunarLander-v3 +46%，Hopper-v4 +22%，同时在Walker2d和Ant上保持稳定性。在非平稳奖励变化下，ARISE比PPO在CartPole上高出75分，在LunarLander上也有相应改进。

Conclusion: ARISE提供了一个简单、架构无关的途径，可以在不改变核心算法结构的情况下，创建更具探索性和鲁棒性的强化学习智能体。消融研究证实群体组件和自适应机制都对性能有贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [83] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出贝叶斯逆博弈框架，通过变分自编码器和可微纳什博弈求解器从交互数据中学习目标分布，量化不确定性，提升下游决策安全性


<details>
  <summary>Details</summary>
Motivation: 现有逆博弈方法仅提供点估计，无法量化估计不确定性，导致下游规划可能过度自信地采取不安全行动。需要一种能够处理多模态观测数据并实时生成后验分布的方法

Method: 提出贝叶斯逆博弈框架，使用结构化变分自编码器嵌入可微纳什博弈求解器，从交互数据中学习先验和后验分布，无需真实目标标签，支持多模态观测

Result: 框架成功学习到先验和后验分布，相比最大似然估计方法提升推理质量，实现更安全的下游决策而不牺牲效率。多模态推理在轨迹信息不足时进一步降低不确定性

Conclusion: 贝叶斯逆博弈方法能够有效量化不确定性，提高自主决策安全性，特别是在观测信息有限的情况下，多模态推理能显著降低不确定性

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [84] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: BSAT模型使用B样条自适应分词器解决长时序预测问题，通过高曲率区域放置token实现自适应分段，结合混合位置编码L-RoPE，在内存受限场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统transformer在长时序预测中存在自注意力二次复杂度问题和均匀分块与数据语义结构不匹配的问题，需要更高效的自适应方法。

Method: 提出B样条自适应分词器(BSAT)，通过B样条拟合时序数据，在高曲率区域自适应放置token；提出混合位置编码L-RoPE，结合可学习加性位置编码和层间可学习基的旋转位置嵌入。

Result: 在多个公开基准测试中表现出色，在高压缩率下保持强大性能，特别适合内存受限的应用场景。

Conclusion: BSAT通过自适应分段和混合位置编码有效解决了长时序预测中的效率和语义对齐问题，为内存受限场景提供了实用解决方案。

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [85] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应精度调优框架，用于线性求解器和其他算法，通过上下文多臂老虎机问题动态选择计算步骤的最佳精度配置，平衡精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在科学计算中，混合精度数值方法需要平衡计算精度和效率。传统方法通常采用固定精度，无法根据具体问题动态调整。本文旨在开发一个自适应框架，能够根据计算状态智能选择最佳精度配置。

Method: 将精度调优问题建模为上下文多臂老虎机问题，使用离散化状态空间和增量动作值估计。采用Q表将离散化特征（如近似条件数和矩阵范数）映射到动作（特定步骤的精度配置），通过epsilon-greedy策略优化，最大化平衡精度和计算成本的多目标奖励函数。

Result: 在线性系统求解的迭代精化应用中，该框架能够有效选择精度配置，在保持与双精度基准相当的精度同时，显著降低计算成本。框架对未见数据具有良好的泛化能力。

Conclusion: 这是首个基于强化学习的精度自动调优工作，并在未见数据集上验证。该框架为科学计算中的混合精度数值方法提供了新思路，可推广到其他数值算法中。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [86] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文提出Distributional Creative Reasoning (DCR)框架，分析当前LLM推理优化方法导致多样性衰减的问题，并提供保持正确性和创造性的理论方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理管道主要优化正确性，导致推理路径分布崩溃，降低语义熵和创造性问题解决能力。需要理论框架来分析这种失败并设计保持多样性的方法。

Method: 提出Distributional Creative Reasoning (DCR)框架，将训练视为通过解决方案轨迹概率测度的梯度流。STaR、GRPO、DPO等方法都是该损失函数的特例。

Result: 1) 多样性衰减定理，描述基于正确性的目标如何导致STaR、GRPO、DPO的不同多样性衰减模式；2) 确保收敛到稳定且多样策略的设计；3) 实际可操作的实现方案。

Conclusion: DCR为LLM提供了首个保持正确性和创造性的原则性方案，解决了当前推理优化方法导致的多样性崩溃问题。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [87] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: 提出基于协变量依赖隐马尔可夫模型(CDHMM)的足球角球防守评估框架，通过球员追踪数据推断盯人和区域防守分配，实现无标签的防守贡献评估和反事实分析。


<details>
  <summary>Details</summary>
Motivation: 传统足球防守评估指标难以捕捉无球防守的协调运动，现有反事实方法（如ghosting模型）依赖"平均"行为模拟而缺乏战术背景，特别是在角球这种高度结构化的场景中。

Method: 针对角球场景开发协变量依赖隐马尔可夫模型(CDHMM)，从球员追踪数据中无监督推断时间分辨的盯人和区域防守分配，并基于此提出防守贡献归因框架和角色条件ghosting方法。

Result: 该方法能够从球员追踪数据中直接推断防守分配，提供可解释的防守贡献评估，并支持基于战术背景的反事实分析，相比传统方法更具上下文感知能力。

Conclusion: CDHMM框架为足球角球防守提供了新的评估工具，能够更准确地评估无球防守表现，为教练和战术分析提供有价值的洞察。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [88] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的持续学习方法，在在线适应学习中显著减少内存占用（降至0.3%），同时保持高知识保留准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的知识容易过时，持续学习需要更新模型而不遗忘旧知识。现有基于记忆库的方法面临内存不断增长的挑战，特别是在大规模数据流场景下。

Method: 提出MBC模型：1) 通过码本优化策略压缩记忆库；2) 引入在线重置机制防止码本崩溃；3) 在注意力层使用Key-Value低秩适配，高效利用压缩后的记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最强基线的0.3%，同时在在线适应学习中保持高保留准确率。

Conclusion: MBC通过记忆库压缩和码本优化，有效解决了持续学习中内存增长问题，为大语言模型的在线适应学习提供了高效解决方案。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [89] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散的软重参数化方法，用于处理分类变量的梯度优化问题，通过高斯噪声过程的去噪器实现训练自由的扩散采样器。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的分类变量优化通常依赖于有噪声的得分函数估计器，或依赖于连续松弛方法，后者虽然能提供路径梯度但优化的是有偏的温度依赖目标。本文旨在扩展这类松弛方法，为分类分布提供更好的优化方案。

Method: 引入基于扩散的软重参数化方法，利用高斯噪声过程下分类分布去噪器的闭式解，构建训练自由的扩散采样器，实现通过该采样器的反向传播。

Result: 在各种基准测试中，提出的重参数化技巧展现出竞争性或改进的优化性能。

Conclusion: 扩散基软重参数化为分类变量的梯度优化提供了一种有效的新方法，能够通过训练自由的扩散采样器实现高效的反向传播，在多个基准测试中表现优异。

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>
