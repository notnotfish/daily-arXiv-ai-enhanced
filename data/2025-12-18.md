<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 87]
- [cs.AI](#cs.AI) [Total: 22]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Where to Explore: A Reach and Cost-Aware Approach for Unbiased Data Collection in Recommender Systems](https://arxiv.org/abs/2512.14733)
*Qiang Chen,Venkatesh Ganapati Hegde*

Main category: cs.IR

TL;DR: 本文提出了一种在大型流媒体平台中安全高效地进行内容探索的方法，通过优化探索内容的位置来平衡短期业务指标和长期推荐质量。


<details>
  <summary>Details</summary>
Motivation: 探索对于提升长期推荐质量至关重要，但在远程电视环境中，探索往往会损害短期业务表现，因为用户被动参与、期望即时相关性且几乎没有纠正机会。需要一种既能收集无偏交互数据又不损害平台观看时间目标的方法。

Method: 通过识别滚动深度区域中参与度较低的位置，策略性地引入名为"Something Completely Different"的专用容器，其中包含随机化内容。该方法不是在整个用户界面中统一强制执行探索，而是根据经验上的低成本、高覆盖位置来条件化其出现，以最小化与平台级观看时间目标的权衡。

Result: 在拥有超过1亿月活跃用户的大规模流媒体平台上部署，广泛的A/B测试显示该策略能够保持业务指标，同时收集无偏交互数据。收集的无偏数据集成到下游候选生成中，显著提高了用户参与度。

Conclusion: 该方法通过引入可部署的、基于行为感知的机制，在现有行内多样化和基于bandit的探索技术基础上进行了补充，能够大规模展示探索性内容。验证了无偏数据对推荐系统的重要价值。

Abstract: Exploration is essential to improve long-term recommendation quality, but it often degrades short-term business performance, especially in remote-first TV environments where users engage passively, expect instant relevance, and offer few chances for correction. This paper introduces an approach for delivering content-level exploration safely and efficiently by optimizing its placement based on reach and opportunity cost. Deployed on a large-scale streaming platform with over 100 million monthly active users, our approach identifies scroll-depth regions with lower engagement and strategically introduces a dedicated container, the "Something Completely Different" row containing randomized content. Rather than enforcing exploration uniformly across the user interface (UI), we condition its appearance on empirically low-cost, high-reach positions to ensure minimal tradeoff against platform-level watch time goals. Extensive A/B testing shows that this strategy preserves business metrics while collecting unbiased interaction data. Our method complements existing intra-row diversification and bandit-based exploration techniques by introducing a deployable, behaviorally informed mechanism for surfacing exploratory content at scale. Moreover, we demonstrate that the collected unbiased data, integrated into downstream candidate generation, significantly improves user engagement, validating its value for recommender systems.

</details>


### [2] [Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models](https://arxiv.org/abs/2512.15372)
*Mikel Williams-Lekuona,Georgina Cosma*

Main category: cs.IR

TL;DR: ICAR提出图像复杂度感知检索方法，让视觉transformer根据图像复杂度动态调整计算量，简单图像使用较少计算，复杂图像使用完整网络深度，同时保持跨模态对齐能力。


<details>
  <summary>Details</summary>
Motivation: 传统视觉transformer对所有图像使用统一计算量（175.33 GFLOPs），无论简单产品照片还是复杂街景都消耗相同计算资源，效率低下。需要一种能根据图像复杂度动态调整计算的方法。

Method: 1. ICAR框架：通过双路径训练产生兼容嵌入，使不同处理深度的图像表示与文本嵌入保持语义空间对齐；2. ConvNeXt-IC：将图像复杂度评估作为分类任务，使用现代分类器骨干实现高效复杂度评估；3. 直接图像-文本匹配，无需昂贵的重排序。

Result: 1. ConvNeXt-IC达到0.959的皮尔逊相关性（与人类判断），速度提升4.4倍；2. ICAR在标准基准测试和真实网络数据上实现20%的实际加速，保持类别级性能，达到实例级性能的95%；3. 维持跨模态对齐，无需额外计算开销。

Conclusion: ICAR通过图像复杂度感知的动态计算分配，在保持视觉-语言模型性能的同时显著提升效率，为实现可持续扩展的视觉-语言系统提供了有效解决方案。

Abstract: Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.

</details>


### [3] [MedNuggetizer: Confidence-Based Information Nugget Extraction from Medical Documents](https://arxiv.org/abs/2512.15384)
*Gregor Donabauer,Samy Ateia,Udo Kruschwitz,Maximilian Burger,Matthias May,Christian Gilfrich,Maximilian Haas,Julio Ruben Rodas Garzaro,Christoph Eckl*

Main category: cs.IR

TL;DR: MedNuggetizer是一个基于大语言模型的工具，用于从医学文档中提取和聚类信息片段，帮助临床医生探索医学证据。


<details>
  <summary>Details</summary>
Motivation: 临床医生需要从大量医学文献中提取可靠证据，但现有方法效率低下。需要一种工具能够从长文档中高效提取查询相关的医学证据。

Method: 基于大语言模型，通过重复提取信息片段并进行聚类，在单个文档内和跨多个文档中生成可靠的医学证据。

Result: 在"前列腺活检前抗生素预防"的临床用例中，使用主要泌尿科指南和PubMed研究进行评估，领域专家评价显示该工具能高效探索长文档并提取可靠的查询相关医学证据。

Conclusion: MedNuggetizer为临床医生和研究人员提供了一种高效探索长文档并轻松提取可靠、查询相关的医学证据的方法。

Abstract: We present MedNuggetizer, https://mednugget-ai.de/; access is available upon request.}, a tool for query-driven extraction and clustering of information nuggets from medical documents to support clinicians in exploring underlying medical evidence. Backed by a large language model (LLM), \textit{MedNuggetizer} performs repeated extractions of information nuggets that are then grouped to generate reliable evidence within and across multiple documents. We demonstrate its utility on the clinical use case of \textit{antibiotic prophylaxis before prostate biopsy} by using major urological guidelines and recent PubMed studies as sources of information. Evaluation by domain experts shows that \textit{MedNuggetizer} provides clinicians and researchers with an efficient way to explore long documents and easily extract reliable, query-focused medical evidence.

</details>


### [4] [BERT and CNN integrated Neural Collaborative Filtering for Recommender Systems](https://arxiv.org/abs/2512.15526)
*Abdullah Al Munem,Sumona Yeasmin,Mohammad Rezwanul Huq*

Main category: cs.IR

TL;DR: 提出了一种结合BERT和CNN的神经协同过滤推荐模型，能够处理数值、分类和图像数据，在MovieLens数据集上优于传统NCF和基于BERT的NCF模型。


<details>
  <summary>Details</summary>
Motivation: 为了提升网站用户交互和利润，需要更强大的推荐系统来根据用户独特偏好推荐项目。现有推荐系统在处理多模态数据方面存在局限性。

Method: 提出了BERT和CNN集成的神经协同过滤模型，能够处理数值、分类和图像数据，从用户和项目配置文件中提取潜在特征，发现用户兴趣。

Result: 在MovieLens数据集上，提出的模型在25个epochs训练后，召回率达到0.72，Hit Ratio @ 10达到0.486，优于简单的NCF和基于BERT的NCF模型。

Conclusion: 同时考虑分类数据和图像数据可以显著提升推荐系统的性能，多模态数据处理对于推荐系统效果有重要影响。

Abstract: Every day, a significant number of users visit the internet for different needs. The owners of a website generate profits from the user interaction with the contents or items of the website. A robust recommendation system can increase user interaction with a website by recommending items according to the user's unique preferences. BERT and CNN-integrated neural collaborative filtering (NCF) have been proposed for the recommendation system in this experiment. The proposed model takes inputs from the user and item profile and finds the user's interest. This model can handle numeric, categorical, and image data to extract the latent features from the inputs. The model is trained and validated on a small sample of the MovieLens dataset for 25 epochs. The same dataset has been used to train and validate a simple NCF and a BERT-based NCF model and compared with the proposed model. The proposed model outperformed those two baseline models. The obtained result for the proposed model is 0.72 recall and 0.486 Hit Ratio @ 10 for 799 users on the MovieLens dataset. This experiment concludes that considering both categorical and image data can improve the performance of a recommendation system.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts](https://arxiv.org/abs/2512.14706)
*Krunal Jesani,Dmitry Ignatov,Radu Timofte*

Main category: cs.LG

TL;DR: 本文提出NN-Caption，一个基于LLM引导的神经架构搜索管道，通过组合CNN编码器和序列解码器自动生成可运行的图像描述模型，在MS COCO数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统神经架构搜索需要大量人工专业知识或自动化试错来设计深度学习模型，本文旨在利用LLM的代码生成能力自动设计图像描述模型架构，减少人工干预。

Method: 使用DeepSeek-R1-0528-Qwen3-8B作为主要生成器，通过提示模板将LEMUR分类主干中的CNN编码器与序列解码器（LSTM/GRU/Transformer）组合，在严格的Net API约束下生成可运行架构，并采用提示规则和迭代代码修复解决生成问题。

Result: LLM生成了数十个图像描述模型，超过一半成功训练并产生有意义的描述。使用不同数量输入模型片段（5 vs 10）时，提供更多候选组件会导致成功率轻微下降。报告了训练动态和最高BLEU-4得分，展示了LLM不仅能提出架构，还能建议超参数和训练实践。

Conclusion: LLM引导的神经架构搜索具有显著潜力，本文提出的管道集成了基于提示的代码生成与自动评估，为LEMUR数据集添加了数十个新颖的图像描述模型，促进了可复现的基准测试和下游AutoML研究。

Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.

</details>


### [6] [Topological Metric for Unsupervised Embedding Quality Evaluation](https://arxiv.org/abs/2512.15285)
*Aleksei Shestov,Anton Klenitskiy,Daria Denisova,Amurkhan Dzagkoev,Daniil Petrovich,Andrey Savchenko,Maksim Makarenko*

Main category: cs.LG

TL;DR: 提出了一种基于持久同调的拓扑感知度量方法Persistence，用于无监督评估嵌入空间的质量，无需标签即可量化几何结构和拓扑丰富性。


<details>
  <summary>Details</summary>
Motivation: 现代表示学习主要依赖无监督和自监督方法，但在没有标签的情况下评估嵌入质量仍然是一个开放挑战。现有方法通常假设线性可分性或依赖协方差结构，无法捕捉全局和多尺度组织。

Method: 提出Persistence度量方法，基于持久同调（persistent homology）技术，能够量化嵌入空间的几何结构和拓扑丰富性。该方法完全无监督，不依赖标签信息，能够捕捉全局和多尺度的空间组织特征。

Result: 在多个领域的实证结果表明，Persistence在下游任务性能相关性方面始终达到顶级水平，优于现有的无监督度量方法，能够可靠地进行模型和超参数选择。

Conclusion: Persistence作为一种拓扑感知的无监督度量方法，能够有效评估嵌入空间的质量，为无监督表示学习提供了可靠的评估工具，解决了无标签情况下嵌入质量评估的挑战。

Abstract: Modern representation learning increasingly relies on unsupervised and self-supervised methods trained on large-scale unlabeled data. While these approaches achieve impressive generalization across tasks and domains, evaluating embedding quality without labels remains an open challenge. In this work, we propose Persistence, a topology-aware metric based on persistent homology that quantifies the geometric structure and topological richness of embedding spaces in a fully unsupervised manner. Unlike metrics that assume linear separability or rely on covariance structure, Persistence captures global and multi-scale organization. Empirical results across diverse domains show that Persistence consistently achieves top-tier correlations with downstream performance, outperforming existing unsupervised metrics and enabling reliable model and hyperparameter selection.

</details>


### [7] [Autonomous Source Knowledge Selection in Multi-Domain Adaptation](https://arxiv.org/abs/2512.14710)
*Keqiuyin Li,Jie Lu,Hua Zuo,Guangquan Zhang*

Main category: cs.LG

TL;DR: AutoS方法通过自主选择源域样本和模型，结合伪标签增强，提升多域自适应性能


<details>
  <summary>Details</summary>
Motivation: 多源域自适应中，多个源域常包含冗余或不相关信息，特别是在大规模源域设置下，这会损害迁移性能。需要开发有效策略从海量源域中识别和选择最具可迁移性的知识来解决目标任务。

Method: 提出AutoS方法：1) 采用密度驱动选择策略在训练中选择源域样本，并确定哪些源模型应对目标预测做出贡献；2) 基于预训练多模态模型构建伪标签增强模块，减轻目标标签噪声并改进自监督。

Result: 在真实世界数据集上的实验表明所提方法的优越性。

Conclusion: AutoS方法能够自主选择相关源域信息和模型，有效提升多域自适应性能，特别是在大规模源域设置下。

Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in massive-source domain settings. It is urgent to develop effective strategies for identifying and selecting the most transferable knowledge from massive source domains to address the target task. In this paper, we propose a multi-domain adaptation method named \underline{\textit{Auto}}nomous Source Knowledge \underline{\textit{S}}election (AutoS) to autonomosly select source training samples and models, enabling the prediction of target task using more relevant and transferable source information. The proposed method employs a density-driven selection strategy to choose source samples during training and to determine which source models should contribute to target prediction. Simulteneously, a pseudo-label enhancement module built on a pre-trained multimodal modal is employed to mitigate target label noise and improve self-supervision. Experiments on real-world datasets indicate the superiority of the proposed method.

</details>


### [8] [SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI](https://arxiv.org/abs/2512.14712)
*Ryan Cartularo*

Main category: cs.LG

TL;DR: 比较了两种脓毒症预测模型架构：端到端深度融合与上下文感知堆叠，发现后者在小型抗生素队列中表现更优，开发了SepsisLateFusion模型实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 脓毒症占ICU入院近20%，但传统预测模型难以有效整合异质数据流，要么模态孤立要么依赖脆弱的早期融合，需要更有效的多模态融合方法

Method: 1. 提出Quad-Modal Hierarchical Gated Attention Network (SepsisFusionFormer)进行端到端深度融合；2. 开发SepsisLateFusion，采用上下文感知混合专家架构，将模态视为正交专家（Historian静态、Monitor时序、Reader NLP），通过CatBoost元学习器动态门控；3. 构建SepsisSuite部署框架

Result: 1. SepsisFusionFormer在小样本抗生素队列中因"注意力饥饿"过拟合（AUC 0.66）；2. SepsisLateFusion在临床发作前4小时预测达到SOTA性能（AUC 0.915），通过校准决策阈值将漏诊减少48%；3. 四模态集成在多类抗生素选择任务中表现最佳（AUC 0.72）

Conclusion: 上下文感知混合专家架构优于端到端深度融合，特别是在小样本场景下；SepsisLateFusion模型实现了临床可用的预防窗口，为及时干预提供了有效工具；开源框架SepsisSuite支持临床决策

Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info

</details>


### [9] [A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour](https://arxiv.org/abs/2512.14713)
*Georges Sfeir,Stephane Hess,Thomas O. Hancock,Filipe Rodrigues,Jamal Amani Rad,Michiel Bliemer,Matthew Beck,Fayyaz Khan*

Main category: cs.LG

TL;DR: 提出了一种潜在类别强化学习（LCRL）模型，用于捕捉旅行决策中的偏好形成过程和个体异质性，并通过驾驶模拟器数据集验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 旅行决策通常涉及经验形成过程，个体随时间学习自己的偏好。同时，不同旅行者之间存在显著的异质性，包括基本偏好和偏好演化方式。现有模型难以同时捕捉这两个现象。

Method: 提出了潜在类别强化学习（LCRL）模型，通过变分贝叶斯方法估计参数。模型应用于驾驶模拟器数据集，识别不同类别的个体偏好演化模式。

Result: 识别出三类明显不同的个体：第一类显示情境依赖偏好和情境特定的利用倾向；第二类遵循持久利用策略，不受情境影响；第三类采用探索策略并结合情境特定偏好。

Conclusion: LCRL模型能够有效捕捉旅行决策中的偏好形成过程和个体异质性，为理解不同旅行者群体的行为模式提供了新的分析框架。

Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model that allows analysts to capture both of these phenomena. We apply the model to a driving simulator dataset and estimate the parameters through Variational Bayes. We identify three distinct classes of individuals that differ markedly in how they adapt their preferences: the first displays context-dependent preferences with context-specific exploitative tendencies; the second follows a persistent exploitative strategy regardless of context; and the third engages in an exploratory strategy combined with context-specific preferences.

</details>


### [10] [Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms](https://arxiv.org/abs/2512.14714)
*Lucas Cesar Ferreira Domingos,Russell Brinkworth,Paulo Eduardo Santos,Karl Sammut*

Main category: cs.LG

TL;DR: 本文提出GSE ResNeXt深度学习架构，结合可学习Gabor卷积层与ResNeXt骨干网络，通过挤压-激励注意力机制提升水下声学目标分类性能，在数据有限场景下显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 水下声学目标的远程检测与分类对环境监测和国防至关重要，但船舶辐射和环境噪声的复杂性给信号处理带来挑战。现有机器学习方法存在数据集有限、缺乏标准化实验等问题，影响模型的泛化能力和鲁棒性。

Method: 提出GSE ResNeXt架构，将可学习Gabor卷积层与ResNeXt骨干网络结合，并引入挤压-激励注意力机制。Gabor滤波器作为二维自适应带通滤波器扩展特征通道表示，结合通道注意力提升训练稳定性和收敛速度。

Result: 在三个复杂度递增的分类任务上评估，GSE ResNeXt在分类性能上持续优于Xception、ResNet和MobileNetV2等基线模型。Gabor卷积的加入使训练时间减少28%，且发现船舶与传感器距离对性能有显著影响。

Conclusion: 结果表明信号处理策略对提升模型在不同环境条件下的可靠性和泛化能力至关重要，特别是在数据有限的水下声学分类场景中。未来研究应关注减轻环境因素对输入信号的影响。

Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues such as limited dataset availability and a lack of standardised experimentation hinder generalisation and robustness. This paper introduces GSE ResNeXt, a deep learning architecture integrating learnable Gabor convolutional layers with a ResNeXt backbone enhanced by squeeze-and-excitation attention mechanisms. The Gabor filters serve as two-dimensional adaptive band-pass filters, extending the feature channel representation. Its combination with channel attention improves training stability and convergence while enhancing the model's ability to extract discriminative features. The model is evaluated on three classification tasks of increasing complexity. In particular, the impact of temporal differences between the training and testing data is explored, revealing that the distance between the vessel and sensor significantly affects performance. Results show that, GSE ResNeXt consistently outperforms baseline models like Xception, ResNet, and MobileNetV2, in terms of classification performance. Regarding stability and convergence, the addition of Gabor convolutions in the initial layers of the model represents a 28% reduction in training time. These results emphasise the importance of signal processing strategies in improving the reliability and generalisation of models under different environmental conditions, especially in data-limited underwater acoustic classification scenarios. Future developments should focus on mitigating the impact of environmental factors on input signals.

</details>


### [11] [How a Bit Becomes a Story: Semantic Steering via Differentiable Fault Injection](https://arxiv.org/abs/2512.14715)
*Zafaryab Haider,Md Hafizur Rahman,Shane Moeykens,Vijay Devabhaktuni,Prabuddha Chakraborty*

Main category: cs.LG

TL;DR: 本文首次研究了对大型语言模型权重的比特级扰动如何影响图像描述生成任务的语义含义，同时保持语法结构完整，提出了基于梯度敏感度估计的BLADE框架来定位语义关键比特。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明硬件比特翻转可能使transformer在非生成任务中变得脆弱，但忽视了生成系统的语义和语言维度。本研究旨在探索低层次比特扰动如何影响LLM生成的图像描述的语义含义，同时保持语法结构，揭示比特级变化如何引导生成式视觉语言模型的高层语义。

Method: 设计了可微分的故障分析框架BLADE，使用基于梯度的敏感度估计来定位语义关键比特，然后通过描述级别的语义-流畅性目标来优化比特选择。该方法不是简单地破坏描述，而是理解语义在比特级别的编码、分布和可改变性。

Result: 研究发现语义漂移不是随机的，而是可以通过模型自身的梯度进行可微估计。即使难以察觉的低层次比特变化也能引导生成式视觉语言模型的高层语义输出，揭示了比特级故障如何重塑模型的语义输出。

Conclusion: 这项工作首次系统研究了比特级扰动对LLM生成语义的影响，提出的BLADE框架不仅揭示了语义在比特级别的编码特性，还为鲁棒性测试、对抗防御和可解释AI开辟了新途径，展示了结构化比特级故障如何重塑模型的语义输出。

Abstract: Hard-to-detect hardware bit flips, from either malicious circuitry or bugs, have already been shown to make transformers vulnerable in non-generative tasks. This work, for the first time, investigates how low-level, bitwise perturbations (fault injection) to the weights of a large language model (LLM) used for image captioning can influence the semantic meaning of its generated descriptions while preserving grammatical structure. While prior fault analysis methods have shown that flipping a few bits can crash classifiers or degrade accuracy, these approaches overlook the semantic and linguistic dimensions of generative systems. In image captioning models, a single flipped bit might subtly alter how visual features map to words, shifting the entire narrative an AI tells about the world. We hypothesize that such semantic drifts are not random but differentiably estimable. That is, the model's own gradients can predict which bits, if perturbed, will most strongly influence meaning while leaving syntax and fluency intact. We design a differentiable fault analysis framework, BLADE (Bit-level Fault Analysis via Differentiable Estimation), that uses gradient-based sensitivity estimation to locate semantically critical bits and then refines their selection through a caption-level semantic-fluency objective. Our goal is not merely to corrupt captions, but to understand how meaning itself is encoded, distributed, and alterable at the bit level, revealing that even imperceptible low-level changes can steer the high-level semantics of generative vision-language models. It also opens pathways for robustness testing, adversarial defense, and explainable AI, by exposing how structured bit-level faults can reshape a model's semantic output.

</details>


### [12] [Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox](https://arxiv.org/abs/2512.14717)
*Ziqian Bi,Danyang Zhang,Junhao Song,Chiung-Yi Tseng*

Main category: cs.LG

TL;DR: GPT-OSS-20B小模型在金融NLP任务中达到与更大模型相当的准确率（65.1% vs 66.5%），同时计算效率显著更高，挑战了模型规模与性能直接相关的传统假设。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在金融服务中的快速应用，需要建立严格的评估框架来评估其性能、效率和实际适用性，以指导生产环境中的部署决策。

Method: 对GPT-OSS模型家族及当代LLMs在10个不同金融NLP任务上进行全面评估，包括情感分析、问答和实体识别任务，使用真实金融数据集（Financial PhraseBank、FiQA-SA、FLARE FINERORD），并引入新的效率指标来衡量性能与资源利用之间的权衡。

Result: GPT-OSS-20B小模型在准确率上与GPT-OSS-120B相当（65.1% vs 66.5%），但计算效率显著更高（198.4 Token Efficiency Score，159.80 tokens/s），且GPT-OSS模型整体优于包括Qwen3-235B在内的更大竞争对手。

Conclusion: GPT-OSS的架构创新和训练策略使小模型能够以显著减少的计算开销实现有竞争力的性能，为金融应用中可持续且经济高效的LLM部署提供了途径。

Abstract: The rapid adoption of large language models in financial services necessitates rigorous evaluation frameworks to assess their performance, efficiency, and practical applicability. This paper conducts a comprehensive evaluation of the GPT-OSS model family alongside contemporary LLMs across ten diverse financial NLP tasks. Through extensive experimentation on 120B and 20B parameter variants of GPT-OSS, we reveal a counterintuitive finding: the smaller GPT-OSS-20B model achieves comparable accuracy (65.1% vs 66.5%) while demonstrating superior computational efficiency with 198.4 Token Efficiency Score and 159.80 tokens per second processing speed [1]. Our evaluation encompasses sentiment analysis, question answering, and entity recognition tasks using real-world financial datasets including Financial PhraseBank, FiQA-SA, and FLARE FINERORD. We introduce novel efficiency metrics that capture the trade-off between model performance and resource utilization, providing critical insights for deployment decisions in production environments. The benchmark reveals that GPT-OSS models consistently outperform larger competitors including Qwen3-235B, challenging the prevailing assumption that model scale directly correlates with task performance [2]. Our findings demonstrate that architectural innovations and training strategies in GPT-OSS enable smaller models to achieve competitive performance with significantly reduced computational overhead, offering a pathway toward sustainable and cost-effective deployment of LLMs in financial applications.

</details>


### [13] [SEED: Spectral Entropy-Guided Evaluation of SpatialTemporal Dependencies for Multivariate Time Series Forecasting](https://arxiv.org/abs/2512.14718)
*Feng Xiong,Zongxia Xie,Yanru Sun,Haoyu Wang,Jianhong Lin*

Main category: cs.LG

TL;DR: SEED是一个基于谱熵引导的时空依赖建模框架，通过动态评估变量间的空间和时间依赖关系，自适应平衡通道独立和通道依赖策略，解决了现有方法中时间自依赖被干扰、负相关被忽略、变量无法感知时间位置等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力或图的方法存在三个关键问题：1）强时间自依赖常被无关变量干扰；2）softmax归一化忽略并反转负相关关系；3）变量难以感知自身的时间位置。这些问题限制了多元时间序列预测中复杂变量间依赖关系的准确建模。

Method: SEED框架包含四个核心组件：1）依赖评估器：利用谱熵动态评估每个变量的时空依赖关系，自适应平衡通道独立和通道依赖策略；2）基于谱熵的融合器：进一步细化依赖权重，分离由其他变量影响而非内在动态产生的时间规律；3）有符号图构造器：支持有符号边权重，克服softmax限制，保留负相关关系；4）上下文空间提取器：利用局部上下文窗口提取空间特征，帮助变量感知时间位置。

Result: 在12个来自不同应用领域的真实世界数据集上进行广泛实验，SEED实现了最先进的性能，验证了其有效性和通用性。

Conclusion: SEED通过谱熵引导的依赖评估框架，有效解决了多元时间序列预测中时空依赖建模的关键问题，在多个领域数据集上表现出优越性能，为复杂变量间依赖关系的准确建模提供了新思路。

Abstract: Effective multivariate time series forecasting often benefits from accurately modeling complex inter-variable dependencies. However, existing attention- or graph-based methods face three key issues: (a) strong temporal self-dependencies are often disrupted by irrelevant variables; (b) softmax normalization ignores and reverses negative correlations; (c) variables struggle to perceive their temporal positions. To address these, we propose \textbf{SEED}, a Spectral Entropy-guided Evaluation framework for spatial-temporal Dependency modeling. SEED introduces a Dependency Evaluator, a key innovation that leverages spectral entropy to dynamically provide a preliminary evaluation of the spatial and temporal dependencies of each variable, enabling the model to adaptively balance Channel Independence (CI) and Channel Dependence (CD) strategies. To account for temporal regularities originating from the influence of other variables rather than intrinsic dynamics, we propose Spectral Entropy-based Fuser to further refine the evaluated dependency weights, effectively separating this part. Moreover, to preserve negative correlations, we introduce a Signed Graph Constructor that enables signed edge weights, overcoming the limitations of softmax. Finally, to help variables perceive their temporal positions and thereby construct more comprehensive spatial features, we introduce the Context Spatial Extractor, which leverages local contextual windows to extract spatial features. Extensive experiments on 12 real-world datasets from various application domains demonstrate that SEED achieves state-of-the-art performance, validating its effectiveness and generality.

</details>


### [14] [Hybrid Attribution Priors for Explainable and Robust Model Training](https://arxiv.org/abs/2512.14719)
*Zhuoran Zhang,Feng Zhang,Shangyuan Li,Yang Shi,Yuanxing Zhang,Wei Chen,Tengjiao Wang,Kam-Fai Wong*

Main category: cs.LG

TL;DR: 该论文提出了一种新的归因先验提取框架CAP，用于解决现有归因方法在分类任务中难以区分语义相似类别的局限性，通过引导语言模型捕捉细粒度类别差异来提升模型的解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有解释引导学习框架中，归因先验的提取面临重大挑战。研究发现，虽然现有归因方法能够可靠地突出类别相关标记，但它们往往关注语义相似类别共享的常见关键词，这些类别在标准训练下已经难以区分，导致归因提供的判别性线索不足，限制了提升模型区分能力的效果。

Method: 提出Class-Aware Attribution Prior (CAP)框架，引导语言模型捕捉细粒度类别差异，产生更显著、更具判别性的归因先验。进一步提出CAP Hybrid方法，将CAP的先验与现有归因技术的先验相结合，形成更全面平衡的监督信号。通过将模型的自归因与这些丰富的先验对齐，鼓励学习多样化、决策相关的特征。

Result: 在完整数据、少样本和对抗场景下的广泛实验表明，该方法在解释性和鲁棒性方面均取得一致提升。

Conclusion: CAP框架能够有效解决现有归因方法在区分语义相似类别方面的局限性，通过引导模型关注细粒度类别差异，显著提升了小语言模型的解释性和鲁棒性，特别是在分类任务中。

Abstract: Small language models (SLMs) are widely used in tasks that require low latency and lightweight deployment, particularly classification. As interpretability and robustness gain increasing importance, explanation-guided learning has emerged as an effective framework by introducing attribution-based supervision during training; however, deriving general and reliable attribution priors remains a significant challenge. Through an analysis of representative attribution methods in classification settings, we find that although these methods can reliably highlight class-relevant tokens, they often focus on common keywords shared by semantically similar classes. Because such classes are already difficult to distinguish under standard training, these attributions provide insufficient discriminative cues, limiting their ability to improve model differentiation. To overcome this limitation, we propose Class-Aware Attribution Prior (CAP), a novel attribution prior extraction framework that guides language models toward capturing fine-grained class distinctions and producing more salient, discriminative attribution priors. Building on this idea, we further introduce CAP Hybrid, which combines priors from CAP with those from existing attribution techniques to form a more comprehensive and balanced supervisory signal. By aligning a model's self-attribution with these enriched priors, our approach encourages the learning of diverse, decision-relevant features. Extensive experiments in full-data, few-shot, and adversarial scenarios demonstrate that our method consistently enhances both interpretability and robustness.

</details>


### [15] [Automatic Extraction of Rules for Generating Synthetic Patient Data From Real-World Population Data Using Glioblastoma as an Example](https://arxiv.org/abs/2512.14721)
*Arno Appenzeller,Nick Terzer,André Hohmeyer,Jan-Philipp Redlich,Sabine Luttmann,Friedrich Feuerhake,Nadine S. Schaadt,Timm Intemann,Sarah Teuber-Hanselmann,Stefan Nikolin,Joachim Weis,Klaus Kraywinkel,Pascal Birnstill*

Main category: cs.LG

TL;DR: 本文提出了一种基于癌症报告统计数据自动生成Synthea规则的方法，并以胶质母细胞瘤为例创建了Synthea模块，生成合成数据集，保留了原始数据的统计特性。


<details>
  <summary>Details</summary>
Motivation: 合成数据生成是隐私合规的医疗数据二次使用的重要技术。Synthea作为流行的规则基础合成数据生成器，其规则创建过程复杂，需要专家知识和真实样本数据。本文旨在解决这一瓶颈，实现从表格数据自动生成Synthea规则。

Method: 提出一种基于癌症报告统计数据自动生成Synthea规则的方法。具体步骤包括：1) 从癌症报告中提取表格数据；2) 基于统计信息自动生成Synthea规则；3) 以胶质母细胞瘤为用例，从真实数据集创建Synthea模块；4) 使用该模块生成合成数据集。

Result: 生成的合成数据集成功再现了已知的疾病病程，并基本保留了原始数据的统计特性。合成数据与原始数据集在疾病发展模式上具有良好的一致性。

Conclusion: 合成患者数据在隐私保护研究中具有巨大潜力，可用于假设制定和原型开发。但医学解释应考虑当前方法的特定局限性，合成数据可作为研究工具但不能完全替代真实数据。

Abstract: The generation of synthetic data is a promising technology to make medical data available for secondary use in a privacy-compliant manner. A popular method for creating realistic patient data is the rule-based Synthea data generator. Synthea generates data based on rules describing the lifetime of a synthetic patient. These rules typically express the probability of a condition occurring, such as a disease, depending on factors like age. Since they only contain statistical information, rules usually have no specific data protection requirements. However, creating meaningful rules can be a very complex process that requires expert knowledge and realistic sample data. In this paper, we introduce and evaluate an approach to automatically generate Synthea rules based on statistics from tabular data, which we extracted from cancer reports. As an example use case, we created a Synthea module for glioblastoma from a real-world dataset and used it to generate a synthetic dataset. Compared to the original dataset, the synthetic data reproduced known disease courses and mostly retained the statistical properties. Overall, synthetic patient data holds great potential for privacy-preserving research. The data can be used to formulate hypotheses and to develop prototypes, but medical interpretation should consider the specific limitations as with any currently available approach.

</details>


### [16] [HATSolver: Learning Groebner Bases with Hierarchical Attention Transformers](https://arxiv.org/abs/2512.14722)
*Mohamed Malhou,Ludovic Perret,Kristin Lauter*

Main category: cs.LG

TL;DR: 本文改进了Kera等人使用Transformer计算Gröbner基的方法，通过引入分层注意力Transformer（HAT）架构来求解多元多项式方程组，显著降低了计算成本并能够处理更大规模的问题。


<details>
  <summary>Details</summary>
Motivation: Gröbner基是计算机代数中的核心概念，有众多实际应用。虽然Kera等人在NeurIPS 2024上首次使用Transformer计算Gröbner基，但传统扁平注意力模型在处理具有层次结构的数据时效率有限。本文旨在通过引入具有树结构归纳偏置的HAT架构来改进这一方法，以更有效地建模数据中的层次关系。

Method: 采用分层注意力Transformer（HAT）架构，该架构包含树结构归纳偏置，能够建模数据中的层次关系。方法推广到任意深度，并包含详细的计算成本分析。结合课程学习策略，使模型能够逐步学习处理更复杂的实例。

Result: 与Kera等人（2024）的扁平注意力模型相比，HAT架构实现了显著的计算节省。结合课程学习后，该方法能够解决比Kera等人论文中实例大得多的多项式方程组问题。

Conclusion: 分层注意力Transformer架构为计算Gröbner基提供了更高效的方法，通过利用数据中的层次结构关系，在计算成本上获得显著优势，并能扩展到更大规模的问题实例。

Abstract: At NeurIPS 2024, Kera et al. introduced the use of transformers for computing Groebner bases, a central object in computer algebra with numerous practical applications. In this paper, we improve this approach by applying Hierarchical Attention Transformers (HATs) to solve systems of multivariate polynomial equations via Groebner bases computation. The HAT architecture incorporates a tree-structured inductive bias that enables the modeling of hierarchical relationships present in the data and thus achieves significant computational savings compared to conventional flat attention models. We generalize to arbitrary depths and include a detailed computational cost analysis. Combined with curriculum learning, our method solves instances that are much larger than those in Kera et al. (2024 Learning to compute Groebner bases)

</details>


### [17] [Generative Urban Flow Modeling: From Geometry to Airflow with Graph Diffusion](https://arxiv.org/abs/2512.14725)
*Francisco Giral,Álvaro Manzano,Ignacio Gómez,Petros Koumoutsakos,Soledad Le Clainche*

Main category: cs.LG

TL;DR: 提出了一种基于生成扩散框架的方法，用于在非结构化网格上合成稳态城市风场，仅需几何信息，无需时间演化或密集测量。


<details>
  <summary>Details</summary>
Motivation: 城市风场建模对空气质量评估和可持续城市规划至关重要，但面临复杂几何形状的挑战。低阶模型难以捕捉几何效应，而高保真CFD模拟成本过高，特别是在处理多种几何形状或风况时。

Method: 结合分层图神经网络与基于分数的扩散建模，构建生成扩散框架，仅使用几何信息在非结构化网格上生成稳态风场。模型在多个网格切片和风向下训练，能够泛化到未见过的几何形状。

Result: 模型能够生成准确多样的速度场，恢复关键流动结构（如尾流和再循环区），提供不确定性感知预测，对网格变化具有鲁棒性，并在不同推理机制下表现良好。

Conclusion: 这是构建建筑环境基础模型的第一步，有助于城市规划者在城市密集化和气候不确定性背景下快速评估设计决策。

Abstract: Urban wind flow modeling and simulation play an important role in air quality assessment and sustainable city planning. A key challenge for modeling and simulation is handling the complex geometries of the urban landscape. Low order models are limited in capturing the effects of geometry, while high-fidelity Computational Fluid Dynamics (CFD) simulations are prohibitively expensive, especially across multiple geometries or wind conditions. Here, we propose a generative diffusion framework for synthesizing steady-state urban wind fields over unstructured meshes that requires only geometry information. The framework combines a hierarchical graph neural network with score-based diffusion modeling to generate accurate and diverse velocity fields without requiring temporal rollouts or dense measurements. Trained across multiple mesh slices and wind angles, the model generalizes to unseen geometries, recovers key flow structures such as wakes and recirculation zones, and offers uncertainty-aware predictions. Ablation studies confirm robustness to mesh variation and performance under different inference regimes. This work develops is the first step towards foundation models for the built environment that can help urban planners rapidly evaluate design decisions under densification and climate uncertainty.

</details>


### [18] [Quantum Decision Transformers (QDT): Synergistic Entanglement and Interference for Offline Reinforcement Learning](https://arxiv.org/abs/2512.14726)
*Abraham Itzhak Weinberg*

Main category: cs.LG

TL;DR: 量子决策变换器(QDT)通过引入量子启发式计算机制，解决了传统决策变换器在长时程信用分配和复杂状态-动作依赖关系上的问题，在连续控制任务中实现了超过2000%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习允许从预收集数据集学习策略而无需环境交互，但现有决策变换器架构在长时程信用分配和复杂状态-动作依赖关系方面存在困难。需要新的架构设计来解决这些挑战。

Method: 提出量子决策变换器(QDT)，包含两个核心组件：1) 量子启发式注意力机制，通过纠缠操作捕获非局部特征相关性；2) 量子前馈网络，具有多路径处理和可学习干扰的自适应计算能力。

Result: 在连续控制任务上，相比标准决策变换器实现了超过2000%的性能提升，并在不同数据质量下表现出优越的泛化能力。消融研究表明量子启发式组件之间存在强烈的协同效应。

Conclusion: 量子启发式架构设计需要整体协同设计相互依赖的机制，而非模块化组件采用。该研究建立了量子启发式设计原则作为推进序列决策中变换器架构的有前景方向，其影响超越强化学习扩展到更广泛的神经架构设计领域。

Abstract: Offline reinforcement learning enables policy learning from pre-collected datasets without environment interaction, but existing Decision Transformer (DT) architectures struggle with long-horizon credit assignment and complex state-action dependencies. We introduce the Quantum Decision Transformer (QDT), a novel architecture incorporating quantum-inspired computational mechanisms to address these challenges. Our approach integrates two core components: Quantum-Inspired Attention with entanglement operations that capture non-local feature correlations, and Quantum Feedforward Networks with multi-path processing and learnable interference for adaptive computation. Through comprehensive experiments on continuous control tasks, we demonstrate over 2,000\% performance improvement compared to standard DTs, with superior generalization across varying data qualities. Critically, our ablation studies reveal strong synergistic effects between quantum-inspired components: neither alone achieves competitive performance, yet their combination produces dramatic improvements far exceeding individual contributions. This synergy demonstrates that effective quantum-inspired architecture design requires holistic co-design of interdependent mechanisms rather than modular component adoption. Our analysis identifies three key computational advantages: enhanced credit assignment through non-local correlations, implicit ensemble behavior via parallel processing, and adaptive resource allocation through learnable interference. These findings establish quantum-inspired design principles as a promising direction for advancing transformer architectures in sequential decision-making, with implications extending beyond reinforcement learning to neural architecture design more broadly.

</details>


### [19] [A Critical Perspective on Finite Sample Conformal Prediction Theory in Medical Applications](https://arxiv.org/abs/2512.14727)
*Klaus-Rudolf Kladny,Bernhard Schölkopf,Lisa Koch,Christian F. Baumgartner,Michael Muehlebach*

Main category: cs.LG

TL;DR: 该论文质疑了共形预测在小校准集情况下的实际效用，尽管其统计保证理论上适用于任意大小的校准集，但在医学图像分类任务中，小校准集的实际效用有限。


<details>
  <summary>Details</summary>
Motivation: 机器学习在医疗领域应用广泛，但临床决策需要可靠的置信度估计。共形预测虽然能提供统计保证，但通常认为其适用于任意大小的校准集，而医疗数据往往稀缺，难以获得大规模校准集，因此需要评估小校准集下共形预测的实际效用。

Method: 通过理论分析和实证验证，在医学图像分类任务上评估共形预测在不同大小校准集下的表现，对比统计保证与实际效用之间的关系。

Result: 研究发现，虽然共形预测的统计保证在理论上适用于任意大小的校准集，但实际效用高度依赖于校准集的大小。小校准集虽然能提供统计保证，但预测集可能过大而不实用，在医疗应用中价值有限。

Conclusion: 共形预测在小校准集情况下的实际效用有限，医疗领域应用时需要谨慎考虑校准集大小对预测质量的影响，不能仅依赖理论统计保证。

Abstract: Machine learning (ML) is transforming healthcare, but safe clinical decisions demand reliable uncertainty estimates that standard ML models fail to provide. Conformal prediction (CP) is a popular tool that allows users to turn heuristic uncertainty estimates into uncertainty estimates with statistical guarantees. CP works by converting predictions of a ML model, together with a calibration sample, into prediction sets that are guaranteed to contain the true label with any desired probability. An often cited advantage is that CP theory holds for calibration samples of arbitrary size, suggesting that uncertainty estimates with practically meaningful statistical guarantees can be achieved even if only small calibration sets are available. We question this promise by showing that, although the statistical guarantees hold for calibration sets of arbitrary size, the practical utility of these guarantees does highly depend on the size of the calibration set. This observation is relevant in medical domains because data is often scarce and obtaining large calibration sets is therefore infeasible. We corroborate our critique in an empirical demonstration on a medical image classification task.

</details>


### [20] [A data-driven approach to inferring travel trajectory during peak hours in urban rail transit systems](https://arxiv.org/abs/2512.14728)
*Jie He,Yong Qin,Jianyuan Guo,Xuan Sun,Xuanchuan Zheng*

Main category: cs.LG

TL;DR: 本文提出了一种基于AFC和AVL数据的城市轨道交通个体出行轨迹推断方法，通过KLEM算法实现数据驱动的参数估计，无需外部调查数据，在高峰时段轨迹推断准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: 城市轨道交通精细化轨迹推断对运营组织具有重要意义，现有方法依赖外部调查数据或合成数据验证，缺乏鲁棒性和适用性。

Method: 基于时空约束建立列车备选集，采用基于KL散度结合EM算法的KLEM方法进行数据驱动自适应轨迹推断，构建完整的出行轨迹。

Result: 该方法在高峰时段城市轨道交通出行轨迹推断中准确率超过90%，实现了高精度乘客轨迹推断。

Conclusion: 提出的数据驱动方法能够有效推断个体出行轨迹，消除了对外部调查数据的依赖，提高了模型的鲁棒性和适用性。

Abstract: Refined trajectory inference of urban rail transit is of great significance to the operation organization. In this paper, we develop a fully data-driven approach to inferring individual travel trajectories in urban rail transit systems. It utilizes data from the Automatic Fare Collection (AFC) and Automatic Vehicle Location (AVL) systems to infer key trajectory elements, such as selected train, access/egress time, and transfer time. The approach includes establishing train alternative sets based on spatio-temporal constraints, data-driven adaptive trajectory inference, and trave l trajectory construction. To realize data-driven adaptive trajectory inference, a data-driven parameter estimation method based on KL divergence combined with EM algorithm (KLEM) was proposed. This method eliminates the reliance on external or survey data for parameter fitting, enhancing the robustness and applicability of the model. Furthermore, to overcome the limitations of using synthetic data to validate the result, this paper employs real individual travel trajectory data for verification. The results show that the approach developed in this paper can achieve high-precision passenger trajectory inference, with an accuracy rate of over 90% in urban rail transit travel trajectory inference during peak hours.

</details>


### [21] [Semantic Geometry for policy-constrained interpretation](https://arxiv.org/abs/2512.14731)
*Nikit Phadke*

Main category: cs.LG

TL;DR: 提出几何框架防止高风险领域中的幻觉承诺，将语义表示为单位球面上的方向，证据建模为见证向量集，可接受解释对应球形凸区域，通过约束优化实现解释，矛盾时产生拒绝结果。


<details>
  <summary>Details</summary>
Motivation: 解决高风险领域中语义解释可能产生幻觉承诺的问题，特别是在金融等受监管领域，需要确保解释不会产生虚假的批准或承诺。

Method: 使用几何框架：1) 语义表示为单位球面上的方向；2) 证据建模为见证向量集；3) 可接受解释对应球形凸区域；4) 政策约束作为显式先验引入；5) 解释简化为约束优化问题；6) 矛盾时产生拒绝结果。

Result: 在大规模受监管金融数据上的实证验证显示，在多种政策制度下实现了零幻觉批准，这是首次在大规模应用中取得这样的结果。

Conclusion: 该几何框架为政策约束的语义解释提供了理论基础，能够防止高风险领域中的幻觉承诺，证明了其复杂性边界在信息论上是最优的，并在实际应用中取得了显著效果。

Abstract: We present a geometric framework for policy-constrained semantic interpretation that provably prevents hallucinated commitments in high-stakes domains. Semantic meaning is represented as direction on a unit sphere, evidence is modeled as sets of witness vectors, and admissible interpretations correspond to spherical convex regions. Policy constraints are introduced as explicit priors defined over the same manifold, separated from evidence geometry. Interpretation reduces to constrained optimization over admissible regions, with refusal emerging as a topologically necessary outcome under contradiction or policy exclusion. We connect this framework to information theory, Bayesian inference, and sheaf-theoretic semantics, proving that our complexity bounds are information-theoretically optimal. Empirical validation on large scale regulated financial data demonstrates zero hallucinated approvals across multiple policy regimes-the first such result at scale.

</details>


### [22] [INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT](https://arxiv.org/abs/2512.14732)
*Idan Tankel,Nir Mazor,Rafi Brada,Christina LeBedis,Guy ben-Yosef*

Main category: cs.LG

TL;DR: 本文提出了一种基于LLM和VLM的规划-执行代理框架，用于自动化腹部CT扫描中偶然发现的检测、分类和报告，相比纯VLM方法在准确性和效率上表现更优。


<details>
  <summary>Details</summary>
Motivation: CT扫描中的偶然发现虽然通常良性，但具有重要临床意义，需要遵循指南报告。传统放射科医生手动检查耗时且存在变异性，需要提高检测效率和精度。

Method: 采用规划-执行代理框架：规划器基于LLM生成Python脚本，使用预定义基础函数；执行器运行脚本，通过VLM、分割模型和图像处理子程序执行必要的检查和检测。

Result: 在腹部CT基准测试中对三个器官进行实验，结果显示该框架在准确性和效率方面优于现有的纯VLM方法。

Conclusion: 提出的规划-执行代理框架能够有效自动化管理偶然发现的过程，提高腹部CT扫描中偶然发现检测、分类和报告的效率和精度。

Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.

</details>


### [23] [Inference Time Feature Injection: A Lightweight Approach for Real-Time Recommendation Freshness](https://arxiv.org/abs/2512.14734)
*Qiang Chen,Venkatesh Ganapati Hegde,Hongfei Li*

Main category: cs.LG

TL;DR: 提出了一种轻量级、模型无关的日内个性化方法，通过推理时选择性注入近期观看历史来更新用户特征，无需模型重训练，显著提升长视频流媒体推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有长视频流媒体推荐系统通常依赖批量训练模型和每日更新的特征，导致用户特征在一天内保持静态，无法纳入用户最新行为，造成推荐结果过时。

Method: 开发了一种轻量级、模型无关的日内个性化方法，在推理时选择性地用近期观看历史覆盖过时的用户特征，使系统能够即时适应用户偏好变化，无需重新训练模型。

Result: 通过将个性化反馈循环从每日缩短到日内，观察到关键用户参与度指标统计显著提升0.47%，这是近期实验周期中观察到的最显著参与度提升之一。

Conclusion: 这是首次发表的证据表明日内个性化能够在长视频流媒体服务中产生有意义的积极影响，为需要模型重训练的完全实时架构提供了一个有吸引力的替代方案。

Abstract: Many recommender systems in long-form video streaming reply on batch-trained models and batch-updated features, where user features are updated daily and served statically throughout the day. While efficient, this approach fails to incorporate a user's most recent actions, often resulting in stale recommendations. In this work, we present a lightweight, model-agnostic approach for intra-day personalization that selectively injects recent watch history at inference time without requiring model retraining. Our approach selectively overrides stale user features at inference time using the recent watch history, allowing the system to adapt instantly to evolving preferences. By reducing the personalization feedback loop from daily to intra-day, we observed a statistically significant 0.47% increase in key user engagement metrics which ranked among the most substantial engagement gains observed in recent experimentation cycles. To our knowledge, this is the first published evidence that intra-day personalization can drive meaningful impact in long-form video streaming service, providing a compelling alternative to full real-time architectures where model retraining is required.

</details>


### [24] [NoveltyRank: Estimating Conceptual Novelty of AI Papers](https://arxiv.org/abs/2512.14738)
*Zhengxu Yan,Han Li,Yuming Feng*

Main category: cs.LG

TL;DR: 开发了一个评估AI论文概念新颖性的模型，通过标题、摘要和语义相似度来量化研究原创性，帮助识别真正创新的工作


<details>
  <summary>Details</summary>
Motivation: 随着学术出版门槛降低，AI领域论文数量激增，真正新颖和有影响力的工作难以脱颖而出，手动评估新颖性不稳定且耗时

Method: 通过论文标题、摘要和与先前文献的语义相似度评估新颖性，探索两种任务形式：二元分类（预测绝对新颖性）和成对新颖性比较（学习相对新颖性），使用Qwen3-4B-Instruct-2507和SciBERT进行微调

Result: 开发了公开可用的新颖性评估系统，并与GPT-5.1进行基准测试，分析任务形式和建模选择对性能的影响

Conclusion: 该研究提供了一个数据驱动、可扩展的研究原创性评估方法，能帮助研究人员高效识别真正创新的论文，为会议评审提供定量一致的新颖性信号

Abstract: With the growing ease of academic publishing, the volume of research papers, especially in AI-related fields, has surged dramatically. This flood of publications makes it difficult for truly novel and impactful work to stand out, and manual novelty assessment is often unstable and time-consuming. Our project aims to develop a model that estimates and ranks the conceptual novelty of AI papers, enabling a data-driven and scalable assessment of research originality. Such a system can help researchers efficiently identify submissions that introduce genuinely innovative ideas rather than minor variants, and provide conference reviewers with a quantitative and consistent signal of novelty. Our approach evaluates novelty primarily through a paper's title, abstract, and semantic similarity to prior literature. Given the motivation of novelty estimation, we explore two task formulations with different modeling objectives, each offering a different perspective: (1) binary classification, which predicts the paper's absolute novelty from learned patterns of prior novel works, and (2) pairwise novelty comparison, which learns to distinguish papers by relative novelty over others. We fine-tune Qwen3-4B-Instruct-2507 and SciBERT on both tasks, benchmarking against GPT-5.1 to analyze how task formulation and modeling choices affect performance. The implementation is publicly available at https://github.com/ZhengxuYan/NoveltyRank.

</details>


### [25] [Guided Discrete Diffusion for Constraint Satisfaction Problems](https://arxiv.org/abs/2512.14765)
*Justin Jung*

Main category: cs.LG

TL;DR: 提出离散扩散引导方法解决约束满足问题，以数独为例展示无监督求解能力


<details>
  <summary>Details</summary>
Motivation: 传统约束满足问题求解方法通常需要监督或特定启发式规则，本文旨在开发一种无需监督的通用求解框架

Method: 采用离散扩散模型结合引导机制，通过逐步去噪过程满足约束条件，特别针对数独等组合优化问题

Result: 成功实现无监督数独求解，验证了离散扩散引导在约束满足问题上的有效性

Conclusion: 离散扩散引导为约束满足问题提供了一种新颖的无监督求解范式，具有扩展到其他组合优化问题的潜力

Abstract: We propose discrete diffusion guidance for constraint satisfaction problems (CSPs) and demonstrate its ability to solve Sudoku puzzles without supervision.

</details>


### [26] [Evaluating Weather Forecasts from a Decision Maker's Perspective](https://arxiv.org/abs/2512.14779)
*Kornelius Raeth,Nicole Ludwig*

Main category: cs.LG

TL;DR: 决策校准框架从决策者角度评估天气预报价值，发现传统预报评估与下游决策表现不一致


<details>
  <summary>Details</summary>
Motivation: 传统天气预报评估主要从预报员角度进行统计比较，但实际应用中预报用于决策，需要从决策者角度评估预报价值

Method: 提出决策校准框架，在决策层面而非预报层面评估预报性能，比较机器学习与传统数值天气预报模型在不同天气依赖决策任务中的表现

Result: 模型在预报层面的性能不能可靠地转化为下游决策表现：一些性能差异只在决策层面显现，不同决策任务中模型排名会变化

Conclusion: 典型预报评估不足以为特定决策任务选择最优预报模型，需要决策层面的评估框架

Abstract: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.

</details>


### [27] [Unreliable Uncertainty Estimates with Monte Carlo Dropout](https://arxiv.org/abs/2512.14851)
*Aslak Djupskås,Alexander Johannes Stasik,Signe Riemer-Sørensen*

Main category: cs.LG

TL;DR: 蒙特卡洛dropout作为贝叶斯推断的高效近似方法，在捕捉真实不确定性方面存在局限性，特别是在外推和内插区域，其不确定性估计不如传统贝叶斯方法可靠。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，可靠的不确定性估计对机器学习模型至关重要。虽然精确的贝叶斯推断提供了原则性方法，但对于深度神经网络计算上不可行。蒙特卡洛dropout被提出作为深度学习贝叶斯推断的高效近似方法。

Method: 通过实证研究蒙特卡洛dropout捕捉真实不确定性的能力，并与高斯过程和贝叶斯神经网络进行比较。蒙特卡洛dropout在推理时应用神经元dropout生成多个子模型，产生预测分布来估计不确定性。

Result: 研究发现蒙特卡洛dropout难以准确反映底层真实不确定性，特别是在外推和内插区域无法捕捉到不确定性增加，而贝叶斯模型在这些区域能够观察到不确定性增加。

Conclusion: 蒙特卡洛dropout的不确定性估计不如传统贝叶斯方法可靠，无法有效捕捉认知不确定性和偶然不确定性。该方法在实现和评估中存在局限性。

Abstract: Reliable uncertainty estimation is crucial for machine learning models, especially in safety-critical domains. While exact Bayesian inference offers a principled approach, it is often computationally infeasible for deep neural networks. Monte Carlo dropout (MCD) was proposed as an efficient approximation to Bayesian inference in deep learning by applying neuron dropout at inference time \citep{gal2016dropout}. Hence, the method generates multiple sub-models yielding a distribution of predictions to estimate uncertainty. We empirically investigate its ability to capture true uncertainty and compare to Gaussian Processes (GP) and Bayesian Neural Networks (BNN). We find that MCD struggles to accurately reflect the underlying true uncertainty, particularly failing to capture increased uncertainty in extrapolation and interpolation regions as observed in Bayesian models. The findings suggest that uncertainty estimates from MCD, as implemented and evaluated in these experiments, is not as reliable as those from traditional Bayesian approaches for capturing epistemic and aleatoric uncertainty.

</details>


### [28] [How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal](https://arxiv.org/abs/2512.14873)
*Sam Jeong,Hae Yong Kim*

Main category: cs.LG

TL;DR: 研究发现FAN网络的改进主要来自正弦激活而非余弦激活，其机制不是周期性特性，而是正弦函数在x=0附近的非零导数缓解了梯度消失问题，特别是解决了ReLU的"死亡神经元"问题。


<details>
  <summary>Details</summary>
Motivation: 虽然Fourier Analysis Network (FAN)被证明能小幅提升神经网络性能，但其改进机制一直不明确。本研究旨在揭示FAN真正有效的机制，并基于此开发更高效的收敛加速器。

Method: 通过分析正弦和余弦激活的各自贡献，发现只有正弦激活有正面效果。进一步研究表明，改进源于正弦函数在x=0附近的局部行为，其非零导数缓解了梯度消失问题。基于此开发了Dual-Activation Layer (DAL)。

Result: 在三个任务上的评估显示：噪声正弦信号与纯噪声分类、MNIST数字分类、ECG生物特征识别中，DAL模型都比传统激活函数收敛更快，达到相同或更高的验证准确率。

Conclusion: FAN的改进机制应从频谱解释转向训练动态分析，其核心是缓解梯度消失问题。基于此开发的DAL能更有效地加速收敛，为神经网络激活函数设计提供了新思路。

Abstract: Fourier Analysis Network (FAN) was recently proposed as a simple way to improve neural network performance by replacing part of ReLU activations with sine and cosine functions. Although several studies have reported small but consistent gains across tasks, the underlying mechanism behind these improvements has remained unclear. In this work, we show that only the sine activation contributes positively to performance, whereas the cosine activation tends to be detrimental. Our analysis reveals that the improvement is not a consequence of the sine function's periodic nature; instead, it stems from the function's local behavior near x = 0, where its non-zero derivative mitigates the vanishing-gradient problem. We further show that FAN primarily alleviates the dying-ReLU problem, in which a neuron consistently receives negative inputs, produces zero gradients, and stops learning. Although modern ReLU-like activations, such as Leaky ReLU, GELU, and Swish, reduce ReLU's zero-gradient region, they still contain input domains where gradients remain significantly diminished, contributing to slower optimization and hindering rapid convergence. FAN addresses this limitation by introducing a more stable gradient pathway. This analysis shifts the understanding of FAN's benefits from a spectral interpretation to a concrete analysis of training dynamics, leading to the development of the Dual-Activation Layer (DAL), a more efficient convergence accelerator. We evaluate DAL on three tasks: classification of noisy sinusoidal signals versus pure noise, MNIST digit classification, and ECG-based biometric recognition. In all cases, DAL models converge faster and achieve equal or higher validation accuracy compared to models with conventional activations.

</details>


### [29] [Entropy-Reservoir Bregman Projection: An Information-Geometric Unification of Model Collapse](https://arxiv.org/abs/2512.14879)
*Jingwei Chen*

Main category: cs.LG

TL;DR: 论文提出ERBP框架，用信息几何方法统一解释自指学习中的模型崩溃现象，并通过熵库注入可控熵流来稳定系统。


<details>
  <summary>Details</summary>
Motivation: 自指学习（模型在自身生成的数据上训练）虽然具有无限扩展潜力，但长期存在模型崩溃问题。尽管实践中采用各种临时修复方法，但缺乏统一的理论框架来解释失败模式和修复方法的成功原理。

Method: 提出熵库Bregman投影（ERBP）框架，将闭环系统建模为分布空间中的随机Bregman投影序列。通过引入熵库（高熵分布混合到每次投影中）注入可控熵流来稳定动力学。

Result: 理论推导出：(1) 模型崩溃的必要条件；(2) 保证非平凡熵底限的充分条件；(3) 仅依赖于样本量和Bregman生成器强凸性/Lipschitz常数的闭式速率。在大语言模型自训练、强化学习和GAN优化实验中验证了预测。

Conclusion: ERBP将各种经验性稳定启发式方法统一为单一量化设计规则：监控和预算熵流。不同的稳定启发式方法对应特定的熵库选择和耦合系数，为自指学习提供了理论基础。

Abstract: Self-referential learning -- training a model on data it generated itself -- promises boundless scalability but chronically suffers from model collapse: language models degenerate into repetitive text, GANs drop modes, and reinforcement-learning policies over-exploit. Although practitioners employ ad~hoc fixes such as real-data mixing, entropy bonuses, knowledge distillation, or retrieval-augmented generation, a single principle that explains both the failure mode and the success of these fixes has remained elusive. We present Entropy-Reservoir Bregman Projection (ERBP), an information-geometric framework that unifies these phenomena. We model the closed loop as a stochastic Bregman projection sequence in distribution space. Without external coupling, finite-sample noise forces the system to project onto an ever-shrinking empirical support, causing exponential entropy decay and eventual collapse. Introducing an Entropy Reservoir -- a high-entropy distribution mixed into each projection -- injects a controllable entropy flux that provably stabilises the dynamics. Our theory yields (i) a necessary condition for collapse, (ii) a sufficient condition that guarantees a non-trivial entropy floor, and (iii) closed-form rates that depend only on sample size and the strong-convexity/Lipschitz constants of the Bregman generator. Experiments on large-language-model self-training, Soft Actor-Critic in reinforcement learning, and GAN optimisation validate our predictions and show that disparate stabilisation heuristics correspond to specific reservoir choices and coupling coefficients. ERBP thus transforms a collection of folk remedies into a single, quantitative design rule: monitor and budget your entropy flux.

</details>


### [30] [Task Matrices: Linear Maps for Cross-Model Finetuning Transfer](https://arxiv.org/abs/2512.14880)
*Darrin O' Brien,Dhikshith Gajulapalli,Eric Xia*

Main category: cs.LG

TL;DR: 论文提出"任务矩阵"概念，证明预训练与微调模型之间存在跨层线性编码，通过线性变换可接近微调效果


<details>
  <summary>Details</summary>
Motivation: 现有研究表明大型视觉和语言模型在上下文提示偏置下学习隐式线性编码，但更一般的适应机制中是否存在类似线性表示尚未得到证实

Method: 开发"任务矩阵"概念，作为从基础模型到微调模型嵌入状态的线性变换；在视觉和文本模型及十个不同数据集上进行验证；使用基于数据的近似方法

Result: 基础模型加上任务矩阵的效果超越线性探针，有时接近微调水平；验证了预训练与微调架构间存在跨层线性编码；基于数据的近似方法既高效又可泛化到多个领域

Conclusion: 任务矩阵概念有效，证明了预训练与微调模型间存在可学习的线性编码，为模型适应提供了高效且可泛化的新方法

Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.

</details>


### [31] [OLR-WA: Online Weighted Average Linear Regression in Multivariate Data Streams](https://arxiv.org/abs/2512.14892)
*Mohammad Abu-Shaira,Alejandro Rodriguez,Greg Speegle,Victor Sheng,Ishfaq Ahmad*

Main category: cs.LG

TL;DR: 提出OLR-WA在线回归模型，通过加权平均处理数据漂移，实现与批处理回归相当的性能，并在收敛速度和置信度处理方面优于现有在线模型。


<details>
  <summary>Details</summary>
Motivation: 在线学习需要增量更新模型以避免大数据存储和昂贵的模型重新计算，同时需要处理数据漂移问题，现有在线回归模型在收敛速度和置信度处理方面存在不足。

Method: 提出OLR-WA（在线回归加权平均）模型，这是一种多变量在线线性回归方法，采用保守更新策略，优先考虑置信度较高的旧数据点，有效处理时间漂移和置信度挑战场景。

Result: OLR-WA在性能上可与批处理回归相媲美，收敛速度优于其他在线模型，即使仅用1%-10%数据初始化也能从第一次迭代就获得高r2值，是唯一能有效处理置信度挑战场景的模型。

Conclusion: OLR-WA通过加权平均和保守更新策略，在多场景下展现出卓越的通用性和实用性，是在线线性回归任务的有价值解决方案。

Abstract: Online learning updates models incrementally with new data, avoiding large storage requirements and costly model recalculations. In this paper, we introduce "OLR-WA; OnLine Regression with Weighted Average", a novel and versatile multivariate online linear regression model. We also investigate scenarios involving drift, where the underlying patterns in the data evolve over time, conduct convergence analysis, and compare our approach with existing online regression models. The results of OLR-WA demonstrate its ability to achieve performance comparable to the batch regression, while also showcasing comparable or superior performance when compared with other state-of-the-art online models, thus establishing its effectiveness. Moreover, OLR-WA exhibits exceptional performance in terms of rapid convergence, surpassing other online models with consistently achieving high r2 values as a performance measure from the first iteration to the last iteration, even when initialized with minimal amount of data points, as little as 1% to 10% of the total data points. In addition to its ability to handle time-based (temporal drift) scenarios, remarkably, OLR-WA stands out as the only model capable of effectively managing confidence-based challenging scenarios. It achieves this by adopting a conservative approach in its updates, giving priority to older data points with higher confidence levels. In summary, OLR-WA's performance further solidifies its versatility and utility across different contexts, making it a valuable solution for online linear regression tasks.

</details>


### [32] [Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections](https://arxiv.org/abs/2512.14895)
*Niklas Lauffer,Xiang Deng,Srivatsa Kundurthy,Brad Kenstler,Jeff Da*

Main category: cs.LG

TL;DR: 该论文提出了一种针对多轮语言模型代理训练的新数据生成方法OEC，通过结合学生模型和专家模型的混合轨迹来解决模仿学习中的协变量偏移问题，在软件工程任务中取得了显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在多轮语言模型代理训练中存在协变量偏移的根本限制：当学生策略的行为偏离专家策略时，会遇到训练数据中不存在的状态，从而降低微调效果。需要解决这一经典问题以提升多轮LLM代理的训练效果。

Method: 提出on-policy expert corrections (OEC)数据生成方法：从学生模型开始生成轨迹，然后在轨迹中途切换到专家模型，生成部分在策略的数据。在软件工程任务中应用该方法，使用拒绝采样结合监督微调技术进行模型训练。

Result: 在SWE-bench验证集上，OEC轨迹相比传统模仿学习方法在7b和32b设置下分别实现了14%和13%的相对改进。实验表明OEC数据在各种在策略和模仿学习方法中表现最佳。

Conclusion: 研究结果表明，将专家演示与在策略数据结合对于有效的多轮语言模型代理训练至关重要。OEC方法为解决协变量偏移问题提供了一种有效的解决方案，在多轮交互任务中显著提升了模型性能。

Abstract: A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.

</details>


### [33] [ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs](https://arxiv.org/abs/2512.14908)
*Turja Kundu,Sanjukta Bhowmick*

Main category: cs.LG

TL;DR: ATLAS是一种新型图学习算法，通过提取多级社区拓扑信息并拼接特征向量，使用MLP替代GNN聚合，解决了异配图精度下降和大图可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络的两个关键挑战：1) 在异配图（heterophilic graphs）上精度下降；2) 迭代特征聚合限制了在大图上的可扩展性。

Method: 提取多级细化的图社区拓扑信息，将社区分配结果与特征向量拼接，然后对得到的表示应用多层感知机（MLP），避免使用聚合操作。

Result: 在多种图上达到与基线方法相当的精度，在具有负结构偏置的异配图上比GCN提高20个百分点，在同配图上比MLP提高11个百分点。多分辨率社区特征能系统性地调节性能。

Conclusion: ATLAS通过拓扑信息提供节点及其邻域的上下文，无需聚合操作，具有更好的可扩展性，并为可解释的图学习开辟了新的路径。

Abstract: We present ATLAS (Adaptive Topology-based Learning at Scale for Homophilic and Heterophilic Graphs), a novel graph learning algorithm that addresses two important challenges in graph neural networks (GNNs). First, the accuracy of GNNs degrades when the graph is heterophilic. Second, iterative feature aggregation limits the scalability of GNNs to large graphs. We address these challenges by extracting topological information about graph communities at multiple levels of refinement, concatenating community assignments to the feature vector, and applying multilayer perceptrons (MLPs) to the resulting representation. This provides topological context about nodes and their neighborhoods without invoking aggregation. Because MLPs are typically more scalable than GNNs, our approach applies to large graphs without the need for sampling. Across a wide set of graphs, ATLAS achieves comparable accuracy to baseline methods, with gains as high as 20 percentage points over GCN for heterophilic graphs with negative structural bias and 11 percentage points over MLP for homophilic graphs. Furthermore, we show how multi-resolution community features systematically modulate performance in both homophilic and heterophilic settings, opening a principled path toward explainable graph learning.

</details>


### [34] [Low-rank MMSE filters, Kronecker-product representation, and regularization: a new perspective](https://arxiv.org/abs/2512.14932)
*Daniel Gomes de Pinho Zanco,Leszek Szczecinski,Jacob Benesty,Eduardo Vinicius Kuhn*

Main category: cs.LG

TL;DR: 提出一种基于Kronecker积表示的低秩MMSE滤波器正则化参数高效选择方法，该方法与秩选择问题相关，在低秩设置中至关重要，仿真验证了其优于常用方法


<details>
  <summary>Details</summary>
Motivation: 在低秩MMSE滤波器设计中，正则化参数的选择对性能至关重要，但现有方法效率不高。作者发现正则化参数与秩选择问题存在意外联系，需要开发更有效的参数选择方法

Method: 基于Kronecker积表示的低秩MMSE滤波器正则化参数选择方法，通过分析正则化参数与秩选择问题的联系，提出高效的参数确定策略

Result: 仿真验证表明，所提方法相比常用方法能获得显著性能增益，证明了该方法在低秩设置中的有效性

Conclusion: 正则化参数选择在低秩MMSE滤波器中至关重要，且与秩选择问题密切相关，提出的基于Kronecker积表示的方法能高效确定最优参数，显著提升滤波器性能

Abstract: In this work, we propose a method to efficiently find the regularization parameter for low-rank MMSE filters based on a Kronecker-product representation. We show that the regularization parameter is surprisingly linked to the problem of rank selection and, thus, properly choosing it, is crucial for low-rank settings. The proposed method is validated through simulations, showing significant gains over commonly used methods.

</details>


### [35] [Deep Learning and Elicitability for McKean-Vlasov FBSDEs With Common Noise](https://arxiv.org/abs/2512.14967)
*Felipe J. P. Antunes,Yuri F. Saporito,Sebastian Jaimungal*

Main category: cs.LG

TL;DR: 提出了一种结合Picard迭代、可引出性和深度学习的新数值方法，用于求解带有共同噪声的McKean-Vlasov前向-后向随机微分方程，避免了计算昂贵的嵌套蒙特卡洛模拟。


<details>
  <summary>Details</summary>
Motivation: 解决带有共同噪声的MV-FBSDEs数值求解问题，传统方法需要昂贵的嵌套蒙特卡洛模拟，计算成本高，需要更高效的数值方法。

Method: 结合Picard迭代、可引出性和深度学习：1) 使用可引出性推导路径损失函数；2) 用循环神经网络参数化平均场交互项；3) 用前馈网络近似解耦场的后向过程；4) 避免嵌套蒙特卡洛模拟。

Result: 1) 在系统性风险银行借贷模型上准确恢复解析解；2) 扩展到分位数介导的交互，展示框架灵活性；3) 应用于非平稳Aiyagari-Bewley-Huggett经济增长模型，展示复杂平均场博弈的适用性。

Conclusion: 该方法为求解带有共同噪声的MV-FBSDEs提供了高效数值框架，结合可引出性和深度学习，避免了昂贵计算，适用于复杂平均场博弈问题，具有良好扩展性。

Abstract: We present a novel numerical method for solving McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) with common noise, combining Picard iterations, elicitability and deep learning. The key innovation involves elicitability to derive a path-wise loss function, enabling efficient training of neural networks to approximate both the backward process and the conditional expectations arising from common noise - without requiring computationally expensive nested Monte Carlo simulations. The mean-field interaction term is parameterized via a recurrent neural network trained to minimize an elicitable score, while the backward process is approximated through a feedforward network representing the decoupling field. We validate the algorithm on a systemic risk inter-bank borrowing and lending model, where analytical solutions exist, demonstrating accurate recovery of the true solution. We further extend the model to quantile-mediated interactions, showcasing the flexibility of the elicitability framework beyond conditional means or moments. Finally, we apply the method to a non-stationary Aiyagari--Bewley--Huggett economic growth model with endogenous interest rates, illustrating its applicability to complex mean-field games without closed-form solutions.

</details>


### [36] [Softly Constrained Denoisers for Diffusion Models](https://arxiv.org/abs/2512.14980)
*Victor M. Yeom Song,Severi Rissanen,Arno Solin,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出一种软约束去噪器方法，通过将约束知识融入去噪器本身，改善扩散模型对约束的遵从性，同时保持对约束误设的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成符合约束的样本方面存在困难，这在科学应用中很常见。现有方法通过在损失函数中添加正则化项或在采样过程中使用引导方法，但这些方法会使生成模型偏离真实数据分布，特别是在约束误设时问题更严重。

Method: 不改变损失函数或采样循环，而是将引导启发的调整集成到去噪器本身，赋予其对约束合规样本的软归纳偏置。

Result: 软约束去噪器利用约束知识提高了对约束的遵从性，同时保持了足够的灵活性，在约束与观测数据存在误设时能够偏离约束。

Conclusion: 通过将约束知识直接融入去噪器架构，可以在不偏离真实数据分布的情况下改善扩散模型对约束的遵从性，同时保持对约束误设的鲁棒性。

Abstract: Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem, especially when the constraint is misspecified, a common issue when formulating constraints on scientific data. In this paper, instead of changing the loss or the sampling loop, we integrate a guidance-inspired adjustment into the denoiser itself, giving it a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, and maintain enough flexibility to deviate from it when there is misspecification with observed data.

</details>


### [37] [Prompt Repetition Improves Non-Reasoning LLMs](https://arxiv.org/abs/2512.14982)
*Yaniv Leviathan,Matan Kalman,Yossi Matias*

Main category: cs.LG

TL;DR: 重复输入提示能提升主流模型性能，无需增加生成token或延迟


<details>
  <summary>Details</summary>
Motivation: 探索在不使用推理的情况下，如何通过简单方法提升主流语言模型的性能表现

Method: 通过重复输入提示的方式，对Gemini、GPT、Claude和Deepseek等主流模型进行测试

Result: 重复输入提示能够有效提升模型性能，且不会增加生成的token数量或延迟

Conclusion: 简单的提示重复策略可以作为提升模型性能的有效方法，具有实用价值

Abstract: When not using reasoning, repeating the input prompt improves performance for popular models (Gemini, GPT, Claude, and Deepseek) without increasing the number of generated tokens or latency.

</details>


### [38] [Adaptive Partitioning and Learning for Stochastic Control of Diffusion Processes](https://arxiv.org/abs/2512.14991)
*Hanqing Jin,Renyuan Xu,Yanzhao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种用于无界连续状态空间扩散过程的自适应分区强化学习算法，通过动态调整状态-动作空间划分来平衡探索与近似，在金融、经济等领域的高维问题中实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 金融、经济和运筹学中的许多问题涉及无界连续状态空间的扩散过程，传统强化学习方法在处理这类高维、无界域时面临挑战，需要新的算法来有效平衡探索与近似。

Method: 提出基于模型的自适应分区算法：1）在联合状态-动作空间进行自适应划分；2）在每个分区内估计漂移、波动率和奖励函数；3）当估计偏差超过统计置信度时细化离散化；4）通过平衡探索与近似来处理无界域。

Result: 建立了后悔界，其依赖于问题时间范围、状态维度、奖励增长阶数以及新定义的无界扩散过程缩放维度。该界将现有有界设置结果作为特例，同时扩展到更广泛的扩散类型问题。数值实验验证了算法在多资产均值-方差投资组合选择等高维问题中的有效性。

Conclusion: 该自适应分区算法能够有效处理无界连续状态空间的扩散过程强化学习问题，为金融、经济等领域的高维应用提供了理论保证和实用工具，扩展了强化学习在扩散类型问题中的应用范围。

Abstract: We study reinforcement learning for controlled diffusion processes with unbounded continuous state spaces, bounded continuous actions, and polynomially growing rewards: settings that arise naturally in finance, economics, and operations research. To overcome the challenges of continuous and high-dimensional domains, we introduce a model-based algorithm that adaptively partitions the joint state-action space. The algorithm maintains estimators of drift, volatility, and rewards within each partition, refining the discretization whenever estimation bias exceeds statistical confidence. This adaptive scheme balances exploration and approximation, enabling efficient learning in unbounded domains. Our analysis establishes regret bounds that depend on the problem horizon, state dimension, reward growth order, and a newly defined notion of zooming dimension tailored to unbounded diffusion processes. The bounds recover existing results for bounded settings as a special case, while extending theoretical guarantees to a broader class of diffusion-type problems. Finally, we validate the effectiveness of our approach through numerical experiments, including applications to high-dimensional problems such as multi-asset mean-variance portfolio selection.

</details>


### [39] [DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding](https://arxiv.org/abs/2512.15000)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-Code：一种专注于代码的流程奖励模型，通过函数链提示策略将函数视为推理步骤，并采用元学习校正机制处理标签噪声，在LiveCodeBench上达到80.9%的pass@1率，超越OpenAI o4-mini。


<details>
  <summary>Details</summary>
Motivation: 当前流程奖励模型在代码任务中效果有限，主要因为代码缺乏有意义的步骤分解，以及蒙特卡洛生成的中间标签存在噪声问题。

Method: 1. 采用函数链提示策略，将函数视为推理步骤，实现模块化代码生成；2. 引入元学习校正机制，利用干净的最终解决方案单元测试标签，通过双层优化精炼中间标签。

Result: 在LiveCodeBench上达到80.9%的pass@1率，实现了最先进的性能，超越了OpenAI o4-mini模型。

Conclusion: DreamPRM-Code通过创新的函数链提示和元学习校正机制，有效解决了代码任务中流程奖励模型的关键挑战，显著提升了代码生成性能。

Abstract: Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.

</details>


### [40] [Stock Pattern Assistant (SPA): A Deterministic and Explainable Framework for Structural Price Run Extraction and Event Correlation in Equity Markets](https://arxiv.org/abs/2512.15008)
*Sandeep Neela*

Main category: cs.LG

TL;DR: SPA是一个确定性框架，用于从股价数据中提取单调价格走势，关联公开事件，并生成事实性、历史性的解释，旨在提高市场分析的透明度和可审计性。


<details>
  <summary>Details</summary>
Motivation: 现有技术指标和预测模型缺乏透明度和解释性，在需要审计和透明度的场景中存在挑战。需要一种能够清晰识别价格结构行为并提供可解释性分析的工具。

Method: SPA框架使用每日OHLCV数据和标准化事件流，通过确定性方法提取单调价格走势，通过对称相关窗口关联公开事件，并生成受约束的事实性历史解释。

Result: 在AAPL、NVDA、SCHW、PGR四只股票上的评估显示，SPA能够稳定地生成结构性分解和上下文叙事。消融实验表明确定性分割、事件对齐和约束解释都对可解释性有贡献。

Conclusion: SPA不是预测系统或交易信号生成器，其价值在于提供透明、可复现的历史价格结构视图，可补充分析师工作流程、风险审查和可解释AI管道。

Abstract: Understanding how prices evolve over time often requires peeling back the layers of market noise to identify clear, structural behavior. Many of the tools commonly used for this purpose technical indicators, chart heuristics, or even sophisticated predictive models leave important questions unanswered. Technical indicators depend on platform-specific rules, and predictive systems typically offer little in terms of explanation. In settings that demand transparency or auditability, this poses a significant challenge. We introduce the Stock Pattern Assistant (SPA), a deterministic framework designed to extract monotonic price runs, attach relevant public events through a symmetric correlation window, and generate explanations that are factual, historical, and guardrailed. SPA relies only on daily OHLCV data and a normalized event stream, making the pipeline straight-forward to audit and easy to reproduce. To illustrate SPA's behavior in practice, we evaluate it across four equities-AAPL, NVDA, SCHW, and PGR-chosen to span a range of volatility regimes and sector characteristics. Although the evaluation period is modest, the results demonstrate how SPA consistently produces stable structural decompositions and contextual narratives. Ablation experiments further show how deterministic segmentation, event alignment, and constrained explanation each contribute to interpretability. SPA is not a forecasting system, nor is it intended to produce trading signals. Its value lies in offering a transparent, reproducible view of historical price structure that can complement analyst workflows, risk reviews, and broader explainable-AI pipelines.

</details>


### [41] [Epistemic diversity across language models mitigates knowledge collapse](https://arxiv.org/abs/2512.15011)
*Damian Hodel,Jevin D. West*

Main category: cs.LG

TL;DR: 研究AI生态系统多样性如何缓解知识崩溃问题，发现适度的认知多样性可以缓解崩溃，但过多或过少都会导致性能下降


<details>
  <summary>Details</summary>
Motivation: 人工智能的广泛使用引发了知识崩溃的担忧，即知识会缩减到最主流和核心的思想集合。先前研究已证明单一模型崩溃现象，本研究从生态学角度出发，探讨AI生态系统多样性（模型间的多样性）是否能缓解这种崩溃。

Method: 基于单一模型方法，但专注于在集体输出上训练的模型生态系统。通过将训练数据在不同语言模型间分割，评估由此产生的生态系统在十次自我训练迭代中的表现，研究多样性对模型性能的影响。

Result: 研究发现增加的认知多样性确实可以缓解崩溃，但有趣的是，只达到一个最优水平。包含太少多样性模型的生态系统无法表达完整真实分布的丰富混合，导致性能迅速衰减；而将数据分布到太多模型中则会降低每个模型对真实分布的近似能力，导致在第一次迭代步骤中性能就较差。

Conclusion: 在AI单一文化的背景下，研究结果表明需要监控AI系统间的多样性，并制定政策激励更多领域和社区特定的模型发展。

Abstract: The growing use of artificial intelligence (AI) raises concerns of knowledge collapse, i.e., a reduction to the most dominant and central set of ideas. Prior work has demonstrated single-model collapse, defined as performance decay in an AI model trained on its own output. Inspired by ecology, we ask whether AI ecosystem diversity, that is, diversity among models, can mitigate such a collapse. We build on the single-model approach but focus on ecosystems of models trained on their collective output. To study the effect of diversity on model performance, we segment the training data across language models and evaluate the resulting ecosystems over ten, self-training iterations. We find that increased epistemic diversity mitigates collapse, but, interestingly, only up to an optimal level. Our results suggest that an ecosystem containing only a few diverse models fails to express the rich mixture of the full, true distribution, resulting in rapid performance decay. Yet distributing the data across too many models reduces each model's approximation capacity on the true distribution, leading to poor performance already in the first iteration step. In the context of AI monoculture, our results suggest the need to monitor diversity across AI systems and to develop policies that incentivize more domain- and community-specific models.

</details>


### [42] [Spectral Representation-based Reinforcement Learning](https://arxiv.org/abs/2512.15036)
*Chenxiao Gao,Haotian Sun,Na Li,Dale Schuurmans,Bo Dai*

Main category: cs.LG

TL;DR: 该论文提出使用谱表示作为强化学习中函数逼近的解决方案，通过谱分解转移算子来抽象系统动态，为策略优化提供理论基础，并在DeepMind Control Suite的20多个任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大型状态和动作空间中通常使用函数逼近（如神经网络），但这些方法存在理论模糊性、优化不稳定性、探索困难以及计算成本高等问题。需要一种既能提供清晰理论特性又能实际有效的表示方法。

Method: 引入谱表示框架，基于转移算子的谱分解来抽象系统动态。针对具有隐变量结构或基于能量结构的转移算子，提出了不同的谱表示构建方法。每种学习方法都实现了有效的强化学习算法，并将该谱视角扩展到部分可观测MDP。

Result: 在DeepMind Control Suite的20多个挑战性任务上验证了算法，性能达到或超过了当前最先进的模型无关和基于模型的基线方法。

Conclusion: 谱表示为强化学习中的函数逼近问题提供了有效的解决方案，既提供了清晰的理论特性，又在实际任务中表现出色，为解决传统方法的问题提供了新视角。

Abstract: In real-world applications with large state and action spaces, reinforcement learning (RL) typically employs function approximations to represent core components like the policies, value functions, and dynamics models. Although powerful approximations such as neural networks offer great expressiveness, they often present theoretical ambiguities, suffer from optimization instability and exploration difficulty, and incur substantial computational costs in practice. In this paper, we introduce the perspective of spectral representations as a solution to address these difficulties in RL. Stemming from the spectral decomposition of the transition operator, this framework yields an effective abstraction of the system dynamics for subsequent policy optimization while also providing a clear theoretical characterization. We reveal how to construct spectral representations for transition operators that possess latent variable structures or energy-based structures, which implies different learning methods to extract spectral representations from data. Notably, each of these learning methods realizes an effective RL algorithm under this framework. We also provably extend this spectral view to partially observable MDPs. Finally, we validate these algorithms on over 20 challenging tasks from the DeepMind Control Suite, where they achieve performances comparable or superior to current state-of-the-art model-free and model-based baselines.

</details>


### [43] [EMFusion: Conditional Diffusion Framework for Trustworthy Frequency Selective EMF Forecasting in Wireless Networks](https://arxiv.org/abs/2512.15067)
*Zijiang Yan,Yixiang Huang,Jianhua Pei,Hina Tabassum,Luca Chiaraviglio*

Main category: cs.LG

TL;DR: EMFusion是一个基于条件多元扩散的概率预测框架，用于电磁场水平预测，通过整合上下文因素和提供不确定性估计，在频率选择性EMF数据集上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 无线基础设施的快速增长需要准确估计和预测电磁场水平以确保合规性、评估健康影响和支持网络规划。现有研究依赖于宽带聚合EMF数据的单变量预测，但需要频率选择性多元预测来捕捉运营商间和频率间的变化，这对主动网络规划至关重要。

Method: 提出EMFusion框架：基于条件多元扩散的概率预测方法，整合时间、季节、节假日等上下文因素；采用残差U-Net骨干网络，通过交叉注意力机制动态整合外部条件；使用基于插值的采样策略，将预测视为结构修复任务以确保时间一致性。

Result: 在频率选择性EMF数据集上的数值实验表明，EMFusion在工作时间上下文信息下优于基线模型：连续排名概率得分(CRPS)提升23.85%，归一化均方根误差提升13.93%，预测CRPS误差降低22.47%。

Conclusion: EMFusion框架能够有效进行电磁场水平的概率预测，提供明确的不确定性量化，支持可信的决策制定，在频率选择性EMF预测任务中表现出优越性能。

Abstract: The rapid growth in wireless infrastructure has increased the need to accurately estimate and forecast electromagnetic field (EMF) levels to ensure ongoing compliance, assess potential health impacts, and support efficient network planning. While existing studies rely on univariate forecasting of wideband aggregate EMF data, frequency-selective multivariate forecasting is needed to capture the inter-operator and inter-frequency variations essential for proactive network planning. To this end, this paper introduces EMFusion, a conditional multivariate diffusion-based probabilistic forecasting framework that integrates diverse contextual factors (e.g., time of day, season, and holidays) while providing explicit uncertainty estimates. The proposed architecture features a residual U-Net backbone enhanced by a cross-attention mechanism that dynamically integrates external conditions to guide the generation process. Furthermore, EMFusion integrates an imputation-based sampling strategy that treats forecasting as a structural inpainting task, ensuring temporal coherence even with irregular measurements. Unlike standard point forecasters, EMFusion generates calibrated probabilistic prediction intervals directly from the learned conditional distribution, providing explicit uncertainty quantification essential for trustworthy decision-making. Numerical experiments conducted on frequency-selective EMF datasets demonstrate that EMFusion with the contextual information of working hours outperforms the baseline models with or without conditions. The EMFusion outperforms the best baseline by 23.85% in continuous ranked probability score (CRPS), 13.93% in normalized root mean square error, and reduces prediction CRPS error by 22.47%.

</details>


### [44] [The Semantic Illusion: Certified Limits of Embedding-Based Hallucination Detection in RAG Systems](https://arxiv.org/abs/2512.15068)
*Debu Sinha*

Main category: cs.LG

TL;DR: 使用保形预测进行RAG幻觉检测，发现基于嵌入的方法在真实基准上存在不可接受的高误报率，而GPT-4作为LLM法官表现更好，揭示了"语义幻觉"问题


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成系统基于检索证据，但仍容易产生幻觉。现有检测方法依赖语义相似性和自然语言推理，但其基本局限性尚未得到严格表征。

Method: 应用保形预测进行幻觉检测，提供有限样本覆盖保证。使用约600个示例的校准集，在合成幻觉上测试，并在三个真实幻觉基准上评估多种嵌入方法和LLM法官。

Result: 在合成幻觉上达到94%覆盖率且0%误报率，但在真实基准上嵌入方法误报率极高：HaluEval 100%、RAGTruth 88%、WikiBio 50%。GPT-4作为LLM法官仅7%误报率，证明任务可通过推理解决。

Conclusion: 揭示了"语义幻觉"现象：语义合理的幻觉保持与源文档相似性但引入嵌入方法无法检测的事实错误。这种局限性跨越嵌入架构、LLM生成器和任务类型，表明基于嵌入的检测不足以用于生产RAG部署。

Abstract: Retrieval-Augmented Generation (RAG) systems remain susceptible to hallucinations despite grounding in retrieved evidence. Current detection methods rely on semantic similarity and natural language inference (NLI), but their fundamental limitations have not been rigorously characterized. We apply conformal prediction to hallucination detection, providing finite-sample coverage guarantees that enable precise quantification of detection capabilities. Using calibration sets of approximately 600 examples, we achieve 94% coverage with 0% false positive rate on synthetic hallucinations (Natural Questions). However, on three real hallucination benchmarks spanning multiple LLMs (GPT-4, ChatGPT, GPT-3, Llama-2, Mistral), embedding-based methods - including state-of-the-art OpenAI text-embedding-3-large and cross-encoder models - exhibit unacceptable false positive rates: 100% on HaluEval, 88% on RAGTruth, and 50% on WikiBio. Crucially, GPT-4 as an LLM judge achieves only 7% FPR (95% CI: [3.4%, 13.7%]) on the same data, proving the task is solvable through reasoning. We term this the "semantic illusion": semantically plausible hallucinations preserve similarity to source documents while introducing factual errors invisible to embeddings. This limitation persists across embedding architectures, LLM generators, and task types, suggesting embedding-based detection is insufficient for production RAG deployment.

</details>


### [45] [The Semantic Architect: How FEAML Bridges Structured Data and LLMs for Multi-Label Tasks](https://arxiv.org/abs/2512.15082)
*Wanfu Gao,Zebin He,Jun Gao*

Main category: cs.LG

TL;DR: FEAML是一种基于大语言模型的多标签学习自动特征工程方法，利用LLM的代码生成能力，通过元数据和标签共现矩阵理解特征与任务关系，生成高质量特征并通过反馈机制持续优化。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的特征工程方法尚未应用于多标签学习任务，缺乏对复杂标签依赖关系的建模能力，且未针对多标签任务特点进行专门适配。

Method: FEAML利用LLM的代码生成能力，通过元数据和标签共现矩阵引导LLM理解数据特征与任务目标的关系，生成高质量特征。使用模型准确率评估特征有效性，皮尔逊相关系数检测冗余，并将评估结果作为反馈驱动LLM在后续迭代中持续优化代码生成。

Result: 在多个多标签数据集上的实证结果表明，FEAML优于其他特征工程方法。

Conclusion: 通过将LLM与反馈机制相结合，FEAML实现了一个高效、可解释且自我改进的特征工程范式。

Abstract: Existing feature engineering methods based on large language models (LLMs) have not yet been applied to multi-label learning tasks. They lack the ability to model complex label dependencies and are not specifically adapted to the characteristics of multi-label tasks. To address the above issues, we propose Feature Engineering Automation for Multi-Label Learning (FEAML), an automated feature engineering method for multi-label classification which leverages the code generation capabilities of LLMs. By utilizing metadata and label co-occurrence matrices, LLMs are guided to understand the relationships between data features and task objectives, based on which high-quality features are generated. The newly generated features are evaluated in terms of model accuracy to assess their effectiveness, while Pearson correlation coefficients are used to detect redundancy. FEAML further incorporates the evaluation results as feedback to drive LLMs to continuously optimize code generation in subsequent iterations. By integrating LLMs with a feedback mechanism, FEAML realizes an efficient, interpretable and self-improving feature engineering paradigm. Empirical results on various multi-label datasets demonstrate that our FEAML outperforms other feature engineering methods.

</details>


### [46] [Neural Modular Physics for Elastic Simulation](https://arxiv.org/abs/2512.15083)
*Yifei Li,Haixu Wu,Zeyi Xu,Tuur Stuyck,Wojciech Matusik*

Main category: cs.LG

TL;DR: 提出Neural Modular Physics (NMP)方法，将弹性模拟分解为物理意义明确的神经模块，结合神经网络近似能力和传统模拟器的物理可靠性，提升物理一致性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的物理模拟方法通常使用端到端的单一神经网络，虽然有效但失去了传统数值模拟器的物理可解释性和可靠性等关键特性。受传统模块化模拟器启发，希望结合神经网络近似能力和传统模拟器的物理可靠性优势。

Method: 提出Neural Modular Physics (NMP)方法，将弹性动力学分解为具有物理意义的神经模块，通过中间物理量连接。采用专门架构和训练策略，将数值计算流程转换为模块化神经模拟器，实现对中间量和物理约束的直接监督。

Result: NMP在未见初始条件和分辨率上表现出优越的泛化能力，实现稳定的长时程模拟，相比其他神经模拟器更好地保持了物理特性，相比传统模拟器在未知底层动力学场景中更具可行性。

Conclusion: 通过模块化设计将神经网络近似能力与传统模拟器的物理可靠性相结合，NMP方法在弹性模拟中实现了更好的物理一致性和泛化性能，为物理模拟提供了新的学习范式。

Abstract: Learning-based methods have made significant progress in physics simulation, typically approximating dynamics with a monolithic end-to-end optimized neural network. Although these models offer an effective way to simulation, they may lose essential features compared to traditional numerical simulators, such as physical interpretability and reliability. Drawing inspiration from classical simulators that operate in a modular fashion, this paper presents Neural Modular Physics (NMP) for elastic simulation, which combines the approximation capacity of neural networks with the physical reliability of traditional simulators. Beyond the previous monolithic learning paradigm, NMP enables direct supervision of intermediate quantities and physical constraints by decomposing elastic dynamics into physically meaningful neural modules connected through intermediate physical quantities. With a specialized architecture and training strategy, our method transforms the numerical computation flow into a modular neural simulator, achieving improved physical consistency and generalizability. Experimentally, NMP demonstrates superior generalization to unseen initial conditions and resolutions, stable long-horizon simulation, better preservation of physical properties compared to other neural simulators, and greater feasibility in scenarios with unknown underlying dynamics than traditional simulators.

</details>


### [47] [PIP$^2$ Net: Physics-informed Partition Penalty Deep Operator Network](https://arxiv.org/abs/2512.15086)
*Hongjin Mi,Huiqiang Lun,Changhong Mou,Yeyu Zhang*

Main category: cs.LG

TL;DR: PIP² Net：一种基于分区惩罚的物理信息深度算子网络，通过简化的分区惩罚机制改进协调分支网络输出，在非线性PDE求解中比现有方法更准确、更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有算子学习方法（如DeepONet和FNO）需要大量训练数据、缺乏显式物理结构、分支网络特征不稳定（存在模式不平衡或崩溃问题），影响算子逼近精度。

Method: 基于经典分区统一方法的稳定性和局部性，开发了PIP² Net（物理信息分区惩罚深度算子网络），引入简化且更原则化的分区惩罚机制，改进协调分支网络输出。

Result: 在三个非线性PDE（粘性Burgers方程、Allen-Cahn方程、扩散-反应系统）上测试，PIP² Net在预测精度和鲁棒性方面一致优于DeepONet、PI-DeepONet和POU-DeepONet。

Conclusion: PIP² Net通过分区惩罚机制有效解决了现有算子学习方法的不稳定问题，提高了表达能力和预测性能，为参数化PDE求解提供了更准确、更鲁棒的算子学习框架。

Abstract: Operator learning has become a powerful tool for accelerating the solution of parameterized partial differential equations (PDEs), enabling rapid prediction of full spatiotemporal fields for new initial conditions or forcing functions. Existing architectures such as DeepONet and the Fourier Neural Operator (FNO) show strong empirical performance but often require large training datasets, lack explicit physical structure, and may suffer from instability in their trunk-network features, where mode imbalance or collapse can hinder accurate operator approximation. Motivated by the stability and locality of classical partition-of-unity (PoU) methods, we investigate PoU-based regularization techniques for operator learning and develop a revised formulation of the existing POU--PI--DeepONet framework. The resulting \emph{P}hysics-\emph{i}nformed \emph{P}artition \emph{P}enalty Deep Operator Network (PIP$^{2}$ Net) introduces a simplified and more principled partition penalty that improved the coordinated trunk outputs that leads to more expressiveness without sacrificing the flexibility of DeepONet. We evaluate PIP$^{2}$ Net on three nonlinear PDEs: the viscous Burgers equation, the Allen--Cahn equation, and a diffusion--reaction system. The results show that it consistently outperforms DeepONet, PI-DeepONet, and POU-DeepONet in prediction accuracy and robustness.

</details>


### [48] [SigMA: Path Signatures and Multi-head Attention for Learning Parameters in fBm-driven SDEs](https://arxiv.org/abs/2512.15088)
*Xianglin Wu,Chiheb Ben Hammouda,Cornelis W. Oosterlee*

Main category: cs.LG

TL;DR: SigMA模型将路径签名与多头注意力结合，用于分数布朗运动驱动的随机微分方程参数估计，在精度、鲁棒性和模型紧凑性方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 分数布朗运动驱动的随机微分方程在金融和可靠性工程中广泛应用，但由于其非马尔可夫性和缺乏半鞅结构，传统参数估计方法不适用或计算复杂。需要开发更有效的参数推断方法。

Method: 提出SigMA（Signature Multi-head Attention）架构，整合路径签名与多头自注意力机制，包含卷积预处理层和多层感知机进行特征编码。模型从分数布朗运动、分数Ornstein-Uhlenbeck和粗糙Heston模型生成的合成路径中学习参数。

Result: 在合成数据和两个真实数据集（股票指数已实现波动率和锂离子电池退化）上的实验表明，SigMA在准确性、鲁棒性和模型紧凑性方面一致优于CNN、LSTM、普通Transformer和深度签名基线方法。

Conclusion: 将签名变换与基于注意力的架构相结合，为具有粗糙或持久时间结构的随机系统中的参数推断提供了一个有效且可扩展的框架。

Abstract: Stochastic differential equations (SDEs) driven by fractional Brownian motion (fBm) are increasingly used to model systems with rough dynamics and long-range dependence, such as those arising in quantitative finance and reliability engineering. However, these processes are non-Markovian and lack a semimartingale structure, rendering many classical parameter estimation techniques inapplicable or computationally intractable beyond very specific cases. This work investigates two central questions: (i) whether integrating path signatures into deep learning architectures can improve the trade-off between estimation accuracy and model complexity, and (ii) what constitutes an effective architecture for leveraging signatures as feature maps. We introduce SigMA (Signature Multi-head Attention), a neural architecture that integrates path signatures with multi-head self-attention, supported by a convolutional preprocessing layer and a multilayer perceptron for effective feature encoding. SigMA learns model parameters from synthetically generated paths of fBm-driven SDEs, including fractional Brownian motion, fractional Ornstein-Uhlenbeck, and rough Heston models, with a particular focus on estimating the Hurst parameter and on joint multi-parameter inference, and it generalizes robustly to unseen trajectories. Extensive experiments on synthetic data and two real-world datasets (i.e., equity-index realized volatility and Li-ion battery degradation) show that SigMA consistently outperforms CNN, LSTM, vanilla Transformer, and Deep Signature baselines in accuracy, robustness, and model compactness. These results demonstrate that combining signature transforms with attention-based architectures provides an effective and scalable framework for parameter inference in stochastic systems with rough or persistent temporal structure.

</details>


### [49] [Feature-Centric Unsupervised Node Representation Learning Without Homophily Assumption](https://arxiv.org/abs/2512.15112)
*Sunwoo Kim,Soo Yong Lee,Kyungho Kim,Hyunjin Hwang,Jaemin Yoo,Kijung Shin*

Main category: cs.LG

TL;DR: FUEL是一种无监督节点表示学习方法，通过自适应调整图卷积的使用程度，在嵌入空间中增强类内相似性和类间分离性，在多种同质性水平的图上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督节点表示学习方法过度依赖图卷积，特别是在非同质性图中，这可能导致特征或拓扑属性不同的节点获得过于相似的嵌入。虽然监督学习环境中已经探索了调整图卷积使用程度的方法，但在无监督场景中这种方法仍然研究不足。

Method: FUEL通过自适应学习适当的图卷积使用程度，旨在增强嵌入空间中的类内相似性和类间分离性。由于类别未知，FUEL利用节点特征识别节点簇，并将这些簇作为类别的代理。

Result: 通过使用15种基线方法和14个基准数据集进行广泛实验，证明了FUEL在下游任务中的有效性，在具有不同同质性水平的图上实现了最先进的性能。

Conclusion: FUEL成功解决了无监督节点表示学习中图卷积使用程度的自适应调整问题，通过增强类内相似性和类间分离性，在各种同质性水平的图上都表现出优异的性能。

Abstract: Unsupervised node representation learning aims to obtain meaningful node embeddings without relying on node labels. To achieve this, graph convolution, which aggregates information from neighboring nodes, is commonly employed to encode node features and graph topology. However, excessive reliance on graph convolution can be suboptimal-especially in non-homophilic graphs-since it may yield unduly similar embeddings for nodes that differ in their features or topological properties. As a result, adjusting the degree of graph convolution usage has been actively explored in supervised learning settings, whereas such approaches remain underexplored in unsupervised scenarios. To tackle this, we propose FUEL, which adaptively learns the adequate degree of graph convolution usage by aiming to enhance intra-class similarity and inter-class separability in the embedding space. Since classes are unknown, FUEL leverages node features to identify node clusters and treats these clusters as proxies for classes. Through extensive experiments using 15 baseline methods and 14 benchmark datasets, we demonstrate the effectiveness of FUEL in downstream tasks, achieving state-of-the-art performance across graphs with diverse levels of homophily.

</details>


### [50] [How Many Heads Make an SSM? A Unified Framework for Attention and State Space Models](https://arxiv.org/abs/2512.15115)
*Ali Ghodsi*

Main category: cs.LG

TL;DR: 论文提出了一个统一框架来分析序列建模架构，揭示了注意力机制与状态空间模型在表达能力和梯度传播之间的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有序列建模架构（从RNN到Transformer和状态空间模型）缺乏统一的理论理解，特别是表达能力和可训练性之间的权衡关系。需要建立一个理论框架来统一分析这些不同的架构。

Method: 引入一个统一框架，通过输入依赖的有效交互算子W_ij(X)表示广泛的序列映射。识别两种构建模式：统一因子化框架（注意力风格混合）和结构化动态（隐式状态空间递归）。在此框架下推导了三个理论结果。

Result: 1. 交互秩间隙：统一因子化框架中的模型（如单头注意力）受限于低维算子空间，无法表示某些结构化动态映射。2. 等价定理：在多头因子化类中，表示线性SSM需要且仅需要k个头。3. 梯度高速公路结果：注意力层允许距离无关的梯度路径，而稳定线性动态则表现出距离相关的梯度衰减。

Conclusion: 该研究形式化了代数表达能力（交互/算子空间）与长距离梯度传播之间的基本权衡，为现代序列架构设计提供了理论基础。注意力机制在梯度传播方面有优势，但在表达能力上有限制；状态空间模型则相反。

Abstract: Sequence modeling has produced diverse architectures -- from classical recurrent neural networks to modern Transformers and state space models (SSMs) -- yet a unified theoretical understanding of expressivity and trainability trade-offs remains limited. We introduce a unified framework that represents a broad class of sequence maps via an input-dependent effective interaction operator $W_{ij}(X)$, making explicit two recurring construction patterns: (i) the Unified Factorized Framework (Explicit) (attention-style mixing), in which $W_{ij}(X)$ varies through scalar coefficients applied to shared value maps, and (ii) Structured Dynamics (Implicit) (state-space recurrences), in which $W_{ij}$ is induced by a latent dynamical system. Using this framework, we derive three theoretical results. First, we establish the Interaction Rank Gap: models in the Unified Factorized Framework, such as single-head attention, are constrained to a low-dimensional operator span and cannot represent certain structured dynamical maps. Second, we prove an Equivalence (Head-Count) Theorem showing that, within our multi-head factorized class, representing a linear SSM whose lag operators span a $k$-dimensional subspace on length-$n$ sequences requires and is achievable with $H=k$ heads. Third, we prove a Gradient Highway Result, showing that attention layers admit inputs with distance-independent gradient paths, whereas stable linear dynamics exhibit distance-dependent gradient attenuation. Together, these results formalize a fundamental trade-off between algebraic expressivity (interaction/operator span) and long-range gradient propagation, providing theoretical grounding for modern sequence architecture design.

</details>


### [51] [FADTI: Fourier and Attention Driven Diffusion for Multivariate Time Series Imputation](https://arxiv.org/abs/2512.15116)
*Runze Li,Hanchen Wang,Wenjie Zhang,Binghao Li,Yu Zhang,Xuemin Lin,Ying Zhang*

Main category: cs.LG

TL;DR: FADTI是一个基于扩散模型的多元时间序列插补框架，通过可学习的傅里叶偏置投影模块注入频率感知特征调制，结合自注意力和门控卷积进行时序建模，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列插补在医疗、交通预测和生物建模等领域至关重要，但现有的Transformer和扩散模型缺乏明确的归纳偏置和频率感知能力，限制了它们在结构化缺失模式和分布偏移下的泛化性能。

Method: 提出FADTI框架，通过可学习的傅里叶偏置投影模块注入频率感知特征调制，支持多种谱基，能够自适应编码平稳和非平稳模式，并结合自注意力和门控卷积进行时序建模。

Result: 在多个基准测试（包括新引入的生物时间序列数据集）上，FADTI始终优于最先进的方法，特别是在高缺失率情况下表现突出。

Conclusion: FADTI通过将频域归纳偏置注入生成式插补过程，有效解决了现有模型在频率感知和泛化能力方面的不足，为多元时间序列插补提供了更强大的解决方案。

Abstract: Multivariate time series imputation is fundamental in applications such as healthcare, traffic forecasting, and biological modeling, where sensor failures and irregular sampling lead to pervasive missing values. However, existing Transformer- and diffusion-based models lack explicit inductive biases and frequency awareness, limiting their generalization under structured missing patterns and distribution shifts. We propose FADTI, a diffusion-based framework that injects frequency-informed feature modulation via a learnable Fourier Bias Projection (FBP) module and combines it with temporal modeling through self-attention and gated convolution. FBP supports multiple spectral bases, enabling adaptive encoding of both stationary and non-stationary patterns. This design injects frequency-domain inductive bias into the generative imputation process. Experiments on multiple benchmarks, including a newly introduced biological time series dataset, show that FADTI consistently outperforms state-of-the-art methods, particularly under high missing rates. Code is available at https://anonymous.4open.science/r/TimeSeriesImputation-52BF

</details>


### [52] [Automatic Reward Shaping from Multi-Objective Human Heuristics](https://arxiv.org/abs/2512.15120)
*Yuqing Xie,Jiayu Chen,Wenhao Tang,Ya Zhang,Chao Yu,Yu Wang*

Main category: cs.LG

TL;DR: MORSE框架通过双层优化自动整合多个启发式奖励，引入随机性鼓励探索，在多目标环境中实现与手动调优相当的性能


<details>
  <summary>Details</summary>
Motivation: 强化学习中设计有效的奖励函数仍然是一个核心挑战，特别是在多目标环境中。现有的手动设计奖励函数方法耗时且难以平衡多个目标。

Method: 提出MORSE框架，将奖励塑造过程建模为双层优化问题：内层训练策略最大化当前塑造的奖励，外层更新奖励函数以优化任务性能。为鼓励探索并避免局部最优，引入基于任务性能和随机初始化神经网络预测误差的噪声注入。

Result: 在MuJoCo和Isaac Sim环境中的实验结果表明，MORSE能够有效平衡各种机器人任务中的多个目标，实现与手动调优奖励函数相当的任务性能。

Conclusion: MORSE为多目标强化学习中的奖励函数设计提供了一个有效的自动化框架，能够自动整合多个启发式奖励并实现良好的性能平衡。

Abstract: Designing effective reward functions remains a central challenge in reinforcement learning, especially in multi-objective environments. In this work, we propose Multi-Objective Reward Shaping with Exploration (MORSE), a general framework that automatically combines multiple human-designed heuristic rewards into a unified reward function. MORSE formulates the shaping process as a bi-level optimization problem: the inner loop trains a policy to maximize the current shaped reward, while the outer loop updates the reward function to optimize task performance. To encourage exploration in the reward space and avoid suboptimal local minima, MORSE introduces stochasticity into the shaping process, injecting noise guided by task performance and the prediction error of a fixed, randomly initialized neural network. Experimental results in MuJoCo and Isaac Sim environments show that MORSE effectively balances multiple objectives across various robotic tasks, achieving task performance comparable to those obtained with manually tuned reward functions.

</details>


### [53] [TrajSyn: Privacy-Preserving Dataset Distillation from Federated Model Trajectories for Server-Side Adversarial Training](https://arxiv.org/abs/2512.15123)
*Mukur Gupta,Niharika Gupta,Saifur Rahman,Shantanu Pal,Chandan Karmakar*

Main category: cs.LG

TL;DR: TrajSyn是一个隐私保护的联邦学习框架，通过从客户端模型更新轨迹中合成代理数据集，实现服务器端的对抗训练，无需访问原始客户端数据。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上部署的深度学习模型在安全关键应用中面临对抗性攻击风险，联邦学习环境中由于客户端数据隐私限制和边缘设备计算资源有限，难以应用对抗训练防御。

Method: TrajSyn框架从客户端模型更新轨迹中合成代理数据集，在服务器端进行对抗训练，保护客户端数据隐私且不增加客户端计算负担。

Result: 在图像分类基准测试中，TrajSyn持续提升对抗鲁棒性，且无需在客户端设备上增加额外计算负担。

Conclusion: TrajSyn为联邦学习环境提供了一种有效的隐私保护对抗训练方法，解决了传统对抗训练在FL中的实施难题。

Abstract: Deep learning models deployed on edge devices are increasingly used in safety-critical applications. However, their vulnerability to adversarial perturbations poses significant risks, especially in Federated Learning (FL) settings where identical models are distributed across thousands of clients. While adversarial training is a strong defense, it is difficult to apply in FL due to strict client-data privacy constraints and the limited compute available on edge devices. In this work, we introduce TrajSyn, a privacy-preserving framework that enables effective server-side adversarial training by synthesizing a proxy dataset from the trajectories of client model updates, without accessing raw client data. We show that TrajSyn consistently improves adversarial robustness on image classification benchmarks with no extra compute burden on the client device.

</details>


### [54] [From Isolation to Entanglement: When Do Interpretability Methods Identify and Disentangle Known Concepts?](https://arxiv.org/abs/2512.15134)
*Aaron Mueller,Andrew Lee,Shruti Joshi,Ekdeep Singh Lubana,Dhanya Sridhar,Patrik Reizinger*

Main category: cs.LG

TL;DR: 该研究提出多概念评估框架，分析稀疏自编码器和稀疏探针在概念相关性增强时的解耦表现，发现特征与概念存在一对多关系，且即使概念分布均匀，特征操纵仍会影响多个概念，表明相关度量不足以评估独立性。


<details>
  <summary>Details</summary>
Motivation: 当前可解释性研究中，概念表示的质量通常基于孤立评估和隐含的独立性假设，但这些假设在实践中可能不成立。需要评估常见特征化方法（如稀疏自编码器和稀疏探针）是否能真正恢复这些概念的解耦表示。

Method: 提出多概念评估框架，控制文本概念（如情感、领域、时态）之间的相关性，分析在相关性增强时的表现。首先评估特征化方法在不同相关强度下学习每个概念解耦表示的能力，然后进行操纵实验，测量每个概念是否可独立操纵。

Result: 观察到特征与概念存在一对多关系：特征最多对应一个概念，但概念分布在多个特征中。即使在均匀概念分布上训练，稀疏自编码器特征在操纵时通常会影响多个概念，表明它们既不具有选择性也不独立；然而特征影响不相交的子空间。

Conclusion: 相关度量通常不足以在操纵时建立独立性，影响不相交子空间也不足以实现概念选择性。这些结果强调了可解释性研究中组合评估的重要性。

Abstract: A central goal of interpretability is to recover representations of causally relevant concepts from the activations of neural networks. The quality of these concept representations is typically evaluated in isolation, and under implicit independence assumptions that may not hold in practice. Thus, it is unclear whether common featurization methods - including sparse autoencoders (SAEs) and sparse probes - recover disentangled representations of these concepts. This study proposes a multi-concept evaluation setting where we control the correlations between textual concepts, such as sentiment, domain, and tense, and analyze performance under increasing correlations between them. We first evaluate the extent to which featurizers can learn disentangled representations of each concept under increasing correlational strengths. We observe a one-to-many relationship from concepts to features: features correspond to no more than one concept, but concepts are distributed across many features. Then, we perform steering experiments, measuring whether each concept is independently manipulable. Even when trained on uniform distributions of concepts, SAE features generally affect many concepts when steered, indicating that they are neither selective nor independent; nonetheless, features affect disjoint subspaces. These results suggest that correlational metrics for measuring disentanglement are generally not sufficient for establishing independence when steering, and that affecting disjoint subspaces is not sufficient for concept selectivity. These results underscore the importance of compositional evaluations in interpretability research.

</details>


### [55] [Generalization and Feature Attribution in Machine Learning Models for Crop Yield and Anomaly Prediction in Germany](https://arxiv.org/abs/2512.15140)
*Roland Baatz*

Main category: cs.LG

TL;DR: 该研究比较了德国农业产量预测中机器学习模型的泛化性能和可解释性，发现模型在时间独立验证中表现显著下降，但SHAP特征重要性仍显示可信，揭示了可解释性方法与泛化能力脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估农业和环境系统中机器学习模型的泛化性能和可解释性可靠性。随着数据驱动农业的发展，需要理解模型在未见时空条件下的表现，以及解释性方法是否在模型泛化失败时仍能提供可信的特征重要性。

Method: 使用德国NUTS-3区域的高质量长期数据集，系统比较集成树模型（XGBoost、随机森林）和深度学习方法（LSTM、TCN）。通过空间分割的传统测试集和时间独立的验证年进行性能评估，并使用SHAP进行特征重要性分析。

Result: 所有模型在空间分割测试集上表现良好，但在时间独立验证年上性能显著下降。有趣的是，即使模型在时间验证中泛化能力差，SHAP特征重要性仍显示为可信。这暴露了事后可解释性方法的脆弱性。

Conclusion: 研究强调农业和环境系统中需要验证感知的ML预测解释。特征重要性不应被表面接受，除非模型明确展示了对未见时空条件的泛化能力。建议采用领域感知验证、混合建模策略，以及对可解释性方法进行更严格的审查。

Abstract: This study examines the generalization performance and interpretability of machine learning (ML) models used for predicting crop yield and yield anomalies in Germany's NUTS-3 regions. Using a high-quality, long-term dataset, the study systematically compares the evaluation and temporal validation behavior of ensemble tree-based models (XGBoost, Random Forest) and deep learning approaches (LSTM, TCN).
  While all models perform well on spatially split, conventional test sets, their performance degrades substantially on temporally independent validation years, revealing persistent limitations in generalization. Notably, models with strong test-set accuracy, but weak temporal validation performance can still produce seemingly credible SHAP feature importance values. This exposes a critical vulnerability in post hoc explainability methods: interpretability may appear reliable even when the underlying model fails to generalize.
  These findings underscore the need for validation-aware interpretation of ML predictions in agricultural and environmental systems. Feature importance should not be accepted at face value unless models are explicitly shown to generalize to unseen temporal and spatial conditions. The study advocates for domain-aware validation, hybrid modeling strategies, and more rigorous scrutiny of explainability methods in data-driven agriculture. Ultimately, this work addresses a growing challenge in environmental data science: how can we evaluate generalization robustly enough to trust model explanations?

</details>


### [56] [An Efficient Gradient-Based Inference Attack for Federated Learning](https://arxiv.org/abs/2512.15143)
*Pablo Montaña-Fernández,Ines Ortega-Fernandez*

Main category: cs.LG

TL;DR: 提出一种新的基于梯度的联邦学习成员推理攻击方法，利用多轮训练中最后一层梯度的时序演化模式，无需访问私有数据集，可扩展至离散属性推理攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然减少了直接数据暴露，但参与者与聚合器之间交换的模型更新仍可能泄露敏感信息。现有攻击方法存在局限性，需要探索更有效的多轮联邦学习隐私攻击。

Method: 使用影子技术学习训练记录的多轮梯度模式，考虑半诚实和恶意攻击者（聚合器或数据所有者）。通过对比不同属性假设下的梯度响应，扩展为离散属性推理攻击。该方法与模型无关，适用于任何基于梯度的模型。

Result: 在CIFAR-100和Purchase100数据集上评估成员推理攻击，在Breast Cancer Wisconsin数据集上评估属性推理攻击。结果显示攻击性能强劲，计算和内存开销与现有攻击相当。多轮联邦学习增加了推理攻击的脆弱性，聚合器比数据所有者威胁更大，高维数据比简单表格数据泄露更严重。

Conclusion: 多轮联邦学习增加了对推理攻击的脆弱性，聚合器构成更大威胁，数据集的丰富性和维度强烈影响攻击性能。需要开发更强的隐私保护机制来应对这类时序梯度攻击。

Abstract: Federated Learning is a machine learning setting that reduces direct data exposure, improving the privacy guarantees of machine learning models. Yet, the exchange of model updates between the participants and the aggregator can still leak sensitive information. In this work, we present a new gradient-based membership inference attack for federated learning scenarios that exploits the temporal evolution of last-layer gradients across multiple federated rounds. Our method uses the shadow technique to learn round-wise gradient patterns of the training records, requiring no access to the private dataset, and is designed to consider both semi-honest and malicious adversaries (aggregators or data owners). Beyond membership inference, we also provide a natural extension of the proposed attack to discrete attribute inference by contrasting gradient responses under alternative attribute hypotheses. The proposed attacks are model-agnostic, and therefore applicable to any gradient-based model and can be applied to both classification and regression settings. We evaluate the attack on CIFAR-100 and Purchase100 datasets for membership inference and on Breast Cancer Wisconsin for attribute inference. Our findings reveal strong attack performance and comparable computational and memory overhead in membership inference when compared to another attack from the literature. The obtained results emphasize that multi-round federated learning can increase the vulnerability to inference attacks, that aggregators pose a more substantial threat than data owners, and that attack performance is strongly influenced by the nature of the training dataset, with richer, high-dimensional data leading to stronger leakage than simpler tabular data.

</details>


### [57] [DEER: Draft with Diffusion, Verify with Autoregressive Models](https://arxiv.org/abs/2512.15176)
*Zicong Cheng,Guo-Wei Yang,Jia Li,Zhijie Deng,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.LG

TL;DR: DEER是一种高效的推测解码框架，使用扩散大语言模型作为草稿器，通过并行解码生成长草稿段，相比传统自回归草稿器显著提升了解码速度。


<details>
  <summary>Details</summary>
Motivation: 自回归解码的固有延迟限制了LLM驱动系统的效率，现有推测解码方法依赖自回归草稿器存在两个根本问题：逐步不确定性累积导致目标模型与草稿器之间的信任逐渐崩溃，以及自回归草稿器的固有顺序解码。这些问题导致速度提升有限。

Method: 提出DEER框架，使用扩散大语言模型作为草稿器，采用两阶段训练流程将dLLM草稿器与目标自回归模型对齐，并采用单步解码生成长草稿段，通过扩散验证自回归的方式进行推测解码。

Result: DEER实现了高达32个标记的草稿接受长度，远超EAGLE-3的10个标记。在HumanEval测试中，使用Qwen3-30B-A3B模型，DEER达到了5.54倍加速，而EAGLE-3仅为2.41倍。

Conclusion: 扩散大语言模型草稿器能够自然克服自回归草稿器的固有缺陷，通过不同的概率建模和高效并行解码策略，显著提升了推测解码的效率，为LLM驱动系统提供了更高效的解决方案。

Abstract: Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/

</details>


### [58] [Understanding NTK Variance in Implicit Neural Representations](https://arxiv.org/abs/2512.15169)
*Chengguang Ou,Yixin Zhuang*

Main category: cs.LG

TL;DR: 该论文提出了一种统一的理论框架，通过分析神经正切核(NTK)的特征值方差来解释不同隐式神经表示(INR)架构如何缓解频谱偏差问题。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)通常收敛缓慢且难以恢复高频细节，这归因于频谱偏差。虽然先前工作将此行为与神经正切核(NTK)联系起来，但具体的架构选择如何影响NTK条件数仍不清楚。

Method: 作者证明许多INR机制可以通过它们对少量成对相似性因子和缩放项的影响来理解，这些因子共同决定NTK特征值方差。对于标准坐标MLPs，有限的输入特征交互导致大的特征值离散和不良条件数。作者推导了常见INR组件的闭式方差分解，并展示了位置编码如何重塑输入相似性，球面归一化通过层间缩放减少方差，Hadamard调制引入严格小于1的额外相似性因子，产生乘法方差减少。

Result: 实验验证了预测的方差减少，并展示了在多个任务中更快、更稳定的收敛以及改进的重建质量。

Conclusion: 这一统一视角解释了不同INR架构如何通过改善NTK条件数来缓解频谱偏差，为理解INR机制提供了理论基础。

Abstract: Implicit Neural Representations (INRs) often converge slowly and struggle to recover high-frequency details due to spectral bias. While prior work links this behavior to the Neural Tangent Kernel (NTK), how specific architectural choices affect NTK conditioning remains unclear. We show that many INR mechanisms can be understood through their impact on a small set of pairwise similarity factors and scaling terms that jointly determine NTK eigenvalue variance. For standard coordinate MLPs, limited input-feature interactions induce large eigenvalue dispersion and poor conditioning. We derive closed-form variance decompositions for common INR components and show that positional encoding reshapes input similarity, spherical normalization reduces variance via layerwise scaling, and Hadamard modulation introduces additional similarity factors strictly below one, yielding multiplicative variance reduction. This unified view explains how diverse INR architectures mitigate spectral bias by improving NTK conditioning. Experiments across multiple tasks confirm the predicted variance reductions and demonstrate faster, more stable convergence with improved reconstruction quality.

</details>


### [59] [Leveraging Foundational Models and Simple Fusion for Multi-modal Physiological Signal Analysis](https://arxiv.org/abs/2512.15250)
*Youssef Ghallab,Omar Iraqy,Mohamed Kandil,Mohamed Ashraf,Saadeldine Eletter,Morougue Ghazal,Ayman Khalafallah,Nagwa El-Makky*

Main category: cs.LG

TL;DR: 该研究提出了一种多模态生理信号融合方法，通过自监督预训练的CBraMod编码器分别处理ECG和EEG信号，采用简单的嵌入拼接进行融合，在情感识别任务上取得了接近最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）和脑电图（EEG）等生理信号为人类健康和认知提供了互补的见解，但多模态整合面临挑战，主要原因是多模态标记数据有限以及模态特异性差异。

Method: 1. 采用CBraMod编码器进行大规模自监督ECG预训练，引入双掩码策略捕捉导联内和导联间依赖关系；2. 使用预训练的CBraMod编码器处理EEG信号；3. 通过简单的嵌入拼接融合两种模态的表示，让分类头学习跨模态交互；4. 在有限的多模态监督下实现有效的下游学习。

Result: 在情感识别任务上，该方法取得了接近最先进的性能，表明精心设计的生理信号编码器即使采用简单的融合方式也能显著提升下游任务性能。

Conclusion: 研究结果凸显了基础模型方法在利用生理信号整体特性方面的潜力，能够为医疗保健和情感计算提供可扩展、标签高效且可泛化的解决方案。

Abstract: Physiological signals such as electrocardiograms (ECG) and electroencephalograms (EEG) provide complementary insights into human health and cognition, yet multi-modal integration is challenging due to limited multi-modal labeled data, and modality-specific differences . In this work, we adapt the CBraMod encoder for large-scale self-supervised ECG pretraining, introducing a dual-masking strategy to capture intra- and inter-lead dependencies. To overcome the above challenges, we utilize a pre-trained CBraMod encoder for EEG and pre-train a symmetric ECG encoder, equipping each modality with a rich foundational representation. These representations are then fused via simple embedding concatenation, allowing the classification head to learn cross-modal interactions, together enabling effective downstream learning despite limited multi-modal supervision. Evaluated on emotion recognition, our approach achieves near state-of-the-art performance, demonstrating that carefully designed physiological encoders, even with straightforward fusion, substantially improve downstream performance. These results highlight the potential of foundation-model approaches to harness the holistic nature of physiological signals, enabling scalable, label-efficient, and generalizable solutions for healthcare and affective computing.

</details>


### [60] [Quantum Machine Learning for Cybersecurity: A Taxonomy and Future Directions](https://arxiv.org/abs/2512.15286)
*Siva Sai,Ishika Goyal,Shubham Sharma,Sri Harshita Manuri,Vinay Chamola,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 这是一篇关于量子机器学习在网络安全领域应用的综述论文，探讨了QML如何应对传统安全防御方法的不足，并系统性地介绍了QNNs、QSVMs、VQCs、QGANs等量子机器学习技术在入侵检测、恶意软件分类等网络安全任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则、签名和经典机器学习的网络安全防御方法已无法应对日益增长的网络威胁、快速演变的攻击手段以及海量数据。量子机器学习作为一种新兴技术，利用量子力学原理进行计算，能够更好地编码和处理高维数据结构，为解决网络安全问题提供了新的可能性。

Method: 本文采用综述研究方法，系统性地回顾和分析了量子机器学习在网络安全领域的应用。具体包括：1）全面概述QNNs、QSVMs、VQCs、QGANs等量子机器学习技术；2）将这些方法映射到监督学习、无监督学习和生成学习范式；3）将这些技术应用于核心网络安全任务，如入侵检测、异常检测、恶意软件和僵尸网络分类、加密流量分析；4）探讨在云计算安全领域的应用。

Result: 论文系统性地展示了量子机器学习在网络安全领域的应用潜力，特别是在处理高维数据和复杂模式识别方面的优势。同时，论文也指出了QML在网络安全应用中的局限性，并提出了未来研究方向，为量子机器学习在网络安全领域的进一步发展提供了理论框架和实践指导。

Conclusion: 量子机器学习为网络安全领域提供了有前景的替代方案，能够应对传统防御方法的不足。尽管目前存在技术限制和挑战，但QML在入侵检测、恶意软件分类等关键安全任务中展现出潜力。未来需要进一步研究解决QML在网络安全应用中的局限性，推动该领域的发展。

Abstract: The increasing number of cyber threats and rapidly evolving tactics, as well as the high volume of data in recent years, have caused classical machine learning, rules, and signature-based defence strategies to fail, rendering them unable to keep up. An alternative, Quantum Machine Learning (QML), has recently emerged, making use of computations based on quantum mechanics. It offers better encoding and processing of high-dimensional structures for certain problems. This survey provides a comprehensive overview of QML techniques relevant to the domain of security, such as Quantum Neural Networks (QNNs), Quantum Support Vector Machines (QSVMs), Variational Quantum Circuits (VQCs), and Quantum Generative Adversarial Networks (QGANs), and discusses the contributions of this paper in relation to existing research in the field and how it improves over them. It also maps these methods across supervised, unsupervised, and generative learning paradigms, and to core cybersecurity tasks, including intrusion and anomaly detection, malware and botnet classification, and encrypted-traffic analytics. It also discusses their application in the domain of cloud computing security, where QML can enhance secure and scalable operations. Many limitations of QML in the domain of cybersecurity have also been discussed, along with the directions for addressing them.

</details>


### [61] [Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT](https://arxiv.org/abs/2512.15206)
*Liyu Zhang,Yejia Liu,Kwun Ho Liu,Runxi Huang,Xiaomin Ouyang*

Main category: cs.LG

TL;DR: Chorus：一种面向物联网的无数据上下文感知模型定制方法，能在无需目标域数据的情况下适应未见过的部署环境，通过跨模态重建学习上下文表示，并动态平衡传感器和上下文贡献。


<details>
  <summary>Details</summary>
Motivation: 现实物联网应用中，传感器数据收集于多样动态的上下文环境中（如传感器放置位置、环境因素等），这些因素显著影响数据模式和下游性能。传统域适应或泛化方法通常忽略上下文信息或使用简单集成策略，导致在部署后面对未见过的上下文变化时效果不佳。

Method: 1. 通过无监督跨模态重建学习上下文表示：在未标记传感器数据和基于语言的上下文嵌入之间进行重建，同时正则化上下文嵌入空间以学习鲁棒、可泛化的上下文表示。2. 训练轻量级门控头：在有限标记样本上训练，动态平衡传感器和上下文贡献——当传感器证据模糊时偏向上下文，反之亦然。3. 上下文缓存机制：重用缓存的上下文表示，仅在检测到上下文变化时更新，以减少推理延迟。

Result: 在IMU、语音和WiFi感知任务中，面对多样上下文变化，Chorus在未见过的上下文环境中比最先进的基线方法性能提升高达11.3%，同时在智能手机和边缘设备上保持可比较的延迟。

Conclusion: Chorus提出了一种有效的上下文感知、无数据模型定制方法，能够适应未见过的部署条件，通过跨模态重建学习上下文表示、动态平衡传感器与上下文贡献以及上下文缓存机制，在保持低延迟的同时显著提升了在未见上下文环境中的性能。

Abstract: In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.

</details>


### [62] [Empirical Investigation of the Impact of Phase Information on Fault Diagnosis of Rotating Machinery](https://arxiv.org/abs/2512.15344)
*Hiroyoshi Nagahama,Katsufumi Inoue,Masayoshi Todorokihara,Michifumi Yoshioka*

Main category: cs.LG

TL;DR: 该论文提出了两种相位感知预处理策略来解决多轴振动数据中的随机相位变化问题，并在转子数据集上验证了这些方法能显著提升深度学习模型的预测维护性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的旋转机械预测维护方法要么在频谱特征提取时丢弃相位信息，要么使用原始时间波形而不显式利用相位信息。多轴振动数据中的随机相位变化影响了模型的性能，需要有效的相位预处理策略。

Method: 提出了两种相位感知预处理策略：1）三轴独立相位调整，将每个轴单独对齐到零相位；2）单轴参考相位调整，通过统一的时间偏移保持轴间关系。使用新构建的同步三轴传感器转子数据集，在两级学习框架下评估了六种深度学习架构。

Result: 三轴独立方法实现了稳定的性能提升（Transformer模型提升2.7%），而单轴参考方法通过保持空间相位关系获得了更优性能，最高达到96.2%的准确率（提升5.4%）。两种方法都带来了与架构无关的改进。

Conclusion: 两种相位对齐策略都是实用且可扩展的预测维护系统增强方法，能有效利用振动信号中的相位信息来提升模型性能，特别是通过保持轴间相位关系的单轴参考方法表现最佳。

Abstract: Predictive maintenance of rotating machinery increasingly relies on vibration signals, yet most learning-based approaches either discard phase during spectral feature extraction or use raw time-waveforms without explicitly leveraging phase information. This paper introduces two phase-aware preprocessing strategies to address random phase variations in multi-axis vibration data: (1) three-axis independent phase adjustment that aligns each axis individually to zero phase (2) single-axis reference phase adjustment that preserves inter-axis relationships by applying uniform time shifts. Using a newly constructed rotor dataset acquired with a synchronized three-axis sensor, we evaluate six deep learning architectures under a two-stage learning framework. Results demonstrate architecture-independent improvements: the three-axis independent method achieves consistent gains (+2.7\% for Transformer), while the single-axis reference approach delivers superior performance with up to 96.2\% accuracy (+5.4\%) by preserving spatial phase relationships. These findings establish both phase alignment strategies as practical and scalable enhancements for predictive maintenance systems.

</details>


### [63] [Accelerating High-Throughput Catalyst Screening by Direct Generation of Equilibrium Adsorption Structures](https://arxiv.org/abs/2512.15228)
*Songze Huo,Xiao-Ming Cao*

Main category: cs.LG

TL;DR: DBCata是一种深度生成模型，通过周期性布朗桥框架和等变图神经网络，在无需能量或力信息的情况下，直接从非弛豫结构生成高保真DFT弛豫吸附结构，显著提升催化剂筛选的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习原子间势（MLIP）的训练数据主要来自近平衡结构，分布有限，导致吸附结构和吸附能预测不可靠。吸附能作为催化剂大规模筛选的关键描述符，需要更可靠的结构生成方法。

Method: 提出DBCata深度生成模型，结合周期性布朗桥框架和等变图神经网络，建立非弛豫结构与DFT弛豫结构之间的低维过渡流形，无需显式的能量或力信息。采用混合化学启发式和自监督异常检测方法来识别和精炼异常预测。

Result: 在Catalysis-Hub数据集上，DBCata生成的吸附几何结构达到0.035Å的原子间距离平均绝对误差（DMAE），比当前最先进的机器学习势模型优越近三倍。通过异常检测和精炼，94%的实例可将DFT精度提高至0.1eV以内。

Conclusion: DBCata在氧还原反应高效合金催化剂的高通量计算筛选中表现出色，展示了其作为催化剂设计和优化的强大工具潜力，能够加速催化剂发现过程。

Abstract: The adsorption energy serves as a crucial descriptor for the large-scale screening of catalysts. Nevertheless, the limited distribution of training data for the extensively utilised machine learning interatomic potential (MLIP), predominantly sourced from near-equilibrium structures, results in unreliable adsorption structures and consequent adsorption energy predictions. In this context, we present DBCata, a deep generative model that integrates a periodic Brownian-bridge framework with an equivariant graph neural network to establish a low-dimensional transition manifold between unrelaxed and DFT-relaxed structures, without requiring explicit energy or force information. Upon training, DBCata effectively generates high-fidelity adsorption geometries, achieving an interatomic distance mean absolute error (DMAE) of 0.035 \textÅ on the Catalysis-Hub dataset, which is nearly three times superior to that of the current state-of-the-art machine learning potential models. Moreover, the corresponding DFT accuracy can be improved within 0.1 eV in 94\% of instances by identifying and refining anomalous predictions through a hybrid chemical-heuristic and self-supervised outlier detection approach. We demonstrate that the remarkable performance of DBCata facilitates accelerated high-throughput computational screening for efficient alloy catalysts in the oxygen reduction reaction, highlighting the potential of DBCata as a powerful tool for catalyst design and optimisation.

</details>


### [64] [O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization](https://arxiv.org/abs/2512.15229)
*Elio Gruttadauria,Mathieu Fontaine,Jonathan Le Roux,Slim Essid*

Main category: cs.LG

TL;DR: O-EENC-SD是一个基于EEND-EDA的端到端在线说话人日志系统，采用RNN拼接机制和质心细化解码器，在双人电话对话场景中与SOTA竞争，在DER和复杂度之间提供良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有在线说话人日志方法存在两个主要问题：无监督聚类方法需要超参数调整，而当前在线端到端方法计算成本高。需要开发一个既无需超参数又计算高效的在线系统。

Method: 基于EEND-EDA框架，开发了基于RNN的在线预测拼接机制，并设计了新颖的质心细化解码器。系统在独立块上工作，无需重叠，提高了效率。

Result: 在CallHome数据集的双人电话对话测试中，O-EENC-SD与最先进方法竞争。通过消融研究验证了质心细化解码器的有效性，系统在DER和复杂度之间提供了良好平衡。

Conclusion: O-EENC-SD是一个高效的在线说话人日志系统，相比无监督聚类方法无需超参数调整，相比当前在线端到端方法计算成本更低，在双人电话对话场景中具有竞争力。

Abstract: We introduce O-EENC-SD: an end-to-end online speaker diarization system based on EEND-EDA, featuring a novel RNN-based stitching mechanism for online prediction. In particular, we develop a novel centroid refinement decoder whose usefulness is assessed through a rigorous ablation study. Our system provides key advantages over existing methods: a hyperparameter-free solution compared to unsupervised clustering approaches, and a more efficient alternative to current online end-to-end methods, which are computationally costly. We demonstrate that O-EENC-SD is competitive with the state of the art in the two-speaker conversational telephone speech domain, as tested on the CallHome dataset. Our results show that O-EENC-SD provides a great trade-off between DER and complexity, even when working on independent chunks with no overlap, making the system extremely efficient.

</details>


### [65] [FM-EAC: Feature Model-based Enhanced Actor-Critic for Multi-Task Control in Dynamic Environments](https://arxiv.org/abs/2512.15430)
*Quanxi Zhou,Wencan Mao,Manabu Tsukada,John C. S. Lui,Yusheng Ji*

Main category: cs.LG

TL;DR: 提出FM-EAC算法，整合基于模型和无模型强化学习，通过特征模型和增强的actor-critic框架提升多任务控制中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习方法在跨任务和场景的有效迁移性方面仍然存在困难，尽管Dyna-Q融合了基于模型和无模型方法，但需要更通用的算法来应对动态环境中的多任务控制。

Method: 提出FM-EAC算法，整合规划、行动和学习，结合基于模型和无模型强化学习的优势，使用基于特征的新模型和增强的actor-critic框架来提升泛化能力。

Result: 在城市和农业应用的模拟中，FM-EAC始终优于许多最先进的基于模型和无模型强化学习方法，且其子网络可根据用户特定需求进行定制。

Conclusion: FM-EAC通过整合规划、行动和学习，结合基于模型和无模型强化学习的优势，有效提升了多任务控制中的泛化能力和性能表现。

Abstract: Model-based reinforcement learning (MBRL) and model-free reinforcement learning (MFRL) evolve along distinct paths but converge in the design of Dyna-Q [1]. However, modern RL methods still struggle with effective transferability across tasks and scenarios. Motivated by this limitation, we propose a generalized algorithm, Feature Model-Based Enhanced Actor-Critic (FM-EAC), that integrates planning, acting, and learning for multi-task control in dynamic environments. FM-EAC combines the strengths of MBRL and MFRL and improves generalizability through the use of novel feature-based models and an enhanced actor-critic framework. Simulations in both urban and agricultural applications demonstrate that FM-EAC consistently outperforms many state-of-the-art MBRL and MFRL methods. More importantly, different sub-networks can be customized within FM-EAC according to user-specific requirements.

</details>


### [66] [Double Horizon Model-Based Policy Optimization](https://arxiv.org/abs/2512.15439)
*Akihiro Kubo,Paavo Parmas,Shin Ishii*

Main category: cs.LG

TL;DR: DHMBPO提出双视野模型强化学习方法，通过长分布视野和短训练视野分离，平衡分布偏移、模型偏差和梯度稳定性问题，提升样本效率和运行速度。


<details>
  <summary>Details</summary>
Motivation: 模型强化学习中，视野长度选择存在两难：长视野能更好保持同策略训练但会放大模型偏差，需要中间视野来缓解分布偏移；同时长视野可能减少价值估计偏差但会增加策略梯度方差。这两个最优视野可能不同，需要解决这一冲突。

Method: 提出双视野模型策略优化(DHMBPO)，将视野过程分为长"分布视野"(DR)和短"训练视野"(TR)。DR生成同策略状态样本来缓解分布偏移，而短TR利用可微分转移提供准确的价值梯度估计，实现稳定的梯度更新，减少更新次数和总体运行时间。

Result: 双视野方法有效平衡了分布偏移、模型偏差和梯度不稳定性，在连续控制基准测试中超越了现有的模型强化学习方法，在样本效率和运行时间方面都表现更好。

Conclusion: DHMBPO通过分离分布视野和训练视野，解决了模型强化学习中视野长度选择的冲突，实现了分布偏移缓解和梯度稳定性之间的平衡，提高了算法性能。

Abstract: Model-based reinforcement learning (MBRL) reduces the cost of real-environment sampling by generating synthetic trajectories (called rollouts) from a learned dynamics model. However, choosing the length of the rollouts poses two dilemmas: (1) Longer rollouts better preserve on-policy training but amplify model bias, indicating the need for an intermediate horizon to mitigate distribution shift (i.e., the gap between on-policy and past off-policy samples). (2) Moreover, a longer model rollout may reduce value estimation bias but raise the variance of policy gradients due to backpropagation through multiple steps, implying another intermediate horizon for stable gradient estimates. However, these two optimal horizons may differ. To resolve this conflict, we propose Double Horizon Model-Based Policy Optimization (DHMBPO), which divides the rollout procedure into a long "distribution rollout" (DR) and a short "training rollout" (TR). The DR generates on-policy state samples for mitigating distribution shift. In contrast, the short TR leverages differentiable transitions to offer accurate value gradient estimation with stable gradient updates, thereby requiring fewer updates and reducing overall runtime. We demonstrate that the double-horizon approach effectively balances distribution shift, model bias, and gradient instability, and surpasses existing MBRL methods on continuous-control benchmarks in terms of both sample efficiency and runtime.

</details>


### [67] [Distillation-Guided Structural Transfer for Continual Learning Beyond Sparse Distributed Memory](https://arxiv.org/abs/2512.15267)
*Huiyan Xue,Xuming Ran,Yaxin Li,Qi Xu,Enhui Li,Yi Xu,Qiang Zhang*

Main category: cs.LG

TL;DR: SSD是一种结构引导的稀疏持续学习框架，通过选择性子网络蒸馏在保持稀疏模块化的同时实现跨任务知识重用，解决了传统稀疏系统知识复用受限的问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏神经系统的刚性模块化限制了跨任务知识重用，导致在高稀疏度下性能下降。现有稀疏分布式内存多层感知机（SDMLP）虽然能抵抗灾难性遗忘，但缺乏有效的知识传递机制。

Method: 提出选择性子网络蒸馏（SSD）框架，将蒸馏视为拓扑对齐的信息通道而非正则化器。通过识别高激活频率的神经元，在先前Top-K子网络和输出logits之间进行选择性知识蒸馏，无需重放或任务标签。

Result: 在Split CIFAR-10、CIFAR-100和MNIST上的实验表明，SSD提高了准确性、保留率和表示覆盖率，为稀疏持续学习提供了结构基础解决方案。

Conclusion: SSD通过结构引导的蒸馏实现了稀疏神经网络中的知识重用，在保持模块化优势的同时改善了持续学习性能，为稀疏系统提供了有效的跨任务知识传递机制。

Abstract: Sparse neural systems are gaining traction for efficient continual learning due to their modularity and low interference. Architectures such as Sparse Distributed Memory Multi-Layer Perceptrons (SDMLP) construct task-specific subnetworks via Top-K activation and have shown resilience against catastrophic forgetting. However, their rigid modularity limits cross-task knowledge reuse and leads to performance degradation under high sparsity. We propose Selective Subnetwork Distillation (SSD), a structurally guided continual learning framework that treats distillation not as a regularizer but as a topology-aligned information conduit. SSD identifies neurons with high activation frequency and selectively distills knowledge within previous Top-K subnetworks and output logits, without requiring replay or task labels. This enables structural realignment while preserving sparse modularity. Experiments on Split CIFAR-10, CIFAR-100, and MNIST demonstrate that SSD improves accuracy, retention, and representation coverage, offering a structurally grounded solution for sparse continual learning.

</details>


### [68] [Soft Geometric Inductive Bias for Object Centric Dynamics](https://arxiv.org/abs/2512.15493)
*Hampus Linander,Conor Heins,Alexander Tschantz,Marco Perin,Christopher Buckley*

Main category: cs.LG

TL;DR: 提出基于几何代数神经网络的物体中心世界模型，为学习物理动力学提供软几何归纳偏置，在2D刚体动力学环境中优于非等变基线模型


<details>
  <summary>Details</summary>
Motivation: 等变性是学习物理动力学的强大先验，但精确的群等变性在对称性被破坏时可能降低性能。需要一种软几何归纳偏置来平衡等变性和灵活性。

Method: 使用几何代数神经网络构建物体中心世界模型，在具有静态障碍物的2D刚体动力学模拟环境中进行训练，采用自回归方式进行下一步预测。

Result: 在长时程推演中，模型的软归纳偏置在物理保真度方面优于非等变基线模型，表明几何代数提供了样本高效的动力学建模方法。

Conclusion: 几何代数在手工物理和无结构深度网络之间提供了有效折中，简单而精心选择的先验能够产生鲁棒的泛化能力，适用于多物体场景的动力学建模。

Abstract: Equivariance is a powerful prior for learning physical dynamics, yet exact group equivariance can degrade performance if the symmetries are broken. We propose object-centric world models built with geometric algebra neural networks, providing a soft geometric inductive bias. Our models are evaluated using simulated environments of 2d rigid body dynamics with static obstacles, where we train for next-step predictions autoregressively. For long-horizon rollouts we show that the soft inductive bias of our models results in better performance in terms of physical fidelity compared to non-equivariant baseline models. The approach complements recent soft-equivariance ideas and aligns with the view that simple, well-chosen priors can yield robust generalization. These results suggest that geometric algebra offers an effective middle ground between hand-crafted physics and unstructured deep nets, delivering sample-efficient dynamics models for multi-object scenes.

</details>


### [69] [How Smoothing is N-simplicial Attention?](https://arxiv.org/abs/2512.15600)
*Alexandre Dussolle,Pietro Liò*

Main category: cs.LG

TL;DR: 提出N-单纯形注意力机制，从成对token相似性扩展到高阶交互，适配RoPE位置编码，并通过成本有效的单纯形选择管理计算复杂度，同时分析其平滑性特征


<details>
  <summary>Details</summary>
Motivation: 当前从纯MLP到可学习图消息传递机制（如GATs或Transformers）虽然取得了最先进结果，但存在计算权衡。为了进一步推进，需要从成对token相似性扩展到更高阶的交互

Method: 引入N-单纯形注意力机制，适配旋转位置编码(RoPE)；提出成本有效的单纯形选择方法，让模型将计算资源集中在任务敏感度更高的交互上；通过推导Lipschitz上界分析平滑性特征

Result: N-单纯形注意力机制能够实现高阶交互，单纯形选择方法有效管理计算复杂度，分析表明该机制本身也存在过度平滑问题，尽管它向高阶交互开放了注意力消息传递

Conclusion: N-单纯形注意力是从成对交互到高阶交互的重要扩展，通过单纯形选择管理计算成本，但需要注意其平滑性特征和潜在的过度平滑问题

Abstract: Going from pure Multilayer Perceptron (MLP) to a learnable graph message-passing mechanism at each layer has been foundational to state-of-the-art results, despite the computational trade-off (e.g. GATs or Transformers). To go a step further, in this work, we introduce N-simplicial attention, going from pairwise token similarity to higher-order interactions, and adapt it for Rotary Position Embeddings (RoPE). To help manage the increased complexity, we propose a cost-effective simplex selection enabling the model to focus its computation load onto the more task-sensitive interactions. Beyond these core mechanisms, we study how smoothing N-simplicial attention is by deriving a Lipschitz upper-bound and by demonstrating that by itself it also suffers from over-smoothing, despite opening the attention message-passing to higher-order interactions.

</details>


### [70] [Bits for Privacy: Evaluating Post-Training Quantization via Membership Inference](https://arxiv.org/abs/2512.15335)
*Chenxiang Zhang,Tongxi Qu,Zhong Li,Tian Zhang,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 量化技术通过降低神经网络参数精度来减少内存和计算成本，但现有隐私分析主要关注全精度模型。本研究首次系统分析了后训练量化中的隐私-效用关系，发现低精度量化可显著降低成员推理攻击的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络广泛采用量化技术来降低内存和计算成本，但现有隐私分析主要针对全精度模型，缺乏对量化如何影响隐私泄露的系统研究。量化会改变模型参数和输出，需要填补这一研究空白。

Method: 使用成员推理攻击作为评估框架，分析三种流行的后训练量化算法（AdaRound、BRECQ和OBC），在多个精度级别（4位、2位和1.58位）上对CIFAR-10、CIFAR-100和TinyImageNet数据集进行系统研究。

Result: 低精度后训练量化可以显著减少隐私泄露。与全精度模型相比，低精度模型在成员推理攻击中的脆弱性最多可降低一个数量级，但会以效用下降为代价。在1.58位量化级别上的额外消融研究表明，仅将最后一层量化为更高精度可以实现对隐私-效用权衡的精细控制。

Conclusion: 低精度量化可以有效降低隐私泄露风险，为实际部署中平衡效率、效用和隐私保护提供了可行的见解。量化技术不仅是效率工具，也是隐私保护的有效手段。

Abstract: Deep neural networks are widely deployed with quantization techniques to reduce memory and computational costs by lowering the numerical precision of their parameters. While quantization alters model parameters and their outputs, existing privacy analyses primarily focus on full-precision models, leaving a gap in understanding how bit-width reduction can affect privacy leakage. We present the first systematic study of the privacy-utility relationship in post-training quantization (PTQ), a versatile family of methods that can be applied to pretrained models without further training. Using membership inference attacks as our evaluation framework, we analyze three popular PTQ algorithms-AdaRound, BRECQ, and OBC-across multiple precision levels (4-bit, 2-bit, and 1.58-bit) on CIFAR-10, CIFAR-100, and TinyImageNet datasets. Our findings consistently show that low-precision PTQs can reduce privacy leakage. In particular, lower-precision models demonstrate up to an order of magnitude reduction in membership inference vulnerability compared to their full-precision counterparts, albeit at the cost of decreased utility. Additional ablation studies on the 1.58-bit quantization level show that quantizing only the last layer at higher precision enables fine-grained control over the privacy-utility trade-off. These results offer actionable insights for practitioners to balance efficiency, utility, and privacy protection in real-world deployments.

</details>


### [71] [Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2512.15687)
*Zhenwen Liang,Sidi Lu,Wenhao Yu,Kishan Panaganti,Yujun Zhou,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: G2RL是一种梯度引导的强化学习框架，通过模型自身的梯度更新几何来指导探索，而不是依赖外部启发式方法，在数学和推理基准测试中显著优于基于熵的方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中的探索机制（如熵奖励和外部语义比较器）与大型语言模型的实际学习方式存在根本性错配，它们鼓励表面变化但不能保证采样轨迹在优化更新方向上存在差异。

Method: G2RL框架利用模型最后一层的敏感性构建序列级特征，通过比较采样组内这些特征来衡量每个轨迹如何重塑策略，为引入新颖梯度方向的轨迹提供有界乘法奖励缩放器，同时弱化冗余或偏离流形的更新。

Result: 在Qwen3基础1.7B和4B模型上，G2RL在数学和通用推理基准（MATH500、AMC、AIME24、AIME25、GPQA、MMLUpro）上持续改进pass@1、maj@16和pass@k指标，优于基于熵的GRPO和外部嵌入方法。

Conclusion: 策略自身的更新空间为大型语言模型强化学习中的探索提供了更忠实和有效的基础，G2RL能够扩展到更多正交且通常相反的梯度方向，同时保持语义连贯性。

Abstract: Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.

</details>


### [72] [A Regime-Aware Fusion Framework for Time Series Classification](https://arxiv.org/abs/2512.15378)
*Honey Singh Chauhan,Zahraa S. Abdallah*

Main category: cs.LG

TL;DR: Fusion-3 (F3)是一个轻量级框架，通过自适应融合Rocket、Sax和Sfa三种时间序列表示方法，在特定类型的数据集上比单独使用Rocket获得一致性的性能提升。


<details>
  <summary>Details</summary>
Motivation: 虽然基于核的方法如Rocket是单变量时间序列分类的有效默认方法，但它们在所有数据集上的表现并不均衡。研究基于不同表示方法能捕捉互补结构的长期直觉，探索通过选择性融合这些表示来获得系统性改进。

Method: 提出Fusion-3框架，自适应融合Rocket、Sax和Sfa三种表示。使用元特征（序列长度、频谱结构、粗糙度、类别不平衡）将UCR数据集聚类为6组，作为可解释的数据结构机制。通过三种互补分析：跨数据集的非参数配对统计、消融研究、SHAP归因分析，以及样本级案例研究来验证方法。

Result: 在113个UCR数据集上的5折交叉验证显示，F3相比Rocket获得小而一致的平均改进，得到频率主义者和贝叶斯证据支持。融合在具有结构化变异性或丰富频率内容的数据机制中表现更好，而在高度不规则或异常值多的设置中收益递减。

Conclusion: 选择性应用的融合为强大的基于核的方法提供了可靠且可解释的扩展，在数据支持的特定情况下精确纠正了这些方法的弱点。融合主要通过挽救特定错误来提升性能，自适应增加频率域权重恰好发生在需要修正的地方。

Abstract: Kernel-based methods such as Rocket are among the most effective default approaches for univariate time series classification (TSC), yet they do not perform equally well across all datasets. We revisit the long-standing intuition that different representations capture complementary structure and show that selectively fusing them can yield consistent improvements over Rocket on specific, systematically identifiable kinds of datasets. We introduce Fusion-3 (F3), a lightweight framework that adaptively fuses Rocket, Sax, and Sfa representations. To understand when fusion helps, we cluster UCR datasets into six groups using meta-features capturing series length, spectral structure, roughness, and class imbalance, and treat these clusters as interpretable data-structure regimes. Our analysis shows that fusion typically outperforms strong baselines in regimes with structured variability or rich frequency content, while offering diminishing returns in highly irregular or outlier-heavy settings. To support these findings, we combine three complementary analyses: non-parametric paired statistics across datasets, ablation studies isolating the roles of individual representations, and attribution via SHAP to identify which dataset properties predict fusion gains. Sample-level case studies further reveal the underlying mechanism: fusion primarily improves performance by rescuing specific errors, with adaptive increases in frequency-domain weighting precisely where corrections occur. Using 5-fold cross-validation on the 113 UCR datasets, F3 yields small but consistent average improvements over Rocket, supported by frequentist and Bayesian evidence and accompanied by clearly identifiable failure cases. Our results show that selectively applied fusion provides dependable and interpretable extension to strong kernel-based methods, correcting their weaknesses precisely where the data support it.

</details>


### [73] [Robustness Evaluation of Machine Learning Models for Fault Classification and Localization In Power System Protection](https://arxiv.org/abs/2512.15385)
*Julian Oelhaf,Mehran Pashaei,Georg Kordowich,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 该论文提出了一个评估电力系统保护中机器学习模型鲁棒性的统一框架，通过高保真电磁暂态仿真模拟传感器故障、采样率降低等实际退化场景，发现故障分类相对稳定而故障定位对电压损失更敏感。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和分布式发电的普及，传统依赖固定设置和本地测量的保护方案面临挑战。机器学习为集中式故障分类和定位提供了数据驱动方案，但实际部署需要确保在传感器数据缺失、噪声或退化情况下的鲁棒性。

Method: 提出了一个统一的框架，使用高保真电磁暂态仿真来建模现实的退化场景，包括传感器故障、采样率降低和瞬态通信丢失。该框架提供了基准测试模型、量化有限可观测性影响以及识别关键测量通道的一致方法。

Result: 结果显示故障分类在大多数退化类型下保持高度稳定，但在单相损失下下降约13%；故障定位总体上更敏感，电压损失会使定位误差增加超过150%。

Conclusion: 该研究为未来机器学习辅助保护系统的鲁棒性设计提供了可操作的指导，强调了在传感器数据退化情况下确保保护算法可靠性的重要性。

Abstract: The growing penetration of renewable and distributed generation is transforming power systems and challenging conventional protection schemes that rely on fixed settings and local measurements. Machine learning (ML) offers a data-driven alternative for centralized fault classification (FC) and fault localization (FL), enabling faster and more adaptive decision-making. However, practical deployment critically depends on robustness. Protection algorithms must remain reliable even when confronted with missing, noisy, or degraded sensor data. This work introduces a unified framework for systematically evaluating the robustness of ML models in power system protection.
  High-fidelity EMT simulations are used to model realistic degradation scenarios, including sensor outages, reduced sampling rates, and transient communication losses. The framework provides a consistent methodology for benchmarking models, quantifying the impact of limited observability, and identifying critical measurement channels required for resilient operation. Results show that FC remains highly stable under most degradation types but drops by about 13% under single-phase loss, while FL is more sensitive overall, with voltage loss increasing localization error by over 150%. These findings offer actionable guidance for robustness-aware design of future ML-assisted protection systems.

</details>


### [74] [EUBRL: Epistemic Uncertainty Directed Bayesian Reinforcement Learning](https://arxiv.org/abs/2512.15405)
*Jianfei Ma,Wee Sun Lee*

Main category: cs.LG

TL;DR: 提出了一种名为EUBRL的贝叶斯强化学习算法，利用认知不确定性指导进行原则性探索，在无限时域折扣MDP中实现了接近极小极大最优的遗憾和样本复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 在已知与未知的边界上，智能体面临探索与利用的困境。认知不确定性反映了这种边界，代表了由于知识有限而产生的系统性不确定性。需要一种能够利用认知指导实现原则性探索的强化学习算法。

Method: 提出EUBRL算法，这是一种贝叶斯强化学习方法，利用认知不确定性指导探索。该算法自适应地减少由估计误差引起的每步遗憾，适用于具有充分表达先验的无限时域折扣马尔可夫决策过程。

Result: 理论分析表明，EUBRL在无限时域折扣MDP中实现了接近极小极大最优的遗憾和样本复杂度保证。实证评估显示，在稀疏奖励、长时域和随机性任务中，EUBRL表现出优异的样本效率、可扩展性和一致性。

Conclusion: EUBRL通过利用认知不确定性指导探索，提供了一种原则性的贝叶斯强化学习方法，在理论和实证上都表现出优越性能，特别是在具有挑战性的强化学习任务中。

Abstract: At the boundary between the known and the unknown, an agent inevitably confronts the dilemma of whether to explore or to exploit. Epistemic uncertainty reflects such boundaries, representing systematic uncertainty due to limited knowledge. In this paper, we propose a Bayesian reinforcement learning (RL) algorithm, $\texttt{EUBRL}$, which leverages epistemic guidance to achieve principled exploration. This guidance adaptively reduces per-step regret arising from estimation errors. We establish nearly minimax-optimal regret and sample complexity guarantees for a class of sufficiently expressive priors in infinite-horizon discounted MDPs. Empirically, we evaluate $\texttt{EUBRL}$ on tasks characterized by sparse rewards, long horizons, and stochasticity. Results demonstrate that $\texttt{EUBRL}$ achieves superior sample efficiency, scalability, and consistency.

</details>


### [75] [FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows](https://arxiv.org/abs/2512.15420)
*Yeonwoo Cha,Semin Kim,Jinhyeon Kwon,Seunghoon Hong*

Main category: cs.LG

TL;DR: FlowBind是一个高效的多模态任意到任意生成框架，通过共享潜在空间和模态特定可逆流实现跨模态转换，相比现有方法参数减少6倍、训练速度提升10倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的方法在多模态任意到任意生成中存在效率低下问题：需要大规模数据集且配对约束严格，联合分布建模计算成本高，依赖复杂的多阶段训练流程。

Method: FlowBind学习一个捕获跨模态信息的共享潜在空间，使用模态特定的可逆流将每个模态桥接到这个潜在空间。两个组件在单一流匹配目标下联合优化，推理时这些可逆流作为编码器和解码器实现跨模态直接转换。

Result: 在文本、图像和音频上的实验表明，FlowBind在保持可比生成质量的同时，参数需求减少高达6倍，训练速度提升10倍，显著降低了数据需求和计算成本。

Conclusion: FlowBind通过共享潜在空间和模态特定可逆流的简单设计，实现了高效的多模态任意到任意生成，解决了现有方法在数据需求、计算成本和训练复杂度方面的挑战。

Abstract: Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. Despite recent success, existing flow-based approaches are challenged by their inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computational cost from modeling joint distribution, and rely on complex multi-stage training. We propose FlowBind, an efficient framework for any-to-any generation. Our approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. Both components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities. By factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost. Experiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6x fewer parameters and training 10x faster than prior methods. The project page with code is available at https://yeonwoo378.github.io/official_flowbind.

</details>


### [76] [Statistics of Min-max Normalized Eigenvalues in Random Matrices](https://arxiv.org/abs/2512.15427)
*Hyakka Nakada,Shu Tanaka*

Main category: cs.LG

TL;DR: 该研究探讨了随机矩阵中min-max归一化特征值的统计特性，包括累积分布的标度律和矩阵分解中的残差误差，并通过数值实验验证理论预测。


<details>
  <summary>Details</summary>
Motivation: 随机矩阵理论在数学、物理和机器学习中很重要。从数据科学实践角度看，输入数据通常需要归一化处理，因此需要研究随机矩阵中min-max归一化特征值的统计特性。

Method: 应用先前提出的归一化特征值有效分布来评估累积分布的标度律，推导矩阵分解中的残差误差，并进行数值实验验证理论预测。

Result: 得到了归一化特征值累积分布的标度律，推导了矩阵分解的残差误差，数值实验结果验证了理论预测的正确性。

Conclusion: 该研究为随机矩阵中归一化特征值的统计特性提供了理论分析和实验验证，对数据科学中的归一化处理有实际意义。

Abstract: Random matrix theory has played an important role in various areas of pure mathematics, mathematical physics, and machine learning. From a practical perspective of data science, input data are usually normalized prior to processing. Thus, this study investigates the statistical properties of min-max normalized eigenvalues in random matrices. Previously, the effective distribution for such normalized eigenvalues has been proposed. In this study, we apply it to evaluate a scaling law of the cumulative distribution. Furthermore, we derive the residual error that arises during matrix factorization of random matrices. We conducted numerical experiments to verify these theoretical predictions.

</details>


### [77] [Copyright Infringement Risk Reduction via Chain-of-Thought and Task Instruction Prompting](https://arxiv.org/abs/2512.15442)
*Neeraj Sarna,Yuanyuan Li,Michael von Gablenz*

Main category: cs.LG

TL;DR: 该论文研究如何通过思维链和任务指令提示等技术减少文本到图像生成模型中的版权内容生成，结合负向提示和提示重写策略，评估不同模型下这些方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像生成模型可能记忆并复制其训练数据中的受版权保护内容，这带来版权侵权风险，可能导致AI用户和开发者面临法律责任和经济损失。

Method: 提出结合思维链和任务指令提示的框架，并与两种版权缓解策略结合：a) 负向提示，b) 提示重写。通过评估生成图像与版权图像的相似度以及与用户输入的相关性来研究这些技术。

Result: 在不同模型上进行了数值实验，提供了关于这些技术在不同模型复杂度下有效性的见解。

Conclusion: 思维链和任务指令提示结合负向提示和提示重写策略可以有效减少文本到图像生成模型中的版权内容生成，为版权风险缓解提供了实用方法。

Abstract: Large scale text-to-image generation models can memorize and reproduce their training dataset. Since the training dataset often contains copyrighted material, reproduction of training dataset poses a copyright infringement risk, which could result in legal liabilities and financial losses for both the AI user and the developer. The current works explores the potential of chain-of-thought and task instruction prompting in reducing copyrighted content generation. To this end, we present a formulation that combines these two techniques with two other copyright mitigation strategies: a) negative prompting, and b) prompt re-writing. We study the generated images in terms their similarity to a copyrighted image and their relevance of the user input. We present numerical experiments on a variety of models and provide insights on the effectiveness of the aforementioned techniques for varying model complexity.

</details>


### [78] [From Risk to Resilience: Towards Assessing and Mitigating the Risk of Data Reconstruction Attacks in Federated Learning](https://arxiv.org/abs/2512.15460)
*Xiangrui Xu,Zhize Li,Yufei Han,Bin Wang,Jiqiang Liu,Wei Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Invertibility Loss的理论框架，用于量化联邦学习系统中数据重构攻击的最大可实现效果，并基于此开发了风险估计器和防御方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统中的数据重构攻击对隐私构成严重威胁，但现有研究缺乏理论基础的量化框架来评估和表征这种风险。

Method: 引入Invertibility Loss来量化数据重构攻击的最大效果，推导其可计算的上界，并基于此开发InvRE风险估计器和两种自适应噪声扰动防御方法。

Result: 实验验证表明，该框架能有效评估DRA风险，揭示风险与雅可比矩阵谱特性的关系，提出的防御方法能在不损害分类准确性的情况下增强隐私保护。

Conclusion: 该工作为联邦学习系统提供了系统性的数据重构攻击风险评估和缓解框架，填补了理论量化方法的空白。

Abstract: Data Reconstruction Attacks (DRA) pose a significant threat to Federated Learning (FL) systems by enabling adversaries to infer sensitive training data from local clients. Despite extensive research, the question of how to characterize and assess the risk of DRAs in FL systems remains unresolved due to the lack of a theoretically-grounded risk quantification framework. In this work, we address this gap by introducing Invertibility Loss (InvLoss) to quantify the maximum achievable effectiveness of DRAs for a given data instance and FL model. We derive a tight and computable upper bound for InvLoss and explore its implications from three perspectives. First, we show that DRA risk is governed by the spectral properties of the Jacobian matrix of exchanged model updates or feature embeddings, providing a unified explanation for the effectiveness of defense methods. Second, we develop InvRE, an InvLoss-based DRA risk estimator that offers attack method-agnostic, comprehensive risk evaluation across data instances and model architectures. Third, we propose two adaptive noise perturbation defenses that enhance FL privacy without harming classification accuracy. Extensive experiments on real-world datasets validate our framework, demonstrating its potential for systematic DRA risk evaluation and mitigation in FL systems.

</details>


### [79] [Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance](https://arxiv.org/abs/2512.15469)
*Ioannis Kalogeropoulos,Giorgos Bouritsas,Yannis Panagakis*

Main category: cs.LG

TL;DR: 提出了一种通过图元网络编辑神经网络的方法，可以在不牺牲模型性能的前提下，高效满足各种合规性、公平性等要求。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在关键领域部署时需要满足多种要求（法规合规、公平性、计算约束等），传统后处理方法会损害性能，而重新训练又耗时耗力，需要一种高效编辑模型的方法。

Method: 采用数据驱动的统一框架，使用图元网络作为编辑器，元网络本身也是神经网络，通过单次推理步骤编辑目标网络，训练时最小化两个目标：满足特定要求和保持模型效用。

Result: 在数据最小化原则、偏见缓解和权重剪枝等任务上进行了实验，相比传统后处理或重新训练方法，在性能、要求满足度和时间效率之间取得了更好的平衡。

Conclusion: 提出的图元网络编辑框架能够高效地修改神经网络以满足各种要求，同时保持模型性能，为解决机器学习模型部署中的合规性和公平性等问题提供了有效方案。

Abstract: As machine learning models are increasingly deployed in high-stakes settings, e.g. as decision support systems in various societal sectors or in critical infrastructure, designers and auditors are facing the need to ensure that models satisfy a wider variety of requirements (e.g. compliance with regulations, fairness, computational constraints) beyond performance. Although most of them are the subject of ongoing studies, typical approaches face critical challenges: post-processing methods tend to compromise performance, which is often counteracted by fine-tuning or, worse, training from scratch, an often time-consuming or even unavailable strategy. This raises the following question: "Can we efficiently edit models to satisfy requirements, without sacrificing their utility?" In this work, we approach this with a unifying framework, in a data-driven manner, i.e. we learn to edit neural networks (NNs), where the editor is an NN itself - a graph metanetwork - and editing amounts to a single inference step. In particular, the metanetwork is trained on NN populations to minimise an objective consisting of two terms: the requirement to be enforced and the preservation of the NN's utility. We experiment with diverse tasks (the data minimisation principle, bias mitigation and weight pruning) improving the trade-offs between performance, requirement satisfaction and time efficiency compared to popular post-processing or re-training alternatives.

</details>


### [80] [Multi-stage Bayesian optimisation for dynamic decision-making in self-driving labs](https://arxiv.org/abs/2512.15483)
*Luca Torresi,Pascal Friederich*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯优化的扩展方法，能够处理多阶段实验流程并利用中间观测值（代理测量）进行决策，相比传统贝叶斯优化在寻找解决方案的时间和最终优化结果上都有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前自驱动实验室中广泛使用的贝叶斯优化算法依赖于固定的实验流程，无法在实验过程中根据中间测量结果动态调整实验计划。这使得许多真实世界的实验需要被简化和适应才能应用于自驱动实验室，限制了实验的复杂性和现实性。

Method: 作者提出了贝叶斯优化的扩展方法，能够灵活采样多阶段工作流程，并基于中间观测值（代理测量）做出最优决策。该方法允许在实验过程中根据中间结果动态调整后续实验步骤。

Result: 在广泛的应用场景中，使用代理测量的方法相比传统贝叶斯优化（仅观察最终测量结果）带来了显著改进：不仅缩短了找到良好解决方案的时间，还提高了所找到解决方案的整体最优性。

Conclusion: 这种方法不仅为在自主实验室中使用更复杂、更现实的实验工作流程铺平了道路，还能在下一代自驱动实验室中平滑地结合模拟和实验，提高实验效率和效果。

Abstract: Self-driving laboratories (SDLs) are combining recent technological advances in robotics, automation, and machine learning based data analysis and decision-making to perform autonomous experimentation toward human-directed goals without requiring any direct human intervention. SDLs are successfully used in materials science, chemistry, and beyond, to optimise processes, materials, and devices in a systematic and data-efficient way. At present, the most widely used algorithm to identify the most informative next experiment is Bayesian optimisation. While relatively simple to apply to a wide range of optimisation problems, standard Bayesian optimisation relies on a fixed experimental workflow with a clear set of optimisation parameters and one or more measurable objective functions. This excludes the possibility of making on-the-fly decisions about changes in the planned sequence of operations and including intermediate measurements in the decision-making process. Therefore, many real-world experiments need to be adapted and simplified to be converted to the common setting in self-driving labs. In this paper, we introduce an extension to Bayesian optimisation that allows flexible sampling of multi-stage workflows and makes optimal decisions based on intermediate observables, which we call proxy measurements. We systematically compare the advantage of taking into account proxy measurements over conventional Bayesian optimisation, in which only the final measurement is observed. We find that over a wide range of scenarios, proxy measurements yield a substantial improvement, both in the time to find good solutions and in the overall optimality of found solutions. This not only paves the way to use more complex and thus more realistic experimental workflows in autonomous labs but also to smoothly combine simulations and experiments in the next generation of SDLs.

</details>


### [81] [Robustness and uncertainty: two complementary aspects of the reliability of the predictions of a classifier](https://arxiv.org/abs/2512.15492)
*Adrián Detavernier,Jasper De Bock*

Main category: cs.LG

TL;DR: 该研究比较了评估分类器个体预测可靠性的两种方法：鲁棒性量化(RQ)和不确定性量化(UQ)，发现两者互补，结合后的混合方法优于单独使用任一种方法。


<details>
  <summary>Details</summary>
Motivation: 评估分类器个体预测的可靠性对于实际应用至关重要，目前存在两种概念上不同的方法：鲁棒性量化(RQ)和不确定性量化(UQ)，但缺乏对这两种方法的系统比较和整合研究。

Method: 在多个基准数据集上系统比较RQ和UQ方法，分析它们的性能差异，并提出一种结合两者优势的混合方法，同时评估每个数据集中不确定性和鲁棒性作为不可靠性来源的相对重要性。

Result: 研究发现RQ和UQ之间没有明显的优劣之分，但两者具有互补性。结合两者的混合方法在性能上优于单独使用RQ或UQ。研究还能为每个数据集提供不确定性和鲁棒性作为不可靠性来源的相对重要性评估。

Conclusion: 鲁棒性量化和不确定性量化是评估分类器预测可靠性的两种互补方法，结合两者的混合方法能够提供更全面的可靠性评估，并为理解不同数据集中不可靠性的主要来源提供洞见。

Abstract: We consider two conceptually different approaches for assessing the reliability of the individual predictions of a classifier: Robustness Quantification (RQ) and Uncertainty Quantification (UQ). We compare both approaches on a number of benchmark datasets and show that there is no clear winner between the two, but that they are complementary and can be combined to obtain a hybrid approach that outperforms both RQ and UQ. As a byproduct of our approach, for each dataset, we also obtain an assessment of the relative importance of uncertainty and robustness as sources of unreliability.

</details>


### [82] [Tracking Temporal Dynamics of Vector Sets with Gaussian Process](https://arxiv.org/abs/2512.15538)
*Taichi Aida,Mamoru Komachi,Toshinobu Ogiso,Hiroya Takamura,Daichi Mochihashi*

Main category: cs.LG

TL;DR: 提出一种基于无限维高斯过程的新方法，用于建模随时间变化的向量集合分布，通过随机傅里叶特征近似获得紧凑可比的时序向量表示，可追踪和可视化向量集合的时空演变。


<details>
  <summary>Details</summary>
Motivation: 理解向量集合的时序演化是生态学、犯罪分析和语言学等多个领域的基础挑战。这些领域中的向量集合（如生态系统结构、犯罪空间分布、词嵌入向量）具有复杂结构且随时间演变，难以分析其动态变化。

Method: 使用无限维高斯过程建模每个向量集合的底层分布，通过随机傅里叶特征近似高斯过程中的潜在函数，获得紧凑且可比较的时序向量表示，从而在低维空间中追踪和可视化向量集合的时空过渡。

Result: 方法应用于社会学数据（犯罪分布）和语言学数据（词嵌入），有效捕捉了时序动态。结果显示该方法提供了可解释且稳健的表示，为分析跨领域时序索引向量集合的结构变化提供了强大框架。

Conclusion: 提出的基于无限维高斯过程和随机傅里叶特征的方法能够有效建模和分析随时间变化的向量集合分布，为跨多个领域的时序向量集合分析提供了强大且可解释的框架。

Abstract: Understanding the temporal evolution of sets of vectors is a fundamental challenge across various domains, including ecology, crime analysis, and linguistics. For instance, ecosystem structures evolve due to interactions among plants, herbivores, and carnivores; the spatial distribution of crimes shifts in response to societal changes; and word embedding vectors reflect cultural and semantic trends over time. However, analyzing such time-varying sets of vectors is challenging due to their complicated structures, which also evolve over time. In this work, we propose a novel method for modeling the distribution underlying each set of vectors using infinite-dimensional Gaussian processes. By approximating the latent function in the Gaussian process with Random Fourier Features, we obtain compact and comparable vector representations over time. This enables us to track and visualize temporal transitions of vector sets in a low-dimensional space. We apply our method to both sociological data (crime distributions) and linguistic data (word embeddings), demonstrating its effectiveness in capturing temporal dynamics. Our results show that the proposed approach provides interpretable and robust representations, offering a powerful framework for analyzing structural changes in temporally indexed vector sets across diverse domains.

</details>


### [83] [Joint Learning of Unsupervised Multi-view Feature and Instance Co-selection with Cross-view Imputation](https://arxiv.org/abs/2512.15574)
*Yuxin Cai,Yanyong Huang,Jinyuan Chang,Dongjie Wang,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: JUICE提出了一种联合学习框架，将多视图数据的不完整数据恢复与特征和实例协同选择统一起来，通过跨视图邻域信息改进缺失值填补，从而选择更具代表性的特征和实例。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理未标记的不完整多视图数据时，通常先填补缺失数据，然后将所有视图拼接成单一数据集进行协同选择。这种策略将协同选择和缺失数据填补视为两个独立过程，忽略了它们之间的潜在相互作用。同时，简单合并多视图数据无法捕捉视图间的互补信息，限制了协同选择的效果。

Method: JUICE首先利用可用观测重建不完整多视图数据，将缺失数据恢复与特征和实例协同选择统一在一个框架中。然后，利用跨视图邻域信息学习样本间关系，在重建过程中进一步细化缺失值的填补，从而选择更具代表性的特征和实例。

Result: 大量实验表明，JUICE在性能上优于现有的最先进方法。

Conclusion: JUICE通过将缺失数据填补与特征和实例协同选择联合学习，并利用跨视图邻域信息改进填补过程，有效提升了多视图数据协同选择的性能。

Abstract: Feature and instance co-selection, which aims to reduce both feature dimensionality and sample size by identifying the most informative features and instances, has attracted considerable attention in recent years. However, when dealing with unlabeled incomplete multi-view data, where some samples are missing in certain views, existing methods typically first impute the missing data and then concatenate all views into a single dataset for subsequent co-selection. Such a strategy treats co-selection and missing data imputation as two independent processes, overlooking potential interactions between them. The inter-sample relationships gleaned from co-selection can aid imputation, which in turn enhances co-selection performance. Additionally, simply merging multi-view data fails to capture the complementary information among views, ultimately limiting co-selection effectiveness. To address these issues, we propose a novel co-selection method, termed Joint learning of Unsupervised multI-view feature and instance Co-selection with cross-viEw imputation (JUICE). JUICE first reconstructs incomplete multi-view data using available observations, bringing missing data recovery and feature and instance co-selection together in a unified framework. Then, JUICE leverages cross-view neighborhood information to learn inter-sample relationships and further refine the imputation of missing values during reconstruction. This enables the selection of more representative features and instances. Extensive experiments demonstrate that JUICE outperforms state-of-the-art methods.

</details>


### [84] [Corrective Diffusion Language Models](https://arxiv.org/abs/2512.15596)
*Shuibai Zhang,Fred Zhangzhi Peng,Yiheng Zhang,Jin Pan,Grigorios G. Chrysos*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散语言模型的校正导向后训练方法，解决了传统掩码扩散训练无法可靠识别和修正错误的问题，显著提升了模型在代码修订任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在结构上适合迭代错误修正，但传统的掩码扩散语言模型训练无法可靠地诱导这种校正行为，模型往往无法识别完整输入中的不可靠标记，导致基于置信度的细化方法失效。

Method: 提出校正导向的后训练原则，明确监督可见的错误标记，使模型具备错误感知的置信度评估和针对性修正能力。同时引入了代码修订基准（CRB），这是一个可控且可执行的基准，用于评估错误定位和原地修正能力。

Result: 实验表明，采用本文方法训练的模型在代码修订任务和受控设置中显著优于标准掩码扩散语言模型，同时还能提升纯补全性能。

Conclusion: 通过校正导向的后训练方法，可以有效地增强扩散语言模型的校正行为，使其能够可靠地识别错误标记并进行迭代修正，同时保持正确内容不变。

Abstract: Diffusion language models are structurally well-suited for iterative error correction, as their non-causal denoising dynamics allow arbitrary positions in a sequence to be revised. However, standard masked diffusion language model (MDLM) training fails to reliably induce this behavior, as models often cannot identify unreliable tokens in a complete input, rendering confidence-guided refinement ineffective. We study corrective behavior in diffusion language models, defined as the ability to assign lower confidence to incorrect tokens and iteratively refine them while preserving correct content. We show that this capability is not induced by conventional masked diffusion objectives and propose a correction-oriented post-training principle that explicitly supervises visible incorrect tokens, enabling error-aware confidence and targeted refinement. To evaluate corrective behavior, we introduce the Code Revision Benchmark (CRB), a controllable and executable benchmark for assessing error localization and in-place correction. Experiments on code revision tasks and controlled settings demonstrate that models trained with our approach substantially outperform standard MDLMs in correction scenarios, while also improving pure completion performance. Our code is publicly available at https://github.com/zhangshuibai/CDLM.

</details>


### [85] [Autoregressive Language Models are Secretly Energy-Based Models: Insights into the Lookahead Capabilities of Next-Token Prediction](https://arxiv.org/abs/2512.15605)
*Mathieu Blondel,Michael E. Sander,Germain Vivier-Ardisson,Tianlin Liu,Vincent Roulet*

Main category: cs.LG

TL;DR: 该论文建立了自回归模型（ARMs）和能量模型（EBMs）之间的统一理论框架，证明了它们在函数空间中的双射关系，并分析了从EBMs到ARMs蒸馏的理论误差界。


<details>
  <summary>Details</summary>
Motivation: 目前自回归模型主导了大语言模型领域，而能量模型虽然在LLM发展中较少使用，但自然地表征了后训练对齐中的最优策略。研究者希望建立这两种模型类别的统一理论框架，理解它们之间的关系。

Method: 以概率的链式法则为起点，在函数空间中建立ARMs和EBMs之间的显式双射关系，证明这对应于最大熵强化学习中软贝尔曼方程的特殊情况。基于此双射，推导ARMs和EBMs监督学习的等价性，并分析从EBMs到ARMs蒸馏的理论误差界。

Result: 建立了ARMs和EBMs之间的理论等价关系，证明了它们可以通过双射相互转换。提供了从EBMs蒸馏到ARMs的理论误差界，为理解ARMs基于下一个token预测范式却能够进行前瞻规划提供了理论见解。

Conclusion: 该研究为自回归模型和能量模型提供了统一的数学框架，揭示了它们之间的深刻联系，有助于理解为什么基于下一个token预测的自回归模型能够表现出前瞻规划能力，为LLM的理论基础提供了新的视角。

Abstract: Autoregressive models (ARMs) currently constitute the dominant paradigm for large language models (LLMs). Energy-based models (EBMs) represent another class of models, which have historically been less prevalent in LLM development, yet naturally characterize the optimal policy in post-training alignment. In this paper, we provide a unified view of these two model classes. Taking the chain rule of probability as a starting point, we establish an explicit bijection between ARMs and EBMs in function space, which we show to correspond to a special case of the soft Bellman equation in maximum entropy reinforcement learning. Building upon this bijection, we derive the equivalence between supervised learning of ARMs and EBMs. Furthermore, we analyze the distillation of EBMs into ARMs by providing theoretical error bounds. Our results provide insights into the ability of ARMs to plan ahead, despite being based on the next-token prediction paradigm.

</details>


### [86] [Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary](https://arxiv.org/abs/2512.15614)
*Xinshun Feng,Mingzhe Liu,Yi Qiao,Tongyu Zhu,Leilei Sun,Shuai Wang*

Main category: cs.LG

TL;DR: BEAT框架将用户和物品行为转化为离散可解释的序列，通过向量量化自编码构建行为词汇表，解耦宏观兴趣和微观意图，实现与语言模型的语义对齐，提升零样本推荐性能并生成连贯解释。


<details>
  <summary>Details</summary>
Motivation: 现有可解释推荐方法依赖ID表示，语义模糊且对语言模型施加结构限制，难以适应开放场景。真实交互中用户意图复杂多样，协作信号与语言语义难以对齐。

Method: 提出BEAT框架：1) 将用户物品行为转化为离散序列；2) 通过向量量化自编码构建行为词汇表，从图表示中解耦宏观兴趣和微观意图；3) 引入多级语义监督桥接行为信号与语言空间；4) 设计语义对齐正则化机制，将行为token嵌入冻结语言模型的输入空间。

Result: 在三个公开数据集上的实验表明，BEAT提升了零样本推荐性能，同时生成连贯且信息丰富的解释。进一步分析显示行为token捕获了细粒度语义，为将复杂行为模式集成到大语言模型中提供了即插即用接口。

Conclusion: BEAT提供了一个统一可迁移的框架，通过将行为转化为离散可解释序列，有效桥接了行为信号与语言语义，解决了现有可解释推荐方法的局限性，为开放场景下的推荐系统提供了新思路。

Abstract: Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.

</details>


### [87] [SoFlow: Solution Flow Models for One-Step Generative Modeling](https://arxiv.org/abs/2512.15657)
*Tianze Luo,Haotian Yuan,Zhuang Liu*

Main category: cs.LG

TL;DR: SoFlow是一种用于一步生成的框架，通过分析速度函数与速度ODE解函数的关系，提出Flow Matching损失和解一致性损失来训练模型，无需计算Jacobian-vector product，在ImageNet 256x256上优于MeanFlow模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和Flow Matching模型中的多步去噪过程导致效率问题，这促使研究者探索少步生成方法。

Method: 提出Solution Flow Models (SoFlow)框架，通过分析速度函数与速度ODE解函数的关系，设计Flow Matching损失和解一致性损失来训练模型。Flow Matching损失允许模型在训练期间为Classifier-Free Guidance提供估计的速度场，而一致性损失无需计算Jacobian-vector product。

Result: 使用相同的Diffusion Transformer (DiT)架构和相同训练轮数从头训练时，SoFlow模型在ImageNet 256x256数据集上获得了比MeanFlow模型更好的FID-50K分数。

Conclusion: SoFlow提供了一种高效的一步生成框架，通过创新的损失函数设计避免了计算复杂的Jacobian-vector product，在图像生成质量上优于现有方法。

Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.

</details>


### [88] [A Multivariate Statistical Framework for Detection, Classification and Pre-localization of Anomalies in Water Distribution Networks](https://arxiv.org/abs/2512.15685)
*Oleg Melnikov,Yurii Dorofieiev,Yurii Shakhnovskiy,Huy Truong,Victoria Degeler*

Main category: cs.LG

TL;DR: SICAMS框架使用多元统计分析检测、分类和初步定位供水管网异常，无需校准水力模型即可实现高灵敏度泄漏检测


<details>
  <summary>Details</summary>
Motivation: 供水管网异常检测需要处理异质传感器数据，传统方法依赖校准水力模型，开发无需模型校准的统计检测框架

Method: 通过白化变换消除压力流量数据的空间相关性，构建Hotelling's T²统计量进行异常检测，开发启发式算法分类异常类型，基于统计贡献度进行粗定位

Result: 在BattLeDIM L-Town基准数据集上表现出高灵敏度和可靠性，即使在多重泄漏下仍保持鲁棒性能，T²统计量与泄漏量相关可用于估算水损失

Conclusion: SICAMS框架为供水管网异常管理提供了有效的统计工具，无需校准水力模型即可应用于实际运营环境

Abstract: This paper presents a unified framework, for the detection, classification, and preliminary localization of anomalies in water distribution networks using multivariate statistical analysis. The approach, termed SICAMS (Statistical Identification and Classification of Anomalies in Mahalanobis Space), processes heterogeneous pressure and flow sensor data through a whitening transformation to eliminate spatial correlations among measurements. Based on the transformed data, the Hotelling's $T^2$ statistic is constructed, enabling the formulation of anomaly detection as a statistical hypothesis test of network conformity to normal operating conditions. It is shown that Hotelling's $T^2$ statistic can serve as an integral indicator of the overall "health" of the system, exhibiting correlation with total leakage volume, and thereby enabling approximate estimation of water losses via a regression model. A heuristic algorithm is developed to analyze the $T^2$ time series and classify detected anomalies into abrupt leaks, incipient leaks, and sensor malfunctions. Furthermore, a coarse leak localization method is proposed, which ranks sensors according to their statistical contribution and employs Laplacian interpolation to approximate the affected region within the network. Application of the proposed framework to the BattLeDIM L-Town benchmark dataset demonstrates high sensitivity and reliability in leak detection, maintaining robust performance even under multiple leaks. These capabilities make the method applicable to real-world operational environments without the need for a calibrated hydraulic model.

</details>


### [89] [Multi-Modal Semantic Communication](https://arxiv.org/abs/2512.15691)
*Matin Mortaheb,Erciyes Karakaya,Sennur Ulukus*

Main category: cs.LG

TL;DR: 提出多模态语义通信框架，通过文本查询引导视觉信息提取，使用跨模态注意力机制融合视觉和语言特征，根据相关度评分和信道带宽自适应传输图像块，实现复杂场景下的高效语义通信。


<details>
  <summary>Details</summary>
Motivation: 传统基于自注意力的语义通信方法在复杂多物体场景中表现不佳，因为缺乏明确的任务指导。需要一种能够根据用户查询引导信息提取的语义通信框架，以提高通信效率。

Method: 提出多模态语义通信框架：1) 使用跨模态注意力机制融合视觉特征和语言嵌入，生成软相关度评分；2) 根据评分和瞬时信道带宽，采用算法自适应传输图像块；3) 使用独立训练的编码器-解码器对进行传输；4) 在接收端重建和组合图像块以保留任务关键信息。

Result: 该框架能够在复杂和带宽受限的环境中实现高效的语义通信，通过目标驱动的设计，在保持任务关键信息的同时匹配信道容量。

Conclusion: 提出的多模态语义通信框架通过整合文本查询指导信息提取，解决了复杂场景中自注意力缺乏任务指导的问题，实现了灵活、目标驱动的高效语义通信。

Abstract: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.

</details>


### [90] [FrontierCS: Evolving Challenges for Evolving Intelligence](https://arxiv.org/abs/2512.15699)
*Qiuyang Mang,Wenhao Chai,Zhifei Li,Huanzhi Mao,Shang Zhou,Alexander Du,Hanchen Li,Shu Liu,Edwin Chen,Yichuan Wang,Xieting Chu,Zerui Cheng,Yuan Xu,Tian Xia,Zirui Wang,Tianneng Shi,Jianzhu Yao,Yilong Zhao,Qizheng Zhang,Charlie Ruan,Zeyu Shen,Kaiyuan Liu,Runyuan He,Dong Xing,Zerui Li,Zirong Zeng,Yige Jiang,Lufeng Cheng,Ziyi Zhao,Youran Sun,Wesley Zheng,Meiyuwang Zhang,Ruyi Ji,Xuechang Tu,Zihan Zheng,Zexing Chen,Kangyang Zhou,Zhaozi Wang,Jingbang Chen,Aleksandra Korolova,Peter Henderson,Pramod Viswanath,Vijay Ganesh,Saining Xie,Zhuang Liu,Dawn Song,Sewon Min,Ion Stoica,Joseph E. Gonzalez,Jingbo Shang,Alvin Cheung*

Main category: cs.LG

TL;DR: FrontierCS是一个包含156个开放式计算机科学问题的基准测试，涵盖算法和研究问题，特点是问题的最优解未知但解决方案质量可客观评估，模型通过实现可执行程序而非直接输出答案来解决问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注已知最优解的任务，而计算机科学前沿存在大量最优解未知但可客观评估的问题。需要创建一个能评估模型在真正前沿计算机科学问题上表现能力的基准测试。

Method: 创建了包含156个开放式问题的基准测试，涵盖算法问题和研究问题。算法问题通常是NP-hard的竞赛编程变体，具有客观部分评分；研究问题具有相同特性。每个问题都提供专家参考解决方案和自动评估器。模型通过实现可执行程序来解决问题。

Result: 前沿推理模型在算法和研究两个赛道上都远远落后于人类专家；仅增加推理预算无法缩小这一差距；模型经常过度优化生成仅能工作的代码，而不是发现高质量的算法和系统设计。

Conclusion: FrontierCS提供了一个处于计算机科学难度前沿的基准测试，结合了开放式设计、可测量进展和专家策划。当前模型在解决真正前沿计算机科学问题方面仍有很大提升空间，需要超越仅生成可工作代码的能力。

Abstract: We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.

</details>


### [91] [Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data](https://arxiv.org/abs/2512.15706)
*Kayode Olumoyin,Lamees El Naqa,Katarzyna Rejniak*

Main category: cs.LG

TL;DR: 使用物理信息神经网络（PINN）在有限数据场景下学习膀胱癌细胞与免疫细胞之间的时变相互作用及其对联合抗癌治疗的反应。


<details>
  <summary>Details</summary>
Motivation: 传统固定参数模型无法捕捉生物体间随时间变化的动态相互作用，特别是在肿瘤学中，实验数据稀疏且仅有少量时间点观测值，需要开发能够处理时变相互作用和外部干预的模型。

Method: 采用物理信息神经网络（PINN）方法，在数据有限的情况下学习膀胱癌细胞与免疫细胞之间的时变相互作用，预测在无观测数据时间点的亚群轨迹，并分析其对联合抗癌治疗的反应。

Result: 该方法能够预测亚群轨迹，且预测结果与生物学解释一致，为学习生物体在外部干预下的演化相互作用提供了框架。

Conclusion: 提出的PINN方法能够在数据稀疏的情况下有效学习生物体间的时变相互作用，为理解肿瘤-免疫系统动态和评估联合治疗策略提供了新工具。

Abstract: In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics. In oncology, this is further exacerbated by the fact that experimental data are often sparse and sometimes are composed of a few time points of tumor volume. In this paper, we propose to learn time-varying interactions between cells, such as those of bladder cancer tumors and immune cells, and their response to a combination of anticancer treatments in a limited data scenario. We employ the physics-informed neural network (PINN) approach to predict possible subpopulation trajectories at time points where no observed data are available. We demonstrate that our approach is consistent with the biological explanation of subpopulation trajectories. Our method provides a framework for learning evolving interactions among biological organisms when external interventions are applied to their environment.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [Attention as Binding: A Vector-Symbolic Perspective on Transformer Reasoning](https://arxiv.org/abs/2512.14709)
*Sahil Rajesh Dhayalkar*

Main category: cs.AI

TL;DR: 该论文将Transformer的自注意力机制解释为近似的向量符号架构（VSA），认为注意力权重执行软解绑，残差连接实现多个绑定结构的叠加，从而为语言模型的推理行为提供统一的理论框架。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型展现出类似推理的行为，但在需要稳定符号操作的任务上仍然脆弱。论文旨在通过向量符号架构的视角，统一理解这些现象，解释模型的成功与失败模式。

Method: 将自注意力机制和残差流解释为近似的VSA实现：查询和键定义角色空间，值编码填充物，注意力权重执行软解绑，残差连接实现绑定结构的叠加。基于此视角提出VSA启发的架构偏置，包括显式的绑定/解绑头和超维记忆层，以及促进角色-填充物分离的训练目标。

Result: 建立了Transformer内部机制与思维链追踪、基于程序的推理和记忆增强工具使用之间的联系，解释了变量混淆和逻辑相关提示间不一致等典型失败模式。提出了衡量"VSA相似性"和逻辑组合性的度量指标。

Conclusion: 将注意力视为软向量符号计算为构建更可解释和逻辑可靠的推理系统提供了原则性路径。论文提出了理论和架构上的开放性问题，认为VSA视角有助于理解并改进Transformer的推理能力。

Abstract: Transformer-based language models display impressive reasoning-like behavior, yet remain brittle on tasks that require stable symbolic manipulation. This paper develops a unified perspective on these phenomena by interpreting self-attention and residual streams as implementing an approximate Vector Symbolic Architecture (VSA). In this view, queries and keys define role spaces, values encode fillers, attention weights perform soft unbinding, and residual connections realize superposition of many bound structures. We use this algebraic lens to relate transformer internals to chain-of-thought traces, program-based reasoning, and memory-augmented tool use, and to explain characteristic failure modes such as variable confusion and inconsistency across logically related prompts. Building on this perspective, we propose VSA-inspired architectural biases, including explicit binding/unbinding heads and hyperdimensional memory layers, and training objectives that promote role-filler separation and robust superposition. Finally, we outline metrics for measuring "VSA-likeness" and logical compositionality, and pose theoretical and architectural open problems. Overall, the paper argues that viewing attention as soft vector-symbolic computation offers a principled route toward more interpretable and logically reliable reasoning systems.

</details>


### [93] [GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge](https://arxiv.org/abs/2512.14766)
*Dongzhuoran Zhou,Yuqicheng Zhu,Xiaxia Wang,Hongkuan Zhou,Jiaoyan Chen,Steffen Staab,Yuan He,Evgeny Kharlamov*

Main category: cs.AI

TL;DR: 该研究针对知识图谱问答中知识图谱不完整的问题，提出了一种构建不完整知识图谱基准的方法，并开发了自适应图推理智能体（GR-Agent）来提升在不完整知识图谱上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答基准大多假设知识图谱是完整的，包含直接支持答案的三元组，但这不符合现实世界中知识图谱通常不完整的实际情况。当前评估主要关注浅层检索，而忽视了在不完整知识图谱上需要推理的能力。

Method: 1. 提出构建不完整知识图谱基准的方法论：移除直接支持的三元组，同时保留推断答案所需的替代推理路径；2. 开发自适应图推理智能体（GR-Agent）：从知识图谱构建交互环境，将知识图谱问答形式化为智能体与环境交互，使用图推理工具作为动作空间，维护包含相关关系和推理路径的潜在支持证据记忆。

Result: 实验表明：1. 使用该方法构建的基准显示现有方法在不完整知识图谱下性能一致下降，暴露其有限的推理能力；2. GR-Agent在完整和不完整设置下均优于非训练基线方法，与基于训练的方法性能相当。

Conclusion: 该研究填补了知识图谱问答评估中不完整知识图谱的空白，提出的GR-Agent通过形式化智能体与环境交互的方式，有效提升了在不完整知识图谱上的推理能力，为知识图谱问答的实际应用提供了更现实的评估框架和解决方案。

Abstract: Large language models (LLMs) achieve strong results on knowledge graph question answering (KGQA), but most benchmarks assume complete knowledge graphs (KGs) where direct supporting triples exist. This reduces evaluation to shallow retrieval and overlooks the reality of incomplete KGs, where many facts are missing and answers must be inferred from existing facts. We bridge this gap by proposing a methodology for constructing benchmarks under KG incompleteness, which removes direct supporting triples while ensuring that alternative reasoning paths required to infer the answer remain. Experiments on benchmarks constructed using our methodology show that existing methods suffer consistent performance degradation under incompleteness, highlighting their limited reasoning ability. To overcome this limitation, we present the Adaptive Graph Reasoning Agent (GR-Agent). It first constructs an interactive environment from the KG, and then formalizes KGQA as agent environment interaction within this environment. GR-Agent operates over an action space comprising graph reasoning tools and maintains a memory of potential supporting reasoning evidence, including relevant relations and reasoning paths. Extensive experiments demonstrate that GR-Agent outperforms non-training baselines and performs comparably to training-based methods under both complete and incomplete settings.

</details>


### [94] [IaC Generation with LLMs: An Error Taxonomy and A Study on Configuration Knowledge Injection](https://arxiv.org/abs/2512.14792)
*Roman Nekrasov,Stefano Fossati,Indika Kumara,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.AI

TL;DR: 该研究通过注入结构化配置知识，将LLM生成Terraform代码的整体成功率从27.1%提升至62.6%，技术验证成功率提升至75.3%，但发现意图对齐存在瓶颈，揭示了"正确性-一致性差距"。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成正确且意图对齐的基础设施即代码方面成功率较低，特别是在Terraform代码生成上表现不佳，需要系统性的改进方法。

Method: 1. 增强IaC-Eval基准测试，增加云仿真和自动化错误分析；2. 开发LLM辅助IaC代码生成的错误分类法；3. 实施一系列知识注入技术，从朴素检索增强生成到更复杂的图RAG方法，包括图组件的语义丰富化和资源间依赖关系建模。

Result: 基线LLM性能较差（整体成功率27.1%），注入结构化配置知识后，技术验证成功率提升至75.3%，整体成功率提升至62.6%。然而意图对齐出现平台期，LLM能成为熟练的"编码者"但在满足细微用户意图方面仍是有限的"架构师"。

Conclusion: 结构化配置知识注入显著提高了LLM生成IaC的技术正确性，但意图对齐仍然存在挑战，揭示了"正确性-一致性差距"，表明LLM在理解复杂用户意图方面仍有局限。

Abstract: Large Language Models (LLMs) currently exhibit low success rates in generating correct and intent-aligned Infrastructure as Code (IaC). This research investigated methods to improve LLM-based IaC generation, specifically for Terraform, by systematically injecting structured configuration knowledge. To facilitate this, an existing IaC-Eval benchmark was significantly enhanced with cloud emulation and automated error analysis. Additionally, a novel error taxonomy for LLM-assisted IaC code generation was developed. A series of knowledge injection techniques was implemented and evaluated, progressing from Naive Retrieval-Augmented Generation (RAG) to more sophisticated Graph RAG approaches. These included semantic enrichment of graph components and modeling inter-resource dependencies. Experimental results demonstrated that while baseline LLM performance was poor (27.1% overall success), injecting structured configuration knowledge increased technical validation success to 75.3% and overall success to 62.6%. Despite these gains in technical correctness, intent alignment plateaued, revealing a "Correctness-Congruence Gap" where LLMs can become proficient "coders" but remain limited "architects" in fulfilling nuanced user intent.

</details>


### [95] [AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally](https://arxiv.org/abs/2512.14910)
*Nadine Angela Cantonjos,Arpita Biswas*

Main category: cs.AI

TL;DR: AgroAskAI是一个用于农业气候适应决策支持的多智能体推理系统，专为农村脆弱社区设计，通过模块化角色架构和链式责任方法协调自主智能体，整合实时工具和数据集，提供可操作、本地化、多语言的决策支持。


<details>
  <summary>Details</summary>
Motivation: 农村农业地区面临干旱、强降雨和天气模式变化等气候相关风险的损害，需要适应性风险管理解决方案和决策策略。现有系统多为单智能体模型或仅用于静态功能的多智能体框架，缺乏支持动态协作推理和情境感知输出的架构。

Method: 提出AgroAskAI多智能体推理系统，采用模块化、角色专业化的架构，使用链式责任方法协调自主智能体，整合实时工具和数据集。系统内置治理机制减少幻觉，支持内部反馈，并提供多语言交互功能。

Result: 在常见农业气候适应查询实验中，通过额外工具和提示优化，AgroAskAI能够提供更可操作、更接地气、更具包容性的输出。实验结果表明系统在农业气候适应决策支持方面具有潜力。

Conclusion: AgroAskAI展示了智能体AI在农业气候适应中可持续和负责任决策支持的潜力，特别是通过多智能体协作推理为农村脆弱社区提供本地化、可访问的解决方案。

Abstract: Agricultural regions in rural areas face damage from climate-related risks, including droughts, heavy rainfall, and shifting weather patterns. Prior research calls for adaptive risk-management solutions and decision-making strategies. To this end, artificial intelligence (AI), particularly agentic AI, offers a promising path forward. Agentic AI systems consist of autonomous, specialized agents capable of solving complex, dynamic tasks. While past systems have relied on single-agent models or have used multi-agent frameworks only for static functions, there is a growing need for architectures that support dynamic collaborative reasoning and context-aware outputs. To bridge this gap, we present AgroAskAI, a multi-agent reasoning system for climate adaptation decision support in agriculture, with a focus on vulnerable rural communities. AgroAskAI features a modular, role-specialized architecture that uses a chain-of-responsibility approach to coordinate autonomous agents, integrating real-time tools and datasets. The system has built-in governance mechanisms that mitigate hallucination and enable internal feedback for coherent, locally relevant strategies. The system also supports multilingual interactions, making it accessible to non-English-speaking farmers. Experiments on common agricultural queries related to climate adaptation show that, with additional tools and prompt refinement, AgroAskAI delivers more actionable, grounded, and inclusive outputs. Our experimental results highlight the potential of agentic AI for sustainable and accountable decision support in climate adaptation for agriculture.

</details>


### [96] [Beyond Accuracy: A Geometric Stability Analysis of Large Language Models in Chess Evaluation](https://arxiv.org/abs/2512.15033)
*Xidan Song,Weiqi Wang,Ruifeng Cao,Qingya Hu*

Main category: cs.AI

TL;DR: 论文提出几何稳定性框架，用于评估大语言模型在象棋领域的真实推理能力，发现准确率与稳定性之间存在悖论：高准确率模型在几何变换下表现崩溃，揭示模型依赖模式匹配而非抽象空间逻辑。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型在复杂推理领域（如象棋）主要依赖与基准引擎的准确率对齐，但高标量准确率不能保证真正的几何推理能力，可能只是对标准棋盘状态的表面记忆。需要区分真实推理能力与数据污染/过拟合。

Method: 提出几何稳定性框架，通过不变变换（棋盘旋转、镜像对称、颜色反转、格式转换）来严格测试模型一致性。对6个SOTA LLM（包括GPT-5.1、Claude Sonnet 4.5、Kimi K2 Turbo等）使用约3000个棋局位置进行对比分析。

Result: 发现显著的准确率-稳定性悖论：GPT-5.1在标准位置接近最优准确率，但在几何扰动下表现灾难性退化，旋转任务错误率飙升600%以上。相反，Claude Sonnet 4.5和Kimi K2 Turbo在所有变换轴上保持高一致性。Gemini 2.5 Flash在非法状态拒绝方面领先（96.0%）。

Conclusion: 几何稳定性为AI评估提供了正交且必要的度量标准，是区分大规模模型中推理能力与数据污染/过拟合的必要代理指标，揭示了当前准确率基准的局限性。

Abstract: The evaluation of Large Language Models (LLMs) in complex reasoning domains typically relies on performance alignment with ground-truth oracles. In the domain of chess, this standard manifests as accuracy benchmarks against strong engines like Stockfish. However, high scalar accuracy does not necessarily imply robust conceptual understanding. This paper argues that standard accuracy metrics fail to distinguish between genuine geometric reasoning and the superficial memorization of canonical board states. To address this gap, we propose a Geometric Stability Framework, a novel evaluation methodology that rigorously tests model consistency under invariant transformations-including board rotation, mirror symmetry, color inversion, and format conversion. We applied this framework to a comparative analysis of six state-of-the-art LLMs including GPT-5.1, Claude Sonnet 4.5, and Kimi K2 Turbo, utilizing a dataset of approximately 3,000 positions. Our results reveal a significant Accuracy-Stability Paradox. While models such as GPT-5.1 achieve near-optimal accuracy on standard positions, they exhibit catastrophic degradation under geometric perturbation, specifically in rotation tasks where error rates surge by over 600%. This disparity suggests a reliance on pattern matching over abstract spatial logic. Conversely, Claude Sonnet 4.5 and Kimi K2 Turbo demonstrate superior dual robustness, maintaining high consistency across all transformation axes. Furthermore, we analyze the trade-off between helpfulness and safety, identifying Gemini 2.5 Flash as the leader in illegal state rejection (96.0%). We conclude that geometric stability provides an orthogonal and essential metric for AI evaluation, offering a necessary proxy for disentangling reasoning capabilities from data contamination and overfitting in large-scale models.

</details>


### [97] [LADY: Linear Attention for Autonomous Driving Efficiency without Transformers](https://arxiv.org/abs/2512.15038)
*Jihao Huang,Xi Xia,Zhiyuan Li,Tianle Liu,Jingke Wang,Junbo Chen,Tengju Ye*

Main category: cs.AI

TL;DR: LADY是首个完全基于线性注意力的端到端自动驾驶生成模型，通过线性注意力机制实现恒定计算和内存成本的长时序建模，在边缘设备上验证了实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的端到端自动驾驶方法存在二次注意力计算成本问题，限制了长时空序列建模能力，特别是在资源受限的边缘平台上。虽然线性注意力机制具有优越的时空复杂度，但现有方法仅限于自注意力，缺乏对自动驾驶至关重要的跨模态和跨时序交互支持。

Method: 提出LADY模型：1）采用完全线性注意力架构，实现推理时融合长时序上下文且计算和内存成本恒定；2）引入轻量级线性交叉注意力机制，支持有效的跨模态信息交换；3）在NAVSIM和Bench2Drive基准上进行实验验证。

Result: 在NAVSIM和Bench2Drive基准测试中，LADY实现了最先进的性能，具有恒定时间和内存复杂度，提供改进的规划性能和显著降低的计算成本。模型已在边缘设备上部署验证，证明了其在资源受限场景中的实用性。

Conclusion: LADY作为首个完全基于线性注意力的端到端自动驾驶生成模型，通过创新的线性注意力机制解决了传统Transformer的计算效率问题，实现了长时序建模的恒定复杂度，为资源受限环境下的自动驾驶部署提供了实用解决方案。

Abstract: End-to-end paradigms have demonstrated great potential for autonomous driving. Additionally, most existing methods are built upon Transformer architectures. However, transformers incur a quadratic attention cost, limiting their ability to model long spatial and temporal sequences-particularly on resource-constrained edge platforms. As autonomous driving inherently demands efficient temporal modeling, this challenge severely limits their deployment and real-time performance. Recently, linear attention mechanisms have gained increasing attention due to their superior spatiotemporal complexity. However, existing linear attention architectures are limited to self-attention, lacking support for cross-modal and cross-temporal interactions-both crucial for autonomous driving. In this work, we propose LADY, the first fully linear attention-based generative model for end-to-end autonomous driving. LADY enables fusion of long-range temporal context at inference with constant computational and memory costs, regardless of the history length of camera and LiDAR features. Additionally, we introduce a lightweight linear cross-attention mechanism that enables effective cross-modal information exchange. Experiments on the NAVSIM and Bench2Drive benchmarks demonstrate that LADY achieves state-of-the-art performance with constant-time and memory complexity, offering improved planning performance and significantly reduced computational cost. Additionally, the model has been deployed and validated on edge devices, demonstrating its practicality in resource-limited scenarios.

</details>


### [98] [Agentic AI for Integrated Sensing and Communication: Analysis, Framework, and Case Study](https://arxiv.org/abs/2512.15044)
*Wenwen Xie,Geng Sun,Ruichen Zhang,Xuejie Liu,Yinqiu Liu,Jiacheng Wang,Dusit Niyato,Ping Zhang*

Main category: cs.AI

TL;DR: 本文探讨了智能体人工智能在集成感知与通信系统中的应用价值与前景，提出了基于智能体AI的ISAC框架，并通过案例研究验证了其在优化ISAC性能方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 随着无线环境日益动态复杂，ISAC系统需要更智能的处理和更自主的操作来保持效率和适应性。智能体AI通过实现动态环境中的持续感知-推理-行动循环，为ISAC系统提供智能、自主、高效运行的可行解决方案。

Method: 1. 全面综述智能体AI和ISAC系统的关键特性；2. 展示ISAC系统的常见优化方法，突出基于生成式AI的智能体AI的显著优势；3. 提出新颖的智能体ISAC框架，并通过案例研究验证其优越性。

Result: 提出的智能体ISAC框架在优化ISAC性能方面表现出优越性，案例研究验证了该框架的有效性。

Conclusion: 智能体AI为ISAC系统提供了强大的技术支持，能够实现智能、自主、高效的运行。未来需要进一步探索智能体AI在ISAC系统中的研究方向和应用前景。

Abstract: Integrated sensing and communication (ISAC) has emerged as a key development direction in the sixth-generation (6G) era, which provides essential support for the collaborative sensing and communication of future intelligent networks. However, as wireless environments become increasingly dynamic and complex, ISAC systems require more intelligent processing and more autonomous operation to maintain efficiency and adaptability. Meanwhile, agentic artificial intelligence (AI) offers a feasible solution to address these challenges by enabling continuous perception-reasoning-action loops in dynamic environments to support intelligent, autonomous, and efficient operation for ISAC systems. As such, we delve into the application value and prospects of agentic AI in ISAC systems in this work. Firstly, we provide a comprehensive review of agentic AI and ISAC systems to demonstrate their key characteristics. Secondly, we show several common optimization approaches for ISAC systems and highlight the significant advantages of generative artificial intelligence (GenAI)-based agentic AI. Thirdly, we propose a novel agentic ISAC framework and prensent a case study to verify its superiority in optimizing ISAC performance. Finally, we clarify future research directions for agentic AI-based ISAC systems.

</details>


### [99] [Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models](https://arxiv.org/abs/2512.15089)
*Jinwu Hu,Dongjin Yang,Langyu Bian,Zhiquan Wen,Yufeng Wang,Yaofo Chen,Bin Xiao,Yuanqing Li,Mingkui Tan*

Main category: cs.AI

TL;DR: CogER是一个受人类分层推理启发的框架，通过动态选择最适合每个查询的推理策略，平衡LLM推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理策略主要依赖LLM自身的快速或慢速模式（如o1思考），难以在不同难度查询间平衡推理效率和准确性。

Method: CogER首先评估查询复杂度并分配到预定义级别，每个级别对应定制处理策略。通过马尔可夫决策过程建模策略选择，使用强化学习训练CogER-Agent，奖励函数平衡解质量和计算成本。对于需要外部工具的查询，引入认知工具辅助推理，使LLM能在思维链中自主调用外部工具。

Result: CogER在广泛实验中优于最先进的测试时缩放方法，在领域内任务上实现至少13%的相对平均精确匹配提升，在领域外任务上实现8%的相对增益。

Conclusion: CogER通过动态策略选择和工具集成，有效解决了LLM在不同难度查询中的推理效率与准确性平衡问题，展示了认知启发方法的优势。

Abstract: Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.

</details>


### [100] [A Clustering-Based Variable Ordering Framework for Relaxed Decision Diagrams for Maximum Weighted Independent Set Problem](https://arxiv.org/abs/2512.15198)
*Mohsen Nafar,Michael Römer,Lin Xie*

Main category: cs.AI

TL;DR: 本文提出了一种基于聚类的变量排序框架，用于提升决策图松弛的质量，通过将变量分区为簇来减少动态排序启发式的搜索空间，在最大加权独立集问题上显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 决策图松弛的质量（即对偶界的紧密度）严重依赖于变量排序和合并决策。虽然动态变量排序启发式能有效收紧边界，但在整个变量集上全局评估时会产生计算开销。需要一种方法来缓解这种权衡。

Method: 提出了基于聚类的变量排序框架：首先将变量分区为簇，然后利用这种结构分解来指导排序过程。研究了两种策略：1) 簇到簇策略：使用问题特定的聚合标准（如MWISP中的累积顶点权重）顺序处理簇；2) 选取排序策略：从每个簇中迭代选择和排序代表性变量，以平衡局部多样性和启发式指导。此外，基于MWISP中决策图大小的理论分析，提出了两种设置簇数量的策略。

Result: 将提出的策略嵌入到基于决策图的分支定界算法中，并在MWISP基准实例上进行了评估。与标准动态变量排序基线相比，所提出的方法始终降低了计算成本。

Conclusion: 基于聚类的变量排序框架通过减少动态排序启发式的搜索空间，有效地缓解了边界紧密度与计算开销之间的权衡，在MWISP问题上取得了显著的计算效率提升。

Abstract: Efficient exact algorithms for Discrete Optimization (DO) rely heavily on strong primal and dual bounds. Relaxed Decision Diagrams (DDs) provide a versatile mechanism for deriving such dual bounds by compactly over-approximating the solution space through node merging. However, the quality of these relaxed diagrams, i.e. the tightness of the resulting dual bounds, depends critically on the variable ordering and the merging decisions executed during compilation. While dynamic variable ordering heuristics effectively tighten bounds, they often incur computational overhead when evaluated globally across the entire variable set. To mitigate this trade-off, this work introduces a novel clustering-based framework for variable ordering. Instead of applying dynamic ordering heuristics to the full set of unfixed variables, we first partition variables into clusters. We then leverage this structural decomposition to guide the ordering process, significantly reducing the heuristic's search space. Within this framework, we investigate two distinct strategies: Cluster-to-Cluster, which processes clusters sequentially using problem-specific aggregate criteria (such as cumulative vertex weights in the Maximum Weighted Independent Set Problem (MWISP)), and Pick-and-Sort, which iteratively selects and sorts representative variables from each cluster to balance local diversity with heuristic guidance. Later on, developing some theoretical results on the growth of the size of DDs for MWISP we propose two different policies for setting the number of clusters within the proposed framework. We embed these strategies into a DD-based branch-and-bound algorithm and evaluate them on the MWISP. Across benchmark instances, the proposed methodology consistently reduces computational costs compared to standard dynamic variable ordering baseline.

</details>


### [101] [CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications](https://arxiv.org/abs/2512.15231)
*Zhengchao Chen,Haoran Wang,Jing Yao,Pedram Ghamisi,Jun Zhou,Peter M. Atkinson,Bing Zhang*

Main category: cs.AI

TL;DR: 本文提出了CangLing-KnowFlow，一个统一的遥感智能代理框架，通过整合程序知识库、动态工作流调整和进化记忆模块，实现了从数据预处理到高级解释的端到端自动化工作流管理。


<details>
  <summary>Details</summary>
Motivation: 现有遥感自动化系统通常是任务特定的，缺乏统一框架来管理从数据预处理到高级解释的端到端工作流，无法适应多样化的遥感应用需求。

Method: 提出CangLing-KnowFlow框架，包含三个核心组件：1) 程序知识库(PKB)，包含162个实际遥感任务的1008个专家验证工作流案例；2) 动态工作流调整，在运行时故障时自主诊断和重新规划恢复策略；3) 进化记忆模块，从事件中持续学习，迭代增强代理知识和性能。

Result: 在KnowFlow-Bench基准测试中（包含324个受真实应用启发的工作流），CangLing-KnowFlow在13个顶级LLM骨干网络上测试，在所有复杂任务中的任务成功率比Reflexion基线至少高出4%。

Conclusion: CangLing-KnowFlow通过将专家知识转化为自适应且可验证的程序，展示了作为复杂地球观测挑战的稳健、高效和可扩展自动化解决方案的巨大潜力。

Abstract: The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).

</details>


### [102] [Graph Contextual Reinforcement Learning for Efficient Directed Controller Synthesis](https://arxiv.org/abs/2512.15295)
*Toshihide Ubukata,Enhong Mu,Takuto Yamauchi,Mingyue Zhang,Jialong Li,Kenji Tei*

Main category: cs.AI

TL;DR: GCRL方法通过图神经网络增强强化学习，在控制器合成中利用探索历史图结构提升学习效率和泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统控制器合成方法中，探索策略通常基于固定规则或仅考虑有限当前特征的强化学习策略，这限制了合成过程的效率。需要一种能够捕捉更广泛、非当前上下文信息的方法来改进探索策略。

Method: 提出GCRL方法，将图神经网络集成到强化学习框架中。该方法将LTS探索历史编码为图结构，使智能体能够捕获更广泛的非当前上下文信息，从而改进探索策略。

Result: 在对比实验中，GCRL在五个基准域中的四个表现出优于现有方法的学习效率和泛化能力。仅在具有高度对称性和严格局部交互特性的特定领域表现不佳。

Conclusion: GCRL通过整合图神经网络成功增强了基于强化学习的控制器合成方法，能够有效利用探索历史信息，在大多数领域显著提升学习效率和泛化性能。

Abstract: Controller synthesis is a formal method approach for automatically generating Labeled Transition System (LTS) controllers that satisfy specified properties. The efficiency of the synthesis process, however, is critically dependent on exploration policies. These policies often rely on fixed rules or strategies learned through reinforcement learning (RL) that consider only a limited set of current features. To address this limitation, this paper introduces GCRL, an approach that enhances RL-based methods by integrating Graph Neural Networks (GNNs). GCRL encodes the history of LTS exploration into a graph structure, allowing it to capture a broader, non-current-based context. In a comparative experiment against state-of-the-art methods, GCRL exhibited superior learning efficiency and generalization across four out of five benchmark domains, except one particular domain characterized by high symmetry and strictly local interactions.

</details>


### [103] [ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I](https://arxiv.org/abs/2512.15298)
*Seok-Hyun Ga,Chun-Yen Chang*

Main category: cs.AI

TL;DR: 该研究通过分析2025年韩国高考地球科学I部分，评估了GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro等大语言模型的多模态科学推理能力和认知局限性，揭示了AI在感知-认知、计算-概念化等方面的缺陷，为设计"AI抗性试题"提供了依据。


<details>
  <summary>Details</summary>
Motivation: 随着学生使用AI完成作业的普及，学术诚信和评估有效性受到挑战。研究旨在深入分析先进大语言模型的多模态科学推理能力及其认知局限，为解决AI在学术评估中的不当使用问题提供实证基础。

Method: 使用2025年韩国高考地球科学I部分作为测试材料，设计了三种实验条件（整页输入、单项输入和优化多模态输入），定量评估GPT-4o、Gemini 2.5 Flash和Gemini 2.5 Pro在不同数据结构下的表现，并进行定性分析。

Result: 非结构化输入因分割和OCR失败导致性能显著下降。即使在优化条件下，模型仍表现出根本性推理缺陷，主要包括：感知错误（感知-认知鸿沟）、计算-概念化差异（能计算但无法应用科学概念）和过程幻觉（跳过视觉验证依赖背景知识）。

Conclusion: 通过针对AI的认知弱点（如感知-认知鸿沟）设计"AI抗性试题"，教育工作者可以区分真实学生能力与AI生成答案，确保评估公平性，为解决课程作业中未经授权使用AI的挑战提供可行方案。

Abstract: The rapid development of Generative AI is bringing innovative changes to education and assessment. As the prevalence of students utilizing AI for assignments increases, concerns regarding academic integrity and the validity of assessments are growing. This study utilizes the Earth Science I section of the 2025 Korean College Scholastic Ability Test (CSAT) to deeply analyze the multimodal scientific reasoning capabilities and cognitive limitations of state-of-the-art Large Language Models (LLMs), including GPT-4o, Gemini 2.5 Flash, and Gemini 2.5 Pro. Three experimental conditions (full-page input, individual item input, and optimized multimodal input) were designed to evaluate model performance across different data structures. Quantitative results indicated that unstructured inputs led to significant performance degradation due to segmentation and Optical Character Recognition (OCR) failures. Even under optimized conditions, models exhibited fundamental reasoning flaws. Qualitative analysis revealed that "Perception Errors" were dominant, highlighting a "Perception-Cognition Gap" where models failed to interpret symbolic meanings in schematic diagrams despite recognizing visual data. Furthermore, models demonstrated a "Calculation-Conceptualization Discrepancy," successfully performing calculations while failing to apply the underlying scientific concepts, and "Process Hallucination," where models skipped visual verification in favor of plausible but unfounded background knowledge. Addressing the challenge of unauthorized AI use in coursework, this study provides actionable cues for designing "AI-resistant questions" that target these specific cognitive vulnerabilities. By exploiting AI's weaknesses, such as the gap between perception and cognition, educators can distinguish genuine student competency from AI-generated responses, thereby ensuring assessment fairness.

</details>


### [104] [SCOPE: Prompt Evolution for Enhancing Agent Effectiveness](https://arxiv.org/abs/2512.15374)
*Zehua Pei,Hui-Ling Zhen,Shixiong Kai,Sinno Jialin Pan,Yunhe Wang,Mingxuan Yuan,Bei Yu*

Main category: cs.AI

TL;DR: SCOPE框架通过在线优化和提示演化，自动管理LLM代理的上下文，将任务成功率从14.23%提升至38.64%


<details>
  <summary>Details</summary>
Motivation: LLM代理在动态环境中面临上下文管理瓶颈，静态提示无法有效处理大规模动态上下文，导致频繁的纠正和增强失败

Method: 提出SCOPE框架，将上下文管理视为在线优化问题，通过双流机制平衡战术特异性（解决即时错误）和战略通用性（演化长期原则），并引入视角驱动探索最大化策略覆盖

Result: 在HLE基准测试中，任务成功率从14.23%提升至38.64%，无需人工干预

Conclusion: SCOPE通过自动化的提示演化有效解决了LLM代理的上下文管理问题，显著提升了任务执行能力

Abstract: Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\% to 38.64\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.

</details>


### [105] [Bilateral Spatial Reasoning about Street Networks: Graph-based RAG with Qualitative Spatial Representations](https://arxiv.org/abs/2512.15388)
*Reinhard Moratz,Niklas Daute,James Ondieki,Markus Kattenbeck,Mario Krajina,Ioannis Giannopoulos*

Main category: cs.AI

TL;DR: 该论文通过定性空间关系改进大型语言模型为行人提供路线指引的能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在提供行人路线指引方面存在局限性，特别是在使用定性空间关系进行导航时，需要改进其空间推理能力

Method: 利用定性空间关系来增强大型语言模型的路线指引能力，可能包括空间关系表示、推理框架或训练方法

Result: 改进后的模型能够更准确地使用定性空间关系为行人提供路线指引

Conclusion: 通过整合定性空间关系，可以显著提升大型语言模型在行人导航任务中的表现

Abstract: This paper deals with improving the capabilities of Large Language Models (LLM) to provide route instructions for pedestrian wayfinders by means of qualitative spatial relations.

</details>


### [106] [Outer-Learning Framework for Playing Multi-Player Trick-Taking Card Games: A Case Study in Skat](https://arxiv.org/abs/2512.15435)
*Stefan Edelkamp*

Main category: cs.AI

TL;DR: 提出一个通过自对弈AI游戏扩展人类专家游戏数据库的通用外学习框架，提高多玩家纸牌游戏早期决策的预测准确性


<details>
  <summary>Details</summary>
Motivation: 在多玩家纸牌游戏中，如叫牌、游戏选择和初始选牌等早期阶段对游戏成功至关重要，但当前计算限制下这些决策依赖人类专家游戏的统计信息，需要改进

Method: 开发通用自举外学习框架，通过自对弈AI游戏生成数百万额外游戏数据来扩展人类专家游戏数据库，使用完美特征哈希函数处理压缩表，创建持续自我改进的游戏引擎

Result: 在Skat游戏中的案例研究表明，该自动化方法可以支持游戏中的各种决策，通过合并自对弈游戏数据提高了预测准确性

Conclusion: 提出的自举外学习框架能够有效扩展游戏数据库，通过自对弈AI游戏增强统计信息，为多玩家纸牌游戏的早期决策提供更好的支持

Abstract: In multi-player card games such as Skat or Bridge, the early stages of the game, such as bidding, game selection, and initial card selection, are often more critical to the success of the play than refined middle- and end-game play. At the current limits of computation, such early decision-making resorts to using statistical information derived from a large corpus of human expert games. In this paper, we derive and evaluate a general bootstrapping outer-learning framework that improves prediction accuracy by expanding the database of human games with millions of self-playing AI games to generate and merge statistics. We implement perfect feature hash functions to address compacted tables, producing a self-improving card game engine, where newly inferred knowledge is continuously improved during self-learning. The case study in Skat shows that the automated approach can be used to support various decisions in the game.

</details>


### [107] [Intent-Driven UAM Rescheduling](https://arxiv.org/abs/2512.15462)
*Jeongseok Kim,Kangjin Kim*

Main category: cs.AI

TL;DR: 该论文提出了一种结合ASP和MILP的集成框架，用于处理城市空中交通中动态运营需求和模糊重调度请求的调度问题。


<details>
  <summary>Details</summary>
Motivation: 由于资源受限，城市空中交通中的垂直起降机场高效调度受到广泛关注。现有调度方法难以同时处理动态运营需求和人类模糊的重调度请求，需要更灵活透明的解决方案。

Method: 采用混合整数线性规划处理资源受限项目调度问题，结合三值逻辑解释模糊用户意图和决策树，提出集成答案集编程和MILP的新系统框架。

Result: 开发了一个集成框架，能够优化调度并透明支持人类输入，为可解释、自适应的UAM调度提供了鲁棒结构。

Conclusion: 提出的ASP与MILP集成系统为城市空中交通调度提供了处理动态需求和模糊请求的有效解决方案，增强了调度的可解释性和适应性。

Abstract: Due to the restricted resources, efficient scheduling in vertiports has received much more attention in the field of Urban Air Mobility (UAM). For the scheduling problem, we utilize a Mixed Integer Linear Programming (MILP), which is often formulated in a resource-restricted project scheduling problem (RCPSP). In this paper, we show our approach to handle both dynamic operation requirements and vague rescheduling requests from humans. Particularly, we utilize a three-valued logic for interpreting ambiguous user intents and a decision tree, proposing a newly integrated system that combines Answer Set Programming (ASP) and MILP. This integrated framework optimizes schedules and supports human inputs transparently. With this system, we provide a robust structure for explainable, adaptive UAM scheduling.

</details>


### [108] [Nemotron-Math: Efficient Long-Context Distillation of Mathematical Reasoning from Multi-Mode Supervision](https://arxiv.org/abs/2512.15489)
*Wei Du,Shubham Toshniwal,Branislav Kisacanin,Sadegh Mahdavi,Ivan Moshkov,George Armstrong,Stephen Ge,Edgar Minasyan,Feng Chen,Igor Gitman*

Main category: cs.AI

TL;DR: Nemotron-Math是一个包含750万条解题轨迹的大规模数学推理数据集，整合了AoPS竞赛题和StackExchange-Math社区问题，支持高中低三种推理模式，并包含Python工具集成推理版本。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集在推理风格多样性、长形式推理轨迹和工具集成方面存在局限，需要更高质量、更大规模的监督数据来提升数学推理模型的性能。

Method: 利用GPT-OSS-120B的多模式生成能力，整合85K AoPS竞赛题和262K StackExchange-Math社区问题，创建包含高中低三种推理模式的数据集，开发序列分桶策略加速长上下文训练。

Result: Nemotron-Math在匹配的AoPS问题上持续优于原始OpenMathReasoning，整合StackExchange-Math显著提升了鲁棒性和泛化能力，在AIME 2024和2025竞赛中实现了100% maj@16准确率。

Conclusion: Nemotron-Math数据集通过大规模、多样化的数学推理轨迹和有效的工具集成，实现了最先进的数学推理性能，为数学AI模型训练提供了高质量的数据资源。

Abstract: High-quality mathematical reasoning supervision requires diverse reasoning styles, long-form traces, and effective tool integration, capabilities that existing datasets provide only in limited form. Leveraging the multi-mode generation ability of gpt-oss-120b, we introduce Nemotron-Math, a large-scale mathematical reasoning dataset containing 7.5M solution traces across high, medium, and low reasoning modes, each available both with and without Python tool-integrated reasoning (TIR).
  The dataset integrates 85K curated AoPS problems with 262K community-sourced StackExchange-Math problems, combining structured competition tasks with diverse real-world mathematical queries. We conduct controlled evaluations to assess the dataset quality.
  Nemotron-Math consistently outperforms the original OpenMathReasoning on matched AoPS problems. Incorporating StackExchange-Math substantially improves robustness and generalization, especially on HLE-Math, while preserving accuracy on math competition benchmarks.
  To support efficient long-context training, we develop a sequential bucketed strategy that accelerates 128K context-length fine-tuning by 2--3$\times$ without significant accuracy loss. Overall, Nemotron-Math enables state-of-the-art performance, including 100\% maj@16 accuracy on AIME 2024 and 2025 with Python TIR.

</details>


### [109] [Evaluating Large Language Models in Scientific Discovery](https://arxiv.org/abs/2512.15567)
*Zhangde Song,Jieyu Lu,Yuanqi Du,Botao Yu,Thomas M. Pruyn,Yue Huang,Kehan Guo,Xiuzhe Luo,Yuanhao Qu,Yi Qu,Yinkai Wang,Haorui Wang,Jeff Guo,Jingru Gan,Parshin Shojaee,Di Luo,Andres M Bran,Gen Li,Qiyuan Zhao,Shao-Xiong Lennon Luo,Yuxuan Zhang,Xiang Zou,Wanru Zhao,Yifan F. Zhang,Wucheng Zhang,Shunan Zheng,Saiyang Zhang,Sartaaj Takrim Khan,Mahyar Rajabi-Kochi,Samantha Paradi-Maropakis,Tony Baltoiu,Fengyu Xie,Tianyang Chen,Kexin Huang,Weiliang Luo,Meijing Fang,Xin Yang,Lixue Cheng,Jiajun He,Soha Hassoun,Xiangliang Zhang,Wei Wang,Chandan K. Reddy,Chao Zhang,Zhiling Zheng,Mengdi Wang,Le Cong,Carla P. Gomes,Chang-Yu Hsieh,Aditya Nandy,Philippe Schwaller,Heather J. Kulik,Haojun Jia,Huan Sun,Seyed Mohamad Moosavi,Chenru Duan*

Main category: cs.AI

TL;DR: 论文提出了一个基于场景的科学发现评估框架，用于评估大语言模型在真实科学研究项目中的能力，发现现有模型在科学发现方面存在系统性弱点，距离通用科学"超级智能"还很遥远。


<details>
  <summary>Details</summary>
Motivation: 现有科学基准测试主要评估去语境化的知识，忽略了驱动科学发现的迭代推理、假设生成和观察解释等关键过程。需要建立一个更贴近真实科学发现过程的评估框架。

Method: 引入基于场景的基准测试，涵盖生物学、化学、材料和物理领域。由领域专家定义真实研究项目，将其分解为模块化研究场景，从中采样经过验证的问题。采用两阶段评估：问题级准确性（场景相关项目）和项目级性能（提出可测试假设、设计实验、解释结果）。

Result: 应用该框架评估最先进的大语言模型发现：1）相对于通用科学基准存在持续性能差距；2）模型规模和推理能力的扩展收益递减；3）不同提供商顶级模型存在系统性弱点；4）研究场景间性能差异大，导致最佳模型选择不稳定；5）尽管场景得分低，模型在某些科学发现项目中仍显示潜力。

Conclusion: 当前所有大语言模型距离通用科学"超级智能"还很遥远，但已在多种科学发现项目中显示出潜力。该评估框架为发现相关的LLM评估提供了可复现的基准，并为其向科学发现方向发展指明了实用路径。

Abstract: Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific "superintelligence". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.

</details>


### [110] [A Decision-Theoretic Approach for Managing Misalignment](https://arxiv.org/abs/2512.15584)
*Daniel A. Herrmann,Abinav Chari,Isabelle Qian,Sree Sharvesh,B. A. Levinstein*

Main category: cs.AI

TL;DR: 论文提出了一个决策理论框架，用于分析在价值未完全对齐的情况下何时应将决策委托给AI系统，区分了通用委托和情境特定委托的不同要求。


<details>
  <summary>Details</summary>
Motivation: 虽然价值对齐文献已发展出塑造AI价值的技术，但较少关注如何在不确定性下确定何时不完美的对齐足以证明委托决策的合理性。需要一种原则性方法来评估委托的风险与回报。

Method: 引入了一个正式的决策理论框架，精确分析委托决策中的权衡：平衡代理的价值（未）对齐、认知准确性和行动范围。开发了新颖的评分框架来量化事前决策。

Result: 分析揭示了两种委托情景的明显区别：通用委托需要近乎完美的价值对齐和完全的认知信任，而情境特定委托即使存在显著的价值未对齐也可能是最优的。代理的优越准确性或扩展的行动范围可能提供更好的整体决策问题。

Conclusion: 这项工作提供了一种原则性方法来确定AI在特定情境下是否足够对齐，将重点从实现完美对齐转向在不确定性下管理委托的风险与回报。

Abstract: When should we delegate decisions to AI systems? While the value alignment literature has developed techniques for shaping AI values, less attention has been paid to how to determine, under uncertainty, when imperfect alignment is good enough to justify delegation. We argue that rational delegation requires balancing an agent's value (mis)alignment with its epistemic accuracy and its reach (the acts it has available). This paper introduces a formal, decision-theoretic framework to analyze this tradeoff precisely accounting for a principal's uncertainty about these factors. Our analysis reveals a sharp distinction between two delegation scenarios. First, universal delegation (trusting an agent with any problem) demands near-perfect value alignment and total epistemic trust, conditions rarely met in practice. Second, we show that context-specific delegation can be optimal even with significant misalignment. An agent's superior accuracy or expanded reach may grant access to better overall decision problems, making delegation rational in expectation. We develop a novel scoring framework to quantify this ex ante decision. Ultimately, our work provides a principled method for determining when an AI is aligned enough for a given context, shifting the focus from achieving perfect alignment to managing the risks and rewards of delegation under uncertainty.

</details>


### [111] [Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning](https://arxiv.org/abs/2512.15662)
*Jiaqi Xu,Cuiling Lan,Xuejin Chen,Yan LU*

Main category: cs.AI

TL;DR: STC框架在LLM中集成推理与自我批判，通过混合强化学习联合优化推理质量和自我评估，在数学推理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有LLM将推理与验证分离：要么生成推理而不进行自我检查，要么依赖外部验证器事后检测错误。前者缺乏即时反馈，后者增加系统复杂性并阻碍同步学习。受人类批判性思维启发，需要统一框架。

Method: 提出Stepwise Think-Critique (STC)框架，在单个模型内每一步交错进行推理和自我批判。使用混合强化学习目标，结合推理奖励和批判一致性奖励，联合优化推理质量和自我评估。

Result: 在数学推理基准测试中，STC展现出强大的批判性思维能力，产生更可解释的推理轨迹，代表了LLM内置批判性思维的重要进展。

Conclusion: STC框架通过统一推理和自我批判，使LLM具备内置的批判性思维能力，是向具有批判性思维的LLMs迈出的重要一步。

Abstract: Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.

</details>


### [112] [Explaining the Reasoning of Large Language Models Using Attribution Graphs](https://arxiv.org/abs/2512.15663)
*Chase Walker,Rickard Ewetz*

Main category: cs.AI

TL;DR: CAGE框架通过引入属性图来改进大型语言模型的上下文归因方法，量化每个生成内容如何受提示和先前生成内容的影响，提升归因忠实度达40%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然表现出色，但其推理过程不透明，存在安全和信任问题。现有的上下文归因方法通过直接将生成的token与提示关联来生成解释，但忽略了生成过程中的代际影响，导致解释不完整。

Method: 提出CAGE（Context Attribution via Graph Explanations）框架，引入属性图——一个有向图，量化每个生成内容如何受提示和所有先前生成内容的影响。该图构造时保持因果性和行随机性两个属性，通过沿图中路径边缘化中间贡献来计算上下文归因。

Result: 在多个模型、数据集、指标和方法上，CAGE显著提高了上下文归因的忠实度，平均提升高达40%。

Conclusion: CAGE框架通过考虑生成过程中的代际影响，提供了更完整和忠实的归因解释，有助于提高大型语言模型的可解释性和可信度。

Abstract: Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.

</details>


### [113] [Artism: AI-Driven Dual-Engine System for Art Generation and Critique](https://arxiv.org/abs/2512.15710)
*Shuai Liu,Yiqing Tian,Yang Chen,Mar Canet Sola*

Main category: cs.AI

TL;DR: 本文提出了一种双引擎AI架构方法，用于探索艺术演化的潜在轨迹，包含AIDA（人工艺术家社交网络）和Ismism Machine（批判分析系统）两个组件。


<details>
  <summary>Details</summary>
Motivation: 解决探索艺术演化潜在轨迹这一复杂问题，从传统的单向批判转向智能、交互式的反思实践模式，为艺术的计算分析提供新可能性。

Method: 采用双引擎AI架构，结合深度学习与多智能体协作，通过AIDA（人工艺术家社交网络）和Ismism Machine（批判分析系统）实现艺术历史发展和概念创新模式的多维模拟。

Result: 目前正在当代艺术概念研究中应用该方法进行实验研究，建立了一种基于AI驱动批判循环的通用方法论。

Conclusion: 该方法通过AI驱动的批判循环，为艺术的计算分析开辟了新途径，实现了从传统批判到智能交互式反思实践的转变。

Abstract: This paper proposes a dual-engine AI architectural method designed to address the complex problem of exploring potential trajectories in the evolution of art. We present two interconnected components: AIDA (an artificial artist social network) and the Ismism Machine, a system for critical analysis. The core innovation lies in leveraging deep learning and multi-agent collaboration to enable multidimensional simulations of art historical developments and conceptual innovation patterns. The framework explores a shift from traditional unidirectional critique toward an intelligent, interactive mode of reflexive practice. We are currently applying this method in experimental studies on contemporary art concepts. This study introduces a general methodology based on AI-driven critical loops, offering new possibilities for computational analysis of art.

</details>
