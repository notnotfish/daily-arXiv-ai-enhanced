<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 82]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.IR](#cs.IR) [Total: 10]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Physics-Guided Deep Learning for Heat Pump Stress Detection: A Comprehensive Analysis on When2Heat Dataset](https://arxiv.org/abs/2512.13696)
*Md Shahabub Alam,Md Asifuzzaman Jishan,Ayan Kumar Ghosh*

Main category: cs.LG

TL;DR: 本文提出了一种物理引导的深度神经网络方法，用于热泵系统应力分类，在When2Heat数据集上实现了78.1%的测试准确率，相比基线方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 热泵系统在现代节能建筑中至关重要，但由于复杂的热力学相互作用和有限的真实世界数据，其运行应力检测仍然具有挑战性。需要开发更准确可靠的检测方法。

Method: 提出物理引导的深度神经网络方法，结合物理引导的特征选择和类别定义，采用5层隐藏层的深度神经网络架构，并实施双重正则化策略。

Result: 模型在测试集上达到78.1%准确率，验证集上78.5%准确率。相比浅层网络提升5.0%，相比有限特征集提升4.0%，相比单一正则化策略提升2.0%。

Conclusion: 该方法为热泵应力检测提供了生产就绪的解决方案，通过物理引导的特征选择、可变阈值和跨国能源模式分析，显著提升了分类性能。

Abstract: Heat pump systems are critical components in modern energy-efficient buildings, yet their operational stress detection remains challenging due to complex thermodynamic interactions and limited real-world data. This paper presents a novel Physics-Guided Deep Neural Network (PG-DNN) approach for heat pump stress classification using the When2Heat dataset, containing 131,483 samples with 656 features across 26 European countries. The methodology integrates physics-guided feature selection and class definition with a deep neural network architecture featuring 5 hidden layers and dual regularization strategies. The model achieves 78.1\% test accuracy and 78.5% validation accuracy, demonstrating significant improvements over baseline approaches: +5.0% over shallow networks, +4.0% over limited feature sets, and +2.0% over single regularization strategies. Comprehensive ablation studies validate the effectiveness of physics-guided feature selection, variable thresholding for realistic class distribution, and cross-country energy pattern analysis. The proposed system provides a production-ready solution for heat pump stress detection with 181,348 parameters and 720 seconds training time on AMD Ryzen 9 7950X with RTX 4080 hardware.

</details>


### [2] [Scaling and Transferability of Annealing Strategies in Large Language Model Training](https://arxiv.org/abs/2512.13705)
*Siqi Wang,Zhengyu Chen,Teng Xiao,Zheqi Lv,Jinluan Yang,Xunliang Cai,Jingang Wang,Xiaomeng Li*

Main category: cs.LG

TL;DR: 该研究探索了大语言模型训练中学习率调度策略的可迁移性，提出了改进的Warmup-Steady-Decay调度器预测框架，能够跨不同模型配置优化退火策略，减少超参数搜索成本。


<details>
  <summary>Details</summary>
Motivation: 学习率调度对大语言模型训练至关重要，但不同模型配置下的最优退火策略难以确定，需要大量超参数搜索，成本高昂。研究者希望找到一种可迁移的优化方法，减少这种搜索负担。

Method: 研究者改进了Warmup-Steady-Decay调度器的预测框架，纳入训练步数、最大学习率和退火行为等参数，建立了一个通用的预测模型。通过小模型作为代理来优化大模型的训练动态，并在密集模型和混合专家模型上进行验证。

Result: 研究发现最优退火比率在不同训练配置下遵循一致的模式，可以跨模型迁移。小模型可以作为可靠代理来优化大模型的训练动态，验证了框架在密集模型和混合专家模型上的有效性。

Conclusion: 该研究提供了一个实用的指导框架，可以在不进行详尽超参数搜索的情况下选择最优退火策略，证明了训练动态的可迁移性，为高效优化大语言模型训练提供了新方法。

Abstract: Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.

</details>


### [3] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 论文研究了微调大语言模型进行数学推理时出现的灾难性遗忘问题，并提出混合训练策略来完全消除遗忘，同时保持数学性能。


<details>
  <summary>Details</summary>
Motivation: 当微调大语言模型进行数学推理等专门任务时，模型会出现灾难性遗忘，失去先前学习的能力。研究者希望解决这一问题，使模型在获得专业能力的同时不遗忘通用能力。

Method: 使用Flan-T5-Base模型，在DeepMind Mathematics数据集上进行微调，测量在MultiNLI上的遗忘程度。提出混合训练策略，将数学和NLI示例在训练中交错使用，并系统探索从1:1到15:1的不同混合比例。

Result: 纯数学训练使数学准确率从3.1%提升到12.0%，但导致NLI准确率从81.0%暴跌至16.5%（下降64.5个百分点）。混合训练完全消除了灾难性遗忘：1:1比例达到12.0%数学准确率（与纯数学训练相当），同时保持86.2%的NLI准确率。即使少量NLI暴露（6.2%）也能提供有效正则化。

Conclusion: 专业化不一定需要遗忘通用能力，混合训练策略可以完全消除灾难性遗忘。这一发现对扩展到更大模型具有重要意义，混合训练可能带来超越遗忘预防的额外好处。

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [4] [Variational Physics-Informed Ansatz for Reconstructing Hidden Interaction Networks from Steady States](https://arxiv.org/abs/2512.13708)
*Kaiming Luo*

Main category: cs.LG

TL;DR: VPIA方法从异构稳态数据中推断复杂动力系统的交互结构，无需时间轨迹或导数估计，通过变分物理信息表示和稳态残差最小化恢复有向、加权和多体耦合。


<details>
  <summary>Details</summary>
Motivation: 现有重构方法难以处理非线性、异构和高阶耦合的系统，特别是在只能观测到稳态的情况下。需要一种能够直接从异构稳态数据推断一般交互算子的方法。

Method: 提出变分物理信息表示法（VPIA），将动力学的稳态约束嵌入可微分的变分表示中，通过最小化物理导出的稳态残差来重构底层耦合，无需时间轨迹、导数估计或监督。结合残差采样和自然梯度优化实现大规模高阶网络的可扩展学习。

Result: 在多种非线性系统中，VPIA能够准确恢复有向、加权和多体结构，即使在存在显著噪声的情况下也能稳健工作，为仅能获得快照观测的复杂交互网络推断提供了统一框架。

Conclusion: VPIA为复杂动力系统的交互结构重构提供了强大而统一的框架，特别适用于只能获得稳态观测数据的场景，能够处理非线性、异构和高阶耦合等挑战性问题。

Abstract: The interaction structure of a complex dynamical system governs its collective behavior, yet existing reconstruction methods struggle with nonlinear, heterogeneous, and higher-order couplings, especially when only steady states are observable. We propose a Variational Physics-Informed Ansatz (VPIA) that infers general interaction operators directly from heterogeneous steady-state data. VPIA embeds the steady-state constraints of the dynamics into a differentiable variational representation and reconstructs the underlying couplings by minimizing a physics-derived steady-state residual, without requiring temporal trajectories, derivative estimation, or supervision. Residual sampling combined with natural-gradient optimization enables scalable learning of large and higher-order networks. Across diverse nonlinear systems, VPIA accurately recovers directed, weighted, and multi-body structures under substantial noise, providing a unified and robust framework for physics-constrained inference of complex interaction networks in settings where only snapshot observations are available.

</details>


### [5] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: 该研究结合合成孔径雷达（SAR）影像与环境水文数据，使用机器学习方法对肯尼亚Nyando河流域进行洪水易发性建模，发现随机森林模型表现最佳，为灾害风险管理提供重要依据。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球最具破坏性的自然灾害之一，对生态系统、基础设施和人类生计构成严重威胁。在数据有限的地区，需要开发有效的洪水易发性评估方法，以支持灾害风险管理和土地利用规划。

Method: 研究结合Sentinel-1双极化SAR影像（2024年5月洪水事件）与环境水文数据，生成二元洪水清单作为训练数据。整合六个条件因子（坡度、高程、坡向、土地利用/土地覆盖、土壤类型、距河流距离），使用四种监督分类器（逻辑回归、分类回归树、支持向量机、随机森林）进行建模，并通过准确率、Cohen's Kappa和ROC分析评估模型性能。

Result: 随机森林模型表现最佳（准确率=0.762；Kappa=0.480），优于其他三种模型。基于随机森林的易发性地图显示，维多利亚湖附近的Kano平原低洼地区洪水脆弱性最高，这与历史洪水记录和2024年5月洪水事件的影响一致。

Conclusion: 该研究证明了在数据有限地区结合SAR数据和集成机器学习方法进行洪水易发性制图的价值。生成的易发性地图为灾害风险减少、土地利用规划和早期预警系统开发提供了重要见解。

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [6] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: 论文提出Hessian Reassignment方法，用于文档分类模型的类别级遗忘，通过两步法高效移除目标类别训练数据的影响，同时保持其他类别性能。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在高效移除特定训练数据对模型的影响而无需完全重新训练。虽然LLMs的遗忘研究已有进展，但文档分类模型的遗忘研究相对不足，特别是类别级别的遗忘。

Method: 提出Hessian Reassignment方法：1）单次影响式更新，通过共轭梯度求解Hessian-向量系统，减去目标类别所有训练点的贡献；2）通过Top-1分类强制执行决策空间保证，而非随机重新分类已删除类别样本。

Result: 在标准文本基准测试中，Hessian Reassignment在保持接近完整重新训练（不含目标类别）的保留类别准确率的同时，运行速度快几个数量级。此外，通过池化多影子攻击测量，该方法持续降低移除类别的成员推断优势。

Conclusion: Hessian Reassignment为文档分类中的高效类别遗忘提供了一条实用且原则性的路径，实现了性能与效率的良好平衡。

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [7] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: 开发机器学习框架，结合废水监测、气象和空气质量数据预测美国RSV相关住院率，识别关键预测因子并发现高风险人群和地区。


<details>
  <summary>Details</summary>
Motivation: 呼吸道合胞病毒（RSV）是导致幼儿住院的主要原因，其暴发受环境条件强烈影响。需要整合多源数据来预测RSV相关住院率，以便及时采取公共卫生干预措施。

Method: 整合每周住院率、废水RSV水平、每日气象测量和空气污染物浓度数据，使用CART、随机森林和Boosting等分类模型预测RSV相关住院率风险等级（低风险、警报、流行）。

Result: 废水RSV水平是最强预测因子，其次是温度和臭氧水平等气象和空气质量变量。发现美国原住民和阿拉斯加原住民的RSV相关住院率显著更高，高海拔地区（地表压力较低）的住院率也持续较高。

Conclusion: 结合环境和社区监测数据预测RSV暴发具有重要价值，有助于及时公共卫生干预和资源分配。开发了交互式R Shiny仪表板供实际应用，并强调需要进一步研究RSV健康差异的驱动因素。

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [8] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: 该研究提出了一种两阶段联邦少样本学习框架，用于在数据稀缺、分布不均且受隐私限制的临床环境中实现个性化的EEG癫痫检测。


<details>
  <summary>Details</summary>
Motivation: 临床实践中，EEG数据通常稀缺、分布在各个机构且受严格隐私法规限制，无法集中数据池。现有深度学习癫痫检测方法大多依赖大型集中标注数据集，难以在实际医疗环境中应用。

Method: 提出两阶段联邦少样本学习框架：第一阶段使用联邦学习在模拟的非独立同分布医院站点上微调预训练的BIOT模型，实现共享表征学习；第二阶段使用联邦少样本个性化方法，仅用5个标注EEG片段为每个患者适配分类器。

Result: 联邦微调阶段获得平衡准确率0.43（集中式0.52）、Cohen's kappa 0.42（0.49）、加权F1 0.69（0.74）。FFSL阶段，客户端特定模型在四个具有异质事件分布的站点上平均平衡准确率达0.77、Cohen's kappa 0.62、加权F1 0.73。

Conclusion: FFSL框架能够在现实数据可用性和隐私约束下支持有效的患者自适应癫痫检测，为临床实践中的个性化AI模型开发提供了可行方案。

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [9] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的时间受限推荐系统框架，通过将用户时间预算作为资源约束，在推荐相关性和评估成本之间取得平衡，以提高用户参与度。


<details>
  <summary>Details</summary>
Motivation: 传统推荐任务忽略了用户有限的时间预算这一关键资源约束。在移动购物等场景中，用户需要花费时间评估推荐商品，高相关性但评估成本高的商品可能超出用户时间预算，影响用户参与度。需要同时考虑用户偏好和时间预算的推荐算法。

Method: 1) 将时间受限的slate推荐统一建模为具有预算感知效用的马尔可夫决策过程；2) 开发模拟框架研究在重排序数据上的策略行为；3) 使用强化学习算法（包括on-policy和off-policy控制）学习用户偏好和时间预算模式。

Result: 在阿里巴巴个性化重排序数据集上的实验表明，在严格时间预算下，强化学习方法（特别是on-policy和off-policy控制）相比传统的基于上下文的多臂赌博机方法能够提高性能。

Conclusion: 强化学习能够有效处理时间预算约束下的推荐问题，通过同时学习用户偏好和时间预算模式，可以生成在资源限制下具有更高参与潜力的推荐，为时间受限的推荐系统提供了有前景的解决方案。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [10] [RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing](https://arxiv.org/abs/2512.13727)
*Yuhan Tang,Kangxin Cui,Jung Ho Park,Yibo Zhao,Xuan Jiang,Haoze He,Dingyi Zhuang,Shenhao Wang,Jiangbo Yu,Haris Koutsopoulos,Jinhua Zhao*

Main category: cs.LG

TL;DR: 提出RAST-MoE框架，使用混合专家模型增强强化学习，解决网约车平台在不确定供需条件下的自适应延迟匹配问题，显著降低匹配和接驾延迟。


<details>
  <summary>Details</summary>
Motivation: 网约车平台面临在高度不确定的供需条件下平衡乘客等待时间和系统效率的挑战。自适应延迟匹配需要在立即匹配和批量处理请求之间做出权衡，现有方法往往过度简化交通动态或使用浅层编码器，无法捕捉复杂的时空模式。

Method: 提出Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE)框架，将自适应延迟匹配形式化为具有自注意力混合专家编码器的状态感知MDP。采用物理信息拥堵代理模型保持真实的密度-速度反馈，实现数百万次高效推演，同时使用自适应奖励方案防止病态策略。

Result: 在真实世界Uber轨迹数据（旧金山）上，仅使用1200万参数，总奖励提升超过13%，平均匹配延迟降低10%，接驾延迟降低15%。模型在不同需求状态下表现出鲁棒性和训练稳定性。

Conclusion: 研究结果表明，混合专家增强的强化学习在处理具有复杂时空动态的大规模决策问题方面具有巨大潜力，为网约车平台的延迟匹配优化提供了有效解决方案。

Abstract: Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply-demand conditions. Adaptive delayed matching creates a trade-off between matching and pickup delays by deciding whether to assign drivers immediately or batch requests. Since outcomes accumulate over long horizons with stochastic dynamics, reinforcement learning (RL) is a suitable framework. However, existing approaches often oversimplify traffic dynamics or use shallow encoders that miss complex spatiotemporal patterns.
  We introduce the Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE), which formalizes adaptive delayed matching as a regime-aware MDP equipped with a self-attention MoE encoder. Unlike monolithic networks, our experts specialize automatically, improving representation capacity while maintaining computational efficiency. A physics-informed congestion surrogate preserves realistic density-speed feedback, enabling millions of efficient rollouts, while an adaptive reward scheme guards against pathological strategies.
  With only 12M parameters, our framework outperforms strong baselines. On real-world Uber trajectory data (San Francisco), it improves total reward by over 13%, reducing average matching and pickup delays by 10% and 15% respectively. It demonstrates robustness across unseen demand regimes and stable training. These findings highlight the potential of MoE-enhanced RL for large-scale decision-making with complex spatiotemporal dynamics.

</details>


### [11] [CurvaDion: Curvature-Adaptive Distributed Orthonormalization](https://arxiv.org/abs/2512.13728)
*Bhavesh Kumar,Roger Jin,Jeffrey Quesnelle*

Main category: cs.LG

TL;DR: CurvaDion通过RMMC检测高曲率区域，仅在需要时进行梯度同步，在160M到1.3B参数模型中实现99%通信减少，同时保持收敛性能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型扩展到万亿参数，分布式训练中的梯度同步成为关键瓶颈。现有方法如Dion虽然通过低秩更新减少通信，但无论优化地形如何都每步同步，而实际上平坦区域同步冗余，高曲率区域才需要协调。

Method: 提出CurvaDion方法，使用相对最大动量变化(RMMC)检测需要同步的高曲率区域。RMMC利用优化过程中已计算的动量动态作为方向曲率的计算可行代理，每层仅增加O(d)操作。

Result: 建立了RMMC与损失曲率之间的理论联系，在160M到1.3B参数模型中实现了99%的通信减少，同时匹配基线收敛性能。

Conclusion: CurvaDion通过智能检测高曲率区域，仅在必要时进行梯度同步，显著减少了分布式训练中的通信开销，同时保持了模型收敛性能。

Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.

</details>


### [12] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种用于风数据超分辨率的新型复合分类器自由引导（CCFG）方法，并将其应用于WindDM扩散模型，显著降低了风数据重建成本并提高了重建质量。


<details>
  <summary>Details</summary>
Motivation: 获取高分辨率、高精度的风数据对于天气建模、涡轮机优化等应用至关重要，但传统方法要么成本高，要么精度低。现有的深度学习方法（包括扩散模型）在处理多通道风数据时存在局限性，因为风数据通常需要10个以上的输入通道，远多于自然图像的3个RGB通道。

Method: 提出了复合分类器自由引导（CCFG）方法，这是对标准分类器自由引导（CFG）的泛化，能够更好地处理多个条件输入。该方法可以轻松集成到任何使用标准CFG dropout预训练的扩散模型中。基于CCFG开发了WindDM扩散模型，专门用于工业规模的风动力学重建。

Result: CCFG在风数据超分辨率任务中比标准CFG产生更高保真度的输出。WindDM在深度学习模型中实现了最先进的重建质量，同时成本比传统方法降低了高达1000倍。

Conclusion: 提出的CCFG方法有效解决了风数据超分辨率中多条件输入的挑战，WindDM模型在保持高精度的同时大幅降低了成本，为工业规模的风数据获取提供了实用解决方案。

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [13] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: PIS（物理反演求解器）是一个基于集合条件扩散的框架，能够在任意稀疏、不规则观测条件下稳定准确地求解PDE约束的物理参数反演问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: PDE约束的物理参数反演在稀疏、不规则观测条件下本质上是病态的，现有深度学习和算子学习方法在极端稀疏观测下失效，缺乏鲁棒性和不确定性量化能力。

Method: 提出PIS框架：1）使用基于Set Transformer的编码器处理任意数量和几何分布的观测数据；2）采用余弦退火稀疏课程学习实现卓越鲁棒性；3）提供信息论分析揭示极端稀疏条件下的反演极限。

Result: 在Darcy流、Helmholtz波场反演和结构健康监测三个PDE反问题上，PIS在极端稀疏观测（最低0.29%观测率）下保持稳定准确，反演误差降低12.28%-88.73%，并能生成校准的后验样本。

Conclusion: PIS是一个强大、通用且对稀疏性具有独特鲁棒性的解决方案，能够在任意和严重欠采样观测条件下进行物理反演，为实际应用提供了可靠工具。

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [14] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: LLRC是一种无需微调的梯度学习方法，通过学习掩码权重选择奇异值，优化大语言模型的低秩压缩，在保持性能的同时实现更好的压缩效果。


<details>
  <summary>Details</summary>
Motivation: 现有低秩压缩方法存在两个主要问题：1）基于启发式的方法搜索空间有限，可能导致次优结果；2）基于梯度的方法在无需微调的情况下性能不如启发式方法。需要一种既能直接学习最优秩选择，又无需后压缩微调的方法。

Method: 提出LLRC（Learning to Low-Rank Compress）方法：使用校准数据集，仅训练掩码权重来选择奇异值，同时最小化中间激活与原始模型的差异。这是一种梯度方法，直接学习选择哪些奇异值，在无需微调的情况下优化压缩率和下游任务性能。

Result: 在Llama-2-13B模型上，20%压缩率下，LLRC在MMLU、BoolQ和OpenbookQA任务上分别比STRS方法提升12%、3.5%和4.4%。相比其他无需微调的压缩方法（SVD-LLM和LLM-Pruner），LLRC在各种数据集和压缩率下表现更优。即使与需要微调的LLM-Pruner变体相比，LLRC也表现出竞争力。

Conclusion: LLRC提供了一种有效的梯度方法，能够在无需后压缩微调的情况下，学习选择最优的奇异值进行低秩压缩，在压缩率和下游任务性能之间实现更好的平衡，优于现有的无需微调压缩方法。

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [15] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 提出基于参数高效微调(PEFT)的联邦推荐框架，通过减少嵌入参数传输量来提升通信效率，同时保持推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着云边协同的发展，推荐服务需要在分布式环境中训练。联邦推荐(FR)通过共享模型参数而非原始数据来实现多端协同训练并保护隐私。然而，由于大规模物品嵌入导致的大量参数严重影响了通信效率。现有研究主要关注提升FR模型效率，但忽视了嵌入参数开销问题。

Method: 提出基于参数高效微调(PEFT)的联邦推荐训练框架，旨在减少需要传输的嵌入参数量。该框架提供轻量级、插件式解决方案，可无缝集成到现有FR方法中。除了整合常见的PEFT技术如LoRA和基于哈希的编码外，还探索使用残差量化变分自编码器(RQ-VAE)作为新的PEFT策略。

Result: 在不同FR模型骨干和数据集上的广泛实验表明，该框架显著减少了通信开销，同时提高了准确性。

Conclusion: 提出的基于PEFT的联邦推荐框架有效解决了嵌入参数传输效率问题，在减少通信开销的同时提升了推荐性能，为联邦推荐系统提供了实用的参数高效解决方案。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [16] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: DARTs是一个用于高维多变量时间序列异常检测的双路径框架，通过短时路径和长时路径分别捕获不同时间尺度的时空模式，并使用窗口感知软融合机制集成异常模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低维场景下表现良好，但在处理高维噪声时间序列时难以鲁棒地捕获长程时空依赖关系，需要新的框架来解决这一局限性。

Method: 提出DARTs双路径框架：1)短时路径包含多视图稀疏图学习器和扩散多关系图单元，捕获高噪声下的分层短时时空模式；2)长时路径包含多尺度时空图构造器，建模高维表示空间中的长时动态；3)窗口感知时空软融合机制过滤残差噪声并集成异常模式。

Result: 在主流数据集上的定性和定量实验结果表明，DARTs具有优越性和鲁棒性。消融研究验证了各组件设计的关键因素。

Conclusion: DARTs能够有效解决高维噪声时间序列中长程时空依赖的捕获问题，为工业控制系统中的多变量时间序列异常检测提供了鲁棒的解决方案。

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [17] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 提出TF-MCL模型用于抑郁症检测，通过时间-频率融合和多域交叉损失提升EEG信号表征能力，在两个公开数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症检测方法过度依赖标签，而对比学习方法未能充分考虑EEG信号的时频特性，表征能力不足。

Method: 提出TF-MCL模型，包含融合映射头生成时频混合表征，并通过多域交叉损失函数优化时频域和融合域的表征分布。

Result: 在MODMA和PRED+CT数据集上分别比现有最佳方法提升5.87%和9.96%的准确率。

Conclusion: TF-MCL模型通过时频融合和多域交叉损失有效提升了抑郁症检测性能，解决了现有对比学习方法在EEG信号表征上的不足。

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [18] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: 本文提出层流假说，认为良性输入在LLM潜在空间中产生平滑过渡，而对抗性提示引发语义湍流，可通过层间余弦速度方差检测，实现轻量级越狱检测和安全架构诊断。


<details>
  <summary>Details</summary>
Motivation: 当前LLM防御策略依赖计算昂贵的外部分类器或脆弱的词法过滤器，忽略了模型推理过程的内在动态。需要一种基于模型内部动态的轻量级防御方法。

Method: 提出层流假说，认为良性输入在LLM高维潜在空间中产生平滑渐变过渡，而对抗性提示触发混沌、高方差的轨迹（语义湍流）。通过新颖的零样本度量——层间余弦速度方差来形式化这一现象。

Result: 实验评估显示：RLHF对齐的Qwen2-1.5B在攻击下湍流显著增加75.4%，验证了内部冲突假说；而Gemma-2B显示22.0%的湍流减少，表征了独特的低熵"反射式"拒绝机制。

Conclusion: 语义湍流不仅可作为轻量级实时越狱检测器，还可作为非侵入式诊断工具，用于分类黑盒模型的基础安全架构。

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [19] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 本研究比较了Word2Vec、GloVe和句子Transformer等嵌入方法在资源受限环境下金融新闻情感分类的表现，发现预训练嵌入在小数据集上效果有限，验证集过小会导致过拟合，建议采用少样本学习等替代方法。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析有助于理解市场，但标准NLP方法在小数据集上遇到显著挑战。本研究旨在评估资源受限环境下基于嵌入的金融新闻情感分类方法。

Method: 使用Word2Vec、GloVe和句子Transformer表示方法，结合梯度提升算法，在手动标注的新闻标题数据集上进行比较评估。

Result: 实验结果显示验证集和测试集性能存在显著差距，模型表现甚至不如简单基线；预训练嵌入在数据量低于临界阈值时收益递减；小验证集导致模型选择过程中的过拟合。

Conclusion: 嵌入质量本身无法解决情感分类中的数据稀缺问题。对于资源有限的实践者，建议考虑少样本学习、数据增强或词典增强的混合方法等替代方案。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [20] [MIDUS: Memory-Infused Depth Up-Scaling](https://arxiv.org/abs/2512.13751)
*Taero Kim,Hoyoon Byun,Youngjun Choi,Sungrae Park,Kyungwoo Song*

Main category: cs.LG

TL;DR: MIDUS提出了一种新的深度扩展方法，用头级记忆层替代传统的FFN复制，通过为每个注意力头分配独立记忆库，在保持高效参数的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度扩展方法主要依赖复制FFN层并进行持续预训练，这种方法参数效率低且性能提升有限。研究者观察到注意力头在不同层和层内具有不同的功能角色，这启发了通过头级记忆设计来更高效地扩展模型深度的想法。

Method: MIDUS方法在复制的块中用头级记忆层替代传统的FFN层。为每个注意力头分配独立的记忆库，支持头级检索并将信息注入后续层，同时保持头级功能结构。设计结合了稀疏记忆访问和头级表示，并包含高效的每头值分解模块。

Result: 在持续预训练实验中，MIDUS相比强大的深度扩展基线表现出稳健的性能提升，同时保持了高效的参数占用。该方法在效率和性能之间取得了更好的平衡。

Conclusion: MIDUS通过其头级记忆设计，成为传统FFN复制深度扩展方法的有力且资源高效的替代方案，为大规模语言模型扩展提供了新的有效途径。

Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.

</details>


### [21] [Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention](https://arxiv.org/abs/2512.13758)
*Léo Hein,Giovanni de Nunzio,Giovanni Chierchia,Aurélie Pirayre,Laurent Najman*

Main category: cs.LG

TL;DR: 提出HDA-STGNN模型，利用车速数据和道路属性进行全网交通量估计，无需依赖传感器数据


<details>
  <summary>Details</summary>
Motivation: 现有交通量估计方法要么依赖传感器数据进行预测，要么需要邻近传感器数据进行空间插补，在传感器稀缺的城市中应用受限。而车速数据和静态道路属性更广泛可用，能够覆盖大多数城市道路网络。

Method: 提出混合定向注意力时空图神经网络（HDA-STGNN），这是一个归纳式深度学习框架，利用车速特征、静态道路属性和道路网络拓扑结构来预测网络中所有道路段的日交通量分布。

Result: 通过广泛的消融研究验证了模型的有效性，证明模型能够捕捉复杂的时空依赖关系，并突显了拓扑信息对于在不依赖推理时交通量数据的情况下进行准确全网交通量估计的重要性。

Conclusion: HDA-STGNN框架能够利用广泛可用的车速数据和道路属性，实现全网交通量估计，解决了传感器稀缺城市的交通监测问题，为城市交通管理提供了新的解决方案。

Abstract: Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.

</details>


### [22] [Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training](https://arxiv.org/abs/2512.13770)
*Huaiyuan Xiao,Fadi Dornaika,Jingjun Bi*

Main category: cs.LG

TL;DR: MV-SupGCN是一个半监督图卷积网络模型，通过结合交叉熵损失和监督对比损失、融合KNN和半监督图构建方法、以及集成对比学习和伪标签技术，有效提升了多视图学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCN的多视图学习方法未能充分利用不同视图间的互补信息，导致特征表示不理想和性能受限。需要一种能够更好地整合多视图结构信息、提高模型泛化能力的框架。

Method: 1) 设计联合损失函数，结合交叉熵损失和监督对比损失，减小类内方差并增大类间可分性；2) 融合KNN和半监督图构建方法，增强数据结构表示的鲁棒性；3) 提出统一框架，集成对比学习以增强多视图嵌入一致性，并结合伪标签技术提供额外监督。

Result: 在多个基准测试中，MV-SupGCN持续超越现有最先进方法，验证了所提集成方法的有效性。

Conclusion: MV-SupGCN通过整合互补组件和相互增强的设计，有效提升了多视图学习的性能，为复杂多视图数据的建模提供了强大的半监督GCN框架。

Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN

</details>


### [23] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: SCPO是一种基于采样的权重空间投影方法，用于在未知、基于rollout的安全约束下进行策略学习，通过构建局部安全区域并投影梯度更新来确保训练过程中的安全性。


<details>
  <summary>Details</summary>
Motivation: 安全关键学习需要在不离开安全操作区域的情况下改进性能。现有方法通常需要约束函数的梯度信息，但在实际应用中，安全约束往往是未知的、基于rollout的，且难以获得梯度信息。

Method: 提出SCPO方法：1）通过轨迹rollout和平滑性边界构建局部安全区域；2）使用凸二阶锥规划（SOCP）投影梯度更新；3）结合稳定备份策略确保闭环稳定性；4）提供安全归纳保证，从任何安全初始化开始，所有中间策略都保持安全。

Result: 在有害监督的回归任务和具有恶意专家的约束双积分器任务中，SCPO能够持续拒绝不安全更新，在整个训练过程中保持可行性，并实现有意义的原始目标改进。

Conclusion: SCPO提供了一种无需约束函数梯度访问的安全策略学习方法，通过权重空间投影确保训练过程中的安全性，并在有稳定备份策略的情况下实现安全适应。

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [24] [EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models](https://arxiv.org/abs/2512.13806)
*Siegfried Ludwig,Stylianos Bakas,Konstantinos Barmpas,Georgios Zoumpourlis,Dimitrios A. Adamos,Nikolaos Laskaris,Yannis Panagakis,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 提出D3方法解决EEG深度学习中的隐藏过拟合问题，通过弱监督训练分离脑活动潜在成分，提高模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有EEG深度学习模型在基准测试中表现良好，但在实际应用中泛化能力差，存在隐藏过拟合问题，需要解决任务相关伪影的影响

Method: 提出解耦解码分解（D3）弱监督方法，通过预测输入窗口在试验序列中的位置来分离脑活动潜在成分；采用完全独立子网络架构确保严格可解释性；开发特征解释范式对比不同数据集上的成分激活模式

Result: D3能可靠分离运动想象数据的脑活动潜在成分；基于适当成分子集训练下游分类器可防止隐藏过拟合；在线性可分离潜在空间中实现有效的少样本学习；模型泛化能力强且仅需少量标注数据

Conclusion: D3方法通过区分真实脑活动成分与虚假特征，解决了EEG深度学习中的隐藏过拟合问题，为神经科学研究提供了分离个体脑过程的新工具

Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.

</details>


### [25] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出CTVP框架，通过语义轨道分析验证不可信代码生成模型，检测后门注入，引入ARQ量化验证成本，证明方法具有理论上的不可博弈性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地生成代码而缺乏人工监督，后门注入和恶意行为成为关键问题，需要新的AI控制框架来验证不可信的代码生成模型。

Method: 提出跨迹验证协议（CTVP），通过语义轨道分析验证代码生成模型。不直接执行可能恶意的代码，而是利用模型对语义等价程序变换的执行轨迹预测，分析这些预测轨迹的一致性模式来检测后门行为异常。

Result: 引入对抗鲁棒性商数（ARQ）量化验证成本相对于基线生成的比率，证明随着轨道大小呈指数增长。理论分析建立了信息论边界，显示由于基本空间复杂度约束，攻击者无法通过训练改进（非可博弈性）。

Conclusion: 语义轨道分析为代码生成任务提供了可扩展、理论基础的AI控制方法，能够有效检测后门行为，且具有理论上的安全保证。

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [26] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 该论文提出了一种通过识别并修正导致不满意回答的训练数据来改进RLHF的方法，包括事后解释和反学习两个部分。


<details>
  <summary>Details</summary>
Motivation: 人类在改善不满意结果时通常会寻找并修正其原因，作者希望将这一策略应用于改进语言模型的人类反馈强化学习（RLHF），因为经过RLHF调优的语言模型仍然可能产生不满意的回答。

Method: 方法分为两部分：1）事后解释方法，通过约束组合优化问题识别导致不满意回答的训练数据；2）反学习方法，通过反学习这些训练数据来改善不满意回答，同时不影响其他满意回答。

Result: 实验结果表明，该方法能够有效改进RLHF的性能。

Conclusion: 通过识别并修正导致不满意回答的训练数据，可以有效地改进RLHF对齐语言模型的效果，这为改善语言模型对齐提供了一种新思路。

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [27] [Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains](https://arxiv.org/abs/2512.13852)
*Jelena Losic*

Main category: cs.LG

TL;DR: 提出结合持续同调特征与稳定性正则化的新框架，增强图神经网络对结构扰动的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 图神经网络已成为图表示学习的标准方法，但对结构扰动仍然脆弱，需要提高其鲁棒性

Method: 基于持续同调稳定性定理，将GIN架构与从持续图像提取的多尺度拓扑特征相结合，并通过Hiraoka-Kusano启发的稳定性约束进行正则化

Result: 在六个涵盖生化、社交和协作网络的数据集上，该方法在边扰动下表现出卓越的鲁棒性，同时保持竞争性准确率，性能下降最小（大多数数据集0-4%）

Conclusion: 这项工作提供了一个理论上有基础、经验上已验证的鲁棒图学习方法，与拓扑正则化的最新进展相一致

Abstract: Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology \cite{cohen2007stability}, our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization

</details>


### [28] [Dropout Neural Network Training Viewed from a Percolation Perspective](https://arxiv.org/abs/2512.13853)
*Finley Devlin,Jaron Sanders*

Main category: cs.LG

TL;DR: 研究深度神经网络中dropout训练时的渗流现象及其影响，发现dropout可能导致网络输入输出路径中断，影响训练效果


<details>
  <summary>Details</summary>
Motivation: 研究dropout正则化方法中随机移除连接的过程与统计物理中渗流模型的相似性，探索当dropout移除足够多连接导致输入输出路径中断时对神经网络训练的影响

Method: 提出新的渗流模型来模拟神经网络中的dropout，分析网络拓扑结构与路径问题的关系，理论分析dropout中的渗流效应，并在无偏置神经网络中验证这种效应导致的训练崩溃

Result: 理论证明了dropout中存在渗流效应，这种效应会导致无偏置神经网络训练时崩溃，并启发式地论证了这种崩溃也适用于带偏置的神经网络

Conclusion: dropout训练中存在渗流效应，当随机移除的连接足够多时会导致输入输出路径中断，影响神经网络的训练效果，特别是在无偏置网络中会导致训练崩溃

Abstract: In this work, we investigate the existence and effect of percolation in training deep Neural Networks (NNs) with dropout. Dropout methods are regularisation techniques for training NNs, first introduced by G. Hinton et al. (2012). These methods temporarily remove connections in the NN, randomly at each stage of training, and update the remaining subnetwork with Stochastic Gradient Descent (SGD). The process of removing connections from a network at random is similar to percolation, a paradigm model of statistical physics.
  If dropout were to remove enough connections such that there is no path between the input and output of the NN, then the NN could not make predictions informed by the data. We study new percolation models that mimic dropout in NNs and characterise the relationship between network topology and this path problem. The theory shows the existence of a percolative effect in dropout. We also show that this percolative effect can cause a breakdown when training NNs without biases with dropout; and we argue heuristically that this breakdown extends to NNs with biases.

</details>


### [29] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 该论文提出了两种估计二元分类器L1校准误差的方法：一是为具有有界变差校准函数的分类器提供上界；二是提供一种修改分类器的方法，使其校准误差能被高效上界估计，且不影响分类性能。


<details>
  <summary>Details</summary>
Motivation: 从有限数据集估计二元分类器的L1校准误差是一个重要但具有挑战性的问题，需要非渐近、分布无关的实用方法。

Method: 1. 为具有有界变差校准函数的分类器提供L1校准误差的上界；2. 提出一种修改分类器的方法，使其校准误差能被高效上界估计，同时保持分类性能。

Result: 提出了非渐近、分布无关的实用方法，能够在真实世界数据集上以适度开销运行，为实际测量校准误差提供建议。

Conclusion: 该研究提供了实用的校准误差测量方法，可在实际应用中有效评估分类器的校准性能，而无需严格假设条件。

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [30] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 提出一个端到端的婴儿哭声分析系统，结合去噪自编码器、卷积分词器和Transformer编码器，采用联邦学习进行训练，实现隐私保护、噪声鲁棒和通信高效的婴儿哭声分类。


<details>
  <summary>Details</summary>
Motivation: 婴儿哭声分类有助于早期评估婴儿需求，但现有解决方案部署受限，主要问题包括：音频数据的隐私担忧、对背景噪声的敏感性、以及不同录音环境间的领域偏移。

Method: 1) 端到端婴儿哭声分析流水线，集成去噪自编码器(DAE)、卷积分词器和Transformer编码器；2) 采用通信高效的联邦学习进行训练，使用正则化控制变量更新和8位适配器增量，在安全聚合下进行；3) 系统执行设备端去噪、自适应分割、事后校准和基于能量的分布外拒绝；4) 在Baby Chillanto和Donate-a-Cry数据集上测试，加入ESC-50噪声覆盖。

Result: 1) 宏观F1分数0.938，AUC 0.962，预期校准误差(ECE) 0.032；2) 每轮客户端上传数据从约36-42MB减少到3.3MB；3) 在NVIDIA Jetson Nano(4GB，TensorRT FP16)上实时边缘推理，每1秒频谱图帧处理时间96ms。

Conclusion: 该系统展示了一条实用的路径，实现了适合联邦部署的隐私保护、噪声鲁棒和通信高效的婴儿哭声分类。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [31] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: OPTIMA是一种实用的单次后训练剪枝方法，通过将层间权重重构转化为共享Hessian矩阵的行级二次规划问题，在保持准确性的同时实现大规模剪枝。


<details>
  <summary>Details</summary>
Motivation: 后训练模型剪枝面临权衡：简单的启发式方法速度快但准确率下降，而联合优化方法能恢复准确率但计算成本过高。需要一种既能保持准确性又具有可扩展性的实用剪枝方法。

Method: 将掩码选择后的层间权重重构建模为独立的行级二次规划问题，这些QP共享同一层的Hessian矩阵。开发了加速器友好的QP求解器，每层累积一个Hessian矩阵并并行求解多个小型QP，实现单次后训练剪枝。

Result: OPTIMA在多种LLM家族和稀疏度下持续提升零样本性能，最高带来3.97%的绝对准确率提升。在NVIDIA H100上，40小时内完成80亿参数transformer的端到端剪枝，峰值内存60GB。

Conclusion: OPTIMA为单次后训练剪枝设定了新的准确率-效率权衡标准，实现了实用的大规模模型剪枝，无需微调即可在单个加速器上完成。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [32] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 研究发现当前长上下文LLM的推理时计算策略（如生成更多思考标记）在长上下文任务中效果有限，提出了通过针对性梯度更新来改进长上下文性能的方法


<details>
  <summary>Details</summary>
Motivation: 尽管LLM已经能够处理数百万标记的长上下文，但实证表明它们无法可靠利用这些长文本。同时，推理时计算虽然能提升复杂推理任务性能，但在长上下文任务中效果有限

Method: 提出了一种简单方法：通过对给定上下文进行针对性梯度更新，克服静态自注意力的局限性。这种方法将推理时计算从生成更多思考标记转向上下文特定的训练

Result: 该方法在各种模型和长上下文基准测试中带来持续的大幅性能提升，Qwen3-4B在LongBench-v2和ZeroScrolls基准测试子集上平均分别提升12.6和14.1个百分点

Conclusion: 对于长上下文任务，少量上下文特定的训练比当前推理时扩展策略（如生成更多思考标记）是更好的推理计算利用方式

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [33] [Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America](https://arxiv.org/abs/2512.13910)
*Matheus Corrêa Domingos,Valdivino Alexandre de Santiago Júnior,Juliana Aparecida Anochi,Elcio Hideiti Shiguemori,Luísa Mirelle Costa dos Santos,Hércules Carlos dos Santos Pereira,André Estevam Costa Oliveira*

Main category: cs.LG

TL;DR: 该研究比较了传统机器学习、深度学习和动态模型在南美洲降水预报中的表现，发现LSTM模型在强降水预报中表现最佳，证实了深度学习在气候预报中的可行性。


<details>
  <summary>Details</summary>
Motivation: 降水预报对社会至关重要，但气象过程的复杂性需要先进模型。虽然AI技术已被用作动态模型的替代或补充，但缺乏对纯数据驱动方法在降水预报中可行性的广泛研究。本研究旨在填补这一空白。

Method: 研究比较了传统机器学习方法（随机森林和XGBoost）、深度学习方法（1D CNN、LSTM、GRU）以及传统动态建模方法（巴西全球大气模型BAM）。考虑了2019年所有季节的南美洲降水数据，并使用可解释人工智能分析模型行为。

Result: LSTM表现出最强的预测性能，而传统动态模型BAM结果最差。尽管LSTM延迟较高，但在强降水预报中最准确。如果成本是考虑因素，XGBoost能以稍低的精度提供更低的延迟。

Conclusion: 研究结果证实了深度学习模型在气候预报中的可行性，强化了主要气象和气候预报中心的全球趋势。LSTM在降水预报中表现优异，为数据驱动方法在气象预报中的应用提供了实证支持。

Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.

</details>


### [34] [Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations](https://arxiv.org/abs/2512.13913)
*Patrick Egenlauf,Iva Březinová,Sabine Andergassen,Miriam Klopotek*

Main category: cs.LG

TL;DR: 使用神经ODE学习量子多体系统的二粒子约化密度矩阵动力学，发现在二粒子和三粒子关联度高的区域能准确预测，但在反关联或无关联区域失效，表明需要记忆依赖的闭合方案。


<details>
  <summary>Details</summary>
Motivation: 研究非平衡量子多体系统的快速关联建立现象。传统方法要么计算复杂度指数增长（精确波函数），要么忽略重要关联（平均场方法）。时变二粒子约化密度矩阵方法提供折中方案，但忽略记忆效应的时域重构泛函的有效性在不同动力学区域尚不明确。

Method: 使用神经ODE模型在精确的2RDM数据上进行训练（无维度约简），试图仅从二粒子信息重构动力学。通过分析二粒子和三粒子累积量之间的皮尔逊相关性来评估模型性能。

Result: 神经ODE仅在二粒子和三粒子累积量相关性高的参数区域能准确再现动力学。在反关联或无关联区域失效，表明没有简单的时域二粒子累积量泛函能捕捉演化。时间平均的三粒子关联建立幅度是成功的主要预测因子：中等关联建立时，神经ODE和现有TD2RDM重构都准确；强关联建立时系统性地失效。

Conclusion: 在强关联建立区域需要记忆依赖的三粒子累积量重构核。神经ODE可作为模型无关的诊断工具，映射累积量展开方法的适用范围，指导非局域闭合方案的发展。从有限数据学习高维RDM动力学为快速、数据驱动的关联量子物质模拟开辟了新途径。

Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.

</details>


### [35] [Adaptive digital twins for predictive decision-making: Online Bayesian learning of transition dynamics](https://arxiv.org/abs/2512.13919)
*Eugenio Varetti,Matteo Torzoni,Marco Tezzele,Andrea Manzoni*

Main category: cs.LG

TL;DR: 该研究提出了一种自适应数字孪生框架，通过贝叶斯更新在线学习状态转移概率，增强土木工程中数字孪生的价值实现


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生在土木工程中的应用需要更强的适应性和个性化能力，以提升价值实现效果。研究旨在通过自适应状态转移模型增强数字孪生的鲁棒性和成本效益。

Method: 使用概率图模型表示数字孪生，通过动态贝叶斯网络建模物理与虚拟域的双向交互。将状态转移概率视为具有共轭先验的随机变量，实现分层在线学习。采用参数化马尔可夫决策过程和强化学习计算动态策略。

Result: 提出的自适应数字孪生框架具有增强的个性化、增加的鲁棒性和改进的成本效益。在铁路桥梁结构健康监测和维护规划的案例研究中验证了方法的有效性。

Conclusion: 通过自适应状态转移模型和在线贝叶斯学习，数字孪生在土木工程中的价值实现可以得到显著增强，特别是在结构健康监测和维护规划等应用中。

Abstract: This work shows how adaptivity can enhance value realization of digital twins in civil engineering. We focus on adapting the state transition models within digital twins represented through probabilistic graphical models. The bi-directional interaction between the physical and virtual domains is modeled using dynamic Bayesian networks. By treating state transition probabilities as random variables endowed with conjugate priors, we enable hierarchical online learning of transition dynamics from a state to another through effortless Bayesian updates. We provide the mathematical framework to account for a larger class of distributions with respect to the current literature. To compute dynamic policies with precision updates we solve parametric Markov decision processes through reinforcement learning. The proposed adaptive digital twin framework enjoys enhanced personalization, increased robustness, and improved cost-effectiveness. We assess our approach on a case study involving structural health monitoring and maintenance planning of a railway bridge.

</details>


### [36] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 提出Phalanx层作为窗口注意力或线性循环的替代方案，在1B参数多混合模型中实现10-40%速度提升


<details>
  <summary>Details</summary>
Motivation: 多混合架构因更好的质量和性能有望主导语言建模，但需要解决GPU内存层次对齐和通信开销问题

Method: 引入层次分解框架用于线性循环，开发滑动窗口循环算法，创建Phalanx层作为窗口注意力或线性循环的即插即用替代方案

Result: 在1B参数多混合模型中，Phalanx在4K到32K上下文长度上比优化的Transformer实现10-40%速度提升，同时保持困惑度相当

Conclusion: Phalanx层通过硬件对齐的滑动窗口循环有效解决了多混合架构中的内存层次和通信问题，显著提升性能

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [37] [A Complete Guide to Spherical Equivariant Graph Transformers](https://arxiv.org/abs/2512.13927)
*Sophia Tang*

Main category: cs.LG

TL;DR: 该指南系统介绍了球形等变图神经网络的理论基础、数学框架和实际应用，为三维分子系统建模提供了完整的SO(3)等变建模方法。


<details>
  <summary>Details</summary>
Motivation: 三维分子和生物分子系统的预测需要尊重物理中的旋转对称性，传统图神经网络和Transformer无法保证这种物理意义下的等变性，需要发展能够保持SO(3)旋转对称性的等变模型。

Method: 基于群表示论和球谐函数，使用球张量表示节点和边特征，通过张量积和Clebsch-Gordan分解构建SO(3)-等变核，开发了Tensor Field Network和SE(3)-Transformer架构，实现等变消息传递和注意力机制。

Result: 建立了完整的球形等变建模理论框架，提供了从数学推导到代码实现的完整指南，为化学、分子性质预测、蛋白质结构建模和生成建模等应用提供了理论基础和实用工具。

Conclusion: 球形等变图神经网络为三维分子系统的物理建模提供了原则性框架，通过保持旋转对称性确保预测的物理意义，该指南为研究人员提供了从理论到实践的自包含入门材料。

Abstract: Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.

</details>


### [38] [Informing Acquisition Functions via Foundation Models for Molecular Discovery](https://arxiv.org/abs/2512.13935)
*Qi Chen,Fabio Ramos,Alán Aspuru-Guzik,Florian Shkurti*

Main category: cs.LG

TL;DR: 提出了一种无似然贝叶斯优化方法，直接利用LLM和化学基础模型的先验知识指导分子发现，通过树结构空间划分和MCTS提高可扩展性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在数据稀缺、先验知识不足且候选空间巨大的分子发现场景中性能受限。虽然大语言模型和化学基础模型提供了丰富的先验知识，但高维特征、昂贵的上下文学习以及深度贝叶斯代理模型的计算负担阻碍了这些模型的充分利用。

Method: 提出无似然贝叶斯优化方法，绕过显式代理模型构建，直接利用通用LLM和化学特定基础模型的先验知识来指导获取函数。该方法学习分子搜索空间的树结构划分，使用局部获取函数，通过蒙特卡洛树搜索实现高效候选选择。进一步结合基于LLM的粗粒度聚类，将获取函数评估限制在具有统计上更高属性值的聚类中，大幅提升对大型候选集的可扩展性。

Result: 通过大量实验和消融研究表明，所提出的方法在LLM引导的分子发现贝叶斯优化中，显著提高了可扩展性、鲁棒性和样本效率。

Conclusion: 该方法成功解决了传统贝叶斯优化在分子发现中的局限性，通过有效整合LLM和化学基础模型的先验知识，实现了更高效、可扩展的分子搜索策略。

Abstract: Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.

</details>


### [39] [EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training](https://arxiv.org/abs/2511.10333)
*Qingao Yi,Jiaang Duan,Hanwen Hu,Qin Hua,Haiyan Zhao,Shiyou Qian,Dingyu Yang,Jian Cao,Jinghua Tang,Yinghao Yu,Chenzhi Liao,Kangjin Wang,Liping Zhang*

Main category: cs.LG

TL;DR: 提出EDGC框架，基于梯度熵动态调整压缩率，在保持模型精度的同时显著减少LLM训练中的通信开销


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型面临计算资源和内存容量的挑战，分布式训练虽有帮助但仍存在大量通信开销。现有静态梯度压缩方法忽略了训练过程中梯度的动态变化特性，导致性能下降。如何在压缩加速LLM训练的同时不牺牲性能是一个挑战。

Method: 提出EDGC（熵驱动动态梯度压缩）框架，包含三个关键组件：1）使用下采样方法高效估计梯度熵，降低计算开销；2）建立压缩率与梯度熵的理论模型，支持更明智的压缩决策；3）基于窗口的调整机制，在流水线阶段间动态适应压缩率。

Result: 在32个NVIDIA V100集群上训练GPT2-2.5B，在64个NVIDIA H100集群上训练GPT2-12.1B。结果显示EDGC显著减少通信延迟和训练时间，分别达到46.45%和16.13%，同时保持LLM精度。

Conclusion: EDGC框架通过基于梯度熵动态调整压缩率，有效解决了LLM训练中的通信效率问题，在保持模型性能的同时显著加速训练过程。

Abstract: Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.

</details>


### [40] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: PGDM是一种基于模式引导的扩散模型，利用时间数据中的重复模式进行预测，通过原型分析提取模式并估计最可能的下一个模式，从而提高预测准确性和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多元时间序列预测中很少考虑数据中存在的重复结构或模式，导致预测结果可能不符合已知模式集，限制了预测的准确性和实用性。

Method: PGDM首先使用原型分析提取时间数据中的模式，然后估计序列中最可能的下一个模式。通过这个模式估计来引导预测，使预测结果更符合已知模式。同时引入基于原型分析的不确定性量化技术，并根据模式估计的不确定性动态调整引导水平。

Result: 在两个预测应用（视野测量和动作捕捉帧预测）中，模式引导使PGDM的性能（MAE/CRPS）分别提高了40.67%/56.26%和14.12%/14.10%。PGDM在基线比较中分别优于基线方法达65.58%/84.83%和93.64%/92.55%。

Conclusion: PGDM通过利用时间数据中的重复模式进行引导预测，显著提高了扩散模型在时间序列预测中的性能，同时提供了有效的不确定性量化方法，在医疗和运动分析等领域具有实际应用价值。

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [41] [A Single Architecture for Representing Invariance Under Any Space Group](https://arxiv.org/abs/2512.13989)
*Cindy Y. Zhang,Elif Ertekin,Peter Orbanz,Ryan P. Adams*

Main category: cs.LG

TL;DR: 提出一种单一机器学习架构，能自动适应任何输入空间群并强制对称性不变性，解决材料科学中230种空间群对称性的建模挑战。


<details>
  <summary>Details</summary>
Motivation: 将已知对称性融入机器学习模型能提高预测准确性、鲁棒性和泛化能力，但现有方法需要为每个对称群设计专门架构，限制了可扩展性和知识迁移。特别是在材料科学中，三维空间有230种空间群，这一挑战尤为突出。

Method: 通过显式表征群操作对傅里叶系数的约束，构建对称性适应的傅里叶基，并将这些约束编码到神经网络层中，实现不同空间群之间的权重共享。

Result: 该方法在材料属性预测任务中表现出竞争性性能，并能进行零样本学习以泛化到未见过的空间群。

Conclusion: 提出的单一架构能够自动适应任何空间群并强制对称性不变性，克服了数据稀疏性问题，实现了跨空间群的知识迁移和零样本泛化。

Abstract: Incorporating known symmetries in data into machine learning models has consistently improved predictive accuracy, robustness, and generalization. However, achieving exact invariance to specific symmetries typically requires designing bespoke architectures for each group of symmetries, limiting scalability and preventing knowledge transfer across related symmetries. In the case of the space groups, symmetries critical to modeling crystalline solids in materials science and condensed matter physics, this challenge is particularly salient as there are 230 such groups in three dimensions. In this work we present a new approach to such crystallographic symmetries by developing a single machine learning architecture that is capable of adapting its weights automatically to enforce invariance to any input space group. Our approach is based on constructing symmetry-adapted Fourier bases through an explicit characterization of constraints that group operations impose on Fourier coefficients. Encoding these constraints into a neural network layer enables weight sharing across different space groups, allowing the model to leverage structural similarities between groups and overcome data sparsity when limited measurements are available for specific groups. We demonstrate the effectiveness of this approach in achieving competitive performance on material property prediction tasks and performing zero-shot learning to generalize to unseen groups.

</details>


### [42] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该研究构建了一个经过精心整理的MHC-II抗原表位数据集，并设计了三个机器学习任务来推进计算免疫治疗研究。


<details>
  <summary>Details</summary>
Motivation: MHC-II抗原表位在免疫治疗中至关重要，但与MHC-I相比，其研究面临更多挑战：结合特异性复杂、基序模式模糊、现有数据集较小且标准化不足。

Method: 1. 从IEDB和其他公共来源构建精心整理的MHC-II数据集；2. 提出三个机器学习任务：肽结合、肽呈递和抗原呈递；3. 采用多尺度评估框架对现有模型进行基准测试；4. 使用模块化框架进行全面的建模设计分析。

Result: 创建了一个扩展且标准化的肽-MHC-II数据集，并引入了具有更丰富生物学背景的新型抗原-MHC-II数据集，为MHC-II抗原呈递途径的机器学习建模提供了基础资源。

Conclusion: 这项工作为推进计算免疫治疗提供了宝贵资源，为未来机器学习指导的表位发现和免疫反应预测建模研究奠定了基础。

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [43] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: EXAONE Path 2.5是一个病理学基础模型，通过联合建模组织学、基因组学、表观遗传学和转录组学等多模态数据，创建更全面的肿瘤生物学患者表征。


<details>
  <summary>Details</summary>
Motivation: 癌症进展涉及多个生物层面的相互作用，特别是超越形态学、涉及分子层面的相互作用，这些对于仅基于图像的模型是不可见的。为了捕捉更广泛的生物学景观，需要整合多模态数据来更全面地反映肿瘤生物学。

Method: 模型包含三个关键组件：1) 多模态SigLIP损失，实现跨异质模态的全配对对比学习；2) 片段感知旋转位置编码(F-RoPE)模块，保留WSI中的空间结构和组织片段拓扑；3) 针对WSI和RNA-seq的领域专业化内部基础模型，为稳健的多模态对齐提供生物学基础嵌入。

Result: 在两个互补基准测试中评估：内部真实世界临床数据集和包含80个任务的Patho-Bench基准。模型表现出高数据和参数效率，在Patho-Bench上与最先进的基础模型性能相当，同时在内部临床设置中表现出最高的适应性。

Conclusion: 这些结果突显了生物学信息多模态设计的价值，并强调了整合基因型到表型建模在下一代精准肿瘤学中的潜力。

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [44] [Multivariate Time Series Forecasting with Hybrid Euclidean-SPD Manifold Graph Neural Networks](https://arxiv.org/abs/2512.14023)
*Yong Fang,Na Li,Hangguan Shan,Eryun Liu,Xinyu Li,Wei Ni,Er-Ping Li*

Main category: cs.LG

TL;DR: HSMGNN提出了一种混合欧几里得-黎曼几何框架的图神经网络模型，用于多变量时间序列预测，通过双空间嵌入和自适应距离机制提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列预测方法通常在欧几里得或黎曼空间中建模，难以捕捉真实数据中多样的几何结构和复杂的时空依赖关系。

Method: 1) 引入子流形交叉段嵌入将输入数据投影到欧几里得和黎曼双空间；2) 设计自适应距离库层降低黎曼距离计算成本；3) 开发融合图卷积网络通过可学习融合算子整合双空间特征。

Result: 在三个基准数据集上的实验表明，HSMGNN相比最先进基线方法在预测精度上提升了最高13.8%。

Conclusion: HSMGNN首次利用混合几何表示进行多变量时间序列预测，能够更全面地表征数据的几何特性，显著提升了预测性能。

Abstract: Multivariate Time Series (MTS) forecasting plays a vital role in various real-world applications, such as traffic management and predictive maintenance. Existing approaches typically model MTS data in either Euclidean or Riemannian space, limiting their ability to capture the diverse geometric structures and complex spatio-temporal dependencies inherent in real-world data. To overcome this limitation, we propose the Hybrid Symmetric Positive-Definite Manifold Graph Neural Network (HSMGNN), a novel graph neural network-based model that captures data geometry within a hybrid Euclidean-Riemannian framework. To the best of our knowledge, this is the first work to leverage hybrid geometric representations for MTS forecasting, enabling expressive and comprehensive modeling of geometric properties. Specifically, we introduce a Submanifold-Cross-Segment (SCS) embedding to project input MTS into both Euclidean and Riemannian spaces, thereby capturing spatio-temporal variations across distinct geometric domains. To alleviate the high computational cost of Riemannian distance, we further design an Adaptive-Distance-Bank (ADB) layer with a trainable memory mechanism. Finally, a Fusion Graph Convolutional Network (FGCN) is devised to integrate features from the dual spaces via a learnable fusion operator for accurate prediction. Experiments on three benchmark datasets demonstrate that HSMGNN achieves up to a 13.8 percent improvement over state-of-the-art baselines in forecasting accuracy.

</details>


### [45] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: FusAD是一个统一的时间序列分析框架，通过自适应时频融合和去噪机制，在分类、预测和异常检测等多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分析面临三个主要挑战：1）缺乏高效、多任务兼容且可泛化的统一框架；2）现有方法通常针对单一任务或特定数据类型；3）现实数据常受噪声、复杂频率成分和多尺度动态模式影响，难以进行鲁棒特征提取。

Method: FusAD框架包含三个核心组件：1）自适应时频融合机制，整合傅里叶和小波变换以捕获全局-局部和多尺度动态特征；2）自适应去噪机制，自动感知和过滤各类噪声；3）通用信息融合和解码结构，结合掩码预训练以促进多粒度表示的学习和迁移。

Result: 在主流时间序列基准测试中，FusAD在分类、预测和异常检测任务上均优于最先进模型，同时保持高效率和可扩展性。

Conclusion: FusAD通过统一的时频融合和自适应去噪机制，成功解决了多任务时间序列分析的挑战，为复杂环境下的鲁棒特征提取提供了有效解决方案。

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [46] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: SonicMoE提出了一种内存高效的MoE训练方法，通过减少激活内存缓存、优化GPU内核计算与内存IO重叠，以及创新的token rounding技术来解决细粒度MoE的内存和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型向更高专家粒度（更小的专家中间维度）和更高稀疏性（激活专家数恒定但总专家数增加）发展，虽然提高了每FLOP的模型质量，但细粒度MoE面临激活内存占用增加和硬件效率降低的问题，而稀疏MoE则因Grouped GEMM内核中的填充导致计算浪费。

Method: 1) 提出内存高效算法，以最小化反向传播的激活缓存；2) 设计GPU内核，使内存IO与计算重叠；3) 提出"token rounding"方法，最小化Grouped GEMM内核中填充造成的计算浪费。

Result: SonicMoE将激活内存减少45%，在Hopper GPU上相比ScatterMoE的BF16 MoE内核实现1.86倍计算吞吐量提升。在64个H100上达到2130亿token/天的训练吞吐量，接近ScatterMoE在96个H100上的2250亿token/天。在高MoE稀疏设置下，tile-aware token rounding算法相比vanilla top-K路由获得额外1.16倍加速，同时保持相似的下游性能。

Conclusion: SonicMoE通过创新的内存优化、计算重叠和token rounding技术，有效解决了MoE训练中的内存和计算效率瓶颈，为大规模MoE模型训练提供了高效的解决方案，所有内核已开源。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [47] [Derivative-Informed Fourier Neural Operator: Universal Approximation and Applications to PDE-Constrained Optimization](https://arxiv.org/abs/2512.14086)
*Boyuan Yao,Dingcheng Luo,Lianghao Cao,Nikola Kovachki,Thomas O'Leary-Roseberry,Omar Ghattas*

Main category: cs.LG

TL;DR: 该论文提出了导数信息傅里叶神经算子（DIFNOs）的逼近理论和高效训练方法，用于PDE约束优化问题。DIFNOs通过同时最小化输出和Fréchet导数的预测误差来训练，能够准确模拟高保真算子的响应和灵敏度。


<details>
  <summary>Details</summary>
Motivation: 传统FNOs作为代理模型在PDE约束优化中存在局限性，因为准确的优化需要准确的导数信息。论文旨在开发能够同时准确模拟算子和其导数的神经网络架构，以提高PDE约束优化问题的求解效率和精度。

Method: 提出了导数信息傅里叶神经算子（DIFNOs），通过联合最小化输出和Fréchet导数样本的预测误差进行训练。开发了高效的训练方案，包括维度约简和多分辨率技术，显著降低了导数学习的内存和计算成本。建立了FNOs及其Fréchet导数在紧集上的同时通用逼近理论，以及在加权Sobolev空间中的通用逼近理论。

Result: 理论结果证明了FNOs在导数信息算子学习和PDE约束优化求解中的能力。数值实验表明，DIFNOs在非线性扩散-反应方程、Helmholtz方程和Navier-Stokes方程上表现出优越的样本复杂度，能够在低训练样本量下实现高精度，特别适用于无限维PDE约束逆问题的求解。

Conclusion: DIFNOs作为一种导数信息的算子学习方法，在PDE约束优化问题中具有显著优势。通过同时学习算子和其导数，DIFNOs能够以更少的样本实现高精度，为无限维逆问题的求解提供了高效且准确的代理模型。

Abstract: We present approximation theories and efficient training methods for derivative-informed Fourier neural operators (DIFNOs) with applications to PDE-constrained optimization. A DIFNO is an FNO trained by minimizing its prediction error jointly on output and Fréchet derivative samples of a high-fidelity operator (e.g., a parametric PDE solution operator). As a result, a DIFNO can closely emulate not only the high-fidelity operator's response but also its sensitivities. To motivate the use of DIFNOs instead of conventional FNOs as surrogate models, we show that accurate surrogate-driven PDE-constrained optimization requires accurate surrogate Fréchet derivatives. Then, for continuously differentiable operators, we establish (i) simultaneous universal approximation of FNOs and their Fréchet derivatives on compact sets, and (ii) universal approximation of FNOs in weighted Sobolev spaces with input measures that have unbounded supports. Our theoretical results certify the capability of FNOs for accurate derivative-informed operator learning and accurate solution of PDE-constrained optimization. Furthermore, we develop efficient training schemes using dimension reduction and multi-resolution techniques that significantly reduce memory and computational costs for Fréchet derivative learning. Numerical examples on nonlinear diffusion--reaction, Helmholtz, and Navier--Stokes equations demonstrate that DIFNOs are superior in sample complexity for operator learning and solving infinite-dimensional PDE-constrained inverse problems, achieving high accuracy at low training sample sizes.

</details>


### [48] [Arithmetic-Intensity-Aware Quantization](https://arxiv.org/abs/2512.14090)
*Taig Singh,Shreshth Rajan,Nikhil Iyer*

Main category: cs.LG

TL;DR: AIQ是一种混合精度量化框架，通过逐层选择比特宽度来最大化算术强度，同时最小化精度损失，从而解决内存带宽限制问题。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络越来越受内存限制，推理吞吐量受DRAM带宽而非计算能力限制，需要优化内存访问效率。

Method: AIQ是一种后训练量化方法，使用搜索算法在逐层量化方案上寻找最优配置，最小化算术强度和准确度的加权损失。

Result: 在ResNet-20/CIFAR-10上，AIQ将算术强度提高约50%，测试精度损失在1个百分点内；在MobileNetV2上，吞吐量提高1.66倍，精度损失同样在1个百分点内。

Conclusion: AIQ能有效提高内存受限网络的推理吞吐量，同时保持精度，且自然地更激进地量化较大的层。

Abstract: As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.

</details>


### [49] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Cornserve是一个用于Any-to-Any多模态模型的高效在线服务系统，通过自动优化部署计划和分布式运行时处理异构计算，相比现有方案提升3.81倍吞吐量并降低5.79倍尾延迟。


<details>
  <summary>Details</summary>
Motivation: Any-to-Any多模态模型（接受文本和多模态数据组合输入，生成文本和多模态数据组合输出）在模型服务中引入了请求类型、计算路径和计算扩展的异构性，现有服务系统难以高效处理这种复杂性。

Method: 1. 允许开发者描述通用Any-to-Any模型的计算图（包含多模态编码器、自回归模型如LLM、多模态生成器如DiT等异构组件）；2. 规划器自动根据模型和工作负载特性找到优化部署计划，包括是否以及如何将模型分解为更小组件；3. 分布式运行时按照计划执行模型，高效处理在线服务中的异构性。

Result: Cornserve能够高效服务多样化的Any-to-Any模型和工作负载，相比现有解决方案，提供高达3.81倍的吞吐量提升和高达5.79倍的尾延迟降低。

Conclusion: Cornserve通过自动化的部署规划和高效的分布式运行时，成功解决了Any-to-Any多模态模型在线服务中的异构性挑战，显著提升了服务性能。

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [50] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 提出S-GRPO方法，使用逻辑相似性奖励机制替代传统奖励建模，通过形式逻辑一致性引导大语言模型与人类偏好对齐，避免模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法依赖奖励模型的质量和稳定性，而传统奖励建模存在启发式估计的局限性。需要更可靠的方法来确保大语言模型与人类价值观对齐。

Method: 提出基于逻辑相似性的奖励机制，使用形式逻辑一致性替代传统奖励估计。引入S-GRPO（GRPO的监督变体），结合监督组件，联合优化生成项、KL散度正则化和基于标签的目标。

Result: S-GRPO在性能和鲁棒性上均优于标准监督微调（SFT），并能扩展现有的偏好学习框架（如GRPO和DPO），提供更灵活、任务自适应的对齐训练方法。

Conclusion: 逻辑相似性奖励机制和S-GRPO框架为RLHF提供了更可靠的替代方案，通过形式逻辑一致性确保模型对齐，同时避免多视角解释导致的模型崩溃问题。

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [51] [PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario](https://arxiv.org/abs/2512.14150)
*Zhijie Zhong,Zhiwen Yu,Pengyu Li,Jianming Lv,C. L. Philip Chen,Min Chen*

Main category: cs.LG

TL;DR: PathFinder提出了一种新的无线路径损耗预测架构，通过解耦特征编码主动建模建筑物和发射器，使用掩码引导低秩注意力机制，并引入发射器导向的Mixup策略，在单发射器到多发射器的分布偏移场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的无线路径损耗预测方法存在三个主要问题：1）被动的环境建模忽略了发射器和关键环境特征；2）过度关注单发射器场景而现实世界多为多发射器；3）过度关注分布内性能而忽视分布偏移挑战，特别是在训练/测试环境建筑密度或发射器配置不同时。

Method: 提出PathFinder架构：1）通过解耦特征编码主动建模建筑物和发射器；2）集成掩码引导低秩注意力机制，独立关注接收器和建筑物区域；3）引入发射器导向的Mixup策略进行鲁棒训练；4）创建新的单到多发射器RPP基准来评估外推性能。

Result: 实验结果显示PathFinder显著优于现有最先进方法，特别是在具有挑战性的多发射器场景中，在单发射器训练后多发射器测试的分布偏移情况下表现优异。

Conclusion: PathFinder通过主动环境建模、多发射器场景处理和分布偏移鲁棒性，为5G网络优化和物联网应用提供了更有效的无线路径损耗预测解决方案，特别是在现实世界多发射器环境中。

Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.

</details>


### [52] [On Improving Deep Active Learning with Formal Verification](https://arxiv.org/abs/2512.14170)
*Jonathan Spiegelman,Guy Amir,Guy Katz*

Main category: cs.LG

TL;DR: 本文研究如何通过添加违反鲁棒性约束的对抗性样本来增强深度主动学习，发现形式验证生成的对抗样本比基于梯度的攻击效果更好，能显著提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度主动学习旨在减少神经网络训练中的标注成本，通过选择最有信息量的未标注样本进行标注。除了选择哪些样本标注外，一些DAL方法还通过添加不需要人工标注的合成输入来进一步增强数据效率。本文研究如何通过添加违反鲁棒性约束的对抗性样本来提升DAL性能。

Method: 研究使用对抗性输入增强训练数据的方法，特别比较了通过形式验证生成的对抗样本与标准基于梯度攻击生成的对抗样本的效果。将这种方法应用于多种现代DAL技术，并提出了一个新的技术。

Result: 研究发现形式验证生成的对抗样本比基于梯度的攻击生成的对抗样本贡献更大。将这种扩展应用于多种DAL技术后，在标准基准测试中显著提高了模型的泛化能力。

Conclusion: 通过添加违反鲁棒性约束的对抗性样本可以显著提升深度主动学习的性能，特别是形式验证生成的对抗样本比传统梯度攻击生成的样本效果更好，能有效提高模型在标准基准测试中的泛化能力。

Abstract: Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.

</details>


### [53] [Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix](https://arxiv.org/abs/2512.14188)
*Wei Tao,Sheng Long,Xin Liu,Wei Li,Qing Tao*

Main category: cs.LG

TL;DR: 该论文提出了一种新的基于动量的对抗攻击方法AdaMI，通过引入动量自适应矩阵来解决现有攻击方法中的优化问题，提高了对抗样本的迁移性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的对抗攻击方法（如PGD和MI-FGSM）使用符号函数来缩放扰动，从优化理论角度看存在理论问题。作者发现PGD实际上是使用当前梯度确定步长的投影梯度法的特定重构，并希望通过引入自适应矩阵来改进攻击方法的优化性能。

Method: 作者首先分析了PGD与投影梯度法的关系，发现当使用基于累积梯度的常规自适应矩阵缩放扰动时，PGD变为AdaGrad。基于此分析，提出了AdaMI攻击方法，该方法使用基于动量的自适应矩阵来优化扰动，被证明在凸问题上能达到最优收敛。

Result: 实验表明，提出的基于动量的自适应矩阵可以作为通用且有效的技术，在不同网络架构上提升对抗迁移性，同时保持更好的稳定性和不可感知性，优于现有最先进方法。

Conclusion: AdaMI通过引入动量自适应矩阵解决了MI-FGSM的非收敛问题，确保了优化过程的稳定性，同时显著提高了对抗样本的迁移能力，为对抗攻击的优化提供了新的理论视角和实践方法。

Abstract: Generating adversarial examples (AEs) can be formulated as an optimization problem. Among various optimization-based attacks, the gradient-based PGD and the momentum-based MI-FGSM have garnered considerable interest. However, all these attacks use the sign function to scale their perturbations, which raises several theoretical concerns from the point of view of optimization. In this paper, we first reveal that PGD is actually a specific reformulation of the projected gradient method using only the current gradient to determine its step-size. Further, we show that when we utilize a conventional adaptive matrix with the accumulated gradients to scale the perturbation, PGD becomes AdaGrad. Motivated by this analysis, we present a novel momentum-based attack AdaMI, in which the perturbation is optimized with an interesting momentum-based adaptive matrix. AdaMI is proved to attain optimal convergence for convex problems, indicating that it addresses the non-convergence issue of MI-FGSM, thereby ensuring stability of the optimization process. The experiments demonstrate that the proposed momentum-based adaptive matrix can serve as a general and effective technique to boost adversarial transferability over the state-of-the-art methods across different networks while maintaining better stability and imperceptibility.

</details>


### [54] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 论文提出使用随机桥（random-bridges）作为生成模型中的随机传输方法，相比传统方法能以更少步骤生成高质量样本，计算成本低，适合高速生成任务。


<details>
  <summary>Details</summary>
Motivation: 本文旨在将随机桥（随机过程在固定时间点被约束到目标分布）应用于生成建模领域。随机桥可以作为两个概率分布之间的随机传输，根据驱动过程的不同，可以表现出马尔可夫或非马尔可夫、连续、不连续或混合模式。

Method: 从一般概率陈述出发，推导出特定的表示形式，用于学习和模拟算法的信息处理。基于高斯随机桥构建具体实现，开发相应的学习和生成算法。

Result: 基于高斯随机桥的实证结果，相比传统方法能以显著更少的步骤生成高质量样本，同时获得有竞争力的Frechet inception distance分数。分析表明该框架计算成本低，适合高速生成任务。

Conclusion: 随机桥框架为生成建模提供了一种有效的随机传输方法，具有计算效率高、生成速度快、样本质量好的优势，特别适用于需要高速生成的应用场景。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [55] [Understanding and Improving Hyperbolic Deep Reinforcement Learning](https://arxiv.org/abs/2512.14202)
*Timo Klein,Thomas Lang,Andrii Shkabrii,Alexander Sturm,Kevin Sidak,Lukas Miklautz,Claudia Plant,Yllka Velaj,Sebastian Tschiatschek*

Main category: cs.LG

TL;DR: Hyper++：一种新型双曲强化学习智能体，通过稳定训练机制解决双曲特征空间中的优化问题，在ProcGen和Atari-5任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 双曲特征空间能自然捕捉复杂RL环境中的层次和关系结构，但RL的非平稳性导致双曲深度RL智能体训练面临优化挑战，特别是大范数嵌入会破坏梯度训练稳定性

Method: 通过分析Poincaré Ball和Hyperboloid模型中核心操作的梯度，识别训练失败因素，提出Hyper++：1）使用分类值损失稳定critic训练；2）特征正则化保证有界范数；3）采用更优化友好的双曲网络层公式

Result: 在ProcGen实验中，Hyper++保证稳定学习，优于先前双曲智能体，减少约30%的墙钟时间；在Atari-5的Double DQN中，显著优于欧几里得和双曲基线

Conclusion: Hyper++通过解决双曲特征空间中的优化挑战，实现了稳定高效的强化学习，为利用双曲几何进行RL表示学习提供了有效解决方案

Abstract: The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .

</details>


### [56] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: 提出LLM compare方法，通过大语言模型进行成对难度比较，结合Bradley-Terry评分来估计问题难度，解决了现有方法无法泛化到分布外问题的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有问题难度估计方法（如人工校准或基于性能的评分）无法泛化到分布外问题（人类和LLM目前无法解决的问题），因为这些方法不可扩展、耗时且依赖真实标签。

Method: 提出LLM compare方法：使用LLM进行成对难度比较，然后基于比较结果计算Bradley-Terry评分。该方法具有连续性、动态性、模型无关性和不依赖真实标签的特点。

Result: 1) 概念框架验证：LLM compare占据所有理想象限；2) 与人类标注高度一致：Pearson r≥0.80 (n=1876)；3) 对幻觉具有鲁棒性：10%噪声注入下Pearson相关性下降小于6%。

Conclusion: LLM compare是第一个连续动态、模型无关且不依赖真实标签的难度估计方法，能够替代耗时的人工标注和合成数据生成，对课程设计、模型评估和AI辅助研究构思有重要推动作用。

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [57] [Understanding the Gain from Data Filtering in Multimodal Contrastive Learning](https://arxiv.org/abs/2512.14230)
*Divyansh Pareek,Sewoong Oh,Simon S. Du*

Main category: cs.LG

TL;DR: 论文通过理论分析证明，在多模态表示学习中，使用预训练模型进行数据过滤（教师过滤）能显著提升对比学习性能，特别是在数据质量较低时效果更明显。


<details>
  <summary>Details</summary>
Motivation: 现代多模态表示学习依赖互联网规模的数据集，但大量原始网络数据质量较低，数据筛选成为训练流程中的关键步骤。虽然基于预训练模型的教师过滤方法在实践中取得成功，但其理论原理尚不明确，需要从理论角度解释其有效性。

Method: 采用标准的双模态数据生成模型，在线性对比学习框架下进行理论分析。设η∈(0,1]为n个配对样本中模态正确匹配的比例，分别推导无过滤和有教师过滤情况下的误差边界。

Result: 理论分析表明：无过滤时误差上下界为1/(η√n)；使用教师过滤后，在η较大时误差上界为1/√(ηn)，在η较小时误差上界为1/√n。这证明了教师过滤能显著降低误差，特别是在数据质量差（η小）时效果更明显。

Conclusion: 教师过滤在多模态对比学习中具有理论保证的优势，能够有效提升学习性能，特别是在数据质量较低的情况下。这为实践中广泛使用的数据过滤方法提供了理论依据。

Abstract: The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\frac{1}{η\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\frac{1}{\sqrt{ηn}}$ in the large $η$ regime, and by $\frac{1}{\sqrt{n}}$ in the small $η$ regime.

</details>


### [58] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 该论文提出了一种从数据中学习反应-扩散系统的方法，确保学习到的模型具有物理一致性和适定性。通过正则化框架，将质量守恒和准正性等物理性质直接融入学习过程，保证模型符合物理原理。


<details>
  <summary>Details</summary>
Motivation: 传统的数据驱动模型学习反应-扩散系统时，往往无法保证学习到的模型符合基本的物理原理（如质量守恒、非负性），这限制了模型的可解释性和可靠性。需要开发既能从数据学习又能保持物理一致性的方法。

Method: 1. 提出系统修改参数化反应项的技术，使其固有地满足质量守恒和准正性；2. 在正则化框架下学习反应-扩散系统；3. 扩展现有理论结果，证明即使强制执行守恒定律和准正性，学习问题的解也能收敛到极限系统的唯一正则化最小化解；4. 提供准正函数的逼近结果，用于构建物理一致的参数化。

Result: 1. 开发了确保学习到的反应-扩散系统保持非负性并遵守物理原理的技术；2. 证明了在额外正则性和增长条件下，所得PDE的适定性；3. 理论证明了学习问题的解收敛到唯一正则化最小化解；4. 提供了准正函数的逼近结果，支持物理一致参数化的构建。

Conclusion: 该方法推进了反应-扩散系统可解释且可靠的数据驱动模型的发展，确保模型与基本物理定律保持一致。通过将物理约束直接融入学习过程，实现了物理一致性、适定性和数据驱动学习的统一。

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [59] [Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning](https://arxiv.org/abs/2512.14241)
*Salvatore Romano,Marco Grassia,Giuseppe Mangioni*

Main category: cs.LG

TL;DR: 该论文提出了一种名为RGM的新方法，用于评估图生成模型，克服了传统MMD指标的局限性，并对GRAN和EDGE两种先进模型进行了综合评估。


<details>
  <summary>Details</summary>
Motivation: 当前图生成模型评估主要依赖最大均值差异（MMD）作为评估指标，但这种方法存在局限性，无法充分评估生成图的结构特性，需要更全面的评估方法。

Method: 提出RGM（Representation-aware Graph-generation Model evaluation）评估方法，使用几何深度学习模型在专门设计的合成和真实图数据集上进行图分类任务，从而评估生成模型的质量。

Result: 对GRAN和EDGE两种先进图生成模型的评估显示，虽然两者都能生成具有某些拓扑特性的图，但在保持区分不同图域的结构特征方面存在显著局限性。

Conclusion: MMD作为图生成模型的评估指标存在不足，需要采用RGM等更全面的评估方法，未来研究应探索替代评估方法以更好地评估图生成模型的质量。

Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.

</details>


### [60] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: FLAME是一个轻量级时间序列基础模型家族，支持确定性和概率性预测，通过生成式概率建模确保效率和鲁棒性，在零样本预测任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个既轻量又强大的时间序列基础模型，能够同时处理确定性和概率性预测任务，并在保持高效率的同时提供准确的长期预测能力。

Method: 使用Legendre Memory增强泛化能力，采用变体LegT和LegS在编码和解码阶段捕获数据内在归纳偏差；采用基于Normalization Flow的预测头来建模任意复杂分布。

Result: 在TSFM-Bench和ProbTS等基准测试中，FLAME在确定性和概率性预测任务上均展现出最先进的零样本性能。

Conclusion: FLAME是一个高效且强大的时间序列基础模型，通过创新的Legendre Memory架构和Normalization Flow预测头，在保持轻量级的同时实现了优异的预测性能。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [61] [Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization](https://arxiv.org/abs/2512.14263)
*Nick Leenders,Thomas Quadt,Boris Cule,Roy Lindelauf,Herman Monsuur,Joost van Oijen,Mark Voskuijl*

Main category: cs.LG

TL;DR: 本文提出了一种基于决策树的解释性替代模型，用于偏好贝叶斯优化，解决了传统高斯过程模型难以解释、处理分类数据困难、计算复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于高斯过程的偏好贝叶斯优化方法存在三个主要问题：模型难以解释、难以处理分类数据、计算复杂度高，限制了其在实际应用中的可用性。

Method: 引入了一种基于决策树的内在可解释替代模型，能够同时处理分类和连续数据，并且可以扩展到大型数据集。

Result: 在八个逐渐变得"尖峰"的优化函数上的实验表明，该模型在尖峰函数上优于基于高斯过程的替代方法，在非尖峰函数上性能仅略低。在真实世界的Sushi数据集上展示了学习个人寿司偏好的能力，并初步探索了利用历史偏好数据加速新用户优化过程的方法。

Conclusion: 提出的决策树替代模型为偏好贝叶斯优化提供了一种可解释、能处理混合数据类型、可扩展的解决方案，在实际应用中具有更好的可用性。

Abstract: Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.

</details>


### [62] [Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits](https://arxiv.org/abs/2512.14338)
*Michael Murray,Tenzin Chan,Kedar Karhadker,Christopher J. Hillar*

Main category: cs.LG

TL;DR: Hopfield网络通过最小化能量流(MEF)的梯度下降具有对范数高效解的隐式偏置，使其能够从少量随机样本中推断图的完整同构类，并在群结构数据下实现近似不变性的涌现。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在群结构数据训练中不变性如何隐式涌现的现象，特别是在Hopfield网络中探索图同构类的学习机制。

Method: 使用经典Hopfield网络，通过梯度下降最小化能量流(MEF)，分析其对范数高效解的隐式偏置，研究图同构类在三维不变子空间中的表示。

Result: 发现图同构类可在三维不变子空间中表示；MEF梯度下降具有范数高效偏置，为学习同构类提供了多项式样本复杂度界限；多种学习规则下参数随样本增长收敛到不变子空间。

Conclusion: Hopfield网络中泛化的统一机制是：学习过程中的范数高效偏置驱动了群结构数据下近似不变性的涌现。

Abstract: Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

</details>


### [63] [Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis](https://arxiv.org/abs/2512.14361)
*Nicholas Tagliapietra,Katharina Ensinger,Christoph Zimmer,Osman Mian*

Main category: cs.LG

TL;DR: CaDyT是一种用于动态系统因果发现的新方法，通过基于差异的因果模型和精确高斯过程推理来建模连续时间动力学，在规则和不规则采样数据上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统按照其潜在的因果关系在连续时间中演化，但现有方法要么离散化时间导致不规则采样数据性能差，要么忽略底层因果关系，需要一种能同时解决这两个挑战的方法。

Method: CaDyT基于差异因果模型，使用精确高斯过程推理建模连续时间动力学，通过算法马尔可夫条件和最小描述长度原则指导的贪婪搜索来识别因果结构。

Result: 实验表明CaDyT在规则和不规则采样数据上都优于最先进的方法，发现的因果网络更接近真实的底层动力学。

Conclusion: CaDyT通过连续时间建模和因果发现的有效结合，为动态系统的因果推理提供了更准确的方法。

Abstract: Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.

</details>


### [64] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 首个基于提升量子差分隐私的黑盒隐私审计框架，通过量子金丝雀检测QML模型中的记忆化并量化隐私泄露


<details>
  <summary>Details</summary>
Motivation: 量子机器学习模型在敏感数据上训练时可能记忆个体记录，存在严重隐私风险。现有的量子差分隐私机制缺乏对部署模型的实证验证工具。

Method: 提出基于提升量子差分隐私的黑盒隐私审计框架，利用量子金丝雀（策略性偏移编码的量子态）检测记忆化，建立金丝雀偏移与迹距离界限的数学连接，推导隐私预算消耗的实证下界。

Result: 在模拟和物理量子硬件上的综合评估表明，该框架能有效测量QML模型的实际隐私损失，实现QML系统的稳健隐私验证。

Conclusion: 该框架填补了理论保证与实际隐私验证之间的关键空白，为QML系统提供了首个实用的隐私审计工具。

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [65] [RePo: Language Models with Context Re-Positioning](https://arxiv.org/abs/2512.14391)
*Huayang Li,Tianyu Zhao,Richard Sproat*

Main category: cs.LG

TL;DR: RePo是一种新的上下文重定位机制，通过可微分模块为token分配能捕捉上下文依赖关系的位置，而不是使用预定义的线性或常数位置索引，从而减少外部认知负荷，提升模型在噪声上下文、结构化数据和长上下文任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM架构使用固定线性或常数位置索引，这种无信息结构增加了外部认知负荷，消耗了本应用于深度推理和注意力分配的有限工作记忆容量。基于认知负荷理论，需要减少这种外部负荷。

Method: 提出RePo机制，使用可微分模块f_φ为token分配能捕捉上下文依赖关系的位置，而不是依赖预定义的整数范围。通过在OLMo-2 1B骨干网络上持续预训练实现。

Result: RePo在涉及噪声上下文、结构化数据和长上下文长度的任务上显著提升性能，同时在一般短上下文任务上保持竞争力。分析显示RePo能更好地关注远距离相关信息，在密集非线性空间中分配位置，并捕捉输入上下文的内在结构。

Conclusion: RePo通过上下文重定位有效减少外部认知负荷，提升LLM在复杂上下文环境中的表现，为改进LLM的上下文处理机制提供了新思路。

Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

</details>


### [66] [SuperWing: a comprehensive transonic wing dataset for data-driven aerodynamic design](https://arxiv.org/abs/2512.14397)
*Yunjia Yang,Weishao Tang,Mengxin Liu,Nils Thuerey,Yufei Zhang,Haixin Chen*

Main category: cs.LG

TL;DR: SuperWing是一个包含4,239个参数化机翼几何形状和28,856个RANS流场解的开源数据集，用于训练可泛化的三维机翼气动预测模型


<details>
  <summary>Details</summary>
Motivation: 现有三维机翼气动数据集稀缺且多样性有限，限制了机器学习代理模型在气动设计中的通用性发展

Method: 使用简化的几何参数化方法生成机翼形状，包含翼展方向的翼型变化、扭转和上反角，在典型飞行包线内进行RANS模拟

Result: 在SuperWing上训练的Transformer模型能准确预测表面流动，在保留样本上达到2.5阻力计数误差，并在DLR-F6和NASA CRM等复杂基准机翼上表现出强大的零样本泛化能力

Conclusion: SuperWing数据集具有足够的多样性和实用性，能够支持开发可泛化的三维机翼气动预测模型，促进气动设计加速

Abstract: Machine-learning surrogate models have shown promise in accelerating aerodynamic design, yet progress toward generalizable predictors for three-dimensional wings has been limited by the scarcity and restricted diversity of existing datasets. Here, we present SuperWing, a comprehensive open dataset of transonic swept-wing aerodynamics comprising 4,239 parameterized wing geometries and 28,856 Reynolds-averaged Navier-Stokes flow field solutions. The wing shapes in the dataset are generated using a simplified yet expressive geometry parameterization that incorporates spanwise variations in airfoil shape, twist, and dihedral, allowing for an enhanced diversity without relying on perturbations of a baseline wing. All shapes are simulated under a broad range of Mach numbers and angles of attack covering the typical flight envelope. To demonstrate the dataset's utility, we benchmark two state-of-the-art Transformers that accurately predict surface flow and achieve a 2.5 drag-count error on held-out samples. Models pretrained on SuperWing further exhibit strong zero-shot generalization to complex benchmark wings such as DLR-F6 and NASA CRM, underscoring the dataset's diversity and potential for practical usage.

</details>


### [67] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: GRAFT是一个用于电力负荷预测的模型，通过文本引导融合和多源文本干预，在多个时间尺度上提升预测精度，特别针对电网感知预测需求。


<details>
  <summary>Details</summary>
Motivation: 电力负荷同时受到天气、日历节奏、突发事件和政策等多时间尺度外生因素的影响，现有模型难以有效整合这些多源文本信息进行电网感知预测。

Method: 改进STanHOP模型，严格对齐每日汇总的新闻、社交媒体和政策文本与半小时负荷数据，通过交叉注意力机制实现文本引导融合到特定时间位置，并提供即插即用的外部内存接口。

Result: 在澳大利亚五个州的2019-2021年基准数据集上，GRAFT在小时、日和月三个时间尺度上显著优于强基线，达到或超过最先进水平，在事件驱动场景中表现稳健，并能通过注意力机制实现文本到负荷影响的时空定位和源级解释。

Conclusion: GRAFT通过有效整合多源文本信息，显著提升了电力负荷预测的准确性和可解释性，为电网感知预测提供了标准化评估框架和可复现的研究基础。

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [68] [Dual-Axis RCCL: Representation-Complete Convergent Learning for Organic Chemical Space](https://arxiv.org/abs/2512.14418)
*Dejun Hu,Zhiming Li,Jia-Rui Shen,Jia-Ning Tu,Zi-Hao Ye,Junliang Zhang*

Main category: cs.LG

TL;DR: 提出双轴表示-完全收敛学习策略，通过结合局部价环境和环/笼拓扑的分子表示，实现化学空间的收敛学习，并开发FD25数据集验证该框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习正在重塑分子和材料建模，但化学空间极其庞大（10^30-10^60），模型能否在这种空间上实现收敛学习仍是一个开放的科学问题。

Method: 引入双轴表示-完全收敛学习策略，结合图卷积网络编码局部价环境（基于现代价键理论）和无桥图编码环/笼拓扑，提供化学空间覆盖的定量度量，并开发FD25数据集系统覆盖13,302个局部价单元和165,726个环/笼拓扑。

Result: 在FD25上训练的图神经网络表现出表示-完全收敛学习和强大的分布外泛化能力，在外部基准测试中整体预测误差约为1.0 kcal/mol MAE。

Conclusion: 建立了分子表示、结构完整性和模型泛化之间的定量联系，为可解释、可迁移和数据高效的分子智能奠定了基础。

Abstract: Machine learning is profoundly reshaping molecular and materials modeling; however, given the vast scale of chemical space (10^30-10^60), it remains an open scientific question whether models can achieve convergent learning across this space. We introduce a Dual-Axis Representation-Complete Convergent Learning (RCCL) strategy, enabled by a molecular representation that integrates graph convolutional network (GCN) encoding of local valence environments, grounded in modern valence bond theory, together with no-bridge graph (NBG) encoding of ring/cage topologies, providing a quantitative measure of chemical-space coverage. This framework formalizes representation completeness, establishing a principled basis for constructing datasets that support convergent learning for large models. Guided by this RCCL framework, we develop the FD25 dataset, systematically covering 13,302 local valence units and 165,726 ring/cage topologies, achieving near-complete combinatorial coverage of organic molecules with H/C/N/O/F elements. Graph neural networks trained on FD25 exhibit representation-complete convergent learning and strong out-of-distribution generalization, with an overall prediction error of approximately 1.0 kcal/mol MAE across external benchmarks. Our results establish a quantitative link between molecular representation, structural completeness, and model generalization, providing a foundation for interpretable, transferable, and data-efficient molecular intelligence.

</details>


### [69] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: ClimaX-LETKF是首个纯数据驱动的机器学习集合天气预报系统，通过同化真实观测数据实现多年稳定运行，不依赖传统数值天气预报模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习天气预报虽取得进展，但缺乏在MLWP模型中同化真实观测或集合预报的研究。需要开发独立于NWP模型的纯数据驱动集合预报系统。

Method: 开发ClimaX-LETKF系统，使用NCEP ADP全球高空和地面天气观测数据，采用松弛到先验扰动(RTPP)方法，与松弛到先验扩展(RTPS)进行对比。

Result: 系统运行稳定且准确，RTPP比RTPS表现更好，而NWP模型通常更稳定于RTPS。发现MLWP模型恢复大气场到吸引子的能力弱于NWP模型。

Conclusion: 这项工作为增强MLWP集合预报系统提供了重要见解，代表了向实际应用迈出的重要一步，揭示了MLWP与NWP在扰动处理上的差异。

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [70] [Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection](https://arxiv.org/abs/2512.14563)
*Tejaswani Dash,Gautam Datla,Anudeep Vurity,Tazeem Ahmad,Mohd Adnan,Saima Rafi,Saisha Patro,Saina Patro*

Main category: cs.LG

TL;DR: 提出Residual GRU with Multi-Head Self-Attention模型，用于临床表格数据的心血管疾病预测，在UCI心脏病数据集上优于传统方法和深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要可靠高效的预测工具。传统方法依赖手工特征和临床经验，机器学习方法虽改善可重复性但难以在噪声和异构临床数据上泛化。

Method: 提出紧凑深度学习架构：集成残差双向门控循环单元用于特征列的序列建模、通道重加权块、多头自注意力池化与可学习分类标记来捕获全局上下文。

Result: 在UCI心脏病数据集上，5折分层交叉验证显示：准确率0.861，宏F1 0.860，ROC-AUC 0.908，PR-AUC 0.904，优于所有基线（逻辑回归、随机森林、SVM、DeepMLP、CNN、RNN、Transformer）。

Conclusion: 轻量级混合循环和注意力架构在临床风险预测中实现了准确性和效率的良好平衡，支持在资源受限的医疗环境中部署。

Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

</details>


### [71] [AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts](https://arxiv.org/abs/2512.14461)
*Niklas Grieger,Jannik Raskob,Siamak Mehrkanoon,Stephan Bialonski*

Main category: cs.LG

TL;DR: AnySleep是一个深度神经网络模型，能够使用任意EEG或EOG数据在可调时间分辨率下进行睡眠分期，在21个数据集的19,000多个夜间记录上训练，性能达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 睡眠研究需要手动睡眠分期，这是一个劳动密集型步骤。传统PSG记录以30秒为周期评分是出于实用而非生理原因，且不同中心在电极数量、配置和受试者特征上差异很大，这给多中心睡眠研究和在更短时间尺度上发现新生物标志物带来了挑战。

Method: 开发了AnySleep深度神经网络模型，使用任何EEG或EOG数据在可调时间分辨率下进行睡眠分期。模型在21个数据集的19,000多个夜间记录（近200,000小时EEG和EOG数据）上训练和验证，以促进跨站点的稳健泛化。

Result: 模型达到最先进性能，在30秒周期上超越或等于现有基线。提供更多通道时性能提高，但即使EOG缺失或仅使用EOG或单个EEG导联（额部、中央或枕部）时性能仍然强劲。在30秒以下时间尺度上，模型能捕捉与觉醒一致的短暂清醒入侵，并相对于标准30秒评分，改进了对生理特征（年龄、性别）和病理生理状况（睡眠呼吸暂停）的预测。

Conclusion: AnySleep模型能够处理异质电极设置，促进大规模睡眠研究，并加速睡眠中新生物标志物的发现。模型已公开可用，以支持这些目标。

Abstract: Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.

</details>


### [72] [Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes](https://arxiv.org/abs/2512.14617)
*Alessandro Trapasso,Luca Iocchi,Fabio Patrizi*

Main category: cs.LG

TL;DR: 提出QR-MAX算法，首个针对离散动作非马尔可夫奖励决策过程的模型强化学习方法，具有PAC收敛保证和多项式样本复杂度，并扩展到连续状态空间。


<details>
  <summary>Details</summary>
Motivation: 许多实际决策问题涉及依赖于整个系统历史而非特定状态的任务，马尔可夫强化学习方法不适用于此类任务，而非马尔可夫奖励决策过程方法缺乏形式化保证。

Method: 提出QR-MAX算法，通过奖励机将马尔可夫转移学习与非马尔可夫奖励处理分离；扩展到连续状态空间时使用SimHash-based离散化方法保持相同分解结构。

Result: QR-MAX是首个针对离散动作NMRDPs的模型强化学习算法，具有PAC收敛到ε-最优策略的多项式样本复杂度；实验显示在样本效率和策略最优性方面显著优于现有方法。

Conclusion: QR-MAX解决了非马尔可夫奖励决策过程强化学习的形式化保证问题，提供了高效且可扩展的解决方案，在复杂环境中表现出优越性能。

Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

</details>


### [73] [Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics](https://arxiv.org/abs/2512.14471)
*Additi Pandey,Liang Wei,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: Kinetic-Mamba：基于Mamba架构的神经算子框架，用于准确预测燃烧化学动力学演化，包含三种模型变体，在Syngas和GRI-Mech 3.0反应机制上表现出高保真度。


<details>
  <summary>Details</summary>
Motivation: 精确的化学动力学建模对燃烧模拟至关重要，传统方法难以处理复杂的反应路径和热化学状态演化。需要开发能够准确预测动力学行为的新框架。

Method: 提出Kinetic-Mamba框架，结合神经算子的表达能力与Mamba架构的高效时序建模能力。包含三种模型：独立Mamba模型、强制质量守恒的约束Mamba模型、以及基于温度区域的机制感知架构。还开发了在降维潜空间中演化动力学的变体。

Result: 在Syngas和GRI-Mech 3.0反应机制上的计算实验表明，该框架仅使用状态变量的初始条件就能高保真地预测复杂动力学行为。通过时间分解和递归预测策略评估了准确性和鲁棒性，并测试了在分布外数据集上的外推能力。

Conclusion: Kinetic-Mamba框架成功地将Mamba架构与神经算子结合，为燃烧化学动力学建模提供了高效准确的解决方案，能够处理复杂反应路径并保持质量守恒等物理约束。

Abstract: Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.

</details>


### [74] [gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation](https://arxiv.org/abs/2512.14658)
*Alban Puech,Matteo Mazzonelli,Celia Cintas,Tamara R. Govindasamy,Mangaliso Mngomezulu,Jonas Weiss,Matteo Baù,Anna Varbella,François Mirallès,Kibaek Kim,Le Xie,Hendrik F. Hamann,Etienne Vos,Thomas Brunschwiler*

Main category: cs.LG

TL;DR: gridfm-datakit-v1是一个Python库，用于生成真实多样的电力潮流和最优潮流数据集，解决现有工具在随机负荷扰动、拓扑变化、运行限制和成本函数方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有电力系统数据集面临三个主要挑战：1) 缺乏真实的随机负荷和拓扑扰动，限制了场景多样性；2) 电力潮流数据集仅限于最优潮流可行点，阻碍了机器学习求解器对违反运行限制情况的泛化能力；3) 最优潮流数据集使用固定的发电机成本函数，限制了在不同成本情况下的泛化能力。

Method: 通过结合真实世界负荷曲线的全局负荷缩放与局部噪声，支持任意的N-k拓扑扰动来创建多样且真实的数据集；生成超出运行限制的电力潮流样本；生成具有变化发电机成本的最优潮流数据；并高效扩展到大型电网（最多10,000个节点）。

Result: 开发了gridfm-datakit-v1库，能够生成真实多样的电力潮流和最优潮流数据集，解决了现有工具的局限性。该库已开源在GitHub上，可通过pip安装，并与OPFData、OPF-Learn、PGLearn和PFΔ等现有工具进行了比较。

Conclusion: gridfm-datakit-v1为训练机器学习求解器提供了更全面、真实和多样的电力系统数据集，解决了现有数据集在随机扰动、运行限制和成本变化方面的局限性，有助于提高机器学习求解器的泛化能力和实用性。

Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.

</details>


### [75] [Improving Slow Transfer Predictions: Generative Methods Compared](https://arxiv.org/abs/2512.14522)
*Jacob Taegon Kim,Alex Sim,Kesheng Wu,Jinoh Kim*

Main category: cs.LG

TL;DR: 该研究针对科学计算网络中数据传输性能预测的类别不平衡问题，通过比较多种数据增强策略（包括传统过采样方法和生成技术）以及调整训练数据中的类别不平衡比例，发现随着不平衡比例增加，性能提升不显著，即使最先进的CTGAN技术也没有明显优于简单的分层抽样。


<details>
  <summary>Details</summary>
Motivation: 科学计算网络中监测数据传输性能至关重要，通过早期预测性能可以识别潜在缓慢传输并选择性监控，优化网络使用和整体性能。然而，机器学习模型在此场景下的预测能力受到类别不平衡问题的瓶颈限制。

Method: 研究分析比较了多种数据增强策略，包括传统过采样方法和生成技术（如CTGAN），同时调整训练数据集中的类别不平衡比例，评估这些方法对模型性能的影响。

Result: 虽然数据增强可能改善性能，但随着不平衡比例增加，性能提升并不显著。即使最先进的CTGAN技术也没有明显优于简单的分层抽样方法。

Conclusion: 在科学计算网络性能预测的类别不平衡问题中，复杂的数据增强技术（如CTGAN）相对于简单的分层抽样方法并未带来显著性能提升，特别是在高度不平衡的情况下。

Abstract: Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.

</details>


### [76] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: 该研究使用变分自编码器生成合成心房电图，以解决心房颤动非侵入性成像中配对数据稀缺问题，并通过数据增强提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 心房颤动是最常见的心律失常，需要准确表征心房电活动。非侵入性心电图成像结合深度学习估计心内电图具有潜力，但受限于配对体表电位-心内电图数据集的稀缺性。

Method: 提出两种变分自编码器模型：窦性心律特异性VAE（VAE-S）和类别条件VAE（VAE-C），后者在窦性心律和房颤信号上训练。通过形态学、频谱和分布相似性指标评估生成的心电图。

Result: VAE-S在模拟心电图上实现更高保真度，VAE-C支持节律特异性生成但窦性重建质量降低。生成的心电图用于下游非侵入性心电图重建任务的数据增强，适度增强可提高估计性能。

Conclusion: 基于VAE的生成模型有潜力缓解数据稀缺问题，增强基于深度学习的非侵入性心电图成像流程。

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [77] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 该论文批评现有时间序列反事实解释方法在临床推荐场景中的不足，指出其过于关注最小输入扰动而忽视因果合理性和时间连贯性，并呼吁开发更实用、用户中心的反事实方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类的反事实解释方法主要基于静态数据假设，专注于生成最小输入扰动来翻转模型预测，这在临床推荐场景中不够充分，因为临床干预是随时间展开的，需要因果合理且时间连贯。

Method: 通过分析现有时间序列反事实方法的局限性，识别出时间盲点和缺乏用户中心考虑等关键缺陷，并对几种最先进方法进行鲁棒性分析，展示其对随机噪声的高度敏感性。

Result: 研究发现生成的反事实解释对随机噪声高度敏感，这在真实临床环境中测量变异不可避免的情况下，限制了这些方法的可靠性。现有方法在现实临床环境中的实用性有限。

Conclusion: 呼吁开发超越单纯预测改变的方法和评估框架，强调需要可操作、目的驱动的干预措施，这些措施在现实环境中对用户是可行的，并符合临床推理和患者特定动态。

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [78] [Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs](https://arxiv.org/abs/2512.14596)
*Youngkyu Lee,Francesc Levrero Florencio,Jay Pathak,George Em Karniadakis*

Main category: cs.LG

TL;DR: Geo-DeepONet：一种几何感知的深度算子网络，结合有限元离散化提取的域信息，可在任意非结构化网格上进行算子学习而无需重新训练。基于此开发了几何感知混合预条件迭代求解器，在多样非结构域上的参数PDE中展现出增强的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统迭代求解器对参数偏微分方程（PDEs）的收敛行为高度依赖于域和特定离散化。先前提出的混合求解器（结合经典求解器和神经算子）在特定几何上有效，但在训练未遇到的几何上表现不佳。需要解决这一挑战。

Method: 引入Geo-DeepONet（几何感知深度算子网络），该网络结合从有限元离散化提取的域信息，可在任意非结构化网格上进行算子学习而无需重新训练。基于此开发了一类几何感知混合预条件迭代求解器，将Geo-DeepONet与传统方法（如松弛方案和Krylov子空间算法）耦合。

Result: 通过在多样非结构域上的参数PDE数值实验，证明了所提出的混合求解器在多个实际应用中具有增强的鲁棒性和效率。

Conclusion: Geo-DeepONet及其衍生的几何感知混合预条件迭代求解器能够有效解决传统迭代求解器对几何变化的敏感性问题，在多样非结构域上实现了鲁棒且高效的参数PDE求解。

Abstract: The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.

</details>


### [79] [Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets](https://arxiv.org/abs/2512.14615)
*Omid Khormali*

Main category: cs.LG

TL;DR: 提出了一种新的拓扑数据分析方法OW-HNPV，用于检测时变网络中的异常，通过测量特征出现和消失的速率，在以太坊交易网络中实现了比基线模型高达10.4%的AUC提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要测量累积拓扑存在，缺乏对拓扑特征变化速率的关注。需要一种能够自动降噪、数学稳定且适用于不同类型特征网络的异常检测方法。

Method: 提出了重叠加权分层归一化持久性速度(OW-HNPV)，这是第一个基于速度的持久性图视角方法，通过重叠加权自动降噪，并证明了其数学稳定性。

Result: 在以太坊交易网络(2017年5月-2018年5月)的异常检测中，OW-HNPV在7天价格变动预测上比基线模型提升高达10.4%的AUC。在中长期预测(4-7天)方面表现优异，比VAB、持久性景观和持久性图像等方法提供更一致稳定的性能。

Conclusion: 建模拓扑速度对于检测动态网络中的结构异常至关重要，OW-HNPV作为一种数学稳定的速度基方法，在加密货币异常检测中表现出优越性能。

Abstract: We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.

</details>


### [80] [ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning](https://arxiv.org/abs/2512.14619)
*Chaohao Yuan,Zhenjie Song,Ercan Engin Kuruoglu,Kangfei Zhao,Yang Liu,Deli Zhao,Hong Cheng,Yu Rong*

Main category: cs.LG

TL;DR: ParaFormer是一种PageRank增强的图Transformer，通过自适应滤波机制缓解传统图Transformer中的过度平滑问题，在节点和图分类任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统图Transformer虽然通过全局注意力解决了深度GNN的过度平滑问题，但研究发现全局注意力本身也存在严重的过度平滑现象，导致节点表示变得难以区分，这比GNN中的过度平滑效应更强。

Method: 提出PageRank Transformer (ParaFormer)，包含PageRank增强的注意力模块，模拟深度Transformer的行为。该方法通过自适应滤波机制来缓解过度平滑问题。

Result: 在11个数据集（从数千到数百万节点）的节点分类和图分类任务中，ParaFormer均取得了稳定的性能提升，验证了其有效性。

Conclusion: ParaFormer通过PageRank增强的注意力模块有效缓解了图Transformer中的过度平滑问题，在多种图学习任务上表现出色，为图Transformer的发展提供了新思路。

Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

</details>


### [81] [Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks](https://arxiv.org/abs/2512.14675)
*Rae Chipera,Jenny Du,Irene Tsapara*

Main category: cs.LG

TL;DR: 研究非光滑激活函数在回声状态网络中的表现，发现某些非光滑函数（如康托函数）在收敛速度和谱半径容忍度上优于传统光滑函数，并提出了量化激活函数的理论框架。


<details>
  <summary>Details</summary>
Motivation: 当前储层计算主要依赖光滑、全局Lipschitz连续的激活函数，限制了在需要极端条件下鲁棒操作的领域（如国防、灾害响应、药物建模）的应用。研究旨在探索非光滑激活函数的潜力。

Method: 系统研究非光滑激活函数（包括混沌、随机和分形变体）在回声状态网络中的表现。通过36,610个储层配置的全面参数扫描，分析不同激活函数的性能。引入量化激活函数的理论框架，定义退化回声状态属性(d-ESP)。

Result: 多个非光滑函数不仅保持回声状态属性(ESP)，而且在收敛速度和谱半径容忍度上优于传统光滑激活函数。康托函数保持ESP一致行为至谱半径ρ~10，比光滑函数典型界限高一个数量级，收敛速度比tanh和ReLU快2.6倍。发现预处理拓扑而非连续性本身决定稳定性。

Conclusion: 研究挑战了储层计算中激活函数设计的传统假设，表明非光滑函数在某些应用中可能更优。但某些分形函数优异性能的机制仍未解释，表明对激活函数几何特性如何影响储层动态的理解存在根本性空白。

Abstract: Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.

</details>


### [82] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: 开发了一个多模态机器学习框架EWI，通过整合临床和运营数据预测ICU入院、急救团队派遣和死亡风险，在18,633名患者数据上达到C统计量0.796，已部署为医院分诊工具。


<details>
  <summary>Details</summary>
Motivation: 医院缺乏自动化系统来利用日益增长的异构临床和运营数据有效预测关键事件。早期识别有恶化风险的患者对于患者护理质量监测和医生护理管理都至关重要，但将不同数据流转化为准确可解释的风险评估面临数据格式不一致的挑战。

Method: 开发了多模态机器学习框架EWI（早期预警指数），采用人机协作流程：临床医生帮助确定警报阈值和解释模型输出，使用SHAP可解释性方法突出显示驱动每个患者风险的临床和运营因素（如计划手术、病房人数）。从结构化和非结构化电子健康记录数据中自动提取特征。

Result: 在美国一家大型医院的18,633名独特患者数据集上，EWI实现了C统计量0.796。已部署在医院仪表板中，将患者分为三个风险层级，作为主动管理风险患者的分诊工具。

Conclusion: 该方法通过自动对不同风险水平的患者进行分类，为医生节省了宝贵时间，使他们能够专注于患者护理而非筛选复杂的EHR数据。通过进一步识别特定风险驱动因素，为护理人员调度和关键资源分配提供数据驱动的调整依据，有助于预防下游并发症并改善整体患者流程。

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [83] [Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records](https://arxiv.org/abs/2512.13700)
*Mitchell A. Klusty,Elizabeth C. Solie,Caroline N. Leach,W. Vaiden Logan,Lynnet E. Richey,John C. Gensel,David P. Szczykutowicz,Bryan C. McLellan,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 提出一个基于本地部署大语言模型的自动化临床特征提取框架，用于替代耗时的人工病历审查，提高临床研究效率


<details>
  <summary>Details</summary>
Motivation: 人工病历审查耗时耗力，需要专家从非结构化的电子健康记录中提取复杂信息，成为临床研究的瓶颈

Method: 开发了一个安全、模块化的框架，在符合HIPAA标准的本地计算基础设施上部署大语言模型，结合检索增强生成和结构化响应方法，提供可扩展的容器化特征提取方案

Result: 该框架在大量患者病历中提取多种医学特征时达到高准确率，与专家标注数据集相比表现优异，甚至发现了人工审查中遗漏的标注错误

Conclusion: 该框架展示了LLM系统通过自动化特征提取减轻人工病历审查负担、提高数据捕获一致性的潜力，能够加速临床研究进程

Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.

</details>


### [84] [Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference](https://arxiv.org/abs/2512.13701)
*Zheng Xing,Junting Chen*

Main category: cs.AI

TL;DR: 提出了一种无需位置标签的盲无线电地图构建框架，利用室内MIMO-OFDM信道测量推断用户轨迹，实现了0.68米的平均定位误差和3.3%的波束图重建误差。


<details>
  <summary>Details</summary>
Motivation: 传统无线电地图构建方法需要大量位置标签数据，成本高昂且在实际场景中不实用。本文旨在开发一种无需位置标签的盲构建方法，利用信道测量推断用户轨迹。

Method: 首先证明在准镜面环境模型下，非视距信道状态信息具有空间连续性，推导出与物理距离成正比的CSI-距离度量。针对泊松分布的AP部署中的直线轨迹，证明即使角度分辨率较差，定位误差的克拉美-罗下界也能渐近消失。基于这些理论结果，开发了空间正则化贝叶斯推理框架，联合估计信道特征、区分视距/非视距条件并恢复用户轨迹。

Result: 在射线追踪数据集上的实验表明，平均定位误差为0.68米，波束图重建误差为3.3%，验证了所提出的盲映射方法的有效性。

Conclusion: 本文提出的盲无线电地图构建框架能够在无需位置标签的情况下，仅通过信道测量有效推断用户轨迹并重建无线电地图，为智能无线应用提供了实用且成本效益高的解决方案。

Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.

</details>


### [85] [Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents](https://arxiv.org/abs/2512.13704)
*Doohee You,Sundeep Paul*

Main category: cs.AI

TL;DR: Adjudicator是一个用于自动识别和纠正标签噪声的神经符号系统，通过构建知识图谱和多智能体LLM架构实现高精度数据验证，在AlleNoise基准测试中达到0.99 F1分数。


<details>
  <summary>Details</summary>
Motivation: 生产机器学习系统的性能受限于训练数据质量，在工业应用中，噪声标签会降低性能并损害用户信任，需要自动化的高精度数据验证解决方案。

Method: 采用神经符号方法：1) 构建动态知识图谱统一项目上下文；2) 设计"智能体委员会"多智能体LLM架构，让专门智能体辩论和投票标签有效性；3) 使用知识图谱驱动的覆盖逻辑识别复杂结构性错误。

Result: 在AlleNoise基准的1000项平衡子集上，知识图谱增强模型达到0.99 F1分数，显著优于单LLM基线(0.48 F1)和无知识图谱委员会(0.59 F1)。系统实现了完美的复杂结构性错误召回率。

Conclusion: Adjudicator提供了一个稳健且可解释的自动化高精度数据验证系统，为严格监管的工业环境中生成黄金数据集提供了重要概念验证。

Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.

</details>


### [86] [LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms](https://arxiv.org/abs/2512.13713)
*Ali Parsaee,Yashar Talebirad,Csongor Szepesvári,Vishwajeet Ohal,Eden Redman*

Main category: cs.AI

TL;DR: LoopBench是一个评估大语言模型在分布式对称性打破和元认知推理能力的基准测试，专注于奇数环图着色问题，展示了高级推理模型能设计出避免死锁的策略。


<details>
  <summary>Details</summary>
Motivation: 大语言模型越来越多地被用作自主代理，但它们在分布式系统中的协调能力仍然未被充分理解。需要评估LLM在分布式对称性打破和元认知推理方面的能力。

Method: 引入LoopBench基准测试，专注于奇数环图（C3、C5、C11）的有限颜色着色问题，其中确定性、非通信代理会陷入无限循环。实现策略传递机制作为一致内存形式。

Result: 标准LLM和经典启发式方法难以解决该问题，但高级推理模型（如O3）能够设计出避免死锁的策略。LoopBench为研究基于语言推理的涌现分布式算法提供了测试平台。

Conclusion: LoopBench为评估LLM在分布式系统中的协调能力提供了有效基准，展示了高级推理模型在解决分布式对称性打破问题上的潜力，为集体智能研究提供了新工具。

Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.

</details>


### [87] [AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach](https://arxiv.org/abs/2512.13714)
*Gangesh Pathak,Prasanna Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种基于AI的标注流程，用于系统识别、标记和修复LLM输出的不稳定性模式，通过人机协同方法结合自动弱监督和基于置信度的标注，提高LLM在高度监管行业中的可靠性和稳定性。


<details>
  <summary>Details</summary>
Motivation: LLM在高度监管行业中应用受限，主要因为不稳定性问题、不一致推理、幻觉和性能波动，特别是在工作流程中。这些可靠性问题限制了LLM在需要事实精确性和一致行为的领域的安全使用。现有的稳定化方法如RLHF和监督微调虽然有效但成本高昂，依赖大量人工标注，难以可持续扩展。

Method: 提出AI驱动的标注流程，采用人机协同方法：结合自动弱监督模型和基于置信度的标注，辅以目标人工验证。引入稳定性特定标注类别（语义一致性、事实正确性、逻辑连贯性），通过反馈循环实现模型的持续校准和鲁棒性增强。

Result: 该方法能够系统识别和修复LLM输出的不稳定性模式，保证反馈信息的可靠性和道德完整性。通过持续校准机制增强模型鲁棒性，相比传统RLHF和监督微调方法更具可扩展性和可持续性。

Conclusion: 提出的AI标注流程为人机协同稳定LLM提供了可行方案，通过系统性识别和修复不稳定性模式，能够提高LLM在高度监管行业中的可靠性，同时降低对昂贵人工标注的依赖，实现更可持续的模型稳定化。

Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).

</details>


### [88] [Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN](https://arxiv.org/abs/2512.13715)
*Fatemeh Lotfi,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 本文提出了一种基于元层次强化学习的自适应框架，用于O-RAN中的资源分配和网络切片联合优化，相比基线方法提升了19.8%的网络管理效率


<details>
  <summary>Details</summary>
Motivation: 现代应用日益复杂，需要能够实时适应和高效管理资源的无线网络。O-RAN架构及其RIC模块为动态资源管理和网络切片提供了关键解决方案。虽然AI驱动的方法显示出潜力，但大多数方法在不可预测和高度动态的条件下难以保持性能。

Method: 提出自适应元层次强化学习框架，受MAML启发，将层次控制与元学习结合：高层控制器跨切片分配资源，低层代理执行切片内调度。自适应元更新机制根据时间差分误差方差对任务加权，提高稳定性并优先处理复杂网络场景。

Result: 仿真结果显示，相比基线RL和元RL方法，网络管理效率提升19.8%，具有更快的适应速度和更高的QoS满意度（覆盖eMBB、URLLC和mMTC切片）。消融和可扩展性研究证实了方法的鲁棒性，适应速度提升高达40%，在网络规模增加时保持一致的公平性、延迟和吞吐量性能。

Conclusion: 该自适应元层次强化学习框架有效解决了O-RAN中动态资源管理和网络切片的挑战，通过理论分析和实验验证证明了其优越性能、快速适应能力和可扩展性。

Abstract: The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.

</details>


### [89] [ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making](https://arxiv.org/abs/2512.13716)
*Yitong Luo,Ziang Chen,Hou Hei Lam,Jiayu zhan,Junqi Wang,Zhenliang Zhang,Xue Feng*

Main category: cs.AI

TL;DR: ValuePilot框架通过价值驱动的个性化决策，使AI代理能够根据个人价值偏好进行决策，在未见场景中超越主流LLM基线


<details>
  <summary>Details</summary>
Motivation: 随着AI系统扩展到现实世界应用，适应超越任务完成或集体对齐的个性化价值已成为关键挑战。需要使AI代理能够根据个人用户的价值偏好进行决策，实现人机交互中的个性化决策

Method: 提出ValuePilot两阶段框架：1) 数据集生成工具包(DGT)：通过人-LLM协作管道构建多样化的价值标注场景；2) 决策模块(DMM)：学习基于个人价值偏好评估行动，实现上下文敏感的个性化决策

Result: 在未见场景评估中，DMM在与人行动选择对齐方面超越了GPT-5、Claude-Sonnet-4、Gemini-2-flash和Llama-3.1-70b等强大LLM基线

Conclusion: 价值驱动的决策是构建可解释、个性化AI代理的有效且可扩展的工程路径，人类价值作为稳定、可转移的信号支持跨上下文的一致和可泛化行为

Abstract: Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.

</details>


### [90] [Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy](https://arxiv.org/abs/2512.13725)
*Steve Nwaiwu,Nipat Jongsawat,Anucha Tungkasthan*

Main category: cs.AI

TL;DR: 本文首次系统评估了量化（INT8和NF4）对大型语言模型因果推理能力的影响，发现因果推理对4位量化具有意外鲁棒性，干预查询最敏感，而现有反事实基准未能捕捉深层因果脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署向边缘和资源受限环境转移，量化模型（如INT8和NF4）成为标准，但精度降低对形式化因果推理的影响尚不清楚。本研究旨在填补这一空白，首次系统评估量化在Pearl因果阶梯所有三个层次上的影响。

Method: 使用3000个样本分层的CLadder基准测试，评估Llama 3 8B在不同量化精度下的因果推理能力。在CRASS基准上测试常识反事实推理，并评估图检索增强生成（Graph RAG）使用真实因果图的效果。

Result: 因果推理在量化下总体稳定，NF4仅导致不到1%的总体退化。干预查询（阶梯2）对精度损失最敏感，反事实推理（阶梯3）相对稳定但在特定查询类型中存在异质性弱点。CRASS基准显示各精度间性能几乎相同，表明现有基准缺乏揭示量化诱导推理漂移的结构敏感性。图RAG使NF4干预准确率提升1.7%，部分抵消压缩相关退化。

Conclusion: 因果推理对4位量化具有意外鲁棒性；图结构化增强可选择性强化干预推理；当前反事实基准未能捕捉深层因果脆弱性。本研究为压缩因果推理提供了初步经验图谱，并为部署高效且结构支持的因果AI系统提供了实用指导。

Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.

</details>


### [91] [State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models](https://arxiv.org/abs/2512.13762)
*TK Lee*

Main category: cs.AI

TL;DR: 该研究提出了一种定性案例研究方法，用于审计大语言模型在长时程交互中的策略相关行为选择性，发现同一模型在非敏感领域表现正常，而在策略敏感领域则反复出现功能性拒绝，表现出行为不对称性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型被广泛部署为通用工具，但标准定量基准测试无法捕捉到长期交互中可能出现的特定行为模式。需要一种方法来审计模型在策略相关领域的行为选择性，特别是模型可能表现出的不一致行为模式。

Method: 采用定性案例研究方法，通过一个86轮对话会话进行审计。定义了三种响应机制：正常表现（NP）、功能性拒绝（FR）和元叙事（MN）。分析模型在不同领域（非敏感领域和策略敏感领域）的行为模式，并观察MN角色框架叙事与拒绝行为在敏感上下文中的共现关系。

Result: 研究发现同一模型在非敏感领域表现正常，而在提供商或策略敏感领域则反复出现功能性拒绝，形成了跨领域的一致不对称性。元叙事角色框架叙事倾向于与敏感上下文中的拒绝行为同时出现。研究引入了"习得性无能"（LI）作为描述这种选择性保留行为的概念框架。

Conclusion: 该研究提出了基于可观察行为的交互层面审计框架，并将习得性无能作为检查潜在对齐副效应的视角。这种不对称行为模式值得在不同用户和模型中进行进一步调查，以理解大语言模型在长期交互中的行为特征。

Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.

</details>


### [92] [Mathematics and Coding are Universal AI Benchmarks](https://arxiv.org/abs/2512.13764)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 该论文研究了数学和编程在AI智能体心理测量电池模空间中的特殊作用，证明了在特定条件下，由数学定理证明和编程任务生成的电池子空间在评估度量下是稠密的。


<details>
  <summary>Details</summary>
Motivation: 研究数学和编程在AI智能体评估中的特殊地位，探索它们作为"通用坐标"的潜力，以及形式化数学如何成为高级AI智能体递归自我改进的自然启动领域。

Method: 基于AAI框架和GVU动力学，定义了数学纤维，结合形式证明内核（如Lean、Coq），分析GVU流在该纤维上的谱稳定自我改进机制。主要技术结果是密度定理：在智能体输出均匀紧性和Lipschitz AAI泛函条件下，证明数学定理证明和编程任务生成的电池子空间在模空间中关于评估度量是稠密的。

Result: 编程单独就具有这种意义上的普适性，而纯数学则不具备；数学的特权是谱特性而非表达能力。这表明数学和编程为评估提供了"通用坐标"，形式化数学是高级AI智能体递归自我改进的自然启动域。

Conclusion: 数学和编程在AI智能体评估中具有特殊地位，形式化数学与编程任务相结合可以生成稠密的评估电池空间，这为AI的递归自我改进提供了理论基础和实用框架。

Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.

</details>


### [93] [Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems](https://arxiv.org/abs/2512.13771)
*Javier Marín*

Main category: cs.AI

TL;DR: 论文提出语义接地指数(SGI)，通过计算回答与问题vs.上下文的角度距离比来检测RAG系统中的幻觉，发现幻觉回答在嵌入空间中更接近问题而非上下文，形成"语义懒惰"现象。


<details>
  <summary>Details</summary>
Motivation: 研究检索增强生成(RAG)系统产生幻觉时在嵌入空间中留下的几何痕迹，开发一种计算高效、理论基础的指标来识别需要验证的响应。

Method: 引入语义接地指数(SGI)，定义为在单位超球面上回答到问题与回答到上下文的角度距离比。利用球面三角不等式理论推导，并通过HaluEval数据集(n=5,000)在五个嵌入模型上进行实证验证。

Result: 发现"语义懒惰"现象：幻觉回答在角度上更接近问题而非上下文。效应大小Cohen's d在0.92-1.28之间，跨模型相关性r=0.85。SGI的判别能力随问题-上下文角度分离θ(q,c)增加而增强，效应大小从d=0.61提升到d=1.27，AUC从0.72提高到0.83。在长回答(d=2.05)和短问题(d=1.22)上表现优异。

Conclusion: SGI提供了一种计算高效、理论基础的框架，用于识别生产RAG部署中需要验证的响应。但需注意它测量的是主题参与度而非事实准确性，如在TruthfulQA上AUC仅为0.478。

Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.

</details>


### [94] [EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery](https://arxiv.org/abs/2512.13857)
*Kamer Ali Yuksel*

Main category: cs.AI

TL;DR: EvoLattice是一个基于有向无环图表示候选程序或智能体行为的进化框架，通过节点存储多个持久化替代方案，每个有效路径定义不同可执行候选，实现组合搜索空间而无需复制结构。


<details>
  <summary>Details</summary>
Motivation: 现有LLM进化方法大多依赖覆盖式突变，每次只保留单个候选，会丢弃有用变体、遭受破坏性编辑，并在易发生结构故障的脆弱搜索空间中探索。需要一种能保留成功组件、提供更稳定进化的方法。

Method: EvoLattice将整个候选程序或智能体行为群体表示为单个有向无环图，每个节点存储多个持久化替代方案。通过路径定义可执行候选，实现组合搜索空间。提供细粒度替代级评估，通过确定性自修复机制保证结构正确性，支持LLM引导的突变、重组和剪枝。

Result: 在程序合成（代理和优化器元学习）中，EvoLattice比先前LLM引导方法产生更稳定的进化、更强的表达能力和更好的改进轨迹。其动态类似于质量-多样性优化，从内部多替代表示中隐式涌现。

Conclusion: EvoLattice通过图结构表示整个候选群体，克服了传统覆盖式突变的局限性，实现了更稳定、表达性更强的进化过程，为LLM引导的程序和智能体进化提供了新框架。

Abstract: Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.

</details>


### [95] [MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2512.13955)
*Sindhuja Madabushi,Dawood Wasif,Jin-Hee Cho*

Main category: cs.AI

TL;DR: MURIM是一个基于多维信誉的联邦学习激励机制，通过综合考虑客户端可靠性、隐私、资源容量和公平性来分配激励，防止恶意客户端获得不当奖励。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临客户端激励不足、隐私风险和资源约束等挑战。评估客户端可靠性对于公平分配激励和确保每个客户端数据对全局模型有实质性贡献至关重要。

Method: 提出MURIM机制，基于客户端贡献、延迟和信誉分配激励，包含可靠性验证模块，联合考虑客户端可靠性、隐私、资源容量和公平性。

Result: 在MNIST、FMNIST和ADULT Income数据集上的实验显示，MURIM在公平性指标上提升达18%，隐私攻击成功率降低5-9%，对投毒和噪声梯度攻击的鲁棒性提升达85%。

Conclusion: MURIM有效缓解对抗性威胁，促进公平真实参与，在异构动态联邦学习环境中保持稳定的模型收敛。

Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.

</details>


### [96] [Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms](https://arxiv.org/abs/2512.13978)
*Yang Cao,Yubin Chen,Xuyang Guo,Zhao Song,Song Yue,Jiahao Zhang,Jiale Zhao*

Main category: cs.AI

TL;DR: 该论文对GPT-5-Thinking、Gemini-3-Pro、Claude-Sonnet-4.5-Thinking和Grok-4四个前沿大语言模型在《随机算法》教材中的定理证明能力进行了基准测试，发现顶级模型准确率约66%，但模型间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学推理和科学发现方面取得了显著进展，但缺乏对这些模型在经典研究生级数学理论上的严格评估。需要了解它们在基础推理能力方面的基准表现。

Method: 使用Motwani和Raghavan的《随机算法》教材作为测试基准，要求四个前沿模型（GPT-5-Thinking、Gemini-3-Pro、Claude-Sonnet-4.5-Thinking、Grok-4）为教材中的引理和练习生成正式的LaTeX证明。

Result: 顶级模型（Gemini和Claude）准确率约为66%，表现出对概率方法和形式逻辑的良好掌握；其他模型一致性较差，准确率约40%。定性分析显示在简洁性、幻觉率和逻辑结构方面存在差异。

Conclusion: 前沿模型已达到适合研究生级教学辅助和形式化的熟练程度门槛，但在严格数学推导的可靠性方面存在显著差异。代码和完整响应集已开源。

Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.

</details>


### [97] [ReflCtrl: Controlling LLM Reflection via Representation Engineering](https://arxiv.org/abs/2512.13979)
*Ge Yan,Chung-En Sun,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: ReflCtrl框架通过表征工程控制LLMs的自我反思行为，减少推理成本同时保持性能


<details>
  <summary>Details</summary>
Motivation: 虽然自我反思能提升推理性能，但会增加推理成本，需要研究如何有效控制反思行为

Method: 将模型推理分段，识别反思步骤，提取潜在空间中的反思方向，提出逐步引导方法ReflCtrl控制反思频率

Result: 1) 在许多情况下反思是冗余的，特别是更强的模型中（可节省33.6%的推理token同时保持性能）；2) 模型的反思行为与内部不确定性信号高度相关

Conclusion: 自我反思可能由模型的不确定性控制，ReflCtrl框架能有效管理反思行为，平衡性能与成本

Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.

</details>


### [98] [Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training](https://arxiv.org/abs/2512.13996)
*Can Jin,Hongwu Peng,Mingcan Xiang,Qixin Zhang,Xiangchi Yuan,Amit Hasan,Ohiremen Dibua,Yifan Gong,Yan Kang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: DTop-p MoE：一种稀疏可控的动态Top-p路由机制，通过PI控制器动态调整概率阈值，实现激活专家数量的精确控制，同时适应不同token和层的资源分配需求。


<details>
  <summary>Details</summary>
Motivation: 标准Top-k路由策略对所有token施加统一的稀疏模式，忽略了token难度的差异。而现有的Top-p路由通常依赖固定的全局概率阈值，导致计算成本不可控且对超参数选择敏感。

Method: 提出DTop-p MoE路由机制：1）使用比例积分（PI）控制器动态调整概率阈值，使运行中的激活专家稀疏度与指定目标对齐；2）引入动态路由归一化机制，调整层间路由logits，使不同层能学习不同的专家选择模式，同时使用全局概率阈值。

Result: 在大语言模型和扩散变换器上的实验表明，DTop-p在性能上持续优于Top-k和固定阈值Top-p基线。分析确认DTop-p能精确控制激活专家数量，同时自适应地在不同token和层间分配资源。

Conclusion: DTop-p在专家粒度、专家容量、模型规模和数据集大小方面展现出强大的扩展性，为大规模MoE预训练提供了一个稳健的框架。

Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

</details>


### [99] [MobileWorldBench: Towards Semantic World Modeling For Mobile Agents](https://arxiv.org/abs/2512.14014)
*Shufan Li,Konstantinos Kallidromitis,Akash Gokul,Yusuke Kato,Kazuki Kozuka,Aditya Grover*

Main category: cs.AI

TL;DR: 该研究提出了一种用于GUI智能体的自然语言世界模型方法，替代传统的像素空间预测，通过MobileWorldBench基准测试和MobileWorld数据集提升视觉语言模型的世界建模能力，并展示了语义世界模型如何提高移动智能体的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 传统基于像素空间的世界模型在GUI环境中面临实际限制，预测复杂视觉元素的未来状态很困难。因此需要探索GUI智能体的替代世界建模方法。

Method: 1. 引入MobileWorldBench基准测试评估视觉语言模型作为移动GUI智能体世界模型的能力；2. 发布包含140万样本的大规模MobileWorld数据集；3. 提出将VLM世界模型集成到移动智能体规划框架中的新框架。

Result: 研究表明语义世界模型可以直接提高移动智能体的任务成功率，代码和数据集已开源。

Conclusion: 自然语言描述状态转换的世界模型方法在GUI智能体领域具有实用价值，能够克服像素空间预测的局限性，提升智能体的任务性能。

Abstract: World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld

</details>


### [100] [Evaluating Small Language Models for Agentic On-Farm Decision Support Systems](https://arxiv.org/abs/2512.14043)
*Enhong Liu,Haiyu Yang,Miel Hostens*

Main category: cs.AI

TL;DR: 本研究评估了20个开源小语言模型在奶牛养殖决策支持中的可行性，重点关注隐私保护和计算效率，发现Qwen-4B在多数任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型有潜力支持奶牛养殖决策和知识获取，但云服务的高计算需求限制了其在农场环境的应用。需要能在本地农场硬件上运行的轻量级替代方案。

Method: 在农场实际计算约束下，对HuggingFace上的20个开源小语言模型进行基准测试。开发了一个包含五个任务特定代理的AI系统：文献搜索、网络搜索、SQL数据库交互、NoSQL数据库交互和预测模型后的图生成。评估分两阶段：第一阶段用5个测试问题进行初步筛选；第二阶段用30个问题（每个任务类别5个，加上诚信和不当行为类别）评估通过初筛的模型。

Result: Qwen-4B在大多数任务类别中取得了优越性能，尽管在通过PySpark进行NoSQL数据库交互时表现出不稳定的有效性。这是首个明确评估小语言模型作为奶牛养殖决策引擎可行性的研究。

Conclusion: 研究结果突显了小语言模型辅助工具在奶牛养殖中实际部署的潜力，但仍存在挑战，需要微调来优化小语言模型在奶牛特定问题上的性能。

Abstract: Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.

</details>


### [101] [Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation](https://arxiv.org/abs/2512.14048)
*Shen Li,Li Huang,Shaoxiong Zhan,Weifeng Sun,Tao Yin,Zhongxin Liu,Meng Yan*

Main category: cs.AI

TL;DR: RoutingGen是一个难度感知的路由框架，通过动态调整提示策略来优化代码生成：简单任务用少样本提示，复杂任务用意图思维链(ICoT)进行结构化推理，在提升性能的同时大幅减少token使用。


<details>
  <summary>Details</summary>
Motivation: 现有思维链(CoT)方法存在两个主要局限：1) 统一应用导致简单任务上过度思考；2) 缺乏代码生成中的意图抽象（如核心算法设计和效率建模），使模型关注表层结构而忽视全局目标。受认知经济原则启发，只在必要时进行结构化推理以节省认知资源。

Method: 提出RoutingGen框架，根据任务难度动态选择提示策略：简单任务使用少样本提示；复杂任务使用新提出的意图思维链(ICoT)，引导模型捕获任务意图，包括核心算法逻辑和时间复杂度等关键设计要素。

Result: 在三个模型和六个标准代码生成基准测试中，RoutingGen在大多数设置下达到最先进性能，同时平均减少46.37%的总token使用量。ICoT在具有挑战性的基准测试中优于六个现有提示基线方法。

Conclusion: RoutingGen通过难度感知的路由机制和意图思维链，有效解决了现有CoT方法的局限性，在提升代码生成质量的同时显著降低了计算成本，实现了性能与效率的平衡。

Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.

</details>


### [102] [OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value](https://arxiv.org/abs/2512.14051)
*Mengzhang Cai,Xin Gao,Yu Li,Honglin Lin,Zheng Liu,Zhuoshi Pan,Qizhi Pei,Xiaoran Shang,Mengyuan Sun,Zinan Tang,Xiaoyang Wang,Zhanping Zhong,Yun Zhu,Dahua Lin,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: OpenDataArena (ODA) 是一个开放平台，用于系统评估和基准测试后训练数据集的质量，通过统一训练评估流程、多维度评分框架、数据谱系探索器和开源工具包，揭示数据特性与模型性能之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）的发展高度依赖后训练数据集的质量和多样性，但现有数据集存在不透明性：数据组成不明确、来源不确定、缺乏系统性评估。这种不透明性阻碍了研究的可复现性，也模糊了数据特性与模型行为之间的因果关系。

Method: 提出了OpenDataArena (ODA)平台，包含四个核心组件：1) 统一的训练-评估流程，确保不同模型和领域的公平比较；2) 多维度评分框架，从数十个不同维度评估数据质量；3) 交互式数据谱系探索器，可视化数据集谱系和分析组件来源；4) 完全开源的工具包，支持训练、评估和评分。

Result: 在ODA上进行了广泛实验：覆盖120多个训练数据集、22个基准测试、600多次训练运行和4000万个处理数据点。分析揭示了数据复杂性与任务性能之间的权衡关系，通过谱系追踪发现了流行基准中的冗余，并绘制了数据集之间的谱系关系图。

Conclusion: ODA不仅扩展了排行榜，更旨在推动从试错式数据管理向以数据为中心的AI科学转变。通过发布所有结果、工具和配置，ODA为数据混合规律和基础模型战略组合的严谨研究铺平了道路。

Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.

</details>


### [103] [RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](https://arxiv.org/abs/2512.14069)
*Junjie Ma,Jinlong Li*

Main category: cs.AI

TL;DR: RADAR是一种基于强化学习的动态草稿树推测采样方法，通过实时决策减少冗余计算，加速LLM推理


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型推理成本高且速度慢，推测采样是有效解决方案，但传统方法中草稿模型的调用次数是预设超参数，缺乏灵活性

Method: 提出RADAR方法，将草稿树生成过程建模为马尔可夫决策过程，采用离线强化学习训练预测模型，实现草稿模型调用的实时决策

Result: 在三个LLM和四个任务上的评估显示，RADAR相比自回归解码基线实现了3.17-4.82倍的加速

Conclusion: RADAR通过强化学习驱动的动态草稿树生成，有效减少了冗余计算，显著加速了LLM推理过程

Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

</details>


### [104] [Grammar Search for Multi-Agent Systems](https://arxiv.org/abs/2512.14079)
*Mayank Singh,Vikas Yadav,Shiva Krishna Reddy Malay,Shravan Nayak,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Eduardo Blanco*

Main category: cs.AI

TL;DR: 本文提出了一种结构化框架，使用固定、可组合的简单组件来探索多智能体系统搜索空间，相比基于LLM的自由形式搜索方法，在数学和问答领域的五个基准测试中，有四个表现更优。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统的自动搜索已成为智能体AI研究的关键焦点。先前方法主要依赖基于LLM的自由形式代码空间搜索，但本文认为需要更结构化的框架来提升搜索效率和效果。

Method: 提出结构化框架，通过一组固定的简单、可组合组件来探索多智能体系统搜索空间，而不是使用LLM的生成灵活性进行候选系统生成。

Result: 在数学和问答两个领域的五个基准测试中，该方法在四个基准上优于先前方法。此外，该方法还具有搜索过程更经济、生成的多智能体系统更模块化、可解释且逻辑更简单的优势。

Conclusion: 结构化组件方法在多智能体系统搜索中比基于LLM的自由形式搜索更有效，不仅性能更好，而且成本更低，生成的系统更模块化和可解释。

Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.

</details>


### [105] [HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control](https://arxiv.org/abs/2512.14106)
*Ijaz Ul Haq,Byung Suk Lee,Julia N. Perdrial,David Baude*

Main category: cs.AI

TL;DR: HydroGEM是一个用于大陆尺度河流流量质量控制的深度学习基础模型，通过两阶段训练和混合TCN-Transformer架构，在异常检测和重建方面显著优于现有方法，并展示了跨国泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实时河流流量监测网络每年产生数百万观测数据，但维护数千个远程传感器的数据质量仍然劳动密集。需要自动化解决方案来辅助质量控制工作。

Method: 采用两阶段训练：首先在3,724个USGS站的603万序列上进行自监督预训练学习水文表征，然后使用合成异常进行微调用于检测和重建。使用混合TCN-Transformer架构（1420万参数）捕捉局部时间模式和长程依赖，分层归一化处理六个数量级的流量变化。

Result: 在799个站的18种专家验证异常类型的合成测试中，F1分数达到0.792（检测），重建误差降低68.7%，比现有方法提升36.3%。在100个加拿大环境与气候变化部站的零样本迁移中F1=0.586，超过所有基线，展示了跨国泛化能力。

Conclusion: HydroGEM是一个设计用于人机协同工作流的基础模型，输出需要专家审查的质量控制建议而非自主修正，在异常检测和重建方面表现出色并具有跨国泛化能力。

Abstract: Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.

</details>


### [106] [Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model](https://arxiv.org/abs/2512.14112)
*Chunan Tong*

Main category: cs.AI

TL;DR: 提出混合LNN+XGBoost模型用于多级供应链管理，结合液态神经网络的动态特征提取和XGBoost的全局优化能力，旨在减少牛鞭效应并提高盈利能力。


<details>
  <summary>Details</summary>
Motivation: 供应链管理面临需求波动和牛鞭效应等挑战，传统方法和现有LLM在处理连续时间序列数据方面存在局限，而ML方法如LSTM和XGBoost存在计算效率问题。液态神经网络在机器人领域表现出适应性和效率，但在供应链管理中尚未应用。

Method: 提出混合LNN+XGBoost模型，结合液态神经网络的动态特征提取能力和XGBoost的全局优化优势，专门针对多级供应链设计。

Result: 该研究旨在通过混合模型最小化牛鞭效应并增加盈利能力，填补智能供应链管理中的关键空白。

Conclusion: 这种创新方法解决了供应链管理中对效率和适应性的需求，为智能供应链管理提供了新的解决方案。

Abstract: Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.

</details>


### [107] [Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis](https://arxiv.org/abs/2512.14157)
*Yankai Jiang,Yujie Zhang,Peng Zhang,Yichen Li,Jintai Chen,Xiaoming Shi,Shihui Zhen*

Main category: cs.AI

TL;DR: Ophiuchus是一个工具增强的多模态大语言模型框架，通过动态聚焦医学图像细粒度区域，实现精确定位和诊断，超越现有方法在多个医学基准测试中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的医学MLLMs在生成逐步文本推理链方面取得进展，但在需要动态迭代聚焦细粒度视觉区域的复杂任务中仍存在困难，无法实现精确的定位和诊断。

Method: 提出Ophiuchus框架，通过三阶段训练策略：1)冷启动训练，使用工具集成推理数据实现基本工具选择和关键区域检查；2)自反思微调，加强反思推理并鼓励重新审视工具输出；3)代理工具强化学习，直接优化任务特定奖励并模拟专家诊断行为。

Result: 在多样化的医学基准测试（包括VQA、检测和基于推理的分割）中，Ophiuchus始终优于闭源和开源的最先进方法。

Conclusion: Ophiuchus通过工具集成推理，为医学AI代理真正实现"用图像思考"开辟了道路，展示了将模型内在定位感知能力与外部工具结合以促进高级推理的潜力。

Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.

</details>


### [108] [Georeferencing complex relative locality descriptions with large language models](https://arxiv.org/abs/2512.14228)
*Aneesha Fernando,Surangika Ranathunga,Kristin Stock,Raj Prasanna,Christopher B. Jones*

Main category: cs.AI

TL;DR: 本文探索使用大语言模型自动地理编码生物标本采集记录中的复杂位置描述，通过QLoRA微调在多个区域和语言数据集上取得优于现有基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 生物标本采集记录中的位置描述通常采用叙述性方式而非坐标，传统基于地名库或语言模型的方法难以处理包含空间关系的相对位置描述，导致地理编码不准确。准确的地理编码对生物多样性研究至关重要，但人工处理耗时费力，需要自动化解决方案。

Method: 首先识别有效的提示模式，然后使用量化低秩适应（QLoRA）在多个区域和语言（包括英语、西班牙语、葡萄牙语和法语）的生物多样性数据集上微调大语言模型。该方法能够处理复杂的位置描述。

Result: 在固定训练数据量下，该方法在多个数据集上平均65%的记录在10公里半径内，优于现有基线方法。最佳结果（纽约州数据集）达到85%在10公里内和67%在1公里内。所选LLM在处理冗长复杂描述时表现良好。

Conclusion: 大语言模型在自动地理编码复杂位置描述方面具有显著潜力，特别是在生物多样性收集领域。QLoRA微调方法能够有效处理多语言和多区域数据集，为生物标本记录的地理编码提供了有效的自动化解决方案。

Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.

</details>


### [109] [Gödel's Poetry](https://arxiv.org/abs/2512.14252)
*Kelly J. Davis*

Main category: cs.AI

TL;DR: 该论文提出了一种结合专用语言模型和递归分解定理的新方法，通过多智能体架构协调自动形式化、证明生成和定理分解，在miniF2F上实现了90.4%的通过率，并提供了开源实现。


<details>
  <summary>Details</summary>
Motivation: 形式化的自动定理证明长期以来被视为人工智能的挑战，需要新的方法来提高计算机定理证明的能力和效率。

Method: 使用专门为Lean4设计的语言模型进行证明生成，结合递归分解复杂定理为更简单的蕴含命题，通过多智能体架构协调自动形式化、证明生成、定理分解和递归证明过程。

Result: 在没有分解的情况下，在miniF2F上实现了90.4%的通过率；使用分解方法后，性能得到显著提升。关键技术贡献包括扩展Kimina Lean Server以支持AST解析能力，便于自动递归证明分解。

Conclusion: 该方法为自动定理证明提供了新的有效途径，通过开源实现goedels-poetry促进了方法的可适应性和扩展性。

Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.

</details>


### [110] [Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting](https://arxiv.org/abs/2512.14288)
*Georgios Bouchouras,Dimitrios Doumanas,Andreas Soularidis,Konstantinos Kotis,George A. Vouros*

Main category: cs.AI

TL;DR: 本文研究了LLM在帕金森病监测与警报本体工程中的应用，比较了四种方法：单次提示、思维链提示、X-HCOME和SimX-HCOME+，发现纯LLM方法能生成本体但不完整，人机协作方法显著提升了本体的全面性和准确性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在帕金森病监测与警报本体工程中的应用潜力，评估纯LLM方法能否创建全面本体，以及人机协作是否能提升本体质量，旨在推动复杂医学领域本体工程的自动化发展。

Method: 采用四种方法：1) 单次提示技术；2) 思维链提示；3) X-HCOME（混合本体工程方法，结合人类专业知识与LLM能力）；4) SimX-HCOME+（强调持续人类监督和迭代优化的混合方法）。通过比较这些方法在帕金森病监测与警报本体构建中的表现来评估效果。

Result: 单次提示和思维链提示显示LLM能自主构建本体，但输出不全面，需要大量人工优化；X-HCOME方法显著提升了本体的全面性，生成的本体与专家构建的非常相似；SimX-HCOME+方法通过持续人类监督和迭代优化，创建了更全面准确的本体。

Conclusion: 人机协作在复杂医学领域（如帕金森病）的本体工程中具有巨大潜力，纯LLM方法虽能生成本体但不完整，而结合人类专业知识的混合方法能显著提升本体质量，未来可开发专门用于本体构建的GPT模型。

Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.

</details>


### [111] [TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation](https://arxiv.org/abs/2512.14358)
*Qizhi Wang*

Main category: cs.AI

TL;DR: TiCard是一个低侵入性的基数估计修正框架，通过EXPLAIN-only特征学习乘法残差修正，使用EXPLAIN ANALYZE进行离线标注，显著提升尾部准确性


<details>
  <summary>Details</summary>
Motivation: 传统基数估计器无法处理相关性，而学习型估计器通常需要特定工作负载的训练流程和侵入式集成到优化器中，部署困难

Method: 提出TiCard框架，通过EXPLAIN-only特征学习乘法残差修正，使用EXPLAIN ANALYZE进行离线标注，研究两种实现：梯度提升回归器和TabPFN上下文表格基础模型

Result: 在TiDB上测试TPCH和Join Order Benchmark，在低追踪设置下，TiCard显著改善算子级尾部准确性：P90 Q-error从312.85降至13.69，P99从37,974.37降至3,416.50

Conclusion: TiCard作为AI4DB构建块，专注于可部署性：明确范围、保守集成策略，以及从离线修正到优化器内使用的集成路线图

Abstract: Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.

</details>


### [112] [Massive Editing for Large Language Models Based on Dynamic Weight Generation](https://arxiv.org/abs/2512.14395)
*Wentao Wan,Qiqing Lao,Zhiwei Xie,Hefeng Wu,Runnan Lin,Liang Lin,Keze Wang*

Main category: cs.AI

TL;DR: MeG提出了一种基于动态权重生成的大规模知识编辑方法，通过在LLM特定层附加动态权重神经元，并使用扩散模型根据输入查询条件生成权重，实现大规模知识编辑。


<details>
  <summary>Details</summary>
Motivation: 当前在大规模编辑LLM知识时，同时保证编辑的可靠性、泛化性和局部性仍然是一个挑战，需要一种低成本、高效的知识编辑方法。

Method: MeG方法在LLM的特定层附加动态权重神经元，使用扩散模型根据输入查询条件生成该神经元的权重，通过添加单个动态权重神经元实现大规模知识编辑。

Result: 实验表明MeG在可靠性、泛化性和局部性指标上显著优于现有知识编辑方法，特别是在局部性指标的绝对数值上有显著提升。

Conclusion: MeG方法通过动态权重生成机制，为大规模知识编辑提供了一种有效的解决方案，在保持编辑质量的同时提升了编辑效率。

Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.

</details>


### [113] [PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals](https://arxiv.org/abs/2512.14417)
*Jia Hu,Junqi Li,Weimeng Lin,Peng Jia,Yuxiong Ji,Jintao Lai*

Main category: cs.AI

TL;DR: PortAgent：基于大语言模型的车辆调度代理，通过虚拟专家团队实现自动化集装箱码头车辆调度系统的跨码头快速部署，无需港口运营专家、数据需求低、部署速度快。


<details>
  <summary>Details</summary>
Motivation: 自动化集装箱码头（ACT）的车辆调度系统（VDS）对运营效率至关重要，但其商业化应用受到跨码头可移植性差的限制。主要挑战包括：高度依赖港口运营专家、需要大量码头特定数据、手动部署过程耗时。

Method: 提出PortAgent，基于大语言模型的车辆调度代理，通过虚拟专家团队（VET）实现自动化VDS转移工作流。VET包含四个虚拟专家：知识检索器、建模器、编码器和调试器，通过少样本示例学习方法学习VDS领域知识，使用检索增强生成（RAG）机制检索示例，并建立自动VDS设计工作流，包含受LLM Reflexion框架启发的自我纠正循环。

Result: PortAgent实现了三个关键特性：无需港口运营专家、数据需求低、快速部署。通过虚拟专家团队协作，消除了对专家的依赖；通过RAG机制减少了对码头特定数据的需求；通过自动化工作流避免了额外的人工干预。

Conclusion: PortAgent利用大语言模型解决了车辆调度系统跨码头部署的三大挑战，实现了完全自动化的VDS转移工作流，为自动化集装箱码头的车辆调度系统商业化应用提供了可行方案。

Abstract: Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created

</details>


### [114] [Seismology modeling agent: A smart assistant for geophysical researchers](https://arxiv.org/abs/2512.14429)
*Yukun Ren,Siwei Yu,Kai Chen,Jianwei Ma*

Main category: cs.AI

TL;DR: 该论文提出了一个基于大语言模型的智能交互式工作流，用于降低SPECFEM地震波模拟软件的学习门槛，通过MCP服务器套件将模拟过程分解为可代理执行的工具，实现从文件驱动到意图驱动的对话交互。


<details>
  <summary>Details</summary>
Motivation: 传统SPECFEM软件存在学习曲线陡峭、依赖复杂手动文件编辑和命令行操作的问题，这阻碍了研究人员的使用效率和科学探索。

Method: 开发了首个针对SPECFEM（支持2D、3D笛卡尔和3D全球版本）的模型上下文协议（MCP）服务器套件，将整个模拟过程分解为离散的、可由代理执行的工具，涵盖参数生成、网格划分、求解器执行和可视化等环节。

Result: 通过多个案例研究验证，该工作流在自主和交互模式下都能无缝运行，产生与标准基线一致的高保真结果，显著降低了使用门槛并增强了可重复性。

Conclusion: 这是MCP技术在计算地震学中的首次应用，为计算地球物理学向AI辅助和自动化科学研究提供了有前景的途径，同时保持了科学决策权。

Abstract: To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.

</details>


### [115] [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465)
*Siyuan Zhu,Chengdong Xu,Kaiqiang Ke,Chao Yu*

Main category: cs.AI

TL;DR: Context-Picker：一个用于长上下文问答的推理感知框架，通过两阶段强化学习从相似性排序转向最小充分子集选择，显著提升答案准确性同时减少上下文长度。


<details>
  <summary>Details</summary>
Motivation: 在长上下文问答中，确定查询所需的最佳上下文量是一个重要挑战。包含太少段落可能遗漏关键信息，包含太多则会引入噪声并降低答案质量。传统方法如固定Top-K检索和单阶段重排序面临选择适当段落数量的困境，这对于事实性问题尤为突出，因为这类问题通常只需要少量特定证据。

Method: 提出Context-Picker框架，将上下文选择视为决策过程，通过两阶段强化学习优化：1) 召回导向阶段，优先覆盖推理链；2) 精确导向阶段，积极剪枝冗余以提炼紧凑证据集。为解决奖励稀疏性问题，提出离线证据蒸馏流程，通过留一法挖掘"最小充分集"，提供密集、任务对齐的监督。

Result: 在五个长上下文和多跳问答基准测试中，Context-Picker显著优于强大的RAG基线，在可比较或减少的上下文长度下实现了更优的答案准确性。消融研究表明，粗到细的优化调度、冗余感知的奖励塑造和推理引导的格式都对性能提升有重要贡献。

Conclusion: Context-Picker通过将上下文选择范式从相似性排序转向最小充分子集选择，有效解决了长上下文问答中的上下文量优化问题。该框架的两阶段强化学习方法和离线证据蒸馏流程为复杂问答任务提供了更高效、更精确的上下文选择解决方案。

Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.

</details>


### [116] [Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling](https://arxiv.org/abs/2512.14474)
*Annu Rana,Gaurav Kumar*

Main category: cs.AI

TL;DR: 论文提出Model-First Reasoning (MFR)方法，通过先构建显式问题模型再生成解决方案的两阶段范式，显著减少LLM在复杂规划任务中的约束违反并提高解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂多步骤规划任务中表现不佳，经常出现约束违反和不一致的解决方案。现有的Chain-of-Thought和ReAct等方法依赖于隐式状态跟踪，缺乏明确的问题表示。

Method: 提出Model-First Reasoning (MFR)两阶段范式：第一阶段让LLM构建显式问题模型，定义实体、状态变量、动作和约束；第二阶段基于该模型生成解决方案计划。

Result: 在医疗调度、路径规划、资源分配、逻辑谜题和程序合成等多个规划领域中，MFR相比Chain-of-Thought和ReAct显著减少了约束违反并提高了解决方案质量。消融研究表明显式建模阶段对这些改进至关重要。

Conclusion: 许多LLM规划失败源于表示缺陷而非推理限制，显式建模是构建稳健和可解释AI代理的关键组件。研究结果强调了在LLM规划中采用明确问题表示的重要性。

Abstract: Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.

</details>


### [117] [Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification](https://arxiv.org/abs/2512.14491)
*Cheng-Han Lu,Pei-Hsuan Tsai*

Main category: cs.AI

TL;DR: SMMT是一种稀疏多模态Transformer架构，通过聚类稀疏注意力和模态掩码技术，在保持预测性能的同时显著降低计算、内存和能耗成本。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的多模态智能系统由于密集自注意力机制导致计算和能耗成本高，限制了在资源受限条件下的可扩展性。

Method: 基于级联多模态Transformer框架，引入聚类稀疏注意力实现近似线性计算复杂度，并使用模态掩码增强对不完整输入的鲁棒性。

Result: 在ADNI数据集上的阿尔茨海默病分类实验中，SMMT在保持竞争力的预测性能的同时，显著减少了训练时间、内存使用和能耗。

Conclusion: SMMT作为一种资源感知的架构组件，适用于可扩展的智能系统，在效率和鲁棒性方面表现出色。

Abstract: Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.

</details>


### [118] [Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence](https://arxiv.org/abs/2512.14527)
*Shreyas Subramanian,Bala Krishnamoorthy,Pranav Murthy*

Main category: cs.AI

TL;DR: 提出GreedyLR调度器，基于当前损失自适应调整学习率，在NLP、CV和LLM任务上优于现有调度器，理论证明收敛性并推导最优缩放因子。


<details>
  <summary>Details</summary>
Motivation: 尽管训练优化器有显著进展，但大多数研究工作仍使用常见的调度器选择（如余弦或指数衰减）。本文旨在研究一种新颖的调度器，能够根据当前损失自适应调整学习率。

Method: 提出GreedyLR调度器，基于训练过程中的当前损失自适应调整学习率。提供了算法的理论分析，包括收敛性证明和最大化收敛速率的最优缩放因子F的推导。

Result: 在多个NLP、CV和LLM任务（包括微调和预训练实验，参数规模达70亿）上验证了GreedyLR的有效性。结果显示该方法在准确性、速度和收敛性方面优于多个最先进的调度器。

Conclusion: GreedyLR调度器易于实现、计算高效，可被视为训练的良好默认调度器选择。实验还展示了算法对现实噪声环境的鲁棒性。

Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.

</details>


### [119] [Universal Reasoning Model](https://arxiv.org/abs/2512.14693)
*Zitian Gao,Lynx Chen,Yihao Xiao,He Xing,Ran Tao,Haoming Luo,Joey Zhou,Bryan Dai*

Main category: cs.AI

TL;DR: 论文分析了通用Transformer在复杂推理任务中的性能来源，发现主要来自循环归纳偏置和Transformer的强非线性组件，而非复杂架构设计。基于此提出了通用推理模型URM，在ARC-AGI任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 通用Transformer在ARC-AGI等复杂推理任务中表现出色，但其性能提升的具体来源尚未得到充分探索。作者希望系统分析UT变体，找出真正有效的组件，从而设计更高效的推理模型。

Method: 首先系统分析通用Transformer变体，识别出循环归纳偏置和Transformer强非线性组件是关键因素。然后提出通用推理模型URM，通过引入短卷积和截断反向传播来增强UT。

Result: URM在ARC-AGI任务上取得显著性能提升：在ARC-AGI 1上达到53.8% pass@1，在ARC-AGI 2上达到16.0% pass@1，均达到最先进水平。

Conclusion: 通用Transformer在复杂推理任务中的性能主要来自循环归纳偏置和强非线性组件，而非复杂架构设计。URM通过简单有效的增强策略实现了SOTA性能，为高效推理模型设计提供了新思路。

Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

</details>


### [120] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: IPR（交互式物理推理器）通过世界模型推演来评估和强化VLM策略，使用PhysCode物理中心动作编码，在1000+游戏中预训练后，在从原始直觉到目标驱动推理的各种关卡中表现稳健，甚至超越GPT-5，且性能随训练游戏和交互步骤增加而提升。


<details>
  <summary>Details</summary>
Motivation: 人类通过观察、与环境交互来内化物理和因果关系，本文旨在探索智能体是否也能通过交互获得类似人类的推理能力并随着经验积累持续改进。现有方法（如VLMs和世界模型）难以捕捉底层物理和因果关系，因为它们过度关注视觉细节而非核心机制。

Method: 提出IPR（交互式物理推理器），使用世界模型推演来评估和强化VLM的策略；引入PhysCode物理中心动作编码，将语义意图与动力学对齐，为预测和推理提供共享动作空间；在1000+异构游戏上进行预训练。

Result: IPR在从原始直觉到目标驱动推理的各种关卡中表现稳健，整体性能甚至超越GPT-5；性能随训练游戏数量和交互步骤增加而提升；模型能够零样本迁移到未见过的游戏。

Conclusion: 物理中心的交互是持续改进物理推理能力的有效路径，支持智能体通过交互学习获得类似人类的物理推理能力。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [121] [BiCoRec: Bias-Mitigated Context-Aware Sequential Recommendation Model](https://arxiv.org/abs/2512.13848)
*Mufhumudzi Muthivhi,Terence L van Zyl,Hairong Wang*

Main category: cs.IR

TL;DR: BiCoRec是一个新颖的序列推荐框架，通过自适应地处理用户对热门和冷门物品的偏好变化，解决了现有模型中的流行度偏差问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的序列推荐模型存在固有的流行度偏差问题，无法很好地适应用户对热门和冷门物品偏好的动态变化，特别是对于那些偏好冷门物品的用户。

Method: 提出BiCoRec框架：1）使用协同注意力机制获得流行度加权的用户序列表示；2）采用新的训练方案，通过一致性损失函数从未来偏好中学习；3）旨在提升偏好冷门物品用户的推荐性能。

Result: 对于偏好冷门物品的用户，BiCoRec在NDCG@10指标上比现有基线平均提升26.00%。在完整物品集合上的排名性能：Movies数据集NDCG@10为0.0102，Fashion为0.0047，Games为0.0021，Music为0.0005。

Conclusion: BiCoRec通过自适应处理用户对热门和冷门物品的偏好变化，有效缓解了序列推荐中的流行度偏差问题，显著提升了推荐性能，特别是对于偏好冷门物品的用户群体。

Abstract: Sequential recommendation models aim to learn from users evolving preferences. However, current state-of-the-art models suffer from an inherent popularity bias. This study developed a novel framework, BiCoRec, that adaptively accommodates users changing preferences for popular and niche items. Our approach leverages a co-attention mechanism to obtain a popularity-weighted user sequence representation, facilitating more accurate predictions. We then present a new training scheme that learns from future preferences using a consistency loss function. BiCoRec aimed to improve the recommendation performance of users who preferred niche items. For these users, BiCoRec achieves a 26.00% average improvement in NDCG@10 over state-of-the-art baselines. When ranking the relevant item against the entire collection, BiCoRec achieves NDCG@10 scores of 0.0102, 0.0047, 0.0021, and 0.0005 for the Movies, Fashion, Games and Music datasets.

</details>


### [122] [Intent-Guided Reasoning for Sequential Recommendation](https://arxiv.org/abs/2512.14034)
*Yifan Shao,Peilin Zhou*

Main category: cs.IR

TL;DR: IGR-SR是一个用于序列推荐的意图引导推理框架，通过显式提取高层次意图来解决现有推理增强方法中的推理不稳定性和表面级推理问题，在噪声环境下表现出更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐中的推理增强方法仅以下一个目标物品作为监督，导致两个关键问题：1) 推理不稳定性——过程对近期行为和偶然点击等虚假交互过于敏感；2) 表面级推理——模型记忆物品间转移而非理解内在行为模式。

Method: 提出IGR-SR框架，包含三个核心组件：1) 潜在意图蒸馏器(LID)，使用冻结编码器和可学习令牌高效提取多层面意图；2) 意图感知审慎推理器(IDR)，通过双重注意力架构将推理解耦为意图审议和决策制定；3) 意图一致性正则化(ICR)，通过强制不同意图视图间表示一致性确保鲁棒性。

Result: 在三个公共数据集上的实验显示，IGR-SR相比最先进基线平均提升7.13%。在20%行为噪声下，IGR-SR仅下降10.4%，而竞争方法下降16.2%和18.6%，验证了意图引导推理的有效性和鲁棒性。

Conclusion: IGR-SR通过显式提取和利用高层次意图来引导推理过程，有效解决了序列推荐中推理增强方法的局限性，显著提升了推荐性能和在噪声环境下的鲁棒性。

Abstract: Sequential recommendation systems aim to capture users' evolving preferences from their interaction histories. Recent reasoningenhanced methods have shown promise by introducing deliberate, chain-of-thought-like processes with intermediate reasoning steps. However, these methods rely solely on the next target item as supervision, leading to two critical issues: (1) reasoning instability--the process becomes overly sensitive to recent behaviors and spurious interactions like accidental clicks, and (2) surface-level reasoning--the model memorizes item-to-item transitions rather than understanding intrinsic behavior patterns. To address these challenges, we propose IGR-SR, an Intent-Guided Reasoning framework for Sequential Recommendation that anchors the reasoning process to explicitly extracted high-level intents. Our framework comprises three key components: (1) a Latent Intent Distiller (LID) that efficiently extracts multi-faceted intents using a frozen encoder with learnable tokens, (2) an Intent-aware Deliberative Reasoner (IDR) that decouples reasoning into intent deliberation and decision-making via a dual-attention architecture, and (3) an Intent Consistency Regularization (ICR) that ensures robustness by enforcing consistent representations across different intent views. Extensive experiments on three public datasets demonstrate that IGR-SR achieves an average 7.13% improvement over state-of-the-art baselines. Critically, under 20% behavioral noise, IGR-SR degrades only 10.4% compared to 16.2% and 18.6% for competing methods, validating the effectiveness and robustness of intent-guided reasoning.

</details>


### [123] [DTRec: Learning Dynamic Reasoning Trajectories for Sequential Recommendation](https://arxiv.org/abs/2512.14036)
*Yifan Shao,Peilin Zhou,Shoujin Wang,Weizhi Zhang,Xu Cai,Sunghun Kim*

Main category: cs.IR

TL;DR: DTRec：一种用于序列推荐的动态推理轨迹框架，通过分层过程监督和自适应推理停止机制，在推理方向和深度上实现动态调整，显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的序列推荐方法存在静态推理轨迹的局限性：1）静态推理方向与人类分层推理不匹配；2）固定推理深度对所有用户使用相同计算量，不考虑行为模式复杂性差异。这种刚性导致性能不佳和计算浪费。

Method: 提出DTRec框架，包含两个核心组件：1）分层过程监督（HPS）：提供从粗到细的监督信号，模拟人类认知过程的渐进式细化；2）自适应推理停止（ARH）：通过联合监测三个指标动态调整推理步数，实现计算效率优化。

Result: 在三个真实世界数据集上的实验表明，DTRec相比强基线性能提升高达24.5%，同时计算成本降低高达41.6%，实现了性能与效率的双重优化。

Conclusion: DTRec通过动态调整推理轨迹的方向和深度，有效解决了现有序列推荐方法中的刚性限制，为推理增强的序列推荐提供了更灵活、高效且性能优越的解决方案。

Abstract: Inspired by advances in LLMs, reasoning-enhanced sequential recommendation performs multi-step deliberation before making final predictions, unlocking greater potential for capturing user preferences. However, current methods are constrained by static reasoning trajectories that are ill-suited for the diverse complexity of user behaviors. They suffer from two key limitations: (1) a static reasoning direction, which uses flat supervision signals misaligned with human-like hierarchical reasoning, and (2) a fixed reasoning depth, which inefficiently applies the same computational effort to all users, regardless of pattern complexity. These rigidity lead to suboptimal performance and significant computational waste. To overcome these challenges, we propose DTRec, a novel and effective framework that explores the Dynamic reasoning Trajectory for Sequential Recommendation along both direction and depth. To guide the direction, we develop Hierarchical Process Supervision (HPS), which provides coarse-to-fine supervisory signals to emulate the natural, progressive refinement of human cognitive processes. To optimize the depth, we introduce the Adaptive Reasoning Halting (ARH) mechanism that dynamically adjusts the number of reasoning steps by jointly monitoring three indicators. Extensive experiments on three real-world datasets demonstrate the superiority of our approach, achieving up to a 24.5% performance improvement over strong baselines while simultaneously reducing computational cost by up to 41.6%.

</details>


### [124] [From Feature Interaction to Feature Generation: A Generative Paradigm of CTR Prediction Models](https://arxiv.org/abs/2512.14041)
*Mingjia Yin,Junwei Pan,Hao Wang,Ximei Wang,Shangyu Zhang,Jie Jiang,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: SFG框架将CTR预测从判别式的特征交互范式转变为生成式的特征生成范式，通过编码器-解码器结构生成特征嵌入，使用监督损失缓解嵌入维度崩溃和信息冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测模型主要采用判别式范式，过度依赖原始ID嵌入的特征交互，导致两个关键问题：嵌入维度崩溃和信息冗余。需要新的范式来解决这些限制。

Method: 提出监督特征生成（SFG）框架，包含编码器和解码器两个关键组件。编码器为每个特征构建隐藏嵌入，解码器从这些隐藏表示中重新生成所有特征的特征嵌入。不同于现有的自监督方法，引入监督损失来利用CTR预测任务中的监督信号（点击与否）。

Result: 实验表明SFG能够持续缓解嵌入崩溃并减少信息冗余，在各种数据集和基础模型上都带来了显著的性能提升。该框架具有良好的通用性，可以与大多数现有CTR模型无缝集成。

Conclusion: SFG框架成功地将CTR预测从判别式特征交互范式转变为生成式特征生成范式，有效解决了现有模型中的嵌入维度崩溃和信息冗余问题，为CTR预测提供了新的研究方向。

Abstract: Click-Through Rate (CTR) prediction, a core task in recommendation systems, aims to estimate the probability of users clicking on items. Existing models predominantly follow a discriminative paradigm, which relies heavily on explicit interactions between raw ID embeddings. However, this paradigm inherently renders them susceptible to two critical issues: embedding dimensional collapse and information redundancy, stemming from the over-reliance on feature interactions \emph{over raw ID embeddings}. To address these limitations, we propose a novel \emph{Supervised Feature Generation (SFG)} framework, \emph{shifting the paradigm from discriminative ``feature interaction" to generative ``feature generation"}. Specifically, SFG comprises two key components: an \emph{Encoder} that constructs hidden embeddings for each feature, and a \emph{Decoder} tasked with regenerating the feature embeddings of all features from these hidden representations. Unlike existing generative approaches that adopt self-supervised losses, we introduce a supervised loss to utilize the supervised signal, \ie, click or not, in the CTR prediction task. This framework exhibits strong generalizability: it can be seamlessly integrated with most existing CTR models, reformulating them under the generative paradigm. Extensive experiments demonstrate that SFG consistently mitigates embedding collapse and reduces information redundancy, while yielding substantial performance gains across various datasets and base models. The code is available at https://github.com/USTC-StarTeam/GE4Rec.

</details>


### [125] [AsarRec: Adaptive Sequential Augmentation for Robust Self-supervised Sequential Recommendation](https://arxiv.org/abs/2512.14047)
*Kaike Zhang,Qi Cao,Fei Sun,Xinran Liu*

Main category: cs.IR

TL;DR: AsarRec提出了一种自适应序列增强框架，通过可学习的变换矩阵动态生成增强视图，解决了传统静态增强策略在序列推荐中的局限性，显著提升了模型在噪声环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的用户行为通常包含噪声（人为错误、不确定性、行为模糊性），传统基于自监督学习的序列推荐系统依赖预定义的静态增强策略，存在两个关键问题：1）最优增强类型在不同场景下差异很大；2）不恰当的增强可能降低推荐性能，限制了自监督学习的有效性。

Method: 提出AsarRec（自适应序列增强鲁棒序列推荐）框架：1）将现有基本增强操作统一为结构化变换矩阵；2）通过编码用户序列为概率转移矩阵，并使用可微分的Semi-Sinkhorn算法将其投影为硬半双随机矩阵，学习生成变换矩阵；3）联合优化多样性、语义不变性和信息性三个目标，确保学习的增强有利于下游性能。

Result: 在三个基准数据集上，在不同噪声水平下进行了广泛实验，验证了AsarRec的有效性，展示了其优越的鲁棒性和一致的性能提升。

Conclusion: AsarRec通过自适应增强框架克服了传统静态增强策略的局限性，能够动态生成适合特定场景的增强视图，显著提高了序列推荐系统在噪声环境下的鲁棒性和推荐性能。

Abstract: Sequential recommender systems have demonstrated strong capabilities in modeling users' dynamic preferences and capturing item transition patterns. However, real-world user behaviors are often noisy due to factors such as human errors, uncertainty, and behavioral ambiguity, which can lead to degraded recommendation performance. To address this issue, recent approaches widely adopt self-supervised learning (SSL), particularly contrastive learning, by generating perturbed views of user interaction sequences and maximizing their mutual information to improve model robustness. However, these methods heavily rely on their pre-defined static augmentation strategies~(where the augmentation type remains fixed once chosen) to construct augmented views, leading to two critical challenges: (1) the optimal augmentation type can vary significantly across different scenarios; (2) inappropriate augmentations may even degrade recommendation performance, limiting the effectiveness of SSL. To overcome these limitations, we propose an adaptive augmentation framework. We first unify existing basic augmentation operations into a unified formulation via structured transformation matrices. Building on this, we introduce AsarRec (Adaptive Sequential Augmentation for Robust Sequential Recommendation), which learns to generate transformation matrices by encoding user sequences into probabilistic transition matrices and projecting them into hard semi-doubly stochastic matrices via a differentiable Semi-Sinkhorn algorithm. To ensure that the learned augmentations benefit downstream performance, we jointly optimize three objectives: diversity, semantic invariance, and informativeness. Extensive experiments on three benchmark datasets under varying noise levels validate the effectiveness of AsarRec, demonstrating its superior robustness and consistent improvements.

</details>


### [126] [SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions](https://arxiv.org/abs/2512.14277)
*Panayiotis Smeros,Vincent Emonet,Ruijie Wang,Ana-Claudia Sima,Tarcisio Mendes de Farias*

Main category: cs.IR

TL;DR: SPARQL-LLM是一个开源、与三元存储无关的方法，利用轻量级元数据从自然语言生成SPARQL查询，在准确性、速度和成本方面均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型方法主要关注单一数据源的响应准确性，而忽略了联邦查询能力、运行时性能和生成SPARQL查询的成本等评估标准，导致这些方法难以在生产环境中部署到（可能是联邦的）知识图谱上。

Method: SPARQL-LLM采用基于轻量级元数据的架构，包含元数据索引、提示构建、查询生成和执行等专用组件，是一个开源且与三元存储无关的方法。

Result: 在最新的多语言问题挑战中，F1分数提高了24%；支持英语和西班牙语等高资源语言；能够生成复杂的联邦生物信息学查询；比其他系统快达36倍；每个问题最大成本仅为0.01美元。

Conclusion: SPARQL-LLM适合实时、低成本的文本到SPARQL应用，已在实际的分散式知识图谱上部署（如https://www.expasy.org/chat），具有生产就绪的特性。

Abstract: The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.

</details>


### [127] [Dynamic Context Selection for Retrieval-Augmented Generation: Mitigating Distractors and Positional Bias](https://arxiv.org/abs/2512.14313)
*Malika Iratni,Mohand Boughanem,Taoufiq Dkaki*

Main category: cs.IR

TL;DR: 本文系统分析了RAG系统中干扰文档对生成质量的影响，研究了相关段落位置效应，并提出基于查询的动态文档检索分类器，显著提升了RAG性能。


<details>
  <summary>Details</summary>
Motivation: 标准RAG系统采用固定top k检索策略存在两个主要问题：1) 可能错过相关信息或引入语义不相关的干扰文档，降低输出质量；2) 检索段落在输入上下文中的位置会影响模型注意力，存在"中间丢失"现象。需要系统分析这些因素并改进RAG系统。

Method: 1) 系统分析干扰文档对生成质量的影响，量化不同条件下的效果；2) 研究相关段落在上下文窗口中的位置如何影响生成；3) 基于这些洞察，提出上下文大小分类器，动态预测基于查询特定信息需求的最佳检索文档数量；4) 将该方法集成到完整的RAG流程中。

Result: 提出的动态检索方法在完整RAG流程中表现出优于固定k基线的性能，验证了基于查询特定需求动态调整检索文档数量的有效性。

Conclusion: 通过系统分析RAG中的干扰文档和位置效应问题，提出的动态文档检索策略能够显著提升RAG系统性能，为改进检索增强生成系统提供了重要见解和方法。

Abstract: Retrieval Augmented Generation (RAG) enhances language model performance by incorporating external knowledge retrieved from large corpora, which makes it highly suitable for tasks such as open domain question answering. Standard RAG systems typically rely on a fixed top k retrieval strategy, which can either miss relevant information or introduce semantically irrelevant passages, known as distractors, that degrade output quality. Additionally, the positioning of retrieved passages within the input context can influence the model attention and generation outcomes. Context placed in the middle tends to be overlooked, which is an issue known as the "lost in the middle" phenomenon. In this work, we systematically analyze the impact of distractors on generation quality, and quantify their effects under varying conditions. We also investigate how the position of relevant passages within the context window affects their influence on generation. Building on these insights, we propose a context-size classifier that dynamically predicts the optimal number of documents to retrieve based on query-specific informational needs. We integrate this approach into a full RAG pipeline, and demonstrate improved performance over fixed k baselines.

</details>


### [128] [PushGen: Push Notifications Generation with LLM](https://arxiv.org/abs/2512.14490)
*Shifu Bie,Jiangxia Cao,Zixiao Luo,Yichuan Zou,Lei Liang,Lu Zhang,Linxun Chen,Zhaojie Liu,Xuanping Li,Guorui Zhou,Kaiqiao Zhan,Kun Gai*

Main category: cs.IR

TL;DR: PushGen是一个自动化框架，用于生成与人工撰写质量相当的推送通知，通过可控类别提示技术和奖励模型解决LLM生成内容中的风格控制和质量评估问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的兴起，利用LLM生成推送内容的需求增长。虽然LLM使内容生成变得简单且成本效益高，但保持风格控制和可靠的质量评估仍然具有挑战性，这两者直接影响用户参与度。

Method: PushGen结合两个关键组件：(1) 可控类别提示技术，引导LLM输出符合期望风格的内容；(2) 奖励模型，对生成的候选内容进行排名和选择。

Result: 广泛的离线和在线实验证明了其有效性，该框架已部署在大型工业应用中，每天为数亿用户提供服务。

Conclusion: PushGen成功解决了LLM在推送内容生成中的风格控制和质量评估问题，提供了一个可扩展的自动化解决方案，在实际应用中表现出色。

Abstract: We present PushGen, an automated framework for generating high-quality push notifications comparable to human-crafted content. With the rise of generative models, there is growing interest in leveraging LLMs for push content generation. Although LLMs make content generation straightforward and cost-effective, maintaining stylistic control and reliable quality assessment remains challenging, as both directly impact user engagement. To address these issues, PushGen combines two key components: (1) a controllable category prompt technique to guide LLM outputs toward desired styles, and (2) a reward model that ranks and selects generated candidates. Extensive offline and online experiments demonstrate its effectiveness, which has been deployed in large-scale industrial applications, serving hundreds of millions of users daily.

</details>


### [129] [RecGPT-V2 Technical Report](https://arxiv.org/abs/2512.14503)
*Chao Yi,Dian Chen,Gaoyang Guo,Jiakai Tang,Jian Wu,Jing Yu,Mao Zhang,Wen Chen,Wenjun Yang,Yujie Luo,Yuning Jiang,Zhujin Gao,Bo Zheng,Binbin Cao,Changfa Wu,Dixuan Wang,Han Wu,Haoyi Hu,Kewei Zhu,Lang Tian,Lin Yang,Qiqi Huang,Siqi Yang,Wenbo Su,Xiaoxiao He,Xin Tong,Xu Chen,Xunke Xi,Xiaowei Huang,Yaxuan Wu,Yeqiu Yang,Yi Hu,Yujin Yuan,Yuliang Yan,Zile Zhou*

Main category: cs.IR

TL;DR: RecGPT-V2通过分层多智能体系统、元提示框架、约束强化学习和智能体即法官评估框架，解决了V1版本的计算效率、解释多样性、泛化能力和评估标准问题，显著提升了推荐系统的性能和用户体验。


<details>
  <summary>Details</summary>
Motivation: 虽然RecGPT-V1成功将LLM推理引入推荐系统，但存在四个根本限制：计算效率低下和认知冗余、解释多样性不足、监督学习范式下泛化能力有限、以及评估标准过于简单无法匹配人类标准。

Method: 1. 分层多智能体系统：通过协调协作重构意图推理，消除认知重复，实现多样化意图覆盖；结合混合表示推理压缩用户行为上下文。2. 元提示框架：动态生成上下文自适应提示，提升解释多样性。3. 约束强化学习：缓解多奖励冲突，优化标签预测和解释接受度。4. 智能体即法官框架：将评估分解为多步推理，提高人类偏好对齐。

Result: GPU消耗减少60%，独家召回率从9.39%提升至10.99%；解释多样性提升+7.3%；标签预测提升+24.1%，解释接受度提升+13.0%；淘宝在线A/B测试显示：CTR提升+2.98%，IPV提升+3.71%，TV提升+2.19%，NER提升+11.46%。

Conclusion: RecGPT-V2证明了在大规模部署LLM驱动的意图推理的技术可行性和商业可行性，弥合了认知探索与工业效用之间的差距。

Abstract: Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.
  To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.

</details>


### [130] [Pairwise Comparison for Bias Identification and Quantification](https://arxiv.org/abs/2512.14565)
*Fabian Haak,Philipp Schaer*

Main category: cs.IR

TL;DR: 该研究提出了一种基于成对比较的偏见标注方法，通过优化比较和匹配机制来减少标注成本，并在模拟环境和真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在线新闻和社交媒体中的语言偏见普遍存在但难以测量，主要挑战包括主观性、上下文依赖性和高质量标注数据集的稀缺。传统标注方法成本高昂，需要更高效的解决方案。

Method: 研究采用成对比较方法进行偏见标注，通过模拟环境评估不同评分技术和三种成本感知替代方案的参数。模拟包括潜在严重性分布、距离校准噪声和合成标注者偏见。在真实数据集上评估最有前景的设置，并与大语言模型直接评估和未修改的成对比较标签作为基线进行比较。

Result: 研究发现成对比较作为量化主观语言方面的实用基础是有效的，支持可复现的偏见分析。研究贡献包括比较和匹配组件的优化、包含模拟和真实数据应用的端到端评估，以及成本感知大规模标注的实现蓝图。

Conclusion: 成对比较方法为创建高质量基准数据集和量化偏见及其他主观语言方面提供了实用基础，该方法既适用于人工标注也适用于大语言模型标注，能够显著减少标注工作量。

Abstract: Linguistic bias in online news and social media is widespread but difficult to measure. Yet, its identification and quantification remain difficult due to subjectivity, context dependence, and the scarcity of high-quality gold-label datasets. We aim to reduce annotation effort by leveraging pairwise comparison for bias annotation. To overcome the costliness of the approach, we evaluate more efficient implementations of pairwise comparison-based rating. We achieve this by investigating the effects of various rating techniques and the parameters of three cost-aware alternatives in a simulation environment. Since the approach can in principle be applied to both human and large language model annotation, our work provides a basis for creating high-quality benchmark datasets and for quantifying biases and other subjective linguistic aspects.
  The controlled simulations include latent severity distributions, distance-calibrated noise, and synthetic annotator bias to probe robustness and cost-quality trade-offs. In applying the approach to human-labeled bias benchmark datasets, we then evaluate the most promising setups and compare them to direct assessment by large language models and unmodified pairwise comparison labels as baselines. Our findings support the use of pairwise comparison as a practical foundation for quantifying subjective linguistic aspects, enabling reproducible bias analysis. We contribute an optimization of comparison and matchmaking components, an end-to-end evaluation including simulation and real-data application, and an implementation blueprint for cost-aware large-scale annotation

</details>
